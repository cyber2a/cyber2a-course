[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "",
    "text": "Course Overview\nAI for Arctic Research represents an introduction to Artificial Intelligence (AI) techniques produced by the Cyber2A project, an innovative training program to empower the Arctic science community with advanced AI-driven data analytics and cyberinfrastructure (CI) skills to tackle the pressing challenges facing the Arctic and thus our planet. Today, Artificial Intelligence has become one of the most powerful tools to analyze Arctic big data and enable new ways of data-driven discovery. However, training on these emerging topics is often missing in current undergraduate and graduate curricula, particularly for active Arctic researchers. This project aims to fill this skills gap in order to foster the growth of an Arctic science workforce with strong data science skills through a series of complementary and mutually reinforcing training activities.\nThe week-long course is designed with a modular curriculum, where each module can be incorporated into learning activities across Universities and other organizations. The curriculum is free to be re-used, licensed under a CC-BY Attribution license and covers 5 main topical areas:\nThe sections presented here fit into a one-week workshop as follows, but the modules can be also used individually:",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nPlease note that by participating in this activity you agree to abide by the NCEAS Code of Conduct.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#session-structure-and-content",
    "href": "index.html#session-structure-and-content",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "Session Structure and Content",
    "text": "Session Structure and Content\nThe course has been designed with several guiding principals in mind.\n\nBalance Theory and Hands-On Work: Spend about one-third of the time on theory and the other two-thirds on hands-on activities.\nBuild Gradually: Start with the basics and build up gradually. Expand both the theory and hands-on tasks as you go along.\nChoose Your Tools: You can use either Jupyter Notebook or VSCode for the hands-on parts of your session. Choose whichever one you’re more comfortable with.\nOpen Data Licensing: Use open data for examples that can be ethically shared and re-used, both within the workshop and when the course materials are used and incorporated into other courses.\nOffer Support: Make sure participants know how to ask for help if they get stuck. Regularly ask participants if they’re keeping up and adjust the pace if needed.\nExtra Resources: Provide additional materials like readings or videos for participants who want to learn more.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "About this book",
    "text": "About this book\nCitation:\n\nWenwen Li, Anna Liljedahl, Matthew B. Jones, Chia-Yu Hsu, Alyona Kosobokova, Jim Regetz, Chandi Witharana, Yili Yang, Ben Galewsky, Minu Mathew, Sandeep Puthanveetil Satheesan, Nicole Greco, Kenton McHenry, Carmen Galaz García, Kate Holman Billmeier. 2024. AI for Arctic Research. Arctic Data Center. doi:10.18739/A2222R77V\n\nThe materials in this book are licensed for reuse, and are available from the cyber2a-course github repository. The book is written in Quarto, a cross platform markdown-based platform for writing books and technical materials that works with Python, R, and other languages.\n\nGetting Started: You can find a guide to getting started with Quarto, including editing and previewing content locally with various tools, here.\nQuarto Guide: You can find a comprehensive guide to Quarto here.\nFormat: You can choose to write your content in either Jupyter Notebook (.ipynb) or Markdown files (.qmd). Quarto can render both formats. And both formats can be easily included in other teaching materials.\nEmbed Notebooks: If you choose to write your content in Markdown but have a separate Jupyter Notebook for the hands-on part, you can embed the notebook. Follow the guide here to learn how to do this.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese course materials were developed with funding for Cyber2A from the National Science Foundation under award # 2230034 to W. Li and M. Jones and award # 2230035 to A. Liljedahl and K. McHenry",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "License",
    "text": "License\n\nCyber2A: AI for Arctic Research is licensed under CC BY 4.0",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "",
    "text": "1.1 The changing Arctic\nThe Arctic is one of the Earth’s remaining frontiers that is critical to the Earth’s climate system. Climate warming and change have pushed the Arctic ecosystem to a tipping point: the frozen is becoming unfrozen with subsequent dramatic impact to its terrestrial and coastal landscapes. Permafrost warming and degradation are documented across the Arctic[1], [2], [3], and are coupled with long-term global warming and extremes in air temperature and precipitation [4], [5], [6]. Further, Arctic sea ice is decreasing rapidly [7], which increases coastal erosion rates across the globe [8]. The Arctic region is remote and is experiencing dramatic changes with local and global implications due to the shift from ice to water: altered soil carbon fluxes [9], changes in vegetation cover [10], shifts in animal behavior [11], and challenges to infrastructure [12]. Accordingly, the transformation of ice to water through degrading permafrost and melting sea and lake ice reverberates through the entire Arctic ecosystem and, therefore, enlists the interest of a broad range of earth, engineering, and social science disciplines [13]. Remote sensing of satellite imagery is an important approach in developing Arctic baseline information, monitoring change, and exploring physical processes [14], [15]. Today, there exist important climatic, geological, biological and sociological data that are yet to be exploited by the Arctic science community. To make the best possible use of these data to address the pressing challenges facing the Arctic environment and Arctic people, the more advanced methods and tools that are available need to be applied. AI-driven analytics, especially those incorporating deep machine learning, can process Arctic big data, automatically detect hidden patterns, and derive new knowledge to enable a new wave of data-driven discovery [16].",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#the-changing-arctic",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#the-changing-arctic",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "",
    "text": "Arctic mountains",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#ai-for-arctic-challenges",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#ai-for-arctic-challenges",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.2 AI for Arctic Challenges",
    "text": "1.2 AI for Arctic Challenges\n\n\n\n\n\n\n“AI will be the most transformative technology since electricity.” – Eric Schmidt\n\n\n\n\n\n\n\n\n\n“AI is just another model.” – Unknown\n\n\n\n\n\n\n\n\n\nArtificial Intelligence\n\n\n\n\n\nArtificial Intelligence (AI) in its broadest sense describes the focus on computing systems that exhibit forms of intelligence. Multiple approaches towards AI have been identified, including:\n\nKnowledge Representation and Reasoning to gain a semantic, logical model of a system\nMachine Learning in which statistical models are used for pattern recognition and prediction\nNatural Language Processing for communication in human languages such as English\nExpert Systems using rule-based logical systems for decision-making\nLarge Language Models for filtering and generating language\n…\n\n\n\n\nThe pursuit of AI as a field has been around since the 1956 with the Dartmouth Workshop, but really took a leap forward in the 2010’s with rising performance of computing hardware and new techniques in machine learning, particularly in the field of deep learning. More recently, AI has entered the public consciousness with the promotion of large language models (LLMs) such as the GPT-3 transformer model and related generative AI systems that are based on foundation models and can quickly generate new outputs [17].\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\nMachine learning (ML) is the subfield of AI concerned with pattern detection using statistical models, which then can be applied to unseen data for prediction and extrapolation without explicit instructions [18]. This mechanistic view of ‘learning’ supports robust evaluation of error and has applications in computer vision, image recognition, speech recognition, text processing and filtering, and many more areas.\n\n\n\nTechniques for machine learning are often divided into three types (supervised, unsupervised, and reinforcement learning). These techiques differ based on the feedback provided to the learning system:\n\nSupervised learning: Training input data are labeled (often manually) by a human, and the algorithm learns by generalizing from these inputs to predict correct outputs\nUnsupervised learning: Without labels, the ML algorithm is designed to detect patterns and structure in the input, often using techniques like gradient descent, clustering, and classification algorithms.\nReinforcement learning: A ML algorithm learns dynamically from interactive input to solve a problem or learn a goal, where correct responses are rewarded (weighted) higher than less correct responses. Learning then becomes an optimization/hill-climbing problem.\n\nThese general approaches all have strengths and weaknesses, and are often used in combination to tackle different aspects of a learning problem.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#geospatial-ai",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#geospatial-ai",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.3 Geospatial AI",
    "text": "1.3 Geospatial AI\nIn this course, we will more narrowly focus on geospatial applications of AI, and particularly on the use of deep learning techniques that employ, for example, convolutional neural networks for feature recognition tasks across massive image datasets such as satellite imagery. As we’ll see during the course, advances in computing hardware, and particularly in available Graphical Processing Unit (GPU) performance have enabled massive growth in the scale of models that can be generated. Today, we can train deep learning models on high-resolution, sub-meter scale satellite imagery (e.g., pan-Arctic, 50cm Maxar imagery), and apply the generated models across the Arctic to better understand change at Arctic scales.\nFor one example, Witharana et al. [19] trained a convolutional neural network model on Maxar imagery, and used the trained model to detect permafrost ice-wedges across the entire Arctic at sub-meter scale [20], producing a map of over a billion vector features, and the first-ever permafrost map at this scale.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#welcome-and-introductions",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#welcome-and-introductions",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.4 Welcome and Introductions",
    "text": "1.4 Welcome and Introductions\nLet’s kick the week off with a warm welcome and round of introductions. We’ll start with our Cyber2A project instructors and speakers, and then introduce each of our participants. Everyone is here due to a deep interest in finding solutions to challenges in Arctic science, and everyone is on their own personal journey through data and science. To learn a little about one another, let’s share:\n\nName and affiliation\nYour data science background (be brief!)\nOne! thing you’d like to get out of the course\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#cyber2a-project",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#cyber2a-project",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.5 Cyber2A project",
    "text": "1.5 Cyber2A project\nDespite the power of these machine learning techniques for Arctic research, the Arctic community has been somewhat delayed compared to other geoscience disciplines in adopting these techniques.\n\nThe Cyber2A project aims to build an Arctic learning community to stimulate the use of GeoAI through data science education. This short-course represents a first pass at a survey of relevant AI techniques that would be useful across Arctic regions and disciplines. The goal is to produce an online curriculum and materials that can be used for self-paced learning by Arctic researchers, and can be included in University graduate and undergraduate courses. While there are many online tutorials on machine learning and AI, these materials will specifically target the types of data and challenges typically found in Arctic research, and focus in on the techniques that will make data science learning more approachable.\nThis course is also a starting point, and not an endpoint. We welcome feedback, suggestions, revisions, and edits to the materials. We want people to adopt, adapt, and revise the materials, and, importantly, contribute those changes back so that others can benefit from these curricular advances. Look for more from Cyber2A as we continue to engage in promoting the use of GeoAI across the Arctic.\n\n\n\n\n\n[1] A. K. Liljedahl et al., “Pan-Arctic ice-wedge degradation in warming permafrost and its influence on tundra hydrology,” Nature Geoscience, vol. 9, no. 4, pp. 312–318, Apr. 2016, doi: 10.1038/ngeo2674.\n\n\n[2] A. A. Vasiliev, D. S. Drozdov, A. G. Gravis, G. V. Malkova, K. E. Nyland, and D. A. Streletskiy, “Permafrost degradation in the Western Russian Arctic,” Environmental Research Letters, vol. 15, no. 4, p. 045001, Apr. 2020, doi: 10.1088/1748-9326/ab6f12.\n\n\n[3] S. L. Smith, H. B. O’Neill, K. Isaksen, J. Noetzli, and V. E. Romanovsky, “The changing thermal state of permafrost,” Nature Reviews Earth & Environment, vol. 3, no. 1, pp. 10–23, Jan. 2022, doi: 10.1038/s43017-021-00240-1.\n\n\n[4] T. A. Douglas, M. R. Turetsky, and C. D. Koven, “Increased rainfall stimulates permafrost thaw across a variety of Interior Alaskan boreal ecosystems,” npj Climate and Atmospheric Science, vol. 3, no. 1, pp. 1–7, Jul. 2020, doi: 10.1038/s41612-020-0130-4.\n\n\n[5] R. Í. Magnússon et al., “Extremely wet summer events enhance permafrost thaw for multiple years in Siberian tundra,” Nature Communications, vol. 13, no. 1, p. 1556, Mar. 2022, doi: 10.1038/s41467-022-29248-x.\n\n\n[6] L. M. Farquharson, V. E. Romanovsky, W. L. Cable, D. A. Walker, S. V. Kokelj, and D. Nicolsky, “Climate Change Drives Widespread and Rapid Thermokarst Development in Very Cold Permafrost in the Canadian High Arctic,” Geophysical Research Letters, vol. 46, no. 12, pp. 6681–6689, 2019, doi: 10.1029/2019GL082187.\n\n\n[7] D. Notz and J. Stroeve, “Observed Arctic sea-ice loss directly follows anthropogenic CO2 emission,” Science, vol. 354, no. 6313, pp. 747–750, Nov. 2016, doi: 10.1126/science.aag2345.\n\n\n[8] D. M. Nielsen, M. Dobrynin, J. Baehr, S. Razumov, and M. Grigoriev, “Coastal Erosion Variability at the Southern Laptev Sea Linked to Winter Sea Ice and the Arctic Oscillation,” Geophysical Research Letters, vol. 47, no. 5, p. e2019GL086876, 2020, doi: 10.1029/2019GL086876.\n\n\n[9] L. Bruhwiler, F.-J. W. Parmentier, P. Crill, M. Leonard, and P. I. Palmer, “The Arctic Carbon Cycle and Its Response to Changing Climate,” Current Climate Change Reports, vol. 7, no. 1, pp. 14–34, Mar. 2021, doi: 10.1007/s40641-020-00169-5.\n\n\n[10] T. K. F. Campbell, T. C. Lantz, R. H. Fraser, and D. Hogan, “High Arctic Vegetation Change Mediated by Hydrological Conditions,” Ecosystems, vol. 24, no. 1, pp. 106–121, Jan. 2021, doi: 10.1007/s10021-020-00506-7.\n\n\n[11] S. C. Davidson et al., “Ecological insights from three decades of animal movement tracking across a changing Arctic,” Science, vol. 370, no. 6517, pp. 712–715, Nov. 2020, doi: 10.1126/science.abb7080.\n\n\n[12] L. Suter, D. Streletskiy, and N. Shiklomanov, “Assessment of the cost of climate change impacts on critical infrastructure in the circumpolar Arctic,” Polar Geography, vol. 42, no. 4, pp. 267–286, Oct. 2019, doi: 10.1080/1088937X.2019.1686082.\n\n\n[13] M. L. Druckenmiller et al., “The Arctic,” Bulletin of the American Meteorological Society, vol. 102, no. 8, pp. S263–S316, Aug. 2021, doi: 10.1175/BAMS-D-21-0086.1.\n\n\n[14] M. Philipp, A. Dietz, S. Buchelt, and C. Kuenzer, “Trends in Satellite Earth Observation for Permafrost Related Analyses—A Review,” Remote Sensing, vol. 13, no. 6, p. 1217, Jan. 2021, doi: 10.3390/rs13061217.\n\n\n[15] “Changing state of Arctic sea ice across all seasons - IOPscience.” Accessed: Oct. 18, 2024. [Online]. Available: https://iopscience.iop.org/article/10.1088/1748-9326/aade56\n\n\n[16] “AI in Analytics: Top Use Cases and Tools.” Accessed: Oct. 18, 2024. [Online]. Available: https://www.marketingaiinstitute.com/blog/how-to-use-artificial-intelligence-for-analytics\n\n\n[17] A. Vaswani et al., “Attention is all you need,” in Advances in neural information processing systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., Curran Associates, Inc., 2017. Available: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n\n\n[18] M. I. Jordan and T. M. Mitchell, “Machine learning: Trends, perspectives, and prospects,” Science, vol. 349, no. 6245, pp. 255–260, Jul. 2015, doi: 10.1126/science.aaa8415.\n\n\n[19] C. Witharana et al., “An Object-Based Approach for Mapping Tundra Ice-Wedge Polygon Troughs from Very High Spatial Resolution Optical Satellite Imagery,” Remote Sensing, vol. 13, no. 4, p. 558, Jan. 2021, doi: 10.3390/rs13040558.\n\n\n[20] C. Witharana et al., “Ice-wedge polygon detection in satellite imagery from pan-Arctic regions, Permafrost Discovery Gateway, 2001-2021,” 2023, doi: 10.18739/A2KW57K57.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html",
    "href": "sections/ai-for-everyone.html",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "",
    "text": "Goals\nThis session aims to introduce AI to a non-specialist audience, ensuring that participants from any background can understand these essential concepts. The focus will be on explaining key terminology and the basic principles of machine learning and deep learning. By the end of this session, participants will have a solid foundational knowledge of key AI concepts, enabling them to better appreciate and engage with more advanced topics in the following sessions.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#the-foundations-of-ai",
    "href": "sections/ai-for-everyone.html#the-foundations-of-ai",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.1 The Foundations of AI",
    "text": "2.1 The Foundations of AI\nBefore reading the definition, take a moment: What do you think AI is? How would you define it?\n\n\n\n\n\n\nWhat is AI?\n\n\n\nArtificial Intelligence (AI) refers to the development of computer systems capable of performing tasks that typically require cognitive functions associated with human intelligence, such as recognizing patterns, learning from data, and making predictions.\n\n\nBut… there is a minor issue with this definition. What exactly is human intelligence?\nRecognizing patterns, learning, and making predictions are all functions of intelligence, but what lies at the core of a “conscious human”? Why is self-awareness important in cognition, and what evolutionary function does subjective, conscious experience serve?\nIn the philosophy of mind, this phenomenon is referred to as qualia, and there is still no definitive scientific answer to why qualia exist—at least, not yet (see theories of consciousness for more information).\nBut today, let’s focus on a simpler question. How do humans think?\nHistorically, before the 1950s–1960s, scientists believed humans think through a series of if/else statements (e.g., “If I drink more coffee, I’ll be jittery,” or “If a seagull spots my pizza, it’ll try to snatch a bite”). Geoffrey Hinton, a cognitive psychologist and computer scientist, was one of the advocates for an opposing idea: that humans think more experientially or probabilistically. For instance, based on the cloud cover today and similar past experiences, there’s a high probability of rain, so I’ll grab an umbrella. This idea laid the foundation for probabilistic algorithms and, ultimately, the field of Machine Learning.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#machine-learning",
    "href": "sections/ai-for-everyone.html#machine-learning",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.2 Machine Learning",
    "text": "2.2 Machine Learning\nTo quickly recap, AI is a broad term encompassing efforts to replicate aspects of human cognition. Machine Learning is a subset of AI that focuses on algorithms enabling computers to learn from data and build probabilistic models.\n\n\n\n\n\n\nWhat is ML?\n\n\n\nMachine Learning (ML) is a subset of AI that specifically focuses on algorithms that allow computers to learn from data and create probabilistic models.\n\n\n\n\n\nSource: Original comic by sandserif\n\n\nMachine Learning includes various types and techniques, but in this workshop we’ll primarily focus on Neural Networks (NNs).",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#neural-networks",
    "href": "sections/ai-for-everyone.html#neural-networks",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.3 Neural Networks",
    "text": "2.3 Neural Networks\nNNs are loosely inspired by the structure of the human brain and consist of interconnected nodes, or neurons, that process information. The principle “neurons that fire together, wire together” [1] captures the idea that the strength of their connections, known as weights, adjusts based on experience.\n\n\n\n\n\nFigure 2.1: Source [2] The parts of neuron: a cell body with a nucleus, branching dendrites, and a long axon connecting with thousands of other neurons at synapses.\n\n\n\n\n\n\nFigure 2.2: Structure of a neural network: Ramón y Cajal’s drawing of the cells of the chick cerebellum, from Estructura de los centros nerviosos de las aves, Madrid, 1905\n\n\n\n\n\n\n\n\n\n\nWhat is NN?\n\n\n\nNeural Network (NN) is a foundational technique within the field of machine learning. NNs are designed to simulate the way the human brain processes information by using a series of connected layers, or neurons, that transform and interpret input data.\n\n\nThe Perceptron [3], one of the earliest neural network models, was invented in 1957 by psychologist Frank Rosenblatt, who unfortunately did not live long enough to witness the far-reaching impact of his work. Rosenblatt’s Perceptron was a physical machine with retina-like sensors as inputs, wires acting as the hidden layers, and a binary output system. This invention marked the early stages of artificial intelligence, laying the groundwork for the powerful neural networks we use today.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#exercise-nn-playground",
    "href": "sections/ai-for-everyone.html#exercise-nn-playground",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.4 Exercise: NN Playground",
    "text": "2.4 Exercise: NN Playground\n\n\n\n\n\n\nWeb-based app, no setup or account required: playground.tensorflow.org\n\n\n\n\n\n\n.\n\n\nLevel 1: Browse around\n\nSwitch between different choices of datasets (on the left). See if anything changes.\nAdjust the ratio of training to test data. Does the quality of the output vary?\nExperiment with noise and batch size parameters. How does the output change?\n\nOrange indicates negative values, while blue represents positive values. Typically, an 80/20 split for training and testing data is used. Smaller datasets may need a 90% training portion for more examples, while larger datasets can reduce training data to increase test samples. Background colors illustrate the network’s predictions, with more intense colors representing higher confidence in its prediction. Adding noise during training helps the model generalize by recognizing true patterns, enhancing robustness and stability with real-world noisy data.\nLevel 2: Things to try\n\nAdd or remove hidden layers. Notice how it affects the neural network’s performance?\n\n\n\n\n\n\n\nHidden Layers\n\n\n\nHidden Layers are the layers that are neither input nor output. You can think of the values computed at each layer of the network as a different representation for the input X. Each layer transforms the representation produced by the preceding layer to produce a new representation.\n\n\n\nChange the number of neurons in the hidden layers. Can you see any impact on model predictions?\n\nStart with one hidden layer and one or two neurons, observing predictions (orange vs. blue background) against actual data points (orange vs. blue dots). With minimal layers and neurons, predictions are often inaccurate. Increasing hidden layers and neurons improves alignment with the actual data, illustrating how added complexity helps the model learn and approximate complex patterns more accurately.\nLevel 3: More things to try!\n\nExperiment with different features to see their impact on predictions.\nManually adjust the weight. You can see that the thickness of the line changed, which represents the strength of the connection.\n\n\n\n\n\n\n\nWeights\n\n\n\nWeights are parameters within the neural network that transform input data as it passes through layers. They determine the strength of connections between neurons, with each weight adjusting how much influence one neuron has on another. During training, the network adjusts these weights to reduce errors in predictions.\n\n\n\nChange the learning rate to observe its effect on training speed and accuracy.\nTry various activation functions to see how they influence model performance.\nExperiment with different problem types (e.g., classification vs. regression) and analyze the outcomes.\n\nAs you press the play button, you can see the number of epochs increase. In an Artificial Neural Network, an epoch represents one complete pass through the training dataset.\n\n\n\n\n\n\nLearning rate\n\n\n\nThe learning rate is a key setting or hyperparameter that controls how much a model adjusts its weights during training. A higher rate speeds up learning but risks overshooting the optimal solution, while a lower rate makes learning more precise but slower. It’s one of the most crucial settings when building a neural network.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#backpropagation",
    "href": "sections/ai-for-everyone.html#backpropagation",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.5 Backpropagation",
    "text": "2.5 Backpropagation\nInitially, neural networks were quite shallow feed-forward networks. Adding more hidden layers made training them difficult. However, in the 1980s—often referred to as the rebirth of AI—the invention of the backpropagation algorithm revolutionized the field. It allowed for efficient error correction and gradient calculation across layers, making it possible to train much deeper networks than before.\n\n\n\n\n\n\nWhat is backpropagation?\n\n\n\nBackpropagation is an algorithm that calculates the error at the output layer of a neural network and then “back propagates” this error through the network, layer by layer. It updates the connections (weights) between neurons to reduce the error, allowing the model to improve its accuracy during training.\n\n\n\n\n\n\n\nFigure 2.3: Backpropagation\n\n\n\n\n\n\nFigure 2.4: Source (3Blue1Brown)\n\n\n\n\nThus, the backpropagation algorithm enabled the training of neural networks with multiple layers, laying the foundation for the field of deep learning.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#deep-learning",
    "href": "sections/ai-for-everyone.html#deep-learning",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.6 Deep Learning",
    "text": "2.6 Deep Learning\n\n\n\n\n\n\nDeep Learning\n\n\n\nDeep Learning (DL) is a subset of ML, that uses multilayered neural networks, called deep neural networks.\n\n\n\n\n\nFigure 2.5: Source: Artificial Intelligence - A modern approach. [2]\n\n\n\nFigure 2.7: (a) A shallow model, such as linear regression, has short computation paths between inputs and output. (b) A decision list network has some long paths for some possible input values, but most paths are short. (c) A deep learning network has longer computation paths, allowing each variable to interact with all the others.\n\nDeep learning (DL) techniques are typically classified into three categories: supervised, semi-supervised, and unsupervised. Additionally, reinforcement learning (RL) is often considered a partially supervised technique, sometimes overlapping with unsupervised methods.\nSupervised Learning involves learning from labeled data, where models directly learn from input-output pairs. Common examples include Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Transformers. These models are generally simpler in terms of training and achieve high performance.\nSemi-Supervised Learning combines a small amount of labeled data with a large amount of unlabeled data, often using auto-labeling techniques. Examples include Self-training models, where a model iteratively labels data to improve, and Graph Neural Networks (GNNs), which are useful for understanding relationships between data points.\nUnsupervised Learning relies on unlabeled data, focusing on identifying patterns or structures. Popular models include Autoencoders, Generative Adversarial Networks (GANs), and Restricted Boltzmann Machines (RBMs).\n\n\n\nFigure 2.6: Organogram of AI algorithms.\n\n\nDespite advances in backpropagation, deep learning, computing power, and optimization, neural networks still face the problem known as catastrophic forgetting — losing old knowledge when trained on new tasks. Current AI models are often “frozen” and specialized, needing complete retraining for updates, unlike even simple animals that can continuously learn without forgetting [4]. This limitation is one of the reason that led to the development of specialized deep learning models, each with unique architectures tailored to specific tasks. Let’s explore how each of these models can be applied in scientific research!\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\n\n\n\nConvolutional Neural Networks (CNN) are artificial neural networks designed to process structured data like images. Originally inspired by the mammalian visual cortex, CNNs attempt to mimic how our brains process visual information in layers. First, brains interpret basic shapes like lines and edges and then move to more complex structures, like recognizing a dog ear or the whole animal. This feature, known as “invariance” allows us to recognize objects even if they’re rotated or appear in a different part of our vision.\nCNNs simulate this by using small, repeated filters, or kernels, that scan parts of an image to find basic shapes, edges, and textures, regardless of their location. This scanning process, called convolution, enables early CNN layers to detect simple patterns (like lines) and deeper layers to identify more complex shapes or objects.\nApplication: CNNs are highly effective for image-related tasks, making them ideal for analyzing satellite or drone imagery in ecology, identifying structures in biomedical imaging, and classifying galaxies in astrophysics.\n\n\n\nFigure 2.7: Source (Bennett, 2023) [4] Convolutional Neural Networks\n\n\nLimitations of CNNs\nIronically, though CNNs were inspired by the mammalian visual system, they struggle with tasks that even simpler animals like fish handle easily. CNNs have trouble with rotated or differently angled objects. Current work around it is to have variations of object images in training data with all kinds of different angles [4].\nWhile CNNs follow a layered structure, recent research reveals that the brain’s visual processing is more flexible and not as hierarchical as once believed. Our visual system can “skip” layers or process information in parallel, allowing simultaneous handling of different types of visual input across various brain regions.\n\n\n\n\n\n\n\n\n\nRecurrent Neural Networks\n\n\n\n\n\nRecurrent Neural Networks (RNN) are artificial neural networks designed to process sequential data. By incorporating cycles in their computation graph, RNNs can “remember” previous inputs, making them especially useful for tasks where context is important.\nApplication: These models are commonly used for time series data, such as weather forecasting, monitoring ecological changes over time, and analyzing temporal patterns in genomic data.\n\n\n\nFigure 2.8: Recurrent Neural Network. Source: dataaspirant.com\n\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning\n\n\n\n\n\nReinforcement Learning (RL) is a learning technique in which an agent interacts with the environment and periodically receives rewards (reinforcements) or penalties to achieve a goal.\nWith supervised learning, an agent learns by passively observing example input/output pairs provided by a “teacher.” Reinforcement Learning is one of the attempts to solve the catastrophic forgetting problem and introduce AI agents that can actively learn from their own experience in a given environment.[2]\n\n\n\nFigure 2.8: Reinforcement Learning\n\n\nApplications: RL is applied in robotics and can also assist with experiment simulation in science, environmental monitoring, autonomous driving, and creating AI opponents in gaming.\nOne example of RL is the Actor-Critic model, which divides the learning process into two roles: the Actor, who explores the environment and makes decisions, and the Critic, who evaluates these actions. The Critic provides feedback on the quality of each action, helping the Actor balance exploration (trying new actions) with exploitation (choosing actions with known rewards). Recent research has explored various algorithms to model curiosity in artificial agents [5] [6].\n\n\n\n\n\n\n\n\n\nLLMs: Transformers\n\n\n\n\n\nLarge Language Models (LLM) are a type of neural network that has revolutionized natural language processing (NLP). Trained on massive datasets, these models can generate human-like text, translate languages, create various forms of content, and answer questions informatively (e.g., GPT-3, Gemini, Llama).\n\n\n\nFigure 2.9: Large Language Models. Source: Artificial Intelligence - A modern approach. [2]\n\n\nTransformers, introduced in 2017, revolutionized NLP by introducing a mechanism called self-attention. Unlike previous models like Recurrent Neural Networks (RNNs) that processed language sequentially, Transformers use self-attention to assess relationships between all words in a sentence simultaneously. This allows them to dynamically focus on different parts of the input text and weigh the importance of each word in relation to others. This allows them to understand context and meaning with much better accuracy.\n\n\n\nFigure 2.10: Evolution of Natural Languge Processing. Source[2]\n\n\nThe success of LLMs has driven AI’s recent surge in popularity and research. Between 2010 and 2022, the volume of AI-related publications nearly tripled, climbing from about 88,000 in 2010 to over 240,000 by 2022. Likewise, AI patent filings have skyrocketed, increasing from roughly 3,500 in 2010 to over 190,000 in 2022. In the first half of 2024 alone, AI and machine learning companies in the United States attracted $38.6 billion in investment out of a total of $93.4 billion. [7]",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#ai-beyond-machine-learning",
    "href": "sections/ai-for-everyone.html#ai-beyond-machine-learning",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.7 AI Beyond Machine Learning",
    "text": "2.7 AI Beyond Machine Learning\nWithin the field of AI, there are many techniques that don’t rely on ML principles.\n\n\n\n\n\n\n\nTechnique\nDescription\n\n\n\n\nIf/Else or Rule-Based Systems\nCollections of predefined rules or conditions (if statements) to make decisions.\n\n\nSymbolic AI (Logic-Based AI)\nLogical rules and symbols to represent knowledge, focusing on reasoning through deductive logic.\n\n\nGenetic Algorithms (Evolutionary Algorithms)\nOptimization algorithms inspired by natural selection.\n\n\nFuzzy Logic\nA form of logic that works with “degrees of truth”, making it useful for uncertain or ambiguous scenarios.\n\n\nKnowledge Representation and Reasoning (KR&R)\nTechniques for structuring and processing information, often using ontologies and semantic networks.\n\n\nBayesian Networks\nProbabilistic graphical models that represent relationships between variables.\n\n\n\nRecent research increasingly combines various AI paradigms, such as symbolic AI and Knowledge Representation and Reasoning (KR&R), with Machine Learning (ML) to achieve a higher level of effectiveness tailored to specific tasks.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#the-future-of-ai-in-science",
    "href": "sections/ai-for-everyone.html#the-future-of-ai-in-science",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.8 The Future of AI in Science",
    "text": "2.8 The Future of AI in Science\nAI is transforming the scientific method by supporting each step of scientific discovery. Let’s consider how various AI techniques can be applied at each stage of the scientific process:\n\nObservation: Using computer vision for data collection.\nHypothesis: Clustering data with unsupervised learning.\nExperiment: Simulating environments through reinforcement learning.\nData Analysis: Simplifying data with PCA and classifying insights using neural networks or SVMs.\nConclusion: Combining LLMs with KR&R to generate complex findings and insights.\n\n\nIn conclusion, regardless of model type, high-quality data is essential for accurate AI predictions and insights. In the next sessions, we’ll explore practical tips for working with well-prepared, high-quality data.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#do-you-have-any-questions",
    "href": "sections/ai-for-everyone.html#do-you-have-any-questions",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "Do You Have Any Questions?",
    "text": "Do You Have Any Questions?\nFeel free to reach out!\nEmail: alyonak@nceas.ucsb.edu\nWebsite: alonakosobokova.com\nYouTube: Dork Matter Girl\n\n\n\n\n[1] D. O. Hebb, The organization of behavior: A neuropsychological theory. New York: Wiley, 1949. Available: https://en.wikipedia.org/wiki/The_Organization_of_Behavior\n\n\n[2] P. Norvig and S. J. Russell, Artificial intelligence: A modern approach, 3rd ed. Pearson, 2004. Available: https://books.google.com/books/about/Artificial_Intelligence.html?id=8jZBksh-bUMC\n\n\n[3] F. Rosenblatt, “The perceptron: A probabilistic model for information storage and organization in the brain,” Psychological Review, vol. 65, no. 6, pp. 386–408, 1958, doi: 10.1037/H0042519.\n\n\n[4] M. Bennett, A brief history of intelligence: Evolution, AI, and the five breakthroughs that made our brains, Hardcover. Harper, 2023.\n\n\n[5] D. Kawahara, S. Ozeki, and I. Mizuuchi, “A curiosity algorithm for robots based on the free energy principle,” pp. 53–59, 2022, doi: 10.1109/SII52469.2022.9708819.\n\n\n[6] T. Wang, F. Wang, Z. Xie, and F. Qin, “Curiosity model policy optimization for robotic manipulator tracking control with input saturation in uncertain environment,” Frontiers in Neurorobotics, vol. 18, 2024, doi: 10.3389/fnbot.2024.1376215.\n\n\n[7] Inc. PitchBook Data, “Artificial intelligence & machine learning report, Q2 2024,” PitchBook, 2024. Available: https://pitchbook.com/news/reports/q2-2024-artificial-intelligence-machine-learning-report",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html",
    "href": "sections/ai-ready-data-in-arctic-research.html",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "",
    "text": "Goal\nThis session dives into the concept of ‘AI-ready data’ in Arctic science and geoscience, highlighting the importance of suitable data for AI applications. Participants will learn about creating and managing metadata and organizing data repositories. We’ll discuss best practices for data preparation and structuring for AI processing. By the end, participants will clearly understand AI-ready data characteristics and the steps to transform raw data for AI applications.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#are-we-ready-for-ai",
    "href": "sections/ai-ready-data-in-arctic-research.html#are-we-ready-for-ai",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.1 Are we ready for AI?",
    "text": "3.1 Are we ready for AI?\n\n\n\n\n\n\nWhat is AI-Ready Data?\n\n\n\nData that are accessible, preferably open, and well-documented, making them easily interpretable and machine-readable to simplify reuse.\n\n\nThis is really a variant on Analysis Ready Data (ARD), or, more recently, “Analysis Ready, Cloud Optimized (ARCO)” data.\n\n\n\nMahecha et al. 2020. [1] Visualization of the implemented Earth system data cube. The figure shows from the top left to bottom right the variables sensible heat (H), latent heat (LE), gross primary production (GPP), surface moisture (SM), land surface temperature (LST), air temperature (Tair), cloudiness (C), precipitation (P), and water vapour (V). The resolution in space is 0.25° and 8 d in time, and we are inspecting the time from May 2008 to May 2010; the spatial range is from 15° S to 60° N, and 10° E to 65° W.\n\n\nWorking with xarray and zarr, one can access many multi-petabyte earth systems datasets like CMIP6 (Coupled Model Intercomparison Project Phase 6) and ERA5 (Earth Re). For an overview of Zarr, see the Arctic Data Center Scalable Computing course chapter on Zarr.\nTake, for example, the ERA5 reanalysis dataset [2], which is normally downloadable in bulk from the Copernicus Data Service. ARCO-ERA5 is an Analysis Ready, Cloud Optimized variant of ERA5 which has been reprocessed into a consistent 0.25° global grid, and chunked and saved in Zarr format with extensive metadata such that spatial and temporal subsets are easily extracted. Hosted on the Google Cloud Storage service in a public bucket (gcp-public-data-arco-era5), anyone can easily access slices of this massive multi-petabyte dataset from anywhere on the Internet, and can be doing analysis in seconds. Let’s take a quick peek at this massive dataset:\nimport xarray\n\nds = xarray.open_zarr(\n    'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3',\n    chunks=None,\n    storage_options=dict(token='anon')\n)\nds\n\nWith one line of code, we accessed 273 climate variables (e.g., 2m_temperature, evaporation, forecast_albedo) spanning 8 decades at hourly time scales. And while this dataset is massive, we can explore it from the comfort of our laptop (not all at once, for which we would need a bigger machine!).\nSo, there’s nothing really special about AI-Ready data, in that a lot of the core requirements for Analysis Ready Data are exactly what are needed for AI modeling as well. Labeling is probably the main difference. Neverthless, many groups have gotten motivated by the promise of AI, and particularly machine learning, across disciplines. For example, the federal government has been ramping up readiness for AI across many agencies. In 2019, the White House Office of Science Technology and Policy (OSTP) started an AI-Readiness matrix, which was followed shortly by the National AI Initiative Act in 2020 [3].\n\nFor example, management agencies have started entire new programs to prepare data and staff for the introduction of AI and machine learning into their processes. One such program with a focus on AI-Ready data is NOAA’s Center for Artificial Intelligence (NCAI).\n\n\n\nNOAA NCAI\n\n\nIn beginning to define AI-Ready data for NOAA, Christensen et al. 2020 defined several axes for evaluation, including data quality, data acess, and data documentation. We’ll be dinving into many of these today and over the course of the week.\n\n\n\n\n\n\n\n\n\n\n\n\nData Quality\n\n\n\n\nCompleteness\nConsistency\nLack of bias\nTimeliness\nProvenance and Integrity\n\n\n\n\n\n\n\n\n\n\n\nData Access\n\n\n\n\nFormats\nDelivery options\nUsage rights\nSecurity / privacy\n\n\n\n\n\n\n\n\n\n\n\nData Documentation\n\n\n\n\nDataset Metadata\nData dictionary\nIdentifier",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#open-data-foundations",
    "href": "sections/ai-ready-data-in-arctic-research.html#open-data-foundations",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.2 Open Data Foundations",
    "text": "3.2 Open Data Foundations\nPreservation and open data access are the foundation of Analysis-Ready and AI-ready data. While all modeling and analysis requires access to data, the ability for AI to encompass massive swaths of information and combine disparate data streams makes open data incredibly valuable. And while the open data movement has seen massive growth and adoption, it’s an unfortunate fact that most research data collected today are still not published and accessible, and challenges to the realization of open data outlined by Reichman et al. (2011) are still prominent today [4].",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#arctic-data-center",
    "href": "sections/ai-ready-data-in-arctic-research.html#arctic-data-center",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.3 Arctic Data Center",
    "text": "3.3 Arctic Data Center\nNevertheless, progress has been made. The National Science Foundation Office of Polar Programs Data, Code, and Sample Management Policy (DCL 22-106) embraces the need to preserve, document, and share the data and results from NSF-funded research, and since 2016 has funded the Arctic Data Center to provide services supporting reseach community data needs. The center provides data submission guidelines and data curation support to create well-documented, understandable, and reusable data from the myriad projects funded by NSF and globally each year. In short, the Arctic Data Center provides a long-term home for over 7000 open, Arctic datasets that are AI-Ready. Researchers increasingly deposit large datasets from remote sensing campaigns using unmanned aerial vehicles (UAV), field expeditions, and observing networks, all of which are prime content for AI.\n\n\n\n\n\n\n\nArctic Data Center Catalog\n\n\n\nIn addition to raw observational data and remote sensing imagery, the ADC also stores and distributes model output, labeled training data, and other derived data products. A recent example comes from the Permafrost Discovery Gateway project, in which Neitze et al. used machine learning on multispectral PlanetScope imagery to extract high-resolution geospatial footprints for retrogressive thaw slumps (RTS) and active layer detachment (ALD) slides across the circum-Arctic permafrost region [5]. In addition, the dataset includes human-generated training labels, processing code, and model checkpoints – just what is needed for further advances in this critical field of climate research.\n\n\n\n\n\n\nDARTS retrogressive thaw slump dataset doi:10.18739/A2RR1PP44\n\n\nWhile this and other valuable data for cross-cutting analysis are available from the Arctic Data Center, there are many other repositories that hold relevant data as well. Regardless of which repository a researher has chosen to share their data, the important thing to remember is to do so – data on your laptop or a University web server are rarely accessible and ready for reuse.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#dataone",
    "href": "sections/ai-ready-data-in-arctic-research.html#dataone",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.4 DataONE",
    "text": "3.4 DataONE\nDataONE is a network designed to connect over 60 global data repositories (and growing) to improve the discoverability and accessiblilty of data from across the world. DataONE provides global data search and discovery by harmonizing myriad metadata standards used across the world, and providing an interoperability API across repositories to make datasets findable and programatically accessible regardless of where they live.\n\nFor example, a query across DataONE in 2024 revealed over 4500 datasets held by 16 different repositories, most of which are not specifically tied to Greenland research, per se.\n\nLooking across the whole of the Arctic, we found over 98,000 datasets from 39 data repositories. It is notable that only 6 of those repositories are focused on Arctic research (like the Arctic Data Center), while the rest are either general repositories or discipline specific repositories. For example, Pangaea as a generalist repository has the most datasets with over 10,000, but there are also significant and important data sets on archeology (TDAR), hydrology (HydroShare) and geochemistry (EarthChem).\n\n\n\nGraph of Arctic Data across DataONE",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#metadata-harmonization",
    "href": "sections/ai-ready-data-in-arctic-research.html#metadata-harmonization",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.5 Metadata harmonization",
    "text": "3.5 Metadata harmonization\nOne of the main roles of DataONE is to promote interoperability and improve the quality and discoverability of global data holdings – all of direct benefit to AI Ready data. DataONE promotes the use of detailed, discipline-specific metadata standards that enable researchers to comprehensively document the structure, contents, context, and protocols used when collecting data. For example, a good metadata record records not only the bibliographic information about the Dataset creators, but also documents the spatial and temporal extent of the data, the methods used to collect it, the types of measured properties that were observed or modeled, and other details that are fundamental to the proper interpretation and reuse of the data. Different disciplines focus on different standards: in ecology and environmental science, where biological metadata on taxonomy are important, the Ecological Metadata Language (EML) is used extensively, whereas in geospatial science where time and space are critical, the emphasis is on the ISO 19115 family of metadata standards. Overall, DataONE supports more than a dozen metadata variants, and can be extended to support more. Across the Arctic, we find datasets that use many different metadata approaches.\n\nDataONE harmonizes these standards by cross-walking them conceptually and making the data available for search through an integrated discovery portal and API. And DataONE promotes semantic labeling of the data as well, particularly for measurement types (e.g., fork length for fish length meeasurements) and dataset classification [6]. These annotations are indexed against controlled, ontologically-precise term labels that are stored in queryable systems. For example, the Ecosystem Ontology (ECSO, the Environment Ontology (ENVO), and many others contain precisely defined terms that are useful for precise dataset labeling to differentiate subtly different terms and concepts.\n\n\n\nA sub-Arctic salmon-related dataset [7], showing annotations for each of the measured variables in the dataset. Each annotation is to a precisely defined concept or term from a controlled vocabulary, allowing subtle differences in methodology to be distinguished, which helps with both data discovery and proper reuse. The underlying metadata model is machine-readable, allowing search systems, and amchine learning harvesters to make use of this structured label data.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#croissant-metadata-for-machine-learning",
    "href": "sections/ai-ready-data-in-arctic-research.html#croissant-metadata-for-machine-learning",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.6 Croissant metadata for machine learning",
    "text": "3.6 Croissant metadata for machine learning\nWhile domain-specific metadata dialects continue to proliferate, an increasing number of data repositories support schema.org as a lingua franca to describe datasets on the web for discoverability. The Science on Schema.org (SOSO) project provides interoperability guidelines for using schema.org metadata in dataset landing pages, and DataONE supports search across repositories that produce schema.org. That said, the dialect is fairly lightweight, somewhat lossely defined, and therefore permits some ambiguity in usage. But it has the major advantage that, as a graph-based metadata dialect, it can be easily extended to support new terms and use cases.\nThe Croissant specification [8] extends schema.org with more precise and structured metadata to enable machine-interpretation and use of datasets across multiple tools. While the vocabulary is not as rich as, for example, the ISO 19115 metadata for geospatial metadata, it does provide a more strict structural definition of data types and contents that plain schema.org. A quote from the specification illustrates its intended scope [8]:\n\nThe Croissant metadata format simplifies how data is used by ML models. It provides a vocabulary for dataset attributes, streamlining how data is loaded across ML frameworks such as PyTorch, TensorFlow or JAX. In doing so, Croissant enables the interchange of datasets between ML frameworks and beyond, tackling a variety of discoverability, portability, reproducibility, and responsible AI (RAI) challenges.\n\nCroissant has also explicitly defined metadata to meet the needs of machine-learning tools and algorithms. For example, Crosissant supports the definition of categorical values, data splits for training, testing, and prediction, labels/annotations, specification of bounding boxes and segmentation masks.\nLabel Data. As an example, Crioissant has specific metadata fields designed to capture which fields with the data contain critical label data, which are used by supervised learning workflows. The Croissant metadata class cr:Label can be used in a RecordSet to indicate that a specific field contians labels that apply to the that record.\n{\n  \"@type\": \"cr:RecordSet\",\n  \"@id\": \"images\",\n  \"field\": [\n    {\n      \"@type\": \"cr:Field\",\n      \"@id\": \"images/image\"\n    },\n    {\n      \"@type\": \"cr:Field\",\n      \"@id\": \"images/label\",\n      \"dataType\": [\"sc:Text\", \"cr:Label\"]\n    }\n  ]\n}\nThe intention is that multiple tools, all supporting the Crosissant metadata model, will be able to exchange ML-related data seamlessly. A number of ML tools support Croissant out-of-the-box, but only a tiny fraction of the datasets available today use this nascent vocabulary. In addition, it lacks the sophistication of Analysis Ready, Cloud Optimized (ARCO) data standards like Zarr that permit seamless access to massive data with minimal overhead. But it has a lot of promise for streamlining AI-Ready data, and can be used on top of exiting standards like Zarr.\n\n\n\nCommon ML data providers like Hugging Face and Kaggle could use Croissant to produce ML-optimized datasets, which in turn can be seamlessly loaded and used with compatible ML libraries such as TensorFlow and PyTorch. Image credit: [8]",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#data-quality-1",
    "href": "sections/ai-ready-data-in-arctic-research.html#data-quality-1",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.7 Data Quality",
    "text": "3.7 Data Quality\nOne of the primary determinants of AI-Ready data is whether the data are of sufficient quality for the intended purpose. While the quality of a dataset may have been high for the iniital hypothesis for which it was generated, it might be quite low (e.g., due to biased or selective sampling) at the scales at which machine learning might operate. Consequently, it is fundamentally important to assess the quality of datasets during the planning and execution of an AI project. Questions such as the following would be of prime interest:\n\nDoes the dataset represent the complete population of interest?\nDoes training data reflect an unbiased sample of that population?\nAre the data well-documented, enabling methodological interpretation?\nDid data collection procedures follow standards for responsible AI and ethical reseatch practices?\nAre the data Tidy (normalized) and structured (see Tidy Data lesson)?\nAre geospatial data accessibly structured, with sufficient metadata (e.g., about the Coordinate Reference System)?\n…\n\n\nMany of these issues are encompassed by the FAIR Principles, which are intended to ensure that published data are Finadable, Accessible, Interoperable, and Reusable [9], [10]. While there are a huge variety of methods to assess data quality in use across disciplines, some groups have started to harmonize rubrics for data quality and how to represent data quality results in metadata records (see [11]).\nDataONE is one such group that has operationalized FAIR Assessment [11]. Within the DataONE network, all compatible datasets are evaluated using an automated FAIR rubric which rates the dataset on 52 FAIR checks [12], [13]. Each of these checks is atomic, and looks at a small facet of the dataset quality, but combined they give a powerful assessment of dataset readiness for various analytical purposes. And these suites of checks are extensible, so different groups can create suites of automated quality assessment checks that match their needs and recommendations as a community.\n\n\n\n\nDataONE FAIR assessment\n\n\n\nWe see a marked improvement in dataset quality across the FAIR axes as datasets go through our curation process at the Arctic Data Center.\nThe Arctic Data Center team is currently working to extend this quality assessment suite to deeper data quality checks. Most of the current checks are based on metadata, mainly because these are accessible through the centralized DataONE network, whereas data are distributed throughout the network, and are much larger. The ADC data quality suite will assess generic quality checks and domain-specific quality checks to produce both dataset-level and project-level quality reports. Some of the key types of checks that are posisble with the system include:\n\n\n\nGeneric Checks\nDomain/Discipline Checks\n\n\n\n\nDomain and range values\nTree diameters don’t get smaller\n\n\nData type conformance\nUnit and measurement semantics\n\n\nData model normal\nSpecies taxonomy consistent\n\n\nChecksum matches metadata\nOutlier flagging\n\n\nMalware scans\nAllometric relations among variables\n\n\nCharacters match encoding\nCalibration and Validation conformance\n\n\nPrimary keys unique\nTemporal autocorrelation\n\n\nForeign keys valid\nData / model agreement",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#esip-ai-readiness-checklist",
    "href": "sections/ai-ready-data-in-arctic-research.html#esip-ai-readiness-checklist",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.8 ESIP AI-Readiness Checklist",
    "text": "3.8 ESIP AI-Readiness Checklist\nTo round out our survey of AI-Readiness in data, let’s look at the AI-Readiness Checklist that has been developed by an inter-agency collaboration at the Earth Science Information Partners (ESIP) [14]. This checklist was designed as a way to evaluate the readiness of a data product for use in specific AI workflows, but it is general enough to apply to a wide variety of datasets. In general, the checklist asks questions about four major areas of readiness:\n\nData Preparation\nData Quality\nData Documentation\nData Access\n\nThe challenge with this checklist and others is that readiness is truly in the eye of the beholder. Each project has unique data needs, and what is a fine dataset for one analytical purpose (e.g., a regional model) may be entirely inadequate for another (e.g., a pan-Arctic model).\n\n\n\n\n\n\nExercise: Assess Dataset Readiness\n\n\n\nThere are a huge variety of datasets available from DataONE and the Arctic Data Center, and many other repositories. In this exercise we will do a quick assessment of AI Readiness for the ice wedge polygon permafrost dataset from the Permafrost Discovery Gateway project [[5]], using the ESIP AI-Readiness Checklist.\nLink to dataset: - doi:10.18739/A2KW57K57 - Visualize Permafrost Ice Wedge Polygon data on PDG:\nWe’re going to break into groups, and each group will work on a portion of the evaluation for the dataset. The groups are:\n\nGroup A (Preparation)\nGroup B (Data Quality)\nGroup C (Data Documentation)\nGroup D (Data Access)\n\nInstructions: - Make a copy of the AI-Readiness Checklist spreadsheet - Split into 4 groups and try to quickly answer the questions (we won’t really have time, so don’t get too bogged down)\nQuestions to consider as you are doing the assessment: - How much time would it take for you to do a true assessment? - How useful would this assessment be to you if it were available for most datasets? - Is there a correct answer to the checklist questions?\n\n\n\n\n\n\n\n[1] M. D. Mahecha et al., “Earth system data cubes unravel global multivariate dynamics,” Earth System Dynamics, vol. 11, no. 1, pp. 201–234, Feb. 2020, doi: 10.5194/esd-11-201-2020.\n\n\n[2] “ERA5 hourly data on single levels from 1940 to present.” doi: https://doi.org/10.24381/cds.adbb2d47.\n\n\n[3] S. Long and T. Romanoff, “AI-Ready Open Data.” AI-Ready Open Data  Bipartisan Policy Center, 2023. Accessed: Oct. 19, 2024. [Online]. Available: https://bipartisanpolicy.org/explainer/ai-ready-open-data/\n\n\n[4] O. J. Reichman, M. B. Jones, and M. P. Schildhauer, “Challenges and opportunities of open data in ecology.” Science (New York, N.Y.), vol. 331, no. 6018, pp. 703–5, Feb. 2011, doi: 10.1126/science.1197962.\n\n\n[5] I. Nitze et al., “DARTS: Multi-year database of AI detected retrogressive thaw slumps (RTS) and active layer detachment slides (ALD) in hotspots of the circum-arctic permafrost region - v1,” 2024, doi: 10.18739/A2RR1PP44.\n\n\n[6] S. S. Chong, M. Schildhauer, M. O’Brien, B. Mecum, and M. B. Jones, “Enhancing the FAIRness of Arctic Research Data Through Semantic Annotation,” Data Science Journal, vol. 23, no. 1, Jan. 2024, doi: 10.5334/dsj-2024-002.\n\n\n[7] A. D. of F. Game, D. of C. and Fisheries, and A.-Y.-K. Region, “Salmon age, sex, and length data from Arctic-Yukon-Kuskokwim Region of Alaska, 1960-2017,” 2018, doi: 10.5063/SN07CZ.\n\n\n[8] O. Benjelloun et al., “Croissant Format Specification,” Croissant site. 2024. Accessed: Oct. 20, 2024. [Online]. Available: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n\n\n[9] M. D. Wilkinson et al., “The FAIR Guiding Principles for scientific data management and stewardship,” Scientific Data, vol. 3, p. 160018, Mar. 2016, doi: 10.1038/sdata.2016.18.\n\n\n[10] M. D. Wilkinson, S.-A. Sansone, E. Schultes, P. Doorn, L. O. Bonino da Silva Santos, and M. Dumontier, “A design framework and exemplar metrics for FAIRness,” Scientific Data, vol. 5, p. 180118, Jun. 2018, doi: 10.1038/sdata.2018.118.\n\n\n[11] G. Peng et al., “Harmonizing quality measures of FAIRness assessment towards machine-actionable quality information,” International Journal of Digital Earth, vol. 17, no. 1, p. 2390431, Dec. 2024, doi: 10.1080/17538947.2024.2390431.\n\n\n[12] M. Jones, P. Slaughter, and T. Habermann, “Quantifying FAIR: Metadata improvement and guidance in the DataONE repository network.” 2019. doi: https://doi.org/10.5063/f1kp80gx.\n\n\n[13] M. Jones et al., “MetaDIG: Engaging Scientists in the Improvement of Metadata and Data,” Figshare, 2016, doi: 10.6084/m9.figshare.4055808.v1.\n\n\n[14] “Checklist to Examine AI-readiness for Open Environmental Datasets,” figshare. Jun. 2022. doi: 10.6084/m9.figshare.19983722.v1.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html",
    "href": "sections/data-annotation.html",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "",
    "text": "Goals\nThis session explores the critical role of training data in deep learning, focusing on data annotation methods, tools, and strategies for acquiring high-quality data. Participants will learn how well-annotated data supports effective deep learning models, understanding the challenges and best practices in data annotation. By the end, participants will be equipped to prepare their datasets for deep learning.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#key-elements",
    "href": "sections/data-annotation.html#key-elements",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "Key Elements",
    "text": "Key Elements\nTraining data’s role, annotation methods/tools, annotated data’s importance, annotation challenges, annotation best practices, dataset preparation",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#annotation-fundamentals",
    "href": "sections/data-annotation.html#annotation-fundamentals",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "4.1 Annotation Fundamentals",
    "text": "4.1 Annotation Fundamentals\n\n\n\n\n\n\nHighlights\n\n\n\n\nReiterate ideas related to supervised learning, and the core idea of learning from examples\nDiscuss key role of labeling/annotation in general for generating examples to learn from\nTake a quick tour of label/annotation examples across various ML applications (structured data, text, audio, image, video, etc)\nTalk about some general challenges of procuring/producing labeled data for Machine Learning\n\n\n\n\n4.1.1 Fueling intelligence: It’s All About the Data!\nThe modern AI renaissance is driven by the synergistic combination of Computing advances, more & better data for training, and algorithmic innovations.\n\n\n\nSource: OECD.ai\n\n\nEach of these is critical, but you really can’t overstate the importance of massively upscaling training and validation data. Indeed, to a large extent, the most important recent advances in algorithms and compute have been those that allow us to efficiently use huge amounts of data. The more data available, the better the model can learn.\nRemember that in Machine Learning:\n\nYou are building a model to produce some desired output for a given input. Imagine handing a model an aerial photo that contains a water body, or a camera trap video that contains a bear, or an audio recording that captures a the song of a particular bird species. In each case, you want the model to correctly detect, recognize, and report the relevant feature.\nTo achieve this, you do not build this model by instructing the computer how to detect the water body or the bear or the bird species. Instead, you assemble many (often many, many!) good examples of the phenomena of interest, and feed them to an algorithm that allows the model to adaptively learn from these examples. Now, in practice there may be rule-based guardrails, but we can talk about that separately later in the course.\n\n\n\n\n.\n\n\nMuch of this course is about understanding what kinds of model structures and learning algorithms allow this seemingly magical learning to happen inside the computer, and what the end-to-end process looks like. But for now, we are going to focus on the input data. And as we embark, it is essential is that this core concept makes sense to you:\nFor any project involving development of an AI model, you will quite likely be starting with a generic algorithm that has limited or even zero specific knowledge of your particular application area. Unlike with “classical” modeling, the way you will adapt it to apply to your project is not by hand-tweaking parameters or choosing functional forms describing your phenomenon of interest, but rather by exposing this generalized algorithm to many relevant examples to learn from.\nBottom line, much like vehicles without fuel, even the best training algorithms in the world will just sit and gather dust if they don’t have sufficient data to learn from!\n\n\n\nSource: Walking Dead Fandom\n\n\nUltimately, although you will need to have an understanding of algorithmic and models, and learn how to operationalize them on compute platforms, your success in applying AI (especially if you are training and/or fine-tuning models, not simply applying pre-trained models) will depend on your ability to implement a robust and effective data pipeline, from data collection methods to data annotation to data curation.\n\n\n\n\n\nSource: DZone\n\n\nIn this module, we focus on data annotation.\n\n\n4.1.2 What is annotation?\nData annotation is the process of labeling or marking up data with information that is not already explicit in the data itself.\nIn general, we do this to provide important and relevant context or meaning to the data. As humans, especially in knowledge work, we do this all the time for the purpose of sharing information with others.\n\n\n\nSource: PowerPoint Tricks\n\n\nIn the context of Machine Learning and AI, our objective is to teach a model how to create accurate and useful annotations itself when it encounters new, unannotated data in the future. In order to do this, we need to provide the model with annotated examples that it can train on.\nTo put it a different way, annotation is the process of taking some data just like the kind of data you will eventually feed into the model, and attaching to it the correct answer to whatever question you will be asking the model about that data.\nSimply put, annotation refers to labeling data with information that a model needs to learn, and is not already inherently present in the data.\n\n\n\n\n\n\nThe term “annotation” is synonymous with “labeling”\n\n\n\n\n4.1.2.1 Examples\n\n\n\n\n\n\n\nTabular Data Annotation\n\n\n\n\n\n\n\nLabel (aka Target) column: Species\n\nWhen working with tabular data, we don’t usually talk about “annotating” the data. Nevertheless, the concept of labeling for supervised learning tasks (such as classification and regression) still applies, and indeed it’s common practice to refer to the data used for classification and regression model training as “labeled data”. Labeled tabular data contains a column designated as the target for learning, i.e. the column containing the value that a model learns to predict. Depending on the context (and background of the writer/speaker), you might also hear this referred to as the label, outcome variable, dependent variable, or even just y variable. If this is not already inherently present in the dataset, it must be added by an annotator before proceeding with modeling.\n\n\n\n\n\n\n\n\n\nText Annotation\n\n\n\n\n\n\n\nSentiment: Positive\nParts of speech: most::adv, beautiful::adj\nNamed entity: Alaska\n\n\n\n\n\n\n\n\n\n\nAudio Annotation\n\n\n\n\n\n\n\nVoice recognition\nSpeech to text\n\n\n\n\n\n\n\n\n\n\nImage Annotation\n\n\n\n\n\n\n… our focus today and this week! See details below.\n\n\n\n\n\n\n\n\n\n\nVideo Annotation\n\n\n\n\n\n\nLike image annotation, but with many frames! The focus is often on tracking movement of objects, detecting change, and recognizing activities.\n\n\n\n\n\n\n4.1.3 Why is annotation so important?\nWe’ve already talked about the critical role of data overall in enabling supervised learning, and the role of annotation in explicitly adding or revealing the information in the data.\n\nMore specifically, the annotated data will be used at training time, when a specific learning algorithm will use the information in your annotated data to update internal parameters to yield a specific parameterized (aka “trained”) version of the model that can do a sufficiently good job at getting the right answer when exposed to new data that it hasn’t seen before, and doesn’t have labels.\nThe overall volume and quality of the annotations will have a huge impact on the following characteristics of a model trained on those data:\n\nAccuracy\nPrecision\nGeneralizability\n\nObviously there is a bit of tension here! The point of training the model is do something for you. But in order for the AI to be able to do this, you have to first teach it how, which means doing the very thing that you want it to do.\nThink of it like hiring a large team of interns. Yes, it takes extra time up front to get them trained up. But once you do that, you’re able to scale up operations far beyond what you could do on your own.\nThis raises a few questions that we’ll touch on as we proceed through the course:\n\nIs there a model out there that already knows at least something about what I’m trying to do, so I’m not training it from scratch? Maybe yes! This is a benefit that foundation models (and more generally, transfer learning) offer. To build on the human intern analogy, if you can hire undergrad researchers studying in a field relevant to the task, you’re likely to move much faster than if you hired a 1st grader!\nHow much annotated data do I need? Unfortunately, there is no simple answer. It depends on complexity of task, the clarity of the information, etc. So as we’ll discuss, best practice is to proceed iteratively.\n\n\n\n4.1.4 Annotation challenges\nBy now it should be clear that your goal in the data annotation phase is to quickly and correctly annotate a large enough corpus of inputs that collectively provide an adequate representation of information you want the model to learn.\nHere are some of the key challenges to this activity:\n\n\n\n\n\n\nScalability\n\n\n\n\n\nSimply put, annotating large datasets can be time-consuming!\nThis is especially the case for more complex annotation tasks. Identifying a penguin standing on a rock is one thing, but comprehensively identifying and label all land cover types present in a satellite image is much more time-consuming. Multiply this task by hundreds or thousands, and you’ve quite a scaling challenge!\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nCost\n\n\n\n\n\nCosts become important in conjunction with the scalability challenge.\nYou may find you need to pay for:\n\nAnnotators’ time, whether they are directly employed or used via a service\nAnnotation software or services, if you go with a commercial tool vendor\nData storage, if you are leveraging your own hardware and/or cloud providers like AWS to store large amounts of data\nCPU/GPU cycles, if you are leveraging your own hardware or cloud services to run annotation software, especially if you are using AI-assisted annotation capabilities\n\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nQuality control\n\n\n\n\n\nAnnotation is not always straightforward and easy, but as we’ve discussed, effective model training depends on producing sufficiently high quality annotations of sufficiently high quality training data.\nSome factors to consider:\n\nSource data quality. Is the information signal clear in the data? And does the input dataset include a sufficiently diverse set of examples that are representive of what the model will encountered when deployed?\nAnnotation consistency. Do the annotations capture information in the same way across images? This becomes an even bigger factor when multiple annotators are involved. Clear annotation guidelines and tracking various consistency metrics can help here.\nAnnotation quality. Are the annotations accurate, precise, and complete? Have annotators introduced bias?\n\nIn the end, you will likely need to strike balance between speed and quality. Determining the right goalposts for “good enough” will require experimentation and iterative model training/testing.\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nSubjectivity\n\n\n\n\n\nIn some applications, there is no clear correct answer! In that case, especially without clear guidelines and training, different annotators can interpret data differently. This can leading to inconsistent labels, which in turn will negatively impact model training and lead to degraded model performance.\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nData and annotation management\n\n\n\n\n\nOn a practical front, effectively managing a large-scale annotation activity also requires managing and organizing all associated annotation artifacts, including both the input data and the generated annotations.\nIf you are performing annotation across a team of people, you also need to likely need to keep track of multiple annotations per data object (performed across multiple annotators), metadata associated with those annotations (e.g., how long each annotator took to complete the task), and various metrics for monitoring annotation and annotator performance over time.\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nData privacy & security\n\n\n\n\n\nThis is especially important if you use a cloud-based tool for annotation.\nAsk yourself: What is their data privacy and security policy, and is it sufficient to meet your needs?\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nBias & Ethics\n\n\n\n\n\nManaging bias and ethics is not an annotation-specific problem, and we’ll discuss this later in the cousre. However, bear in mind that annotation can be a major factor, because it is a step in the modeling process when some specific human knowledge (i.e., what the annotators know) is attached to the input data, and will very directly exposed to the model during training. This creates an opportunity for injecting bias, exposing sensitive or private information, among other things.\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nCallout: Annotating satellite imagery\n\n\n\n\nLabeling of satellite imagery brings its own specific challenges. Consider:\n\nScenes are often highly complex and rich in detail\nGeographic distortion: Angle of sensor\nAtmospheric distortion: Haze, fog, clouds\nVariability over time:\n\nWhat time of day? Angle of the sun affects visible characteristics\nWhat time of year? Many features change seasonally (e.g. deciduous forest, grasslands in seasonally arid environments, snow cover, etc)\nFeatures change! Forests are cut, etc. Be mindful of the difference between labeling an image and labeling a patch of the earth’s surface.\n\nIt’s often desirable to maintain the correspondence between pixels and their geospatial location, for cross-reference with maps and/or other imagery\n\n\n\n\n\n4.1.5 Annotation best practices\nThis list could certainly be longer, but if you remember and apply these practices, you’ll start off on a good foot.\n\n\n\n\n\n\nDevelop a thorough annotation protocol\n\n\n\n\n\nCreate and maintain clear labeling instructions.\n\n\n\n\n\n\n\n\n\nProvide annotator training\n\n\n\n\n\n\nWork with annotators to make sure they understand the domain, use cases, and overall purpose of the project.\nProvide specific guidance about what to do in ambiguous or difficult cases, in order to help standardize annotations.\nConsider having new annotators apply annotations on a set of sample inputs, assess those annotations, and provide clear feedback with reference to what they could or should do better.\n\n\n\n\n\n\n\n\n\n\nHave a quality control process\n\n\n\n\n\nTo ensure sufficient quality, plan on doing regular checks, running cross-validations, and having feedback loops.\nFirst, periodically conduct manual annotation reviews to ensure compliance with instructions. This might include having a recognized expert on the team randomly selecting a subset of annotated images to assess.\nSecond, identify and calculate quality metrics on an ongoing basis, targeting each of the following:\nConsensus. To measure the degree to which different annotators on the team are providing similar annotations, have multiple annotors annotate some of the same images, and calculate a consensus measure like Inter-annotator agreement (IAA). Several flavors of this metric exist, such as Cohen’s kappa (to compare 2 labelers) and Fleiss’ kappa (to compare &gt;2 labelers).\nAccuracy. In cases where there’s a known “correct” answer, either for all images or some subset thereof, calculate annotation performance metrics. Here are a couple of examples: - For bounding boxes, calculate a metric like Intersection over union (IoU): Take the area of overlap between the ground truth box and the annotated box, and divide by total area of the (unioned) boxes. - For detected objects overall, calculate standard metrics like precision (proportion of labeled objects that are correctly labeled) and recall (proportion of all objects that were correctly labeled)\nCompleteness. Keep track of annotation completeness overall. For example, when doing bounding box annotation for an object detection task, ensure that all drawn boxes are associated with a valid label.\n\n\n\n\n\n\n\n\n\nProceed iteratively!\n\n\n\n\n\nIn a nutshell:\n\nStart small\nRefine and improve as you go\nScale gradually",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#image-annotation-methodology",
    "href": "sections/data-annotation.html#image-annotation-methodology",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "4.2 Image Annotation Methodology",
    "text": "4.2 Image Annotation Methodology\n\n\n\n\n\n\nHighlights\n\n\n\n\nDiscuss the primary types of image annotations\nDiscuss the common image-related AI/ML tasks requiring annotation\nDiscuss different methods for annotating images\nDescribe a high level annotation workflow\n\n\n\nIt’s important to understand and recognize the difference between image annotation types, tasks, and methods. Note that this isn’t universal or standardized terminology, but it’s pretty widespread.\nIn this context:\n\nAn annotation type describes the specific format or structure of the annotation used to convey information about the data critical for supporting the task.\nAn annotation task is the specific objective that the annotations are meant to support, i.e. the job you want your AI application to do. In the computer vision context, this typically means identifying or understanding something about an image, and conveying that information in some specific form.\nAn annotation method refers to the process or approach used to create the annotations.\n\n\n4.2.1 Image Annotation Types\nThe type of annotation you apply will depend partly on the task (see next section), as different annotation types are naturally suited for different tasks. However, the decision will also be driven in part by time, cost, and accuracy considerations.\n\n\n\n\n\n\nImage tags\n\n\n\n\n\nTags are categorical labels, words, or phrases associated with the image as a whole, without explicit linkage to any localized portion of the image. \n\nLabel: beach\nCaption: “Embracing the serenity of the shore, where the sky meets the ocean #outdoor #beachlife #nature”\n\n\n\n\n\n\n\n\n\n\nBounding boxes\n\n\n\n\n\nBounding boxes are rectangles drawn around objects to localize them within an image. \nTypically they are axis-aligned, meaning two sides are parallel with the image top/bottom, and two sides are parallel with the image sides, but sometimes rotation is supported.\n\n\n\n\n\n\n\n\n\nPolygons\n\n\n\n\n\nGeneralizing the bounding box concept, polygons are a series of 3 or more connected line segments (each with definable end coordinates) that form a closed shape (i.e. the end of the last segment is the beginning of the first segment), used to more precisely localize objects or areas by outlining their shape. \n\n\n\n\n\n\n\n\n\nSegmentations\n\n\n\n\n\nSegmentations involve assigning a class label to individual pixels (or collectively, to regions of individual pixels) in an image. Segmentation may be done either fully for all pixels, or partially only for pixels associated with phenomena of interest.\nIn practice, segmentations are produced either by drawing a polygon to circumscribe relevant pixels, or using a brush tool to select them in entire swaths at a time\n\n\n\n\n\n\n\n\n\n\nKeypoints\n\n\n\n\n\nKeypoints are simply points, used for denoting specific landmarks or features (e.g., skeletal points in human pose estimation). \n\n\n\n\n\n\n\n\n\nPolylines\n\n\n\n\n\nPolylines are conceptually similar to polygons, but they do not form a closed shape. Instead, the lines are used to mark linear features such as roads, rivers, powerlines, or boundaries. \n\n\n\n\n\n\n\n\n\n3D Cuboids\n\n\n\n\n\n3D cuboids are bounding boxes extended to three dimensions. These are often used in LiDAR data which is represented as a 3-dimensional point cloud, but can also be used to indicate depth of field in a 2D image when the modeling task involves understanding position in three dimensions. \n\n\n\n\n\n4.2.2 Image Annotation Tasks\nThe task you choose will depend on the type of information you want the model to extract from the images. Here are the key types of annotation tasks in computer vision:\n\n\n\n\n\n\nImage Classification\n\n\n\n\n\nImage classification is the task of assigning an entire image to a category.\nThe classification typically refers to some singular dominant object or feature (e.g., “Polar bear”) within the image, or some defining characteristic of the image (e.g., “Grassland”), but the details depend on the specific use case motivating the modeling exercise.\n\n\n\n\n\n\n\n\n\nImage Captioning\n\n\n\n\n\nImage captioning is the task of generating textual descriptions of the image. It is conceptually similar to image classification, but involves producing freeform text for each image rather than assigning the image to one of a set of pre-defined categorical classifications.\n\n\n\n\n\n\n\n\n\nObject Detection\n\n\n\n\n\nObject detection is the task of identifying one or more objects or discrete entities within an image.\nNote that object detection involves two distinct sub-tasks:\n\nLocalization: Where is the object within the image?\nClassification: What is the localized object?\n\n\n\n\n\n\n\n\n\n\nImage Segmentation\n\n\n\n\n\nSegmentation is the task of associating individual pixels with labels for purpose of enabling detailed image analysis (e.g., land-use segmentation). In some sense, you can think of it as object detection reported at the pixel level.\nThere are three distinct kinds of segmentation, illustrated below for the following image:\n\nSemantic Segmentation assigns a class label to each pixel in the image, without differentiating individual instances of that class. It is best for amorphous and uncountable “stuff”. In the image below, notice the segmentation and separation of the foreground grass from the background trees from the water in the middle. Also notice that the bears are all lumped together in one segment.\n\nInstance Segmentation separately detects and segments each object instance. It’s therefore similar to semantic segmentation, but identifies the existence, location, shape, and count of objects. It is best for distinct and countable “things”. Notice the separately identified four bears in the image below:\n\nPanoptic Segmentation) combines semantic segmentation + instance segmentation by labeling all pixels, including differentiation of discrete and separately objects within categories. Notice the complete segmentation in the image below, including both the various background types as well as the four distinct bears.\n\nFor more on Panoptic Segmentation, check out the research publication.\n\n\n\n\n\n\n\n\n\nTemporal Annotation\n\n\n\n\n\nTemporal annotation is the task of labeling satellite images over time to track changes in environmental features.\n\n\n\n\n\n4.2.3 Image Annotation Methods\nThe annotation method largely boils down to whether annotations are done manually versus with some level of supporting automation. Ultimately, the choice involves project-specific determination of the cost, speed, and quality of human annotation relative to what can be achieved with available AI assistance.\n\n\n\n\n\n\nManual Annotation\n\n\n\n\n\nWith purely manual annotation, all labeling is done by human annotators.\nNote that good tooling may help make this process easier and more efficient, but ultimately it is up to the human annotator to fully apply annotations to unlabeled inputs.\n\n\n\n\n\n\n\n\n\nSemi-Automated Annotation\n\n\n\n\n\nWith semi-automated annotation, machines assist humans in generating annotations, but humans are still heavily involved in real time with labeling decisions, ranging from actually applying the annotations to refining AI-generated annotations.\nThis can take a few different forms. For example:\n\nModel-based filtering: A model is trained to recognize images with any candidate objects (as compared to empty scenes), and is used to reduce the number of images passed to the human annotator.\nModel-assisted labeling: A pre-trained model generates a candidate annotation, which the human can accept, reject, or modify in some way (e.g., size, position, category).\nActive Learning: A model is learning how to annotate the images alongside the human, and actively decides which images the human should label to accelerate model training the fastest.\n\n\n\n\n\n\n\n\n\n\nAutomated Annotation with Human Validation\n\n\n\n\n\nAt the level of automated annotation with human validation, AI models generate most annotations autonomously. Humans only review the results after the fact, typically checking accuracy metrics at a high level and perhaps inspecting a random sample of annotations, rather than reviewing every annotation.\nExample: A pre-trained model processes satellite images and automatically labels roads, rivers, and forests across thousands of images. A human reviewer then inspects a small percentage of these results to confirm the annotations are accurate, fixing any errors and perhaps fine-tuning the model before the dataset is finalized.\nAt first glance, it might seem illogical that this scenario could exist! If you already have a model that can do the annotation, then don’t you already have a model to do the actual task you want to do?\nIn practice, however, there are some cases where this might be applicable:\n\nOne scenario involves model distillation. Imagine there exists a big, expensive, and/or proprietary (i.e., hidden behind an API) model that does the task you want, and perhaps a lot more. You can use this model to annotate a dataset that you use to train a more compact or economical model that you own and control. In the end, you have effectively distilled the source model’s capability into your own model, through the annotated training data set.\nA second scenario is when you do indeed already have a trained model to perform annotations, whether your own or someone else’s, and are now using it to automatically annotate vasts amounts of data that will serve as inputs to some other machine learning or analysis pipeline. Indeed, in research settings, this is usually the end objective! When you reach this point in the process, you will effectively be doing automated annotation with human validation to ensure that the results are reasonable in aggregate.\n\n\n\n\n\n\n\n\n\n\nFully Automated Annotation\n\n\n\n\n\nRare in practice! Under fully automated annotation, trained models generate annotations with no human involvement, and the quality is deemed sufficient without review.\nThis is typically only relevant in very specific settings, namely in environments where the image data is very highly controlled. For example, consider images that were produced in a lab setting where the composition of the images is highly controlled, or images that were generated synthetically by some known computational agent (e.g., in video games). A related approach with synthetic data involves using trained AI models to generate both the images and their corresponding annotations, in which case the annotation ground truth for each image.\n\n\n\n\n\n4.2.4 Data Annotation Workflow\n\n\n\n\n\n\n1 - Data collection\n\n\n\n\n\nFirst step: Get a sufficiently large and diverse set of data to annotate and subsequently train on.\nYou may already have a set of images from your own research, e.g. from a set of camera traps or aerial flights. Or perhaps you already have a clear use case around detecting features in a particular satellite dataset, and have already procured the imagery. If so, great.\nIf you don’t have your own imagery – and maybe even if you do – you may want to consider augmenting it with additional images if you don’t have enough diversity or content in your own imagery. Depending on your use cases, you may want to poke around public mage datasets like ImageNet.\n\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n2 - Tool Selection\n\n\n\n\n\nTime to choose your annotation tool/platform!\nThere are many options, and lots of factors to consider. See the next section for plenty more detail.\n\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n3 - Data preprocessing\n\n\n\n\n\nBefore proceeding, it’s almost always useful (some sometimes essential) to apply various preprocessing tasks to your data to make it easeir to annotatate and/or eventually train on.\n\n\n\nSource: Medium\n\n\nHere are some categories of common preprocessing tasks:\nReformatting. If relevant, you may need to convert your source images into a better file format for your task. Beyond this, it may be useful to rotate, crop, rescale, and/or reproject your images to get them into a consistent structural format.\nBasic data cleaning. - For example, with satellite or aerial imagery, you may find it useful to apply pre-processing stesp such as filtering to removing noise, correcting for atmospheric conditions, correcting other distortion, adjusting brightness/contrast/color.\nFeature enhancement. Other context-specific transformations may be useful for “bringing out” information for the model (and human annotators) to use, leading to faster and/or better model outcomes. For an example, list to this story about how careful transformations of Sentinel 2 imagery provided a huge boost in the detection of field boundaries as part of the UKFields project.\n\n\n\n\n\n\n\n\n\n4 - Guideline Development\n\n\n\n\n\nAs we discussed earlier, before you begin in earnest, it’s critical that you develop specific guidelines for annotators to follow when doing the annotation using the selected tool.\nNote: These should be written down! Some annotation platforms provide a way to document instructions within the tool, but if yours doesn’t (and probably even if it does), you should create and maintain your own written documentation\n\n\n\nSource: Acquiro\n\n\nOften this will be based on a combination of prior knowledge and task familiarity. To the extent that nobody on the project has extensive experience with the task at hand, it’s often helpful to do some prototyping to inform development of the guidelines.\n\n\n\n\n\n\n\n\n\n5 - Annotation\n\n\n\n\n\nIt’s time to annotate!\n\n\n\nSource: shaip\n\n\nKeep in mind the following image annotation best practices. They may not always hold, but in general:\n\nKeeping bounding boxes and polygons “tight” to the object:\nFor occluded objects, annotate as if the entire object were in view\nIn general, label partial objects cut off at the edge\nLabel all relevant objects in the image. Otherwise, “negative” labels will hamper model learning.\n\nAbove all else, remember, consistency is critical!\n\n\n\n\n\n\n\n\n\n6 - Quality Assurance\n\n\n\n\n\nReview the annotations for quality, and if needed, refine by returning to an earlier step in the workflow.\n\nNote that although QA is identified here as a discrete stage in the workflow, in practice quality is achieved through deliberate attention at multiple stages in the process, including:\n\nInitial annotator workforce training before any annotation is done\nContinuous monitoring during the annotation process\nFinal post-annotation review\n\n\n\n\n\n\n\n\n\n\n7 - Data Export\n\n\n\n\n\nFinalize and output the annotated data for model training.\n\nTypically you will need to get the data into some particular format before proceeding with model training. If your annotation tool can export in this format, you’re all set. If not, you’ll need to export in some other format and then use a conversion tool that you either find or create yourself.\n\n\n\nFrom here, presumably you’ll move on to model training!\nRemember this key best practice: Iterate! You will almost certain not proceed through the annotation workflow in one straight shot. Plan to do some annotations, train, test, fix annotations, figure out whether/how to do more and/or better annotations, refine your annotation approaches, etc.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#annotation-tools-platforms",
    "href": "sections/data-annotation.html#annotation-tools-platforms",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "4.3 Annotation Tools & Platforms",
    "text": "4.3 Annotation Tools & Platforms\n\n\n\n\n\n\nHighlights\n\n\n\n\nGet a sense of what kind of tools are out there today!\nDiscuss high level considerations for choosing a tool\nReview some specific tools out there today\nHighlight how fast things are changing!\n\n\n\n\n4.3.1 High level considerations\nHere are some questions you should be asking…\n\n\n\n\n\n\nWhat annotation types are supported?\n\n\n\n\n\nDoes the tooling allow you do create the kinds of annotations necessary for your task? This probably the first and most fundamental question you should be asking!\n\n\n\n\n\n\n\n\n\nWhat import image formats are supported?\n\n\n\n\n\nCan the tool read images in the right format?\nFortunately, most tools can automatically take a wide range standard image formats including JPG, PNG, BMP, and TIF, and more.\nHowever, if you are working with spatial imagery, including GeoTIFFs, most tools will not natively read in your data. You will need to convert between formats, or choose a tool that is explicitly designed to handle that kind of data.\n\n\n\n\n\n\n\n\n\nWhat output annotation formats are supported?\n\n\n\n\n\nWhile image formats are reasonably standardized, image annotation formats are more diverse. In general, the format you need will be dictated by the constraints of whatever modeling tasks and tooling you will be using to train and validate a model with your annotated data.\nSome annotation software, especially the major players and cloud-based offerings, support a diverse set of output formats, whereas others output only a limited number of formats – or even just their own idiosyncratic format! In that case, you may need to do a conversion to get your annotations in the right format. Fortunately, there’s a good chance that somebody else has already been down this path, and if you search around, you may find a script or package that can do it for you.\nExample formats (not exhaustive!):\n\nVarious JSON formats\n\nCOCO JSON\nVGG Image Annotator JSON\nLabelMe JSON\n\nYOLO TXT\nPascal VOC XML\nTensorFlow TFRecord\n… and lots more …\n\nSee this great page for exploring many different formats.\n\n\n\n\n\n\n\n\n\nWho will be doing the annotation?\n\n\n\n\n\n\nIn-house: You and your team.\nCrowdsource: The broader community.\nOutsource: External people with whom you contract, either directly or through a 3rd party annotation services company. Yes, these do exist!\n\n\n\n\n\n\n\n\n\n\nHow can I assess annotation quality?\n\n\n\n\n\nWe’ve discussed the importance of having high quality annotations, and briefly covered various types of quality assessment. Some tools leave it entirely up to you to handle this, but others have features that help in this area. This can include:\n\nAutomatic calculation of various quality metrics\nConfigurable mechanisms for distributing images among annotators, and choosing how many annotators will see each image\nVarious other forms of annotation process metadata and analytics\n\n\n\n\n\n\n\n\n\n\nIs the tool easy to use?\n\n\n\n\n\nAs with any category of software, some options will be easier to use than others. For image annotation, where you are likely going to want to scale up to a large number of images, small speed-ups in the annotation process will really start to add up over time.\nConsider:\n\nIs the software easy to navigate in general?\nDoes the annotation interface have responsive, reliable, and easy-to-use UI elements for creating, modifying, and deleting image annotations?\nAre there effective keyboard shortcuts to help speed up manual annotations?\nDoes the tool offer effective model-assisted or other “smart” annotation capabilities?\nAre there well-designed features for managing your images, annotations, and ovearll workflow?\nIs there any useful API support to enable programmatic upload, download, or other automation?\n\n\n\n\n\n\n\n\n\n\nHow much am I willing to pay for tooling?\n\n\n\n\n\nIn short, some software options are free, wherease others are commercial offerings with varying costs and prices tiers. As you compare features, consider what you’re willing (and able) to pay for.\n\n\n\n\n\n\n\n\n\nHow is the software licensed?\n\n\n\n\n\nSome annotation software apps and libraries are open source, whereas others are proprietary. You may want to lean toward the open source options if you want to be able to review the source code and understand how it works, and/or (perhaps more importantly) have the option of modifying it to better meet your needs. Of course, general speaking, the open source options will typically also be free, whereas proprietary software is more likely to come with costs.\n\n\n\n\n\n\n\n\n\nWhere does the software run?\n\n\n\n\n\nDo you care if the software runs on your local computer? Do you want it to be something that you deploy and run on your own managed server, either locally or on a VM hosted in a public cloud? Or would you prefer to use a pure cloud-based annotation platform (i.e., a SaaS offering) that somebody else maintains and you access via a browser and/or API?\nAs with any software decision, there are pros and cons to each option.\nBear in mind that with image annotation, any cloud-based offering raises security and privacy considerations, as your images and annotations will reside on somebody else’s server. Consider whether this is a concern for you.\n\n\n\n\n\n\n\n\n\nWhat collaboration features are there?\n\n\n\n\n\n\nWhat collaborative features are offered?\n\n\n\n\n\n\n4.3.2 Tools & services galore\nNote that for geospatial image data annotation in particular, historically there’s been a divide between these two approaches:\n\nMature GIS platforms (QGIS, ArcGIS, etc) -\n\nFirst-class geospatial data and imagery support\nNative capabilities for drawing and editing features like points, lines, and polygons\nBut all of menus and heavyweight UI around robust spatial feature management can impede fast & efficient annotation\nLimited or no support for the broader annotation workflow and lifecycle\n\nImage annotation software and platforms (LabelBox, RoboFlow)\n\nReally nice and constantly improving\nMostly generic with respect to supporting annotation for Computer Vision tasks, not full-featured around environmental research applications, especially with respect to Remote Sensing imagery with spatial component, multispectral bands, etc\n\n\nIn between, you’ll find a few dedicated software packages for environmental and/or spatial image annotation. However, because this is a small niche, you’ll find that they’re often rough around the edges, and likely have a very focused (i.e., limited) set of features addressing only the specific use cases of relevance to the development team. On the plus side, usually they are developed as open source projects, so if you’re up for the investment, you may want to consider contributing or extending these tools to meet your needs.\n\n4.3.2.1 Open-Source Tools for Image Annotation\n\nLabelImg\n\nHigh level: An open-source tool for creating bounding boxes.\nUsed for object detection mainly, maybe??\nOnly supports bounding boxes for annotation\n“Graphical image annotation tool and label object bounding boxes in images”\nIt is written in Python and uses Qt for its graphical interface.\nAnnotations are saved as XML files in PASCAL VOC format, the format used by ImageNet. Besides, it also supports YOLO and CreateML formats\nSee this third-party video tutorial\n\nVGG Image Annotator (VIA)\n\nHigh level: A flexible (but manual) tool for image, video, and audio annotation.\nServerless web application, runs locally and self-contained in a browser, with no network connection required\nReleased in 2016, still maintained, based out of Oxford\nSee demo\n\nLabelMe\n\nOriginally built as an online annotation tool, now distributed\nNow distributed as a deployable web application that you can ran on a local web server\nNot to be confused with this independent Python/QT port of labelme\nWait and what about this labelme GitHub repo??\n\nIRIS (Intelligently Reinforced Image Segmentation)\n\nProvides semi-automated annotation for image segmentation, geared toward multi-band satellite imagery\n\n\n\n\n4.3.2.2 GIS platforms with annotation plugins\n\nQGIS\nArcGIS\n\n\n\n4.3.2.3 Hybrid solutions with both desktop and hosted options\n\nCVAT (Computer Vision Annotation Tool):\n\nOpen-source tool for video and image annotation, widely used in computer vision projects.\nUses pre-trained models to assist annotation?\nSee GitHub repository\nAlso has cloud-based offering and offers annotation services\nSupports:\n\nlabeling images\ndrawing bounding boxes\nmodel assisted labeling using models like YOLO \nmanual semantic segmentation \nautomatic semantic segmentation with SAM \n\n\n🔥 Label Studio\n\nMulti-type data labeling and annotation tool with standardized output format\nWorks on various data types (text, image, audio)\nHas both open source option and paid cloud service\nSee online playground\n\nMicrosoft’s Spatial imagely labeling toolkit\nimglab\n\n\n\n4.3.2.4 Commercial apps\n\nRectLabel\n\nOffline image annotation tool for object detection and segmentation\nHas regular and Pro version\nBuilt for Mac\nSee support page\n\n\n\n\n4.3.2.5 Commercial services\n\nLabelbox\n\nCloud-based commercial platform, albeit with possible free options for academic researchers\n\nRoboflow annotate\n\nOnline platform, with limited free tier\nFree tier does not offer any privacy\n\nSuperAnnotate\n\nHigh level: Full-featured collaborative annotation and modeling platform\nCommercial offering with free tier\n\nMakeSense.ai\n\nIncludes AI models!\nGitHub0\n\nSupervise.ly (commercial with free version)\nLabelerr (commercial with free researcher tier)\nRMSI annotation tools & services\nKili annotation platform (see geoannotation docs)\nSegments.ai labeling platform\nSama\nScaleAI\nDiffgram (see tech docs and GiHub) – commercial but locally installed? Hard to tell!\nDarkLabel\nGroundwork professional labeling services\n\n\n\n4.3.2.6 Fully managed AI & annotation services\n\nAlegion\nManthano\n\n\n\n4.3.2.7 Other platforms\n\nZooniverse? Crowd-sourcing annotation platform\n\nE.g. The Arctic Bears Project\n\nDeepForest\n\nFrom the Weecology lab\nPython package for training and predicting ecological objects in airborne imagery\nComes with a tree crown object detection model and a bird detection model\nSee GitHub repo\n\n\n\n\n\n4.3.3 Miscellaneous links\n\nSatellite image deep learning (Robin Cole’s site)\nOpen Source Data Annotation & Labeling Tools",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html",
    "href": "sections/hands-on-lab-data-annotation.html",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "",
    "text": "Goal\nThis hands-on lab session is designed to give participants practical experience in data annotation for deep learning. Participants will apply the methods, tools, and best practices discussed in the previous session, working directly with datasets to annotate data effectively.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#key-elements",
    "href": "sections/hands-on-lab-data-annotation.html#key-elements",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "Key Elements",
    "text": "Key Elements\nUse of annotation methods and tools, direct dataset interaction",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#choose-your-own-adventures",
    "href": "sections/hands-on-lab-data-annotation.html#choose-your-own-adventures",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "Choose your own adventure(s)",
    "text": "Choose your own adventure(s)\nIn this section, we’ll provide some links, basic information, and suggested starter activities for variety of annotation tools available today. Have a look and get your hands dirty!\nNote: You’ll need some images to annotate in each case. Feel free to use any relevant images you might already have, or just do a web search and find something interesting. Of course, when experimeting with the web-based annotation platforms, be sure not to use upload anything personal, private, or otherwise sensitive.\nIdeally you’ll cover:\n\nSimple bounding box annotation\nPolygon, line, and point annotation\nInteractive model-assisted segmentation\nInspecting annotation output files in various formats, including COCO JSON\nOne or more cloud (web-based) tools\n(For the even more adventurous) One or more locally installed tools",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-make-sense",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-make-sense",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.1 Adventure: Make Sense",
    "text": "5.1 Adventure: Make Sense\n\n\n\n\n\n\nWeb-based app, no setup or account required\n\n\n\n\n\n\n.\n\n\n\nMakeSense.ai is a simple, single-user, browser-based image annotation app\nSupports annotation via bounding boxes, poylgons, points, and lines\nUpload one or more images, apply/edit annotations, then export annotations\nOffers model-based semi-automated annotation with an accept/reject interface\nIf you prefer, you can also grab the source code and run it locally using npm or Docker\n\nThings to try\n\nUpload one or more images\nPlay around with manually creating various annotations of various classes. What is the experience?\nUse Actions to edit label names, colors, etc\nUse Actions to export annotations. What formats are offered?\nTry exporting polygon annotations in both VGG and COCO format. How do they compare?\nUse Actions to run the COCO SSD model locally to suggest boxes. How well does it work?\nWhen you’re done: Evaluate this tool with respect to the software considerations in Section 4.3.1",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-roboflow",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-roboflow",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.2 Adventure: Roboflow",
    "text": "5.2 Adventure: Roboflow\n\n\n\n\n\n\nWeb-based app, requires (free) account signup\n\n\n\n\n\n\n.\n\n\n\nRoboflow offers a cloud-hosted, web-based platform for computer vision, including tooling for data annotation along with model training and deployment\nThey offer a limited free tier, which does not offer any privacy (project and images are automatically public)\nNice interface for doing annotations, managing artifacts, managing team\n\nThings to try\n\nCreate an account and test project\nUpload one or more images\nGo to the Annotate interface and experiment with different annotation types. How easy is it to create, edit, and delete annotations?\nUse the Smart Polygon tool to create polygons by clicking on an object, then refining by adding more clicks inside and outside the object. What is the experience like? Does this speed up your annotations?\nGo back to the main Annotate menu and note how it is organized to support a coherent, team-based annotation workflow. Check out their collaboration documentation. Imagine how you might use this for a multi-person project.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-cvat",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-cvat",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.3 Adventure: CVAT",
    "text": "5.3 Adventure: CVAT\n\n\n\n\n\n\nWeb-based app, requires (free) account signup\n\n\n\n\n\n\n.\n\n\n\nCVAT can be used as a desktop application that you install & run on your own local computer or server.\nHowever, for today, consider creating your own (free) account for annotating using their hosted platform\nThe V7 cvat guide might be helpful\n\nThings to try\n\nCreate a free account\nLog in and create a test Project. At this stage, you’ll need to define at least one relevant label under the Constructor tab (you can edit these later)\nCreate a Task (i.e., a collection of images to annotate) under your Project, and upload one or more images.\nStart an annotation Job within the task. What do think of the interface? Is the documentation helpful?\nUsing the menu bar on the left, try creating box, polygon, line, and point annotations. Note: Click the Shape button to start each annotation. How is the experience?\nAlso try creating a 3D cuboid annotation. Figure out how to resize and orient the cube. What do you think?\nLastly, try doing brush-based segmentations.\nAfter doing some annotations, go to Jobs, use the 3-dots selector on your job to open the action menu, and export annotations in a couple different formats. How do they compare?\nAs a another Jobs action, you can do click on View analytics and run a performance report.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-zooniverse",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-zooniverse",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.4 Adventure: Zooniverse",
    "text": "5.4 Adventure: Zooniverse\n\n\n\n\n\n\nWeb-based app, no setup or account required\n\n\n\n\n\n\n.\n\n\nZooniverse is a cool community crowdsourcing platform on the web, for data annotation and digitization.\nThings to try\n\nCheck out the Penguin Watch project.\n\nVisit the About, Talk, and Collect pages. Imagine how you might set up your own project to encourage and support a crowdsourced annotation community\nVisit the Classify page, go through the Tutorial, and then see how the Task works.\n\nAlso check out the Arctic Bears image classification and interpretation project\nFeel free to search the site for other projects",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-iris-intelligently-reinforced-image-segmentation",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-iris-intelligently-reinforced-image-segmentation",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.5 Adventure: IRIS (Intelligently Reinforced Image Segmentation)",
    "text": "5.5 Adventure: IRIS (Intelligently Reinforced Image Segmentation)\n\n\n\n\n\n\nRequires local installation (Python + JavaScript application).\nSee Installation instructions with for setting IRIS up locally with Python/pip.\n\n\n\n\n\n\n.\n\n\nIRIS is a tool for doing semi-automated image segmentation of satellite imagery (or images in general), with a goal of accelerating the creation of ML training datasets for Earth Observation. The user interface provides configurable simultaneous views of the same image for multispectral imagery, along with interactive AI-assisted segmentation.\nUnlike much of the ML we’ll encounter this week, the backend model in this case is a gradient boosted decision tree. The reason this works sufficiently well is that IRIS is geared toward segmenting multispectral imagery into a small number of classes, training from scratch on each image; the model is able to learn the correlation structure between features and labels by leveraging multiple features per pixel after the human-in-the-loop manually segments and labels pixels.\nFor more information, check out the YouTube video with the main creator Alistar Francis.\nThings to try\n\nRun the IRIS demo that comes with the code\nUsing the onboard help widgets, figure out how to navigate the interface\nTry iteratively labeling some pixels and running the AI. How does it do?\nExperiment with changing the views. How does that improve your ability to manually distinguish and label features like clouds?",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-segment-geospatial-samgeo",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-segment-geospatial-samgeo",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.6 Adventure: Segment-Geospatial (samgeo)",
    "text": "5.6 Adventure: Segment-Geospatial (samgeo)\n\n\n\n\n\n\nRequires local installation (Python library), or can be run in a hosted notebook environment (JupyterLab, Google Collab, etc).\nSee Installation notes.\n\n\n\nThis is an open source tool that you can either install locally or run in JupyterLab (or Google colab).\nFirst check out the online Segment Anything Model (SAM) demo. SAM was developed by Meta AI. It is trained as a generalized segmentation model that is able to segment (but not label) arbitrary objects in an image. It is designed as a promptable tool, which means a user can provide initial point(s) or box(es) that roughly localize an object within an image, and SAM will try to fully segment that object. Alternatively, it can automatically segment an entier image, effectively by self-promtping with a complete grid of points, and then intelligently merging the corresponding segments.\nToday, SAM is used by numerous image annotation tools to provide interactive, AI-assisted segmentation capabilities.\nOne such tool is the segment-geospatial Python package, which provides some base functionality for applying SAM to geospatial data, either programatically or interactively.\n\n\nNote that in addition to using segment-geospatial directly using Python in a notebook or other environment, you can also play with SAM-assisted segmentation in QGIS and ArcGIS.\nThings to try\n\nRun one or more examples\nAfter running automated segmentation, what do you think about the results?\nWhen doing interactive segmentation, how does it do, and what do you think about the results?",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-label-studio",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-label-studio",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.7 Adventure: Label Studio",
    "text": "5.7 Adventure: Label Studio\n\n\n\n\n\n\nPython app must be installed and run locally (unless you pay for an Enterprise cloud account)\nSee Quick start document with instructions for installing with pip and running locally in a web browser.\n\n\n\n\nMulti-type data labeling and annotation tool with standardized output format\nWorks on various data types (text, image, audio)\nHas both open source option and paid cloud service\nSee online playground",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#other-things-to-try",
    "href": "sections/hands-on-lab-data-annotation.html#other-things-to-try",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.8 Other things to try",
    "text": "5.8 Other things to try\n\nVGG Image Annotator (VIA)\n\nTry local installation?",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html",
    "href": "sections/the-building-blocks-of-nn-and-dl.html",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "",
    "text": "Overview\nWelcome to your first step into deep learning! This lession will help you understand deep learning in a simple and friendly way. Think of it as your first journey into an exciting new world of artificial intelligence.\nIn this session, we’ll explore:\nBy the end of this lesson, you’ll understand these key ideas. These building blocks will help you see how artificial intelligence works and how you can use these cool skills in real-life situations.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#overview",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#overview",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "",
    "text": "Data and models: Learn why data is super important and how models work like smart brains to make sense of information.\nLoss functions and optimization algorithms: Discover how computers learn by understanding the mistakes. We’ll explore how loss functions and optimization algorithms help computers get better and smarter.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#introduction",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#introduction",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nThink of deep learning as a tool to help us achieve goals and solve problems, similar to how you drive a car to get to your destination. Just as you start driving by learning only the basics (without diving into all the complex mechanics), your journey into deep learning begins with understanding its essential components.\n\n    \n        Balance Wheelie Viralhog GIF\n        from Balance Wheelie GIFs\n    \n\n\n\n6.1.1 What does this tool (deep learning) do?\n\nFinding a function automatically that maps given inputs to desired outputs 1.\n\nIn everyday terms, deep learning helps computers learn patterns from data to make predictions or decisions without being explicitly programmed with rules. It’s like teaching a child to recognize dogs by showing many dog pictures rather than listing all the details that define a dog.\nFor example:\n\n\n\nInputs\nOutputs\nFunctions\n\n\n\n\nA sentence or prompt\nText completion\nLLM (e.g., ChatGPT 2)\n\n\nA caption/description\nAn image\nDALL-E 3\n\n\nHistorical weather data\nWeather forecasting\nGraphCast 4\n\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nThink about a real problem you want to solve with deep learning. What information would you put into the system? What would you want to get out of it?\n\n\n\n\n6.1.2 Key questions and building blocks of deep learning\nTo find the right function using deep learning, we can break down the process into four key questions and building blocks:\n\nWhat are the inputs and outputs? This relates to the data we use.\nWhat functions can we possibly use? This relates to the models that define how inputs connect to outputs.\nHow do we evaluate the function? This is where loss functions help us.\nHow do we find the best possible function? This is done through optimization algorithms.\n\nThese questions and building blocks create the core of deep learning. Two more important parts — training and inference — help connect these building blocks and make the models work in real situations.\n\n\n\nDeep learning building blocks\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember, the building blocks of deep learning are closely connected. They affect each other, and there are always trade-offs to consider.\n\n\nLet’s begin our exploration into the building blocks of deep learning.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#data",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#data",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.2 Data",
    "text": "6.2 Data\nData is the start point of deep learning, forming the inputs and outputs that define the function we want to learn.\n\n\n\n\n\n\nNote\n\n\n\nPlease see the AI-ready Data section for discussions on data for Arctic research.\n\n\n\n6.2.1 Inputs\nBuilding a deep learning application starts with defining and preparing the data. Data is the foundation that helps models discover patterns and make predictions. Before you begin, think about these key questions:\n\nWhat type of data are you working with? (images, text, audio, etc.)\nHow much data do you have? Is it representative of real-world situations?\nAre there any special requirements for your task? (granularity, temporal consistency, spatial coverage, etc.)\nIs the data clean and usable? (missing values, unusual data points, background noise)\n\nHere are some key steps for preparing input data:\n\n\n\n\n\n\nData collection\n\n\n\nFind and gather data from reliable sources.\n\nUse public datasets or domain-specific repositories to save time and effort 5 6.\nInclude diverse data to cover different scenarios 7.\nCheck data quality by looking for problems like inconsistencies, duplicates, or noise 8.\n\n\n\n\n\n\n\n\n\nData preparation\n\n\n\nClean and prepare data for training the model.\n\nHandle missing or unusual data points by filling in values or removing them9 10 11 12.\nStandardize or normalize data to bring different features to a similar scale. This helps model learn faster 13.\n\n\n\n\n\n\n\n\n\nSplitting data\n\n\n\nDivide data into training, validation, and testing sets.\n\nEnsure each set represents real-world data distribution, avoiding data leakage 14.\nConsider the types of data you’re working with 15:\n\nImbalanced data: Use stratified sampling to keep the right mix of different classes 16 17.\nTemporal data: Split based on time.\nSpatial data: Split based on geographic areas 18 19.\n\n\n\n\n\n\n\n\n\n\nData augmentation\n\n\n\nApply transformations to the training data to increase the size and variety of your data.\n\nUse changes that make sense for your data type (e.g., rotate images, but don’t do this with text) 20 21.\nBe careful not to create unrealistic data 22.\nLook for special techniques for specific areas like medical imaging or satellite pictures 23.\n\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nHow well do you understand your data? Think about how it’s collected and its quality.\nEven famous datasets like ImageNet can have issues 24.\n\n\n\n\n6.2.2 Outputs\nDefining outputs is just as important as preparing inputs. Outputs show how the model makes predictions and must match your project’s goals. Consider:\n\nWhat type of output do you need? (labels, numbers, detailed results)\nHow should the outputs look? (probability list, single number, detailed information)\nAre there any special requirements for the outputs, e.g., a specific range?\n\nHere are some key steps for preparing output data:\n\n\n\n\n\n\nIdentify output type\n\n\n\nChoose the right type of output based on your specific problem.\n\nUse classification for tasks like sorting images or checking sentiment.\nUse regression to predict exact numbers, like sea ice concentration.\nUse structured outputs for complex tasks, like finding objects in an image 25.\n\n\n\n\n\n\n\n\n\nFormat outputs\n\n\n\nChoose a format that works with your model and loss functions.\n\nUse one-hot encoding for category-based tasks.\nNormalize continuous outputs to match the scale of the model’s inputs.\nConsider using embeddings for structured outputs to capture relationships between categories.\n\n\n\n\n\n\n\n\n\nData labeling\n\n\n\nAdd labels to your data to provide a reference for training.\nSee the Data annotation section for more details.\n\n\n\n\n\n\n\n\nNote\n\n\n\nBalance the detail of your outputs with the model’s complexity, available data, and computational resources. For example, predict sea ice concentration precisely (0-100%) or use broader categories like low (&lt;15%) and high (&gt;85%).\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nIn a classification problem, what’s the difference between one-hot encoding and label encoding?\n\n\n\n\n6.2.3 Quantity and quality\n\n\n\n\n\n\nQuantity: Does more data always mean better results?\n\n\n\n\n\n\nHoﬀmann et al., (2022)\n\n\nLarge datasets often improve performance, but they don’t guarantee success. Research shows that creating compute-optimal models means balancing data size, model complexity, and computational power 26.\n\nFor various model sizes, we choose the number of training tokens such that the final FLOPs is a constant. The cosine cycle length is set to match the target FLOP count. We find a clear valley in loss, meaning that for a given FLOP budget there is an optimal model to train\n\n\n\n\n\n\n\n\n\nQuality: What makes data high-quality?\n\n\n\nQuality is often more important than quantity. Common issues include:\n\nIncorrect or inconsistent labels\nNoise and irrelevant information\nPoorly filtered datasets\n\nResearch highlights the importance of data quality:\n\nRae et al. (2021) 27.\n\n\nOur data pipeline (Section A.1.1) includes text quality filtering, removal of repetitious text, deduplication of similar documents, and removal of documents with significant test-set overlap. We find that successive stages of this pipeline improve language model downstream performance (Section A.3.2), emphasising the importance of dataset quality.\n\n\nHoffmann et al., (2022) 28.\n\n\nNonetheless, large language models face several challenges, including their overwhelming computational requirements (the cost of training and inference increase with model size) (Rae et al., 2021; Thoppilan et al., 2022) and the need for acquiring more high-quality training data. In fact, in this work we find that larger, high quality datasets will play a key role in any further scaling of language models.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#models",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#models",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.3 Models",
    "text": "6.3 Models\nModels are the foundation of deep learning. They work as function sets that transform inputs into outputs.\n\n\n\n\n\n\nNote\n\n\n\nThe exact function is created by first choosing a model architecture and then getting a specific set of parameters by training the model on data.\n\n\n\n6.3.1 Layers\nDeep learning models are built from layers. A layer works like a step that processes data and sends it to the next layer. Different layer types have different jobs. Here are some examples:\n\n\n\n\n\n\nNote\n\n\n\nTo get started, don’t get overwhelmed by all the different layer types. Just get a sense of their basic purposes and focus on using complete models for practical tasks. You can always learn more about individual layers later.\n\n\n\n\n\n\n\n\nFully-connected (dense) layer\n\n\n\nA fully-connected layer connects every input to every output through learnable weights, allowing the network to combine features and make predictions.\nHow it works:\n\nEach input connects to every output.\nEach connection has a weight (a number that can be adjusted).\nFor each output:\n\nThe layer multiplies each input by its connection weight.\nIt adds all these multiplied values together.\n\n\nThink of it like a voting system: each input “votes” for different outputs with different strengths (weights).\n\nUses:\n\nAdjust the size of your data (dimensionality).\nCombine features to make decisions, e.g., identifying classes or predicting values 29.\n\nAdvantages:\n\nThey learn patterns across all features.\nThey are simple to add to your network.\n\nChallenges:\n\nThey need lots of computing power and memory due to the dense connections.\nThey don’t understand spatial relationships well (unlike layers designed for images or sequences).\n\n\n\n\n\n\n\n\nInteractive visualization of a fully-connected layer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional layer\n\n\n\nA convolutional layer applies filters to input data to detect patterns like edges, textures, and shapes, making it ideal for processing images and other spatial data 30.\nHow it works:\n\nSmall filters (also called kernels) slide across the input data.\nEach filter looks at a small area at a time.\nFor each position:\n\nThe layer multiplies each input value by the corresponding filter value.\nIt adds all these multiplied values together.\n\nStride controls how many positions the kernel moves each step.\nPadding adds zeros around the input to control output dimensions.\nOutput size = (Input size + 2 × Padding - Kernel size) / Stride + 1\n\nThink of it like a spotlight that moves across an image, highlighting specific patterns whenever they appear.\n\nUses:\n\nExtract features from spatial data (like images).\nDetect patterns regardless of where they appear in the input.\nReduce the data size while keeping important information.\n\nAdvantages:\n\nThey need fewer parameters than fully-connected layers.\nThey preserve spatial relationships in the data.\nThey can find the same pattern anywhere in the input.\n\nChallenges:\n\nThey may miss global patterns that span the entire input.\nSetting the right filter size and number requires careful design.\n\n\n\n\n\n\n\n\nInteractive visualization of a convolutional layer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPooling layer\n\n\n\nA pooling layer reduces the size of data by keeping only the most important information, making processing faster and helping the network focus on key features.\nHow it works:\n\nThe layer divides input data into small regions.\nFor each region, it keeps only one value (e.g., the maximum or average value).\nThe creates a smaller output with fewer details.\n\nThink of it like summarizing a detailed picture by keeping only the brightest point in each region.\n\nUses:\n\nReduce data size to save memory and computation.\nMake the network less sensitive to small input changes.\nFocus on the most important features.\n\nAdvantages:\n\nThey significantly reduce data size.\nThey make the network more resistant to small input changes.\nThey help extract key features regardless of exact position.\n\nChallenges:\n\nThey permanently lose some information.\nThey might discard details that are important for the task.\n\n\nTypes of pooling layers:\n\nMax pooling: Keeps the maximum value in a region.\nAverage pooling: Calculates the average value in a region.\nGlobal pooling: Averages information across the entire feature map, often used to reduce each feature map to a single value 31.\n\n\n\n\n\n\n\nInteractive visualization of a pooling layer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivation layer\n\n\n\nAn activation layer adds non-linearity to the network, allowing it to learn complex patterns that go beyond simple calculations.\nHow it works:\n\nThe layer takes each input value individually.\nIt applies a mathematical function (like ReLU, sigmoid, or tanh).\nThis transforms values in a non-linear way.\n\nThink of it like adding decision points in the network: “If the value is below 0, ignore it. If above, keep it” (for ReLU).\n\nUses:\n\nEnable to network to learn complex, non-linear relationships.\nControl the range of output values.\n\nAdvantages:\n\nThey allow networks to learn complicated patterns.\nThey control how information flows through the network.\nDifferent activations work well for different problems 32.\n\nChallenges:\n\nSome activations can cause training problems (like vanishing gradients) 33.\nChoosing the right activation requires understanding the problem.\n\n\nSome examples:\n\nReLU: \\(f(x) = \\max(0, x)\\).\nSigmoid: \\(f(x) = \\frac{1}{1 + e^{-x}}\\).\nTanh: \\(f(x) = \\frac{e^{2x} - 1}{e^{2x} + 1}\\).\n\n\n\n\n\n\n\nInteractive visualization of an activation layer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecurrent layer\n\n\n\nA recurrent layer processes sequences by maintaining a memory of previous inputs, making it suitable for text, speech, and time-series data.\nHow it works:\n\nIt maintains an internal state (memory) between processing steps.\nFor each item in a sequence:\n\nIt combines the current input with its internal state.\nIt updates its state based on this combination.\n\nThis allows information to persist across the sequence.\n\nThink of it like reading a book while keeping track of the story so far, using previous context to understand each new sentence.\n\nUses:\n\nProcess sequential data like text or time series.\nRemember information from earlier in a sequence.\nGenerate sequential outputs based on context.\n\nAdvantages:\n\nThey can capture dependencies across sequence elements.\nThey can process sequences of variable length.\nVariants like LSTM 34 35 and GRU 36 can remember information for long periods.\n\nChallenges:\n\nThey can be slow to train due to sequential processing.\nThey may suffer from vanishing or exploding gradients 37.\n\n\n\n\n\n\n\n\nVisualization of a LSTM cell\n\n\n\n\n\n\n    \n\n\n    Source: Reddit r/TheInsaneApp\n\nThe key components of an LSTM cell are:\n\nThree gates (shown as X symbols in circles) from left to right:\n\nForget gate: This decides what information to throw away or keep from memory.\nInput gate: This decides what new information to add to memory.\nOutput gate: This decides what information to share with the next cell.\n\nInputs:\n\nCurrent input \\(x_t\\)\nPrevious hidden state \\(h_{t-1}\\)\nPrevious cell state \\(C_{t-1}\\)\n\nOutputs:\n\nCurrent hidden state \\(h_t\\)\nCurrent cell state \\(C_t\\)\n\nThe blue box represents the sigmoid function, which outputs a value between 0 and 1. It controls how much information passes through each gate, like a filter that can be partially open or closed.\nThe purple box represents the tanh function, which outputs a value between -1 and 1. It scales the input values.\n\nThe LSTM cell works as follows:\n\n\\(h_{t-1}\\) and \\(x_t\\) are combined together and passed through sigmoid functions as gate control signals.\nThe forget gate determines how much of the previous cell state \\(C_{t-1}\\) (the previous memory) is passed to the next cell. Think of this like deciding which old memories to keep or discard.\nThe input gate determines how much of the current input \\(x_t\\) is added to the cell state. This is like deciding which new information is worth remembering.\nThe output gate determines how much of the current cell state \\(C_t\\) is passed to the next hidden state \\(h_t\\). This is like deciding which parts of your memory to actively think about right now.\n\n\n\n\n\n\n\n\n\n\n\n\nAttention layer\n\n\n\n\n\nAn attention layer helps a network focus on relevant parts of the input data, similar to how humans concentrate on important details rather than everything at once.\nHow it works:\n\nIt calculates how important each input element is for the current task.\nIt assigns attention weights to each element based on this importance.\nIt creates a weighted combination of the inputs according to these weights.\n\nThink of it like reading with a highlighter: marking and focusing on key phrases rather than every word equally.\n\nUses:\n\nFind relationships between different parts of the input.\nFocus on relevant information for a specific task.\nHandle long-range dependencies in sequences.\n\nAdvantages:\n\nThey significantly improve performance on complex tasks.\nThey create interpretable weightings that show what the network focuses on.\nThey enable processing of very long sequences effectively.\n\nChallenges:\n\nThey can be computationally expensive, especially for long sequences.\nSelf-attention specifically scales quadratically with sequence length.\nDesigning the right attention mechanism requires careful consideration.\n\n\n\n\n\n\n\n\nVisualization of image attention mechanism (conceptual flow)\n\n\n\n\n\nAttention mechanism is originally proposed for natural language processing tasks. The following visualization shows how it works for images. You may check how it works for text here: How LLMs work and Attention in transformers.\n\n\n\n\n\n\n\n\n\n\n6.3.2 Models\nDeep learning models are architectures composed of layers. Each model architecture has unique characteristics and is suited for particular tasks. Here are some examples:\n\n\n\n\n\n\nConvolutional Neural Networks (CNNs)\n\n\n\n\n\nBest for image-related tasks.\n\nConvolutional Neural Networks (CNNs) are designed to process and analyze images. They are characterized by:\n\nConvolutional Layers: Detecting patterns like edges, textures, and shapes.\nPooling Layers: Reducing spatial dimensions to prevent overfitting.\nFully-Connected Layers: Combining features for predictions.\n\nApplications of CNNs:\n\nImage classification, object detection, and segmentation.\nMedical imaging analysis.\nRemote sensing and satellite image processing.\n\n\n\n\n\n\n\n\n\n\n\nLong Short-Term Memory Networks (LSTMs)\n\n\n\n\n\nDesigned for sequential data like time series or text.\n\nLong Short-Term Memory Networks (LSTMs) are recurrent neural networks that maintain a hidden state to capture temporal dependencies. They are characterized by:\n\nMemory Cells: Capturing long-term dependencies in sequential data.\nGates: Regulating the flow of information to prevent vanishing gradients.\nHidden State: Maintaining a memory of past inputs to inform future predictions.\n\nApplications of LSTMs:\n\nTime series forecasting.\nNatural language processing tasks like language modeling and machine translation.\nSpeech recognition and synthesis.\n\n\n\n\n\n\n\n\n\n\n\nTransformers\n\n\n\n\n\nThe backbone of modern natural language processing and vision models.\n\nTransformers are models that process sequences of data using self-attention mechanisms. They are characterized by:\n\nSelf-Attention: Computing relationships between elements in the data.\nMulti-Head Attention: Capturing different types of relationships in the data.\nPositional Encoding: Incorporating positional information into the model.\n\nApplications of Transformers:\n\nNatural language processing tasks like machine translation, text generation, and sentiment analysis.\nImage analysis and computer vision tasks like object detection and image captioning.\n\n\n\n\n\n\n\n\n\n\n\nGraph Neural Networks (GNNs)\n\n\n\n\n\nDesigned for graph-structured data like social networks, molecular structures, and knowledge graphs.\n\nGraph Neural Networks (GNNs) are specialized models for processing graph-structured data. They are characterized by:\n\nGraph Convolutional Layers: Propagating information between nodes in the graph.\nNode Embeddings: Learning representations for nodes in the graph.\nGraph Pooling: Aggregating information from subgraphs.\n\nApplications of GNNs:\n\nSocial network analysis and link prediction.\nDrug discovery and molecular property prediction.\nKnowledge graph completion and recommendation systems.\n\n\n\n\n\n\n\n\n\n\n\nAutoencoders\n\n\n\n\n\nUsed for unsupervised learning and dimensionality reduction.\n\nAutoencoders are neural networks that learn to encode and decode data, enabling tasks like:\n\nDimensionality Reduction: Learning compact representations of data.\nAnomaly Detection: Identifying outliers or unusual patterns in the data.\nGenerative Modeling: Generating new data samples similar to the input.\n\nVariants of autoencoders include:\n\nVariational Autoencoders (VAEs): Learn probabilistic encodings for generative modeling.\nDenoising Autoencoders: Train on noisy data to learn robust representations.\nSparse Autoencoders: Encourage sparsity in the learned representations.\n\nApplications of autoencoders:\n\nImage denoising and reconstruction.\nAnomaly detection in cybersecurity and fraud detection.\nGenerative modeling for data augmentation and synthesis.\n\n\n\n\n\n\n\n6.3.3 Pre-trained models and transfer learning\n\n\n\n\n\n\nNote\n\n\n\nIn the realm of deep learning, building models from scratch can be both time-consuming and resource-intensive. Fortunately, pre-trained models and transfer learning offer a pratical solution to these challenges. They enables scientists to leverage existing models and achieve better performance with minimal efforts.\n\n\n\n\n\n\n\n\nPre-trained models\n\n\n\n\n\nPre-trained models are deep learning models that have been previously trained on extensive datasets. These models can serve as a solid foundation for solving similar tasks in different domains. By utilizing the knowledge captured in pre-trained models, you can achieve faster training times and often better performance.\n\n\n\n\n\n\n\n\n\nTransfer learning\n\n\n\n\n\nTransfer learning is a technique where a model developed for a particular task is reused as the starting point for a model on a second task. This approach is particularly beneficial when the second task has limited data. Instead of training a new model from scratch, you can adapt an existing model that has already learned useful features from a large dataset.\n\n\n\n\n\n\n\n\n\nBenefits of using pre-trained models\n\n\n\n\n\n\nFaster Training: Pre-trained models provide a head start by leveraging knowledge from previous tasks, reducing the time and resources needed for training.\nImproved Performance: Transfer learning allows you to benefit from the generalization capabilities of pre-trained models, often leading to better performance on new tasks.\nDomain Adaptation: Pre-trained models can be fine-tuned on domain-specific data to adapt to new environments or tasks.\n\n\n\n\n\n\n\n\n\n\nHow to implement transfer learning\n\n\n\n\n\n\nSelect a Pre-trained Model: Choose a pre-trained model that is well-suited for your task. It depends on the nature of the data and the target task.\nCustomize the Model: Adapt the pre-trained model to your specific task. The customization may occur at various stages:\n\nInput adaptation: Adjust the model to handle different types of input data. This might involve changing the input layer to accommodate data with more channels, such as multispectral images, or adapting it for temporal data like time series.\nOutput adaptation: Modify the output layers to match your task requirements. This could mean changing the number of output classes for classification tasks. You can also use the pre-trained model as a backbone and build additional task-specific modules on top of it, such as object detection heads for image segmentation tasks.\n\nFine-tune the Model: Train the adapted model on your dataset. You can choose to freeze some of the earlier layers to preserve the learned features, while tuning the later layers to adapt to your task.\n\n\n\n\n\n\n\n\n\n\nPractical applications\n\n\n\n\n\n\nImage Classification: Use pre-trained models like ResNet or Swin Transformer for classifying images into different categories.\nObject Detection: Utilize pre-trained models like Faster R-CNN, YOLO, or RetinaNet for object detection tasks.\nNatural Language Processing: Apply pre-trained models like BERT, GPT, or RoBERTa for text classification, sentiment analysis, or question answering.\n\n\n\n\n\n\n6.3.4 Model customization\n In deep learning, models can be thought of as consisting of three core components: input adaptation, a feature extractor, and output adaptation. Understanding and customizing these components is crucial for effectively applying pre-trained models to new tasks.\n\n\n\n\n\n\nFeature extractor\n\n\n\n\n\nThe feature extractor is the heart of the model, transforming data into informative representations that highlight essential patterns relevant to the task. Pre-trained models often excel in this role, as they have already learned rich feature sets from large datasets. By using a pre-trained model as a feature extractor, you can leverage existing knowledge and focus on adapting it to your specific needs.\n\n\n\n\n\n\n\n\n\nInput adaptation\n\n\n\n\n\nInput adaptation involves transforming your data into a format that the feature extractor can process. This might mean:\n\nAdjusting the input layer to accommodate different data types, such as adding channels for multispectral images or handling temporal sequences for time series data.\nPreprocessing data to match the scale or format expected by the pre-trained model.\n\n\n\n\n\n\n\n\n\n\nOutput adaptation\n\n\n\n\n\nOutput adaptation transforms the extracted features into usable outputs for your specific task. This often involves:\n\nModifying the output layer to match the number of classes in your classification task.\nAdding specialized layers, such as segmentation heads for image segmentation tasks, or regression layers for predicting continuous values.\n\n\n\n\n\n\n\n\n\n\nConsiderations for model customization\n\n\n\n\n\n\nTask Similarity: The extent of adaptation needed depends on how closely the pre-trained model’s original task aligns with your target task. More divergent tasks may require extensive customization and additional data for fine-tuning. Therefore, selecting a pre-trained model that closely resembles your task can simplify the adaptation process.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#loss-functions",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#loss-functions",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.4 Loss functions",
    "text": "6.4 Loss functions\nA loss function quantifies the difference between the predicted outputs and the actual target values, providing essential feedback for optimization.\nFor example, consider a classification task using a softmax output layer:\n\nPredicted Output: [0.6, 0.2, 0.2]\nTarget Output: [1, 0, 0]\n\nUsing the Cross-Entropy loss function, the loss value is calculated as:\n\\[\n\\text{Cross-Entropy Loss} = -\\sum_{i} y_i \\log(p_i) = -\\log(0.6) = 0.51\n\\]\nwhere \\(y_i\\) is the target output and \\(p_i\\) is the predicted output.\nMean Absolute Error (MAE) can also be used to evaluate the prediction:\n\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i} |y_i - p_i| = \\frac{|1 - 0.6| + |0 - 0.2| + |0 - 0.2|}{3} = \\frac{0.4 + 0.2 + 0.2}{3} = 0.27\n\\]\n\n\n\n\n\n\nQuick Thought\n\n\n\nIn the example above, both Cross-Entropy and MAE can evaluate the prediction’s accuracy. Consider these questions:\n\nHow do the values of the two loss functions change when predictions are closer to or further from the target?\nWhat is the impact of each loss function on the model training process?\n\n\n\n\n6.4.1 Types of Loss Functions\nSelecting the right loss function is essential for optimizing model performance across various tasks. Here are some common types of loss functions:\n\n\n\n\n\n\nTask-Specific Loss Functions\n\n\n\n\n\n\nRegression: Measure error for continuous outputs.\n\nMean Squared Error (MSE): Computes the average squared difference between predictions and targets.\nMean Absolute Error (MAE): Calculates the average absolute difference between predictions and targets.\nHuber Loss: Combines MSE and MAE, less sensitive to outliers than MSE.\n\nClassification: Evaluate probability distributions.\n\nCross-Entropy Loss: Measures the difference between predicted and target distributions, used with softmax outputs.\nBinary Cross-Entropy: Specifically for binary classification tasks.\nHinge Loss: Used for “maximum-margin” classification, mainly with support vector machines.\n\nSequence Prediction: Handle variable-length outputs.\n\nConnectionist Temporal Classification (CTC): Aligns input and output sequences, used in tasks like speech recognition.\nSequence-to-Sequence Loss: Often combines cross-entropy with attention mechanisms.\n\n\n\n\n\n\n\n\n\n\n\nPurpose-Specific Loss Functions\n\n\n\n\n\n\nImbalanced Data:\n\nFocal Loss: Mitigates class imbalance by focusing on hard-to-classify examples.\nWeighted Cross-Entropy: Assigns different weights to classes to balance their impact.\n\nMulti-Objective Tasks:\n\nMulti-Task Loss: Combines multiple loss functions with weighting factors to optimize for several objectives simultaneously.\n\nRobustness to Outliers:\n\nLog-Cosh Loss: Similar to MSE but less sensitive to outliers, using the hyperbolic cosine of prediction errors.\n\nImage Processing:\n\nDice Loss: Used for image segmentation tasks to measure overlap between predicted and target areas.\nIoU Loss (Intersection over Union): Measures the overlap between predicted and actual bounding boxes, often used in object detection.\n\n\n\n\n\n\n\n6.4.2 Training and validation loss\nTraining and validation loss are metrics used to evaluate and fine-tune the performance of machine learning models. They provide insights into how well a model is learning and can indicate potential issues like overfitting or underfitting.\n\nTraining Loss: This is the error calculated on the training dataset after each iteration. It reflects how well the model is learning the training data.\nValidation Loss: This is the error calculated on a separate validation dataset that the model has not seen during training. It provides an indication of how well the model generalizes to unseen data.\n\n\n\n\n\n\n\nInterpreting training and validation loss\n\n\n\n\n\nThe relationship between training and validation loss can reveal important information about the model’s performance:\n\nBoth losses decrease: If both training and validation losses decrease and stabilize at a low value, it suggests that the model is learning well and generalizing effectively to the validation set.\nTraining loss decreases, validation loss increases: This pattern indicates overfitting. The model is learning the training data too well, including its noise, and is not generalizing effectively to new data. Regularization techniques or a simpler model might be needed.\nBoth losses are high: If both losses remain high, it may indicate underfitting. The model is not complex enough to capture the underlying patterns in the data. Consider increasing model capacity or improving feature engineering.\nTraining loss stable, validation loss fluctuates: Fluctuating validation loss with stable training loss may suggest that the model is sensitive to the specific validation data. This could be due to a small validation set size or data noise.\n\n\n\n\n\n\n\n\n\n\nStrategies to manage training and validation loss\n\n\n\n\n\nTo address common issues related to training and validation loss, consider the following strategies:\n\nRegularization: Techniques like L1/L2 regularization, dropout, and early stopping can help mitigate overfitting.\nData Augmentation: Increasing the diversity of the training data can improve model generalization.\nCross-Validation: Using k-fold cross-validation provides a more reliable estimate of model performance on unseen data.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#optimization-algorithms",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#optimization-algorithms",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.5 Optimization Algorithms",
    "text": "6.5 Optimization Algorithms\nOptimization algorithms adjust model parameters to minimize the loss function, guiding the model towards better performance.\n\n6.5.1 Introduction to gradient descent\nGradient Descent is the foundational algorithm used in deep learning for optimization:\n\nObjective: The aim is to find the minimum of a function by iteratively adjusting parameters.\nGradient: Represents the direction of the steepest ascent. In optimization, we move in the opposite direction to find the minimum.\nLearning rate: A crucial hyperparameter that controls the size of the steps taken towards the minimum.\n\n\n\n6.5.2 Variants of gradient descent\nTo enhance the efficiency and performance of gradient descent, several variants have been developed:\n\n\n\n\n\n\nStochastic gradient descent (SGD)\n\n\n\n\n\nUpdate parameters using a single training example per iteration, leading to faster but noisier convergence.\n\n\n\n\n\n\n\n\n\nMini-batch gradient descent\n\n\n\n\n\nIt strikes a balance between batch and stochastic gradient descent by updating parameters using a small subset (mini-batch) of the training data, improving convergence stability and speed.\n\n\n\n\n\n\n\n\n\nMomentum\n\n\n\n\n\nThis method accelerates convergence by considering past gradients, helping the algorithm navigate through ravines and avoid oscillations.\n\n\n\n\n\n\n\n\n\nAdam (Adaptive moment estimation)\n\n\n\n\n\nAdam combines the benefits of momentum and RMSprop, adjusting learning rates for each parameter based on historical gradients, making it one of the most popular optimization methods.\n\n\n\n\n\n6.5.3 Key Hyperparameters\nOptimization algorithms rely on hyperparameters that need to be carefully tuned for optimal performance:\n\n\n\n\n\n\nLearning rate\n\n\n\n\n\nThe learning rate determines how quickly or slowly the model learns. It needs to be carefully selected to balance convergence speed and stability.\n\n\n\n\n\n\n\n\n\nBatch size\n\n\n\n\n\nThe batch size refers to the number of training samples used in one iteration. Smaller batch sizes can lead to faster convergence but noisier updates, while larger batch sizes provide smoother updates but require more computational resources.\n\n\n\n\n\n\n\n\n\nMomentum rate\n\n\n\n\n\nThe momentum rate determines the influence of past gradients on the current update, helping to smooth the optimization path.\n\n\n\n\n\n\n\n\n\nRegularization strength\n\n\n\n\n\nA factor used to prevent overfitting by penalizing complex models, ensuring simpler and more generalizable solutions.\n\n\n\n\n\n6.5.4 Learning rate scheduling\nAdjusting the learning rate over time can impact model performance:\n\n\n\n\n\n\nFixed scheduling\n\n\n\n\n\nMaintains a constant learning rate throughout training, simplifying the optimization process.\n\n\n\n\n\n\n\n\n\nStep decay\n\n\n\n\n\nReduces the learning rate at regular intervals, allowing the model to refine its parameters as it approaches convergence.\n\n\n\n\n\n\n\n\n\nExponential decay\n\n\n\n\n\nGradually decreases the learning rate exponentially, enabling fine-tuning of the model as training progresses.\n\n\n\n\n\n\n\n\n\nCyclical learning rates\n\n\n\n\n\nVary the learning rate cyclically, encouraging exploration of different regions of the loss landscape for potentially better minima.\n\n\n\n\n\n6.5.5 Adaptive learning rates\nThese methods automatically adjust the learning rate during training:\n\n\n\n\n\n\nAdam\n\n\n\n\n\nAdaptive moment estimation that combines momentum and RMSprop, providing an efficient and effective optimization approach.\n\n\n\n\n\n\n\n\n\nRMSprop (Root Mean Square Propagation)\n\n\n\n\n\nDivides the learning rate by a moving average of the squared gradients, adapting the learning rate for each parameter dynamically.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#training-and-inference",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#training-and-inference",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.6 Training and Inference",
    "text": "6.6 Training and Inference\nTraining and inference are the key processes that integrate the essential components for deep learning applications: data, models, loss functions, and optimization algorithms.\n\n6.6.1 Training\nTraining is the phase where the model learns from the data by optimizing its parameters to minimize the loss function.\n\n\n\n\n\n\nSteps\n\n\n\n\n\n\nData preparation: Gather and preprocess data into a suitable format for the model.\nForward pass: The model processes the input data to generate predictions.\nLoss calculation: The predictions are compared against the target outputs using a loss function to quantify the error.\nBackward pass: Compute gradients of the loss with respect to the model parameters using backpropagation.\nParameter ppdate: Utilize optimization algorithms (e.g., Gradient Descent, Adam) to update the model’s weights based on the computed gradients, iteratively improving the model’s performance.\n\n\n\n\n\n\n6.6.2 Inference\nInference is the phase where the trained model is used to make predictions on new, unseen data.\n\n\n\n\n\n\nSteps\n\n\n\n\n\n\nData preparation: Prepare new data in the same way as the training data for consistency.\nForward pass: The model processes the input data to generate predictions, leveraging the learned parameters.\nOutput generation: Convert raw model outputs into interpretable results, such as class labels or continuous values.\nPost-processing: Apply additional processing steps like thresholding for binary classification or filtering to refine results.\nResult interpretation: Analyze the model’s outputs to make informed decisions, often integrating domain-specific knowledge or business logic.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#footnotes",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#footnotes",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "",
    "text": "https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/ML%20basic%20(v8).pdf↩︎\nhttps://openai.com/chatgpt/overview/↩︎\nhttps://openai.com/index/dall-e/↩︎\nhttps://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/↩︎\nhttps://arcticdata.io/↩︎\nhttps://arctic.noaa.gov/data/↩︎\nhttps://www.snowflake.com/en/blog/five-steps-data-diversity-for-smarter-ai-models/↩︎\nhttps://www.markovml.com/blog/data-quality-validation↩︎\nhttps://www.geeksforgeeks.org/ml-handling-missing-values/↩︎\nhttps://www.mastersindatascience.org/learning/how-to-deal-with-missing-data/↩︎\nhttps://www.freecodecamp.org/news/how-to-detect-outliers-in-machine-learning/↩︎\nhttps://www.geeksforgeeks.org/detect-and-remove-the-outliers-using-python/↩︎\nhttps://developers.google.com/machine-learning/crash-course/numerical-data/normalization↩︎\nhttps://www.alooba.com/skills/concepts/deep-learning/data-splitting/↩︎\nhttps://datascience.stanford.edu/news/splitting-data-randomly-can-ruin-your-model↩︎\nhttps://en.wikipedia.org/wiki/Stratified_sampling↩︎\nhttps://www.baeldung.com/cs/ml-stratified-sampling↩︎\nhttps://www.geeksforgeeks.org/spatial-data-analysis-with-python/↩︎\nLi, W., Hsu, C. Y., Wang, S., & Kedron, P. (2024). GeoAI Reproducibility and Replicability: a computational and spatial perspective. Annals of the American Association of Geographers, 114(9), 2085-2103.↩︎\nhttps://anushsom.medium.com/image-augmentation-for-creating-datasets-using-pytorch-for-dummies-by-a-dummy-a7c2b08c5bcb↩︎\nhttps://www.datacamp.com/tutorial/complete-guide-data-augmentation↩︎\nhttps://ubiai.tools/what-are-the-advantages-anddisadvantages-of-data-augmentation-2023-update/↩︎\nRatner, A. J., Ehrenberg, H., Hussain, Z., Dunnmon, J., & Ré, C. (2017). Learning to compose domain-specific transformations for data augmentation. Advances in neural information processing systems, 30.↩︎\nBeyer, L., Hénaff, O. J., Kolesnikov, A., Zhai, X., & Oord, A. V. D. (2020). Are we done with imagenet?. arXiv preprint arXiv:2006.07159.↩︎\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-in-formats.html↩︎\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., … & Sifre, L. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.↩︎\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., … & Irving, G. (2021). Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.↩︎\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., … & Sifre, L. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.↩︎\nhttps://adamharley.com/nn_vis/mlp/3d.html↩︎\nhttps://cs231n.github.io/convolutional-networks/↩︎\nLin, M., Chen, Q., & Yan, S. (2013). Network in network. arXiv preprint arXiv:1312.4400.↩︎\nhttps://www.v7labs.com/blog/neural-networks-activation-functions↩︎\nhttps://www.geeksforgeeks.org/tanh-vs-sigmoid-vs-relu/↩︎\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.↩︎\nUnderstanding LSTM Networks↩︎\nCho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.↩︎\nhttps://www.superdatascience.com/blogs/recurrent-neural-networks-rnn-the-vanishing-gradient-problem↩︎",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html",
    "href": "sections/intro-to-pytorch.html",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "",
    "text": "Goal\nThis session introduces PyTorch, one of the most popular deep learning frameworks, known for its flexibility and ease of use. Participants will learn the basic operations in PyTorch, go through the building blocks of a deep learning model with PyTorch, and understand some common pitfalls and best practices in PyTorch.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#what-is-pytorch",
    "href": "sections/intro-to-pytorch.html#what-is-pytorch",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.1 What is PyTorch?",
    "text": "7.1 What is PyTorch?\nPyTorch is an open-source deep learning framework developed by Meta’s AI Research lab (FAIR) 1. It is designed to provide flexibility and efficiency in building and deploying machine learning models.\n\n\n\n\n\n\nA simple example of using PyTorch vs. Numpy to create a convolutional layer\n\n\n\n\n\nUsing Numpy:\nimport numpy as np\n\n# Input data\nX = np.random.randn(10, 3, 32, 32)\nW = np.random.randn(20, 3, 5, 5)\nb = np.random.randn(20)\n\n# Convolution operation\nout = np.zeros((10, 20, 28, 28))\nfor i in range(10):\n    for j in range(20):\n        for k in range(28):\n            for l in range(28):\n                out[i, j, k, l] = np.sum(X[i, :, k:k+5, l:l+5] * W[j]) + b[j]\nUsing PyTorch:\nimport torch\n\n# Input data\nX = torch.randn(10, 3, 32, 32)\n\n# Define a convolutional layer\nconv = torch.nn.Conv2d(in_channels=3, out_channels=20, kernel_size=5)\n\n# Convolution operation\nout = conv(X)\nThis example highlights how PyTorch simplifies deep learning model development. It provides a glimpse of the framework’s power and ease of use. We’ll explore more of PyTorch’s features in the following sections.\n\n\n\n\n7.1.1 Key features of PyTorch\nPyTorch offers several key features that make it a popular choice among deep learning practitioners:\n\n\n\n\n\n\nAutomatic differentiation\n\n\n\n\n\nPyTorch provides automatic differentiation, a key feature for training deep learning models. This feature allows users to compute gradients of tensors with respect to a loss function without explicitly defining the backward pass. This simplifies the process of implementing complex neural network architectures and training algorithms.\n\n\n\n\n\n\n\n\n\nStrong GPU acceleration\n\n\n\n\n\nPyTorch seamlessly integrates with GPUs, enabling accelerated computation for training and inference. This feature is essential for handling large-scale deep learning tasks and achieving faster training times.\n\n\n\n\n\n\n\n\n\nDynamic computation graphs\n\n\n\n\n\nPyTorch uses dynamic computation graphs, allowing for more flexibility and intuitive model building. Unlike frameworks with static computation graphs, PyTorch constructs the graph on-the-fly during runtime, making it easier to work with variable input sizes and complex data-dependent control flows.\n\n\n\n\n\n\n\n\n\nPythonic nature\n\n\n\n\n\nPyTorch’s API is designed to be intuitive and easy to use, closely resembling standard Python code. This makes it easy for Python programmers to learn and adopt PyTorch quickly, leading to faster prototyping and development.\n\n\n\n\n\n\n\n\n\nRich ecosystem\n\n\n\n\n\nPyTorch has a rich ecosystem with libraries and tools that support various machine learning and deep learning tasks. This includes TorchVision for computer vision, TorchText for natural language processing, and TorchAudio for audio processing. These libraries provide pre-built components and utilities for common deep learning tasks.\n\n\n\n\n\n\n\n\n\nPre-trained models and extensions\n\n\n\n\n\nPyTorch’s active community contributes pre-trained models, extensions, and utilities that enhance the framework’s capabilities. These resources help users leverage state-of-the-art models and accelerate their deep learning projects.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#basic-operations-in-pytorch",
    "href": "sections/intro-to-pytorch.html#basic-operations-in-pytorch",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.2 Basic operations in PyTorch",
    "text": "7.2 Basic operations in PyTorch\n\n7.2.1 Tensors\nTensors are the fundamental data structure in PyTorch, similar to arrays in Numpy. They represent multi-dimensional arrays with support for GPU acceleration and automatic differentiation. They are used to store and manipulate data in PyTorch.\n\n\n7.2.2 Basic tensor operations\n\n\n\n\n\n\nCreating tensors\n\n\n\n\n\nimport torch\n\n# Create a tensor from a list\ntensor_a = torch.tensor([1, 2, 3, 4, 5])\n\n# Create a tensor from a NumPy array\nimport numpy as np\narray = np.array([1, 2, 3, 4, 5])\ntensor_b = torch.tensor(array)\n\n# Create a tensor of zeros\ntensor_zeros = torch.zeros(3, 4)\n\n# Create a tensor of ones\ntensor_ones = torch.ones(2, 3)\n\n# Create a random tensor\ntensor_rand = torch.randn(3, 3)\n\n\n\n\n\n\n\n\n\nChecking tensor properties\n\n\n\n\n\n# Get the shape of a tensor\nprint(tensor_a.shape)\n\n# Get the data type of a tensor\nprint(tensor_a.dtype)\n\n# Get the device of a tensor\nprint(tensor_a.device)\n\n\n\n\n\n\nNote\n\n\n\n\nShape: The shape of a tensor represents its dimensions, such as the number of rows and columns.\nData type: The data type of a tensor indicates the type of values it stores, such as integers or floating-point numbers. Check available data types here.\nDevice: The device of a tensor indicates whether it is stored on the CPU or GPU.\n\n\n\n\n\n\n\n\n\n\n\n\nTensor operations\n\n\n\n\n\n# Element-wise addition\nresult = tensor_a + tensor_b\n\n# Element-wise multiplication\nresult = tensor_a * tensor_b\n\n# Matrix multiplication\nresult = torch.matmul(tensor_a, tensor_b)\n\n# Transpose a tensor\nresult = tensor_a.T\n\n# Reshape a tensor\nresult = tensor_a.reshape(2, 3)\n\n# Permute dimensions\nresult = tensor_a.permute(1, 0) # Swap dimensions 0 and 1\n\n# Concatenate tensors\nresult = torch.cat((tensor_a, tensor_b), dim=0)\n\n# Expand a tensor\nresult = tensor_a.unsqueeze(0) # Adds a dimension at index 0\n\n## Move a tensor to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntensor_a = tensor_a.to(device)\n\n\n\n\n\n\nWhat’s the difference?\n\n\n\n\ntorch.view() vs. torch.reshape()\ntorch.cat() vs. torch.stack()\ntorch.unsqueeze() vs. torch.squeeze()\ntorch.cuda.FloatTensor vs. torch.FloatTensor\n\"cpu\" vs. \"cuda\" vs. \"cuda:0\"\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou don’t need to memorize all these operations. You can always refer to the PyTorch documentation for a comprehensive list of tensor operations and functions.\n\n\n\n\n\nIn the next sections, we will explore the building blocks of a deep learning application in PyTorch.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#data-handling-in-pytorch",
    "href": "sections/intro-to-pytorch.html#data-handling-in-pytorch",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.3 Data handling in PyTorch",
    "text": "7.3 Data handling in PyTorch\nPyTorch provides several utilities for handling data, including datasets, data loaders, and transformations. These components help manage input data, preprocess it, and feed it into deep learning models efficiently.\n\n7.3.1 Datasets\nDatasets in PyTorch represent collections of data samples, typically stored in memory or on disk. They provide an interface to access individual data points and their corresponding labels.\nYou can create custom datasets by subclassing the torch.utils.data.Dataset class and implementing the __len__ and __getitem__ methods.\n\n__len__: Returns the size of the dataset.\n__getitem__: Returns a data sample and its label given an index.\n\nThis structure allows you to handle any type of data, such as images, text, or time series, in a uniform way.\n\n\n\n\n\n\nCreating a custom dataset\n\n\n\n\n\nimport torch\nfrom torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        x = self.data[index]\n        y = self.labels[index]\n\n        if self.transform:\n            x = self.transform(x)\n\n        return x, y\n\n    def some_custom_method(self):\n        # You can define custom methods for the dataset\n        pass\n\n# Create a custom dataset\ndata = [torch.randn(3, 32, 32) for _ in range(100)] # 100 random tensors\nlabels = torch.randint(0, 10, (100,)) # 100 random labels\ncustom_dataset = CustomDataset(data, labels)\n\n# Access a data sample\nsample, label = custom_dataset[0]\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPyTorch also provides built-in datasets like MNIST, CIFAR-10, and ImageNet, which can be easily loaded and used for training and evaluation. These datasets are available through the torchvision.datasets module. Check the official documentation for more details.\n\n\n\n\n7.3.2 Data loaders\nWhile a Dataset provides access to individual data samples, a DataLoader in PyTorch wraps a dataset and provides an iterable over it. It provides several features to facilitate efficient data loading and processing:\n\nBatching: Groups a set number of samples into a batch, which speeds up training by processing multiple samples in parallel.\nShuffling: Randomizes the order of samples in each epoch to prevent overfitting and improve generalization.\nParallel data loading: Uses multiple subprocesses to load data concurrently, reducing the time spent on data I/O operations.\n\nYou can create a DataLoader by passing a Dataset object and specifying batch size, shuffling, and other parameters.\n\n\n\n\n\n\nCreating a data loader\n\n\n\n\n\nfrom torch.utils.data import DataLoader\n\n# Create a DataLoader\ndata_loader = DataLoader(custom_dataset, batch_size=32, shuffle=True)\n\n# Iterate over the DataLoader\nfor batch in data_loader:\n    inputs, labels = batch\n    # Process the batch\n\n# Access a single batch\ninputs, labels = next(iter(data_loader))\n\n\n\n\n\n7.3.3 Transformations\nTransformations in PyTorch are operations applied to data samples during loading or preprocessing. They are commonly used to perform data augmentation, normalization, and other preprocessing steps before feeding the data into a model.\nYou can define custom transformations using the torchvision.transforms module or create a custom transformation class by subclassing torchvision.transforms.Transform.\n\n\n\n\n\n\nApplying transformations\n\n\n\n\n\nfrom torchvision import transforms\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Apply transformations to a dataset\ntransformed_dataset = CustomDataset(data, labels, transform=transform)",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#model-building-in-pytorch",
    "href": "sections/intro-to-pytorch.html#model-building-in-pytorch",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.4 Model building in PyTorch",
    "text": "7.4 Model building in PyTorch\nBuilding deep learning models in PyTorch is a straightforward process. The torch.nn module provides a wide range of neural network layers that can be easily combined to create complex architectures.\n\n7.4.1 Layers\nLayers in PyTorch are building blocks for constructing neural networks. They perform specific operations on input data. PyTorch provides a wide range of pre-defined layers, such as Linear and Conv2d. You can also create custom layers by subclassing torch.nn.Module and implementing the forward method.\n\n\n\n\n\n\nUsing pre-defined layers\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n# Input data\ninput_data = torch.randn(32, 10)\n\n# Define a fully connected (linear) layer\nlinear_layer = nn.Linear(10, 20)\n\n# Apply the layer to input data\noutput = linear_layer(input_data)\n\n\n\n\n\n\n\n\n\nCreating custom layers\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n# Define a custom layer\nclass CustomLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(CustomLayer, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.activation(x)\n        return x\n\n# Create an instance of the custom layer\ncustom_layer = CustomLayer(10, 20)\n\n# Apply the layer to input data\ninput_data = torch.randn(32, 10)\noutput = custom_layer(input_data)\n\n\n\n\n\n\nNote\n\n\n\nThe backward method is automatically defined by PyTorch’s autograd system, which computes the gradients of the loss with respect to the layer’s parameters during backpropagation. You don’t need to implement the backward pass manually when using PyTorch’s pre-defined layers or custom layers that use PyTorch operations.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a comprehensive list of pre-defined layers and modules available in PyTorch, refer to the official documentation.\n\n\n\n\n7.4.2 Models\nModels in PyTorch are neural network architectures composed of layers and modules. It is similar to defining custom layers. You can create models by subclassing torch.nn.Module and defining the network structure in the forward method. This method specifies how input data flows through the layers to produce the output.\n\n\n\n\n\n\nDefining a model\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n# Define a custom model\nclass CustomModel(nn.Module):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n        self.linear1 = nn.Linear(10, 20)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(20, 10)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        return x\n\n# Create an instance of the model\nmodel = CustomModel()\n\n# Apply the model to input data\ninput_data = torch.randn(32, 10)\noutput = model(input_data)\n\n\n\n\n\n\n\n\n\nDefining a model with another model as a layer\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n# Define a custom model\nclass CustomModel2(nn.Module):\n    def __init__(self):\n        super(CustomModel2, self).__init__()\n        self.model1 = CustomModel()\n        self.linear = nn.Linear(10, 5)\n\n    def forward(self, x):\n        x = self.model1(x)\n        x = self.linear(x)\n        return x\n\n# Create an instance of the model\nmodel2 = CustomModel2()\n\n# Apply the model to input data\ninput_data = torch.randn(32, 10)\noutput = model2(input_data)\n\n\n\nWhen defining models, you can nest layers and models within each other to create complex architectures. You can also use Sequential, ModuleList, and ModuleDict to organize layers and modules in a structured way. They\n\nSequential: A container that allows you to stack layers sequentially and apply them in order.\nModuleList: A list-like container that holds layers and modules, allowing for flexible indexing and iteration.\nModuleDict: A dictionary-like container that maps keys to layers and modules, enabling named access to individual components.\n\n\n\n\n\n\n\nNote\n\n\n\nModuleList and ModuleDict are like Python lists and dictionaries, respectively, but they are designed to work with PyTorch modules. They provide additional functionality for managing layers and modules within a model, e.g., parameter registration and device allocation.\n\n\n\n\n\n\n\n\nUsing Sequential to define a model\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n# Define a model using Sequential\nclass SequentialModel(nn.Module):\n    def __init__(self):\n        super(SequentialModel, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(10, 20),\n            nn.ReLU(),\n            nn.Linear(20, 10)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\n\n\n\n\n\n\n\nUsing ModuleList to define a model\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n# Define a model using ModuleList\nclass ModuleListModel(nn.Module):\n    def __init__(self):\n        super(ModuleListModel, self).__init__()\n        self.layers = nn.ModuleList([\n            nn.Linear(10, 20),\n            nn.ReLU(),\n            nn.Linear(20, 10)\n        ])\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\n\n\n\n\nNote\n\n\n\nModuleList is useful when the layers are not applied sequentially and require more complex control flow or operations.\n\n\n\n\n\n\n\n\n\n\n\nUsing ModuleDict to define a model\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n# Define a model using ModuleDict\nclass ModuleDictModel(nn.Module):\n    def __init__(self):\n        super(ModuleDictModel, self).__init__()\n        self.layers = nn.ModuleDict({\n            'linear1': nn.Linear(10, 20),\n            'relu': nn.ReLU(),\n            'linear2': nn.Linear(20, 10)\n        })\n\n    def forward(self, x):\n        x = self.layers['linear1'](x)\n        x = self.layers['relu'](x)\n        x = self.layers['linear2'](x)\n        return x\n\n\n\n\n\n\nNote\n\n\n\nModuleDict is useful when you want to access layers by name or key, providing a more structured and organized way to define models with named components.\n\n\n\n\n\n\n\n\n\n\n\nTry it out!\n\n\n\n\nDefine a custom model with Sequential, ModuleList, or ModuleDict. Initialize the model and use print(model) to inspect the differences in the model structure.\n\n\n\nYou can check the model’s architecture by printing the model object or using model.parameters(), model.named_parameters(), and model.children() to access the model’s parameters, named parameters, and child modules, respectively.\n\n\n\n\n\n\nInspecting model parameters\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n# Define a custom model\nclass CustomModel(nn.Module):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n        self.linear1 = nn.Linear(10, 20)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(20, 10)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        return x\n\n# Create an instance of the model\nmodel = CustomModel()\n\n# Print the model architecture\nprint(model)\n\n# Access model parameters\nfor name, param in model.named_parameters():\n    print(name, param.shape)\n\n# Access child modules\nfor child in model.children():\n    print(child)\n\n# Access model parameters\nfor param in model.parameters():\n    print(param.shape)\n\n\n\n\n\n7.4.3 Pre-trained models\nPyTorch provides a wide range of pre-trained models through the torchvision.models module. These models are trained on large-scale datasets like ImageNet and can be used for various tasks such as image classification, object detection, and segmentation.\n\n\n\n\n\n\nLoading a pre-trained model\n\n\n\n\n\nimport torch\nimport torchvision.models as models\n\n# Load a pre-trained ResNet model\nresnet = models.resnet18(pretrained=True)\n\n# Apply the model to input data\ninput_data = torch.randn(32, 3, 224, 224)\noutput = resnet(input_data)\n\n# Get the predicted class\n_, predicted_class = torch.max(output, 1)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe pretrained=True argument loads the pre-trained weights of the model, allowing you to use the model for inference or fine-tuning on your specific tasks. If it is set to False, the model will be initialized with random weights.\n\n\n\n\n7.4.4 Model customization\nYou can customize pre-trained models by modifying their architecture, freezing or fine-tuning specific layers, or replacing parts of the model with custom layers. This allows you to adapt pre-trained models to your specific tasks and datasets.\n\n\n\n\n\n\nCustomizing a pre-trained model\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# Load a pre-trained ResNet model\nresnet = models.resnet18(pretrained=True)\n\n# Modify the model architecture\nnum_classes = 10\nresnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n\n# Apply the modified model to input data\ninput_data = torch.randn(32, 3, 224, 224)\noutput = resnet(input_data)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen customizing pre-trained models, you need to ensure that the input dimensions and output dimensions of the modified model match your specific task requirements. You can inspect the model architecture using print(model) or model.parameters() to understand the structure of the model and its parameters.\n\n\n\n\n7.4.5 Freezing model parameters\nFreezing a model’s parameters means preventing them from being updated during training. It is achieved by setting the requires_grad attribute of the parameters to False. Freezing specific layers or parameters can be useful when you want to fine-tune only certain parts of a pre-trained model while keeping the rest fixed.\n\n\n\n\n\n\nFreezing model parameters\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# Load a pre-trained ResNet model\nresnet = models.resnet18(pretrained=True)\n\n# Freeze the model parameters\nfor param in resnet.parameters():\n    param.requires_grad = False\n\n# Modify the model architecture\nnum_classes = 10\nresnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n\n# Apply the modified model to input data\ninput_data = torch.randn(32, 3, 224, 224)\noutput = resnet(input_data)",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#loss-functions-in-pytorch",
    "href": "sections/intro-to-pytorch.html#loss-functions-in-pytorch",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.5 Loss functions in PyTorch",
    "text": "7.5 Loss functions in PyTorch\nPyTorch provides a wide range of loss functions through the torch.nn module. These functions cover various tasks such as classification, regression, and generative modeling. You can choose the appropriate loss function based on the nature of your task and the type of output your model produces.\n\n\n\n\n\n\nCommon loss functions\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n# Define predicted outputs and ground truth labels\noutputs = torch.randn(32, 10) # Random predictions\nlabels = torch.randint(0, 10, (32,)) # Random labels\n\n# Cross-entropy loss for classification\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(outputs, labels)\n\n# Mean squared error loss for regression\ncriterion = nn.MSELoss()\nloss = criterion(outputs, labels.float())\n\n# Binary cross-entropy loss for binary classification\ncriterion = nn.BCELoss()\nloss = criterion(torch.sigmoid(outputs), labels.float())\n\n\n\nYou can also create custom loss functions.\n\n\n\n\n\n\nCreating a custom loss function\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n# Define a custom loss function\ndef custom_loss(outputs, labels):\n    loss = torch.mean((outputs - labels) ** 2)\n    return loss\n\n# Use the custom loss function\noutputs = torch.randn(32, 10) # Random predictions\nlabels = torch.randn(32, 10) # Random labels\nloss = custom_loss(outputs, labels)",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#optimizers-in-pytorch",
    "href": "sections/intro-to-pytorch.html#optimizers-in-pytorch",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.6 Optimizers in PyTorch",
    "text": "7.6 Optimizers in PyTorch\nOptimizers in PyTorch are used to update the parameters of a model during training. PyTorch provides a wide range of optimizers through the torch.optim module, such as SGD, Adam, RMSprop, and more.\nUsing an optimizer involves several steps:\n\nzero_grad(): Clear the gradients of the model parameters from the previous iteration (otherwise, gradients accumulate).\nEach parameter in the model has a grad attribute that stores the gradient of the loss with respect to that parameter.If you don’t set the gradients to zero before backpropagation, the gradients will accumulate across iterations.\nForward pass: Compute the output of the model given the input data. Refer to Models building in PyTorch for more details.\nCompute loss: Calculate the loss between the predicted output and the ground truth labels. Refer to Loss functions in PyTorch for more details.\nbackward(): Compute the gradients of the loss with respect to the model parameters using backpropagation.\nThis step performs backpropagation, a process in which the loss is differentiated with respect to each parameter in the model to compute the gradients. The gradients indicate how much each parameter should be adjusted to minimize the loss.\nstep(): Update the model parameters using the computed gradients and the optimizer’s update rule.\nThe optimizer uses the gradients to update the model parameters based on the chosen optimization algorithm (e.g., SGD, Adam, RMSprop). This step is where the actual parameter updates occur.\n\n\n\n\n\n\n\nUsing an optimizer\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define a model\nmodel = nn.Linear(10, 1)\n\n# Define an optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Update the model parameters\noptimizer.zero_grad() \noutputs = model(torch.randn(32, 10))\nloss = torch.mean(outputs)\nloss.backward()\noptimizer.step()",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#training-a-model-in-pytorch",
    "href": "sections/intro-to-pytorch.html#training-a-model-in-pytorch",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.7 Training a model in PyTorch",
    "text": "7.7 Training a model in PyTorch\nTraining a deep learning model in PyTorch involves combining the building blocks we’ve discussed so far: data handling, model building, loss computation, and optimization. The basic training loop consists of the following steps:\n\nData loading: Load the training data using a DataLoader and iterate over the batches.\nForward pass: Pass the input data through the model to compute the predicted output.\nCompute loss: Calculate the loss between the predicted output and the ground truth labels.\nBackward pass: Compute the gradients of the loss with respect to the model parameters using backpropagation.\nUpdate model parameters: Update the model parameters using the computed gradients and the optimizer’s update rule.\n\nAfter training the model for a specified number of epochs, you can save the model’s parameters with torch.save() and use the model for inference on new data.\n\n\n\n\n\n\nBasic training loop\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Load the training data\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Load a pre-trained model\nmodel = models.resnet18(pretrained=True)\n\n# Define a loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\n\n# Use the model's parameters that require gradients\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Train the model\nnum_epochs = 10 # Number of training epochs. One epoch is a complete pass through the training data.\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train() # Set the model to training mode\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n\n# Save the trained model\ntorch.save(model.state_dict(), 'model.pth')\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlways remember to set the model to training mode using model.train() before training. This ensures that layers like dropout and batch normalization behave correctly during training.\nYou can customize the training loop by adding additional components such as loss recording, evaluation, learning rate scheduling, early stopping, and model checkpointing to monitor and improve the training process.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#evaluation-of-a-model-in-pytorch",
    "href": "sections/intro-to-pytorch.html#evaluation-of-a-model-in-pytorch",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.8 Evaluation of a model in PyTorch",
    "text": "7.8 Evaluation of a model in PyTorch\nEvaluating a deep learning model in PyTorch involves running the model on a validation or test dataset and computing metrics to assess its performance. The evaluation process is similar to the training process but without the gradient computation and parameter updates.\nThe basic evaluation loop consists of the following steps:\n\nData loading: Load the validation or test data using a DataLoader and iterate over the batches.\nForward pass: Pass the input data through the model to compute the predicted output.\nCompute metrics: Calculate evaluation metrics such as accuracy, precision, recall, or F1 score based on the predicted output and ground truth labels.\n\n\n\n\n\n\n\nBasic evaluation loop\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\n# Load the validation data\nval_loader = DataLoader(val_dataset, batch_size=32)\n\n# Load the trained model\nmodel = models.resnet18(pretrained=True)\nmodel.load_state_dict(torch.load('model.pth'))\n\n# Define a loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Evaluate the model\nmodel.eval() # Set the model to evaluation mode\ntotal_loss = 0.0\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for inputs, labels in val_loader:\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        total_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\naccuracy = correct / total\naverage_loss = total_loss / len(val_loader)\n\nprint(f'Validation Loss: {average_loss}, Accuracy: {accuracy}')\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlways remember to set the model to evaluation mode using model.eval() before evaluation. This ensures that layers like dropout and batch normalization behave correctly during evaluation.\ntorch.no_grad() is used to disable gradient computation during evaluation, reducing memory consumption and speeding up the evaluation process.\nYou can integrate the evaluation step into the training loop to monitor the model’s performance during training and make decisions based on the evaluation metrics.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#common-pitfalls-and-best-practices",
    "href": "sections/intro-to-pytorch.html#common-pitfalls-and-best-practices",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.9 Common pitfalls and best practices",
    "text": "7.9 Common pitfalls and best practices\nWhile working with PyTorch, you may encounter common errors that can be challenging to debug. Here are some common pitfalls and best practices to help you avoid these errors:\n\nIncorrect tensor shapes: Ensure that the input data and model parameters have compatible shapes. Mismatched tensor shapes can lead to errors during forward and backward passes.\nMissing .to(device): If you’re using a GPU, make sure to move tensors and models to the appropriate device (CPU or GPU) using .to(device). Forgetting this step can result in runtime errors.\nData on different devices: Ensure that all data (inputs, labels, and model parameters) are on the same device (CPU or GPU) to avoid compatibility issues.\nMismatched data types: Some operations require specific data types (e.g., float or integer). Make sure that the data types of tensors are compatible with the operations you’re performing.\nCuda out of memory: When working with large models or datasets on a GPU, you may encounter out-of-memory errors. Reduce the batch size, use gradient accumulation, freeze unnecessary layers, or use a smaller model to address this issue.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#footnotes",
    "href": "sections/intro-to-pytorch.html#footnotes",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "",
    "text": "https://pytorch.org↩︎",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html",
    "href": "sections/hands-on-lab-pytorch.html",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "",
    "text": "Overview\nThis hands-on lab session offers participants practical experience with PyTorch for building, training, and evaluating neural network models. Participants will work with a sample dataset, load a pre-trained model, and fine-tune it to enhance performance. The session will guide participants through the building blocks of a deep learning application, including data, models, loss functions, optimization algorithms, training and evaluation, inference and visualization.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#overview",
    "href": "sections/hands-on-lab-pytorch.html#overview",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "",
    "text": "Note\n\n\n\nYou can access the Jupyter notebook for this hands-on lab on Google Colab.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#data",
    "href": "sections/hands-on-lab-pytorch.html#data",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.1 Data",
    "text": "8.1 Data\n\n8.1.1 Dataset overview\nIn this hands-on lab, we will use a sample RTS (Retrogressive Thaw Slumps) dataset from Dr. Yili Yang’s research. While the RTS dataset was originally used for semantic segmentation, we will repurpose it for a classification task. The goal is to classify the number of RTS present in each image, with counts ranging from 1 to 10, which will serve as the ground truth for our model.\nThe dataset structure and required files for this hands-on lab are as follows:\ncyber2a\n│--- rts\n│    │--- images  # Folder containing RGB images\n│    │    │--- train_nitze_000.jpg\n│    │    │--- train_nitze_001.jpg\n│    │    │--- ...\n│--- data_split.json\n│--- rts_cls.json\n\ndata_split.json: A dictionary with two keys: train and valtest:\n\ntrain: A list of image filenames for training.\nvaltest: A list of image filenames for validation and testing.\n\nrts_cls.json: A dictionary with image filenames as keys and the number of RTS in each image as values.\n\n\n\n8.1.2 Download the dataset\nTo download the dataset, run the following commands:\n\n\n\n\n\n\nDownload the dataset\n\n\n\n\n\n!wget --content-disposition https://www.dropbox.com/scl/fi/1pz52tq3puomi0185ccyq/cyber2a.zip?rlkey=3dgf4gfrj9yk1k4p2znn9grso&st=bapbt1bq&dl=0\n\n!unzip -o cyber2a.zip\n\n\n\n\n\n8.1.3 Visualize the dataset\nLet’s visualize the dataset by displaying one image and its corresponding label:\n\n\n\n\n\n\nVisualize the dataset\n\n\n\n\n\nimport os\nimport json \n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Define the directory where images are stored\nimg_dir = \"cyber2a/rts/images/\"\n\n# Load the data split file to get lists of training and validation/test images\nwith open(\"cyber2a/data_split.json\", 'r') as f:\n    data_split = json.load(f)\n\n# Retrieve the list of training images\nimg_list = data_split[\"train\"]\n\n# Load the image labels, where each image name maps to the number of RTS in the image\nwith open(\"cyber2a/rts_cls.json\", 'r') as f:\n    img_labels = json.load(f)\n\n# Select the first image file name from the training list and get its corresponding label\nimg_name = img_list[0]\nimg_label = img_labels[img_name]\n\n# Print the image file name and its corresponding number of RTS\nprint(f\"Image Name: {img_name}, Number of RTS: {img_label}\")\n\n# Construct the full path to the image file\nimg_path = os.path.join(img_dir, img_name)\n\n# Open the image and convert it to RGB format\nimage = Image.open(img_path).convert(\"RGB\")\n\n# Convert the label to 0-indexed format for classification tasks\nlabel = int(img_label) - 1\n\n# Display the image using matplotlib\nfig, ax = plt.subplots()\nax.imshow(image)\nax.axis('off')  # Hide the axis\nax.set_title(f\"Label: {label}\")  # Set the title to the 0-indexed label\n\n# Show the plot\nplt.show()\n\n\n\n\n\n8.1.4 Build a custom dataset\nTo build a custom dataset, we will create a PyTorch dataset class that loads the images and their corresponding labels. The dataset class will implement the following methods:\n\n__init__: Initialize the dataset by loading the image filenames and labels.\n__len__: Return the total number of images in the dataset.\n__getitem__: Load an image and its corresponding label based on the index.\n\n\n\n\n\n\n\nBuild a custom dataset\n\n\n\n\n\nfrom torch.utils.data import Dataset\n\nclass RTSDataset(Dataset):\n    def __init__(self, split, transform=None):\n        \"\"\"\n        Args:\n            split (str): One of 'train' or 'valtest' to specify the dataset split.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        # Define the directory where images are stored\n        self.img_dir = \"cyber2a/rts/images/\"\n        \n        # Load the list of images based on the split (train/valtest)\n        with open(\"cyber2a/data_split.json\") as f:\n            data_split = json.load(f)\n            \n        if split == 'train':\n            self.img_list = data_split['train']\n        elif split == 'valtest':\n            self.img_list = data_split['valtest']\n        else:\n            raise ValueError(\"Invalid split: choose either 'train' or 'valtest'\")\n    \n        # Load the image labels\n        with open(\"cyber2a/rts_cls.json\") as f:\n            self.img_labels = json.load(f)\n\n        # Store the transform to be applied to images\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"Return the total number of images.\"\"\"\n        return len(self.img_list)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): Index of the image to retrieve.\n        \n        Returns:\n            tuple: (image, label) where image is the image tensor and label is the corresponding label.\n        \"\"\"\n        # Retrieve the image name using the index\n        img_name = self.img_list[idx]\n      \n        # Construct the full path to the image file\n        img_path = os.path.join(self.img_dir, img_name)\n        \n        # Open the image and convert it to RGB format\n        image = Image.open(img_path).convert('RGB')\n        \n        # Get the corresponding label and adjust it to be 0-indexed\n        label = self.img_labels[img_name] - 1\n\n        # apply transform if specified\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n\n\n\n\n8.1.5 Test the custom dataset\nTo test the custom dataset, we will create an instance of the RTSDataset class for the training split and display the first image and its corresponding label:\n\n\n\n\n\n\nTest the custom dataset\n\n\n\n\n\ndef display_sample_images(dataset, num_images=3):\n    \"\"\"\n    Display sample images from the dataset.\n\n    Args:\n        dataset (Dataset): The dataset to sample images from.\n        num_images (int): Number of images to display.\n        save_path (str): Path to save the displayed images.\n    \"\"\"\n    data, label = dataset[0]\n    if type(data) is dict:\n        num_modalities = len(data)\n        fig, axs = plt.subplots(num_modalities, num_images, figsize=(20, 5))\n        for i in range(num_images):\n            data, label = dataset[i]\n            for j, modality in enumerate(data):\n                axs[j, i].imshow(data[modality])\n                if j == 0:\n                    axs[j, i].set_title(f\"label: {label}\")\n                else:\n                    axs[j, i].set_title(f\"modality: {modality}\")\n                axs[j, i].axis(\"off\")\n\n    else:\n        fig, axs = plt.subplots(1, num_images, figsize=(20, 5))\n        for i in range(num_images):\n            data, label = dataset[i]\n            axs[i].imshow(data)\n            axs[i].set_title(f\"Label: {label}\")\n            axs[i].axis(\"off\")\n\n    plt.show()\n\n# Create the training dataset\ntrain_dataset = RTSDataset(\"train\")\n\n# Display and save sample images from the training dataset\ndisplay_sample_images(train_dataset)\n\n\n\n\n\n8.1.6 Define data transforms and data loaders\nTo prepare the data for training, we will define data transforms to normalize the images and convert them to PyTorch tensors. We will also create data loaders to load the data in batches during training and validation.\n\n\n\n\n\n\nDefine data transforms and data loaders\n\n\n\n\n\nimport torch\nimport torchvision.transforms as T\n\n# Define the transform for the dataset\ntransform = T.Compose([\n    T.Resize((256, 256)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create the training and validation datasets with transforms\ntrain_dataset = RTSDataset(\"train\", transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n\nval_dataset = RTSDataset(\"valtest\", transform=transform)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False)",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#models",
    "href": "sections/hands-on-lab-pytorch.html#models",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.2 Models",
    "text": "8.2 Models\n\n8.2.1 Load a pre-trained model\nIn this hands-on lab, we will use a pre-trained ResNet-18 model as the backbone for our classification task. We will load the pre-trained ResNet-18 model from the torchvision library and modify the final fully connected layer to output 10 classes corresponding to the number of RTS in the images.\n\n\n\n\n\n\nLoad a pre-trained model and modify the final layer\n\n\n\n\n\nfrom torchvision import models # https://pytorch.org/vision/stable/models.html\nfrom torchvision.models.resnet import ResNet18_Weights\n\n# Load the pretrained ResNet18 model\nmodel = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n\n# Modify the final layer to match the number of classes\nnum_ftrs = model.fc.in_features\nmodel.fc = torch.nn.Linear(num_ftrs, 10)\n\n# print the model to observe the new `fc` layer\nprint(model)",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#loss-functions",
    "href": "sections/hands-on-lab-pytorch.html#loss-functions",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.3 Loss functions",
    "text": "8.3 Loss functions\nFor the classification task, we will use the cross-entropy loss function, which is commonly used for multi-class classification problems.\n\n\n\n\n\n\nDefine the loss function\n\n\n\n\n\nimport torch\n\n# Define the loss function\ncriterion = torch.nn.CrossEntropyLoss()",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#optimization-algorithms",
    "href": "sections/hands-on-lab-pytorch.html#optimization-algorithms",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.4 Optimization algorithms",
    "text": "8.4 Optimization algorithms\nWe will use the SGD (Stochastic Gradient Descent) optimizer to train the model.\n\n\n\n\n\n\nDefine the optimizer\n\n\n\n\n\nimport torch.optim as optim\n\n# Define the optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#training-and-evaluation",
    "href": "sections/hands-on-lab-pytorch.html#training-and-evaluation",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.5 Training and evaluation",
    "text": "8.5 Training and evaluation\n\n8.5.1 Define the training and evaluation functions\nThe training function will iterate over the training dataset, compute the loss, backpropagate the gradients, and update the model parameters. The evaluation function will iterate over the validation dataset and compute the accuracy of the model.\n\n\n\n\n\n\nDefine the training and evaluation functions\n\n\n\n\n\nimport torch\nfrom tqdm import tqdm\n\ndef train(model, criterion, optimizer, train_loader, val_loader, num_epochs=5):\n    \"\"\"\n    Train the model.\n    \n    Args:\n        model: The model to train.\n        criterion: The loss function.\n        optimizer: The optimizer.\n        train_loader: DataLoader for the training data.\n        val_loader: DataLoader for the validation data.\n        num_epochs (int): Number of epochs to train.\n    \n    Returns:\n        model: The trained model.\n    \"\"\"\n    for epoch in range(num_epochs):\n        # Set model to training mode\n        model.train()\n        running_loss = 0.0\n        for i, data in enumerate(tqdm(train_loader)):\n            inputs, labels = data\n            \n            # get model's device\n            device = next(model.parameters()).device\n            \n            # Move data to the appropriate device\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass to get model outputs\n            outputs = model(inputs)\n\n            # Compute the loss\n            loss = criterion(outputs, labels)\n            # Backward pass to compute gradients\n            loss.backward()\n            # Update model parameters\n            optimizer.step()\n\n            # Accumulate the running loss\n            running_loss += loss.item()\n        \n        epoch_loss = running_loss / len(train_loader)\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")\n        \n        # Validation phase\n        # set the model to validation mode\n        model.eval()\n        correct = 0\n        total = 0\n        # Disable gradient computation for validation\n        with torch.no_grad():\n            for data in val_loader:\n                images, labels = data\n                # Move validation data to the appropriate device\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                # Get the predicted class\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        print(f\"Validation accuracy: {100 * correct / total}%\")\n\n    return model\n\n\n\n\n\n8.5.2 Train the model\nLet’s train the model using the training and evaluation functions defined above.\n\n\n\n\n\n\nTrain the model\n\n\n\n\n\n# move model to gpu is available \nif torch.cuda.is_available():\n    model = model.to('cuda')\n\nmodel = train(model, criterion, optimizer, train_loader, val_loader, num_epochs=5)",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#inference",
    "href": "sections/hands-on-lab-pytorch.html#inference",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.6 Inference",
    "text": "8.6 Inference\nTo perform inference on new images, we will define a function that takes an image as input, preprocesses it, and passes it through the model to get the predicted class.\n\n\n\n\n\n\nInference\n\n\n\n\n\ndef predict_image(model, image_path):\n    \"\"\"\n    Predict the class of a sample image.\n    \n    Args:\n        model: The trained model.\n        image_path (str): Path to the image to predict.\n    \n    Returns:\n        int: Predicted class label.\n    \"\"\"\n    transform = T.Compose([\n        T.Resize((256, 256)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    image = Image.open(image_path).convert(\"RGB\")\n    # Apply the transformations and add a batch dimension\n    image = transform(image).unsqueeze(0)\n    device = next(model.parameters()).device\n    image = image.to(device)\n\n    model.eval() # Set the model to evaluation mode\n    with torch.no_grad():\n        outputs = model(image)\n        _, predicted = torch.max(outputs, 1)\n        return predicted.item()\n\nimg_name = \"valtest_yg_070.jpg\"\nimg_dir = \"./cyber2a/rts/images\"\nimg_path = os.path.join(img_dir, img_name)\n\npredicted_class = predict_image(model, img_path)\n\nprint(f\"Predicted class for {img_name}: {predicted_class}\")",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#visualization",
    "href": "sections/hands-on-lab-pytorch.html#visualization",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.7 Visualization",
    "text": "8.7 Visualization\nTo visualize the model’s predictions, we will display a sample image from the validation set along with the predicted class.\n\n\n\n\n\n\nVisualization\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\ndef display_image_with_annotations(image_name, image_folder):\n    \"\"\"\n    Display an image with its annotations.\n\n    Parameters:\n    - image_name: str, the name of the image file to display.\n    - image_folder: str, the folder where the images are stored.\n\n    Returns:\n    - cv2_image: The annotated image.\n    \"\"\"\n    # Load the COCO annotations from the JSON file\n    with open(\"cyber2a/rts_coco.json\", \"r\") as f:\n        rts_coco = json.load(f)\n\n    # Get the image ID for the image\n    image_id = None\n    for image in rts_coco[\"images\"]:\n        if image[\"file_name\"] == image_name:\n            image_id = image[\"id\"]\n            break\n\n    if image_id is None:\n        raise ValueError(f\"Image {image_name} not found in COCO JSON file.\")\n\n    # Get the annotations for the image\n    annotations = []\n    for annotation in rts_coco[\"annotations\"]:\n        if annotation[\"image_id\"] == image_id:\n            annotations.append(annotation[\"segmentation\"])\n\n    # Read the image\n    cv2_image = cv2.imread(f\"{image_folder}/{image_name}\")\n    if cv2_image is None:\n        raise FileNotFoundError(\n            f\"Image file {image_name} not found in folder {image_folder}.\"\n        )\n\n    # Overlay the polygons on top of the image\n    for annotation in annotations:\n        for polygon in annotation:\n            # Reshape polygon to an appropriate format for cv2.polylines\n            polygon = np.array(polygon, dtype=np.int32).reshape((-1, 2))\n            cv2.polylines(\n                cv2_image, [polygon], isClosed=True, color=(0, 255, 0), thickness=2\n            )\n\n    cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n\n    return cv2_image\n\n\nimage = display_image_with_annotations(img_name, img_dir)\n\nfig, ax = plt.subplots()\nax.imshow(image)\nax.set_title(f'number of predicted RTS: {predicted_class + 1}')\nplt.show()",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/permafrost-discovery-gateway.html",
    "href": "sections/permafrost-discovery-gateway.html",
    "title": "9  Permafrost Discovery Gateway",
    "section": "",
    "text": "Overview\nWe have more satellite imagery data than what we know what to do with. There are many people with different knowledges and common passions for permafrost-affected landscapes. At the same time, Alaska and the Arctic at large are starving for basic geospatial information about it’s permafrost-affected landscape and infrastructure and people may have a hard time finding others to help make visions come true. The Permafrost Discovery Gateway (PDG) is an online free tool meant to empower a) researchers to produce and do science with big geospatial data (think sub-meter resolution maps across Alaska and the entire Arctic) and b) agencies and community leaders in land and infrastructure management that involves permafrost. PDG currently offers easy visual exploration via a regular web-browser of datasets that otherwise crush traditional GIS software due to the file size. In the works are, for example, new big datasets of permafrost thaw features and infrastructure, partial dataset download tool, the incorporation of statistical summaries and AI tools that help the user find interesting stories in the big data products, plug-and-play workflows to help you develop your own big dataset, and the monitoring of permafrost thaw near-real time. The PDG can become a gateway where data and people can connect, where technology enables anyone with an internet connection, no matter your technical skills and resources, to connect, explore, and together create.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Permafrost Discovery Gateway</span>"
    ]
  },
  {
    "objectID": "sections/permafrost-discovery-gateway.html#slides",
    "href": "sections/permafrost-discovery-gateway.html#slides",
    "title": "9  Permafrost Discovery Gateway",
    "section": "Slides",
    "text": "Slides\nDownload Slides",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Permafrost Discovery Gateway</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html",
    "href": "sections/ai-ethics.html",
    "title": "10  AI Ethics",
    "section": "",
    "text": "Goal\nReview FAIR and CARE Principles, and their relevance to data ethics. Examine how ethical considerations are shared and considered at the Arctic Data Center. Discuss ethical considerations in AI and machine learning.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#intro-to-data-ethics",
    "href": "sections/ai-ethics.html#intro-to-data-ethics",
    "title": "10  AI Ethics",
    "section": "10.1 Intro to Data Ethics",
    "text": "10.1 Intro to Data Ethics\nThe Arctic Data Center is an openly-accessible data repository. The data published through it is open for anyone to reuse, subject to one of two license: CC-0 Public Domain and CC-By Attribution 4.0. As an open access repository, we prioritize long-term preservation and embrace principles from the data stewardship community, which established a set of best practices for open data management. n adherence, two principles adopted by the Arctic Data Center are FAIR Principles (Findable, Accessible, Interoperable, and Reproducible) and CARE Principles for Indigenous Governance (Collective Benefit, Authority to Control, Responsibility, Ethics). Both of which serve as frameworks in how to consider data ethics.\nThe FAIR Principles\nFAIR speaks to how metadata is managed, stored, and shared.\n\nWhat is the difference between FAIR principles and open science?\nFAIR principles and open science are overlapping concepts, but are distinctive from one another. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data. The FAIR principles place emphasis on machine readability, “distinct from peer initiatives that focus on the human scholar” (Wilkinson et al 2016) and as such, do not fully engage with sensitive data considerations and with Indigenous rights and interests (Research Data Alliance International Indigenous Data Sovereignty Interest Group, 2019). Metadata can be FAIR but not open. For example, sensitive data (data that contains personal information) may not be appropriate to share, however sharing the anonymized metadata that is easily understandable will reduce research redundancy.\n\nResearch has historically perpetuated colonialism and represented extractive practices, meaning that the research results were not mutually beneficial. These issues also related to how data was owned, shared, and used. To address issues like these, the Global Indigenous Data Alliance (GIDA) introduced CARE Principles for Indigenous Data Governance to support Indigenous data sovereignty. CARE Principles speak directly to how the data is stored and shared in the context of Indigenous data sovereignty. CARE Principles (Carroll et al. 2020) stand for:\n\nCollective Benefit - Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data\nAuthority to Control - Indigenous Peoples’ rights and interests in Indigenous data must be recognized and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledges and geographical indicators, are represented and identified within data.\nResponsibility - Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples.\nEthics - Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n\nTo many, the FAIR and CARE principles are viewed by many as complementary: CARE aligns with FAIR by outlining guidelines for publishing data that contributes to open-science and at the same time, accounts for Indigenous’ Peoples rights and interests (Carroll et al. 2020).\nIn Arctic-based research, there is a paradigm shift to include more local Indigenous Peoples, their concerns, and knowledge throughout the research process (Loseto 2020). At the 2019 ArcticNet Annual Scientific Meeting (ASM), a 4-hour workshop was held between Indigenous and non-Indigenous participants to address the challenge of peer-reviewed publications arising when there is a lack of co-production and co-management in the research process between both groups. In the context of peer review, involving Indigenous People and Indigenous Knowledge (IK) not only can increase the validity of research findings, but also ensure the research is meaningful to those most impacted by it. Moreover, it gives power back to the appropriate people to decide who can be knowledge holders of Indigenous knowledge (Loseto et al. 2020). This example underscores the advocacy CARE framework for Indigenous sovereignty, emphasizing the essential integration of people and purpose in the peer review publication stage. Failure to do so perpetuates power imbalances between science institutions and Indigenous communities. Hence, an equitable framework would adhere to the idea ‘Not about us without us’. As an Arctic research community, it is important to reflect on ways we can continue to engage and incorporate Indigenous communities and if there are gaps to address. However, it is important there is no ‘one size fits all’ approach.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#ethics-at-the-arctic-data-center",
    "href": "sections/ai-ethics.html#ethics-at-the-arctic-data-center",
    "title": "10  AI Ethics",
    "section": "10.2 Ethics at the Arctic Data Center",
    "text": "10.2 Ethics at the Arctic Data Center\nTransparency in data ethics is a vital part of open science. Regardless of discipline, various ethical concerns are always present, including professional ethics such as plagiarism, false authorship, or falsification of data, to ethics regarding the handling of animals, to concerns relevant to human subjects research. As the primary repository for the Arctic program of the National Science Foundation, the Arctic Data Center accepts Arctic data from all disciplines. Recently, a new submission feature was released which asks researchers to describe the ethical considerations that are apparent in their research. This question is asked to all researchers, regardless of disciplines.\nSharing ethical practices openly, similar in the way that data is shared, enables deeper discussion about data management practices, data reuse, sensitivity, sovereignty and other considerations. Further, such transparency promotes awareness and adoption of ethical practices.\n \nInspired by CARE Principles for Indigenous Data Governance (Collective Benefit, Authority to Control, Responsibility, Ethics) and FAIR Principles (Findable, Accessible, Interoperable, Reproducible), we include a space in the data submission process for researchers to describe their ethical research practices. These statements are published with each dataset, and the purpose of these statements is to promote greater transparency in data collection and to guide other researchers. For more information about the ethical research practices statement, check out this blog.\nTo help guide researchers as they write their ethical research statements, we have listed the following ethical considerations that are available on our website. The concerns are organized first by concerns that should be addressed by all researchers, and then by discipline.\nConsider the following ethical considerations that are relevant for your field of research.\n\n10.2.1 Ethical Considerations for all Arctic Researchers\n\nResearch Planning\n\nWere any permits required for your research?\nWas there a code of conduct for the research team decided upon prior to beginning data collection?\nWas institutional or local permission required for sampling?\nWhat impact will your research have on local communities or nearby communities (meaning the nearest community within a 100 mile radius)?\n\nData Collection\n\nWere any local community members involved at any point of the research process, including study site identification, sampling, camp setup, consultation or synthesis?\nWere the sample sites near or on Indigenous land or communities?\n\nData Sharing and Publication\n\nHow were the following concerns accounted for: misrepresentation of results, misrepresentation of experience, plagiarism, improper authorship, or the falsification or data?\nIf this data is intended for publication, are authorship expectations clear for everyone involved? Other professional ethics can be found here",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#ai-ethics",
    "href": "sections/ai-ethics.html#ai-ethics",
    "title": "10  AI Ethics",
    "section": "10.3 AI Ethics",
    "text": "10.3 AI Ethics\nArtificial Intelligence (AI) can be thought of as the development of computer systems that can perform tasks we usually think require human intelligence, such as image recognition, language translation, or autonomous movement. The rapid development and adoption of AI tools in the past years, particularly machine learning algorithms, has revolutionized how big datasets are analyzed, transforming decision-making in all sectors of society. However, frameworks to examine the ethical considerations of AI are just emerging, and careful consideration of how to best develop and apply AI systems is essential to the responsible use of these new, rapidly changing tools. In this section, we will give an overview of the FAST Principles put forward by the Alan Turing Institute in their guide for the responsible design and implementation of AI systems [1].",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#the-fast-principles",
    "href": "sections/ai-ethics.html#the-fast-principles",
    "title": "10  AI Ethics",
    "section": "10.4 The FAST Principles",
    "text": "10.4 The FAST Principles\nFAST stands for Fairness, Accountability, Sustainability, and Transparency. The FAST principles aim to guide the ethical development of AI projects from their inception to deployment. The continuous involvement and commitment of software developers, domain experts, technical leads, project managers, rightsholders, and collaborators involved in the AI project is crucial to implement these principles successfully. The following is a brief overview of each of the FAST principles, we greatly encourage you to read through the Alan Turing Institute guide to learn more!\n\n\n\nLeslie, 2019",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#fairness",
    "href": "sections/ai-ethics.html#fairness",
    "title": "10  AI Ethics",
    "section": "10.5 Fairness",
    "text": "10.5 Fairness\nBias can enter at any point of a research project, from data collection and preprocessing, to model design and implementation. This is because AI projects, as any other, are created by human beings who (even with the best of intentions) can introduce error, prejudice, or misjudgement into a system. Fairness refers to the active minimization of bias and commitment to not harm others through the outcomes of an AI system. The FAST principles [1] suggest the following baseline for fairness:\n\nThe designers and users of AI systems ensure that the decisions and behaviours of their models do not generate discriminatory or inequitable impacts on affected individuals and communities. This entails that these designers and users ensure that the AI systems they are developing and deploying:\n\nAre trained and tested on properly representative, relevant, accurate, and generalisable datasets (Data Fairness)\nHave model architectures that do not include target variables, features, processes, or analytical structures (correlations, interactions, and inferences) which are unreasonable, morally objectionable, or unjustifiable (Design Fairness)\nDo not have discriminatory or inequitable impacts on the lives of the people they affect (Outcome Fairness)\nAre deployed by users sufficiently trained to implement them responsibly and without bias (Implementation Fairness)\n\n\n\n\n\n\n\n\nReal-life Example : Insufficient Radar Network\n\n\n\n\n\nThe following figure [2] shows coverage of the national Doppler weather network (green and yellow circles) over a demographic map of the Black population in the southeast US. This would be an example of an issue in data fairness, since radar coverage does not represent the population uniformly, leaving out areas with higher Black population. Problems with outcome fairness could ensue if this non-representative biases an AI model to under-predict weather impacts in such populations.\n\n\n\nMcGovern et al., 2022 by courtesy of Jack Sillin (CC BY 4.0).",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#accountability",
    "href": "sections/ai-ethics.html#accountability",
    "title": "10  AI Ethics",
    "section": "10.6 Accountability",
    "text": "10.6 Accountability\nAccountability in AI projects stems from the shared view that isolated AI models used to automate decisions are not morally responsible in the same way as a decision-making human. As outputs from AI models are increasingly used to make decisions that affect the environment and human lives, there is a critical need for competent human authorities to offer explanations and justifications for the development process, outputs, and ensuing decisions made by AI systems. Such answerability assignments can be challenging, as AI implementations are often the product of big development teams where the responsibility to answer for a project’s outcome may not be delineated, creating an issue known as “the problem of many hands.” The FAST principles encourage the following accountability implementation:\nAccountability by Design: All AI systems must be designed to facilitate end-to-end answerability and auditability. This requires both responsible humans-in-the-loop across the entire design and implementation chain as well as activity monitoring protocols that enable end-to-end oversight and review.\n\n\n\n\n\n\nReal-life Example: AI for natural disasters response\n\n\n\n\n\nAccountability and the ability to audit AI methods can be crucial when model outputs support critical decision-making, such as in natural disasters. In 2021, a New York Times investigation [3]] covered a private company’s premature release of outputs about neighborhoods most affected by potential earthquakes in Seattle. While the initial release erroneously did not show threats for non-residential areas, ensuing updated versions showed non-compatible predictions again. Although the company acknowledged that its AI models would not replace the first responder’s judgment, the lack of audibility and opacity in the model development hindered accountability for any party, ultimately eroding the public confidence in the tools and leading to a loss of public resources.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#sustainability",
    "href": "sections/ai-ethics.html#sustainability",
    "title": "10  AI Ethics",
    "section": "10.7 Sustainability",
    "text": "10.7 Sustainability\nSustainability in the FAST principles includes continuous assessment of the social impacts of an AI system and technical sustainability of the AI model. In the first consideration, the FAST principles advocate for performing a Stakeholder Impact Assessment (SIA) at different stages to help build confidence in the project and uncover unexpected risks or biases, among other benefits. The Alan Turing Institute guide shares a prototype of an SIA [1]. The core of technical sustainability is creating safe, accurate, reliable, secure, and robust AI systems. To achieve these technical goals, teams must implement thorough testing, performance metrics, uncertainty quantification, and be aware of changes to the underlying distribution of data, among other essential practices.\n\n\n\n\n\n\nReal-life Example: SpaceCows\n\n\n\n\n\nThe SpaceCows project [4], [5] in northern Australia is a collaboration between scientists, industry leaders, and local indigenous communities developing AI centered platforms to analyze GPS tracking data collected from feral cows alongside satellite imagery and weather data. Indigenous knowledge and traditional land owners have been at the center of the development, providing guidance and ultimately benefiting from the AI tools to protect their land and cultural sites.\n\n\n\nImportant indigenous cultural sites can be damaged by feral cattle. Image from CSIRO, SpaceCows: Using AI to tackle feral herds in the Top End.\n\n\nVideos with more information on SpaceCows:\nCSIRO rolls out world’s largest remote ‘space cows’ herd management system\nSpaceCows: Using AI to tackle feral herds in the Top End",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#transparency",
    "href": "sections/ai-ethics.html#transparency",
    "title": "10  AI Ethics",
    "section": "10.8 Transparency",
    "text": "10.8 Transparency\nUnder the FAST principles, transparency in AI projects refers to transparency about how an AI project was designed and implemented and the content and justification of the outcome produced by the AI model. To ensure process transparency, the project should show how the design and implementation included ethical, safety, and fairness considerations throughout the project. To clarify the content and explain the outcomes of an AI system, the project should offer plain language, non-technical explanations accessible to non-specialists that convey how and why a model performed the way it did. In this direction, it is essential to avoid a ‘mathematical glass box’ where the code and mathematics behind the algorithm are openly available, but there is a lack of rationale about how or why the model goes from input to output. Finally, the explanations about how the outcomes were produced should become the basis to justify the outcomes in terms of ethical permissibility, fairness, and trustworthiness. A careful consideration of the balance between the sustainability and transparency principles is necessary when dealing with protected or private data.\n\n\n\n\n\n\nReal-life Example: France’s Digital Republic Act\n\n\n\n\n\nThe concern for transparency in using personal data is an active space for debate. In 2018, the French government passed a law to protect citizens’ privacy, establishing the citizen’s “right to an explanation” regarding, among other things, how an algorithm contributed to decisions on their persona and which data was processed [6], [7]. Overall, this legislation aims to create a fairer and more transparent digital environment where everyone can enjoy equal opportunities.\n\n\n\nPhoto by Google DeepMind",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#discussion-activity",
    "href": "sections/ai-ethics.html#discussion-activity",
    "title": "10  AI Ethics",
    "section": "10.9 Discussion Activity",
    "text": "10.9 Discussion Activity\nGiven the discussion of the CARE priciples and the FAST principles, let’s discuss what responsible AI considerations might exist in the context of Arctic research, particularly with respect to Indigneous peoples of the Arctic. Geospatial data spanning the Arctic typically includes the traditional lands and waters of Arctic Indigenous peoples, and often intersects with current local communities distributed throughout the Arctic. Recent work by projects like Abundant Intelligences are starting to explore the intersection of Indigineous Knowledge systems, Artifical Intelligence models, and how to guide the “development of AI [to support] a more humane future”.\nLet’s take, for example, a researcher that wants to run an machine learning model to detect changes in environmental features at a large regional or Arctic scale. We’ve seen several of these so far, including 1) AI predictions of the distribution of permafrost ice wedges and retrogressive thaw slumps across the Arctic; 2) use of AI to detect changes in surface water extent and lake drainage events across the Arctic; 3) use of AI in a mechanistic process models that helps understand the global source/sink tradeoff of permafrost loss and its impact on climate.\n\n\n\n\n\n\nDiscussion questions\n\n\n\nDivide into 5 groups of 4, find a comfortable place to sit, and pick a large-scale AI application that is of interest to the group. Let’s discuss some of the following questions, and more…\n\nThinking of CARE, does that model provide Collective Benefit to Indigenous populations that it might impact?\nThinking of FAST, what would researchers need to do to ensure that their research process could meet the goals of the four categories of Fairness (Outcome Fairness, Data Fairness, Design Fairness, and Implementation Fairness) for Indigenous people in their research area?",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#conclusion",
    "href": "sections/ai-ethics.html#conclusion",
    "title": "10  AI Ethics",
    "section": "10.10 Conclusion",
    "text": "10.10 Conclusion\nAs new AI developments and applications rapidly emerge and transform everyday life, we need to pause and ensure these technologies are fair, sustainable, and transparent. We must acknowledge human responsibility in designing and implementing AI systems to use these novel tools fairly and with accountability. Finally, we acknowledge that the information covered here is a lightning introduction to AI’s ethical considerations and implications. Whether you are a researcher interested in using AI for the first time or a seasoned ML practitioner, we urge you to dive into the necessary and ever-expanding AI ethics work to learn how to best incorporate these concepts into your work.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#further-reading",
    "href": "sections/ai-ethics.html#further-reading",
    "title": "10  AI Ethics",
    "section": "10.11 Further Reading",
    "text": "10.11 Further Reading\nAcademic Data Science Alliance (ADSA) (2024) The Data Sciene Ethos https://ethos.academicdatascience.org [8]\nChen, W., & Quan-Haase, A. (2020) Big Data Ethics and Politics: Towards New Understandings. Social Science Computer Review. [9]\nCrawford, K., & Paglen, T. (2019) Excavating AI: The Politics of Training Sets for Machine Learning. [10]\nGray, J., & Witt, A. (2021) A feminist data ethics of care framework for machine learning: The what, why, who and how. First Monday, 26(12), Article number: 11833 [11]\n\n\n\n\n[1] D. Leslie, “Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector,” Zenodo, Jun. 2019. doi: 10.5281/zenodo.3240529.\n\n\n[2] A. McGovern, I. Ebert-Uphoff, D. J. G. Ii, and A. Bostrom, “Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science,” Environmental Data Science, vol. 1, p. e6, Jan. 2022, doi: 10.1017/eds.2022.5.\n\n\n[3] S. Fink, “This High-Tech Solution to Disaster Response May Be Too Good to Be True,” The New York Times, Aug. 2019, Accessed: Oct. 19, 2024. [Online]. Available: https://www.nytimes.com/2019/08/09/us/emergency-response-disaster-technology.html\n\n\n[4] T. Shepherd, “Indigenous rangers to use SpaceCows program to protect sacred sites and rock art from feral herds,” The Guardian, Sep. 2021, Accessed: Oct. 19, 2024. [Online]. Available: https://www.theguardian.com/australia-news/2021/sep/15/indigenous-rangers-to-use-spacecows-program-to-protect-sacred-sites-and-rock-art-from-feral-herds\n\n\n[5] CSIRO, “SpaceCows: Using AI to tackle feral herds in the Top End.” Accessed: Oct. 19, 2024. [Online]. Available: https://www.csiro.au/en/news/All/News/2021/September/SpaceCows-Using-AI-to-tackle-feral-herds-in-the-Top-End\n\n\n[6] L. Edwards and M. Veale, “Enslaving the Algorithm: From a ‘Right to an Explanation’ to a ‘Right to Better Decisions’?” Social Science Research Network, Rochester, NY, 2018. doi: 10.2139/ssrn.3052831.\n\n\n[7] S. Lo Piano, “Ethical principles in machine learning and artificial intelligence: Cases from the field and possible ways forward,” Humanities and Social Sciences Communications, vol. 7, no. 1, pp. 1–7, Jun. 2020, doi: 10.1057/s41599-020-0501-9.\n\n\n[8] A. D. S. A. (ADSA), “The Data Science Ethos - Operationalizing Ethics in Data Science,” The Data Science Ethos. Accessed: Oct. 19, 2024. [Online]. Available: https://ethos.academicdatascience.org/\n\n\n[9] W. Chen and A. Quan-Haase, “Big Data Ethics and Politics: Toward New Understandings,” Social Science Computer Review, vol. 38, no. 1, pp. 3–9, Feb. 2020, doi: 10.1177/0894439318810734.\n\n\n[10] “Excavating AI.” Accessed: Oct. 19, 2024. [Online]. Available: https://excavating.ai/\n\n\n[11] J. Gray and A. Witt, “A feminist data ethics of care for machine learning: The what, why, who and how,” First Monday, Dec. 2021, doi: 10.5210/fm.v26i12.11833.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html",
    "href": "sections/guest-lecture-yili-arts-dataset.html",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "",
    "text": "Overview\nIn this session, we will introduce and explore the Arctic Retrogressive Thaw Slump (ARTS) dataset. We aim to illuminate the background and motivation behind the ARTS dataset, detail its design elements including functions, metadata, and usage, and underscore its defining features such as scalability, scientific integrity, and the potential for community contribution.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html#outline",
    "href": "sections/guest-lecture-yili-arts-dataset.html#outline",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "Outline",
    "text": "Outline\n\nBackground and motivation of the Arctic\nRetrogressive Thaw Slump (ARTS) data set.\nSource data for the ARTS data set\nDesign of the data set - functions, metadata, usage\nFeatures of the data set - scalable, scientific, contributable\nData Curation Framework - standards, protocols\nThe ARTS repository - user and contributor guideline\nQuestions and discussions",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html#lecturer",
    "href": "sections/guest-lecture-yili-arts-dataset.html#lecturer",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "Lecturer",
    "text": "Lecturer\nYili Yang, Ph.D.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html#slides",
    "href": "sections/guest-lecture-yili-arts-dataset.html#slides",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "Slides",
    "text": "Slides\nDownload Slides",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html#reference",
    "href": "sections/guest-lecture-yili-arts-dataset.html#reference",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "Reference",
    "text": "Reference\n\nhttps://github.com/whrc/ARTS",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html",
    "href": "sections/exploring-advanced-neural-networks.html",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "",
    "text": "Overview\nIn this section, we will explore the journey of the evolution of neural networks, starting with basic classification networks and progressing to more complex tasks of object detection and instance segmentation.\nWe’ll begin with a recap of classification networks, which are designed to identify the presence of objects in images and assign class labels. From there, we’ll move to the realm of object detection, exploring the development of R-CNN 1, Fast R-CNN 2, and Faster R-CNN 3. Each of these models represents a step forward in detecting and localizing objects within images, with Faster R-CNN introducing a more efficient and integrated approach.\nFinally, we’ll delve into Mask R-CNN 4, which extends the capabilities of Faster R-CNN by adding a branch for predicting segmentation masks. This enhancement allows us to not only detect objects but also delineate their exact shapes, enabling precise instance segmentation.\nOur goal is to build on your existing understanding of classification networks, providing you with practical insights into how these networks can be adapted and expanded for more advanced computer vision applications. By the end of this section, you will be equipped with the knowledge to understand and apply these techniques to real-world challenges.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#what-is-instance-segmentation",
    "href": "sections/exploring-advanced-neural-networks.html#what-is-instance-segmentation",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "12.1 What is instance segmentation?",
    "text": "12.1 What is instance segmentation?\n\n\n\nSource: https://manipulation.csail.mit.edu/segmentation.html\n\n\nTo grasp the concept of instance segmentation, let’s compare it with other related tasks:\n\nImage recognition: This is the most basic task, where the model identifies the presence of objects in an image and assigns class probabilities. It’s like saying, “There are sheep and a dog here,” without specifying their locations.\nSemantic segmentation: This advances to classifying each pixel into a category, effectively coloring the entire image based on object classes. However, it doesn’t distinguish between individual instances of the same class. For example, all sheep might be colored the same, treating them as a single entity.\nObject detection: This task detects objects and distinguishes them by drawing bounding boxes around each, providing more detail than image recognition. It identifies individual objects but lacks fine-grained, pixel-level detail.\nInstance segmentation: This is the most detailed task, combining the strengths of object detection and semantic segmentation. It not only distinguishes individual objects but also delineates their exact shapes at the pixel level. It allows for precise analysis and manipulation.\n\n\n\n\n\n\n\nNote\n\n\n\nInstance segmentation is more challenging and computationally intensive than other tasks, and it’s not always necessary for every application.\n\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nConsider which of these tasks would be most useful for your current projects or applications.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#importance-and-applications-of-instance-segmentation",
    "href": "sections/exploring-advanced-neural-networks.html#importance-and-applications-of-instance-segmentation",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "12.2 Importance and applications of instance segmentation",
    "text": "12.2 Importance and applications of instance segmentation\nInstance segmentation plays a crucial role in various fields. Its importance stems from the following factors:\n\nFine-grained precision: Unlike object detection, which only provides bounding boxes, instance segmentation offers pixel-level accuracy. This precision is essential for tasks where understanding the exact shape and position of objects matters.\nObject differentiation: It not only identifies the presence of objects but also distinguishes between individual instances of the same class. This capability is valuable in scenarios where multiple objects of the same type need to be analyzed separately.\nIntegration in complex systems: By providing detailed object masks, instance segmentation enables more sophisticated interactions with objects in the environment, such as robotics and augmented reality applications.\n\nApplications:\n\nMedical imaging: In medical applications, instance segmentation can help identify and analyze specific structures within images, such as tumors or organs 5.\nRobotics: In robotics, instance segmentation enables robots to perceive and interact with their environment more effectively, distinguishing between different objects and obstacles 6.\nEnvironmental monitoring: Instance segmentation can be used in environmental mapping to detect and classify natural features from satellite imagery. This approach enhances the ability to monitor changes in landscapes across large areas with high accuracy 78.\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nConsider how the ability to identify not just the presence of objects, but also their precise contours, might enhance applications you are familiar with.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#transitioning-from-backbone-networks-to-classification-networks",
    "href": "sections/exploring-advanced-neural-networks.html#transitioning-from-backbone-networks-to-classification-networks",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "12.3 Transitioning from backbone networks to classification networks",
    "text": "12.3 Transitioning from backbone networks to classification networks\n In the previous section, we discussed how deep learning models consist of three core components: input adaptation, feature extractor, and output adaptation. Now, let’s explore how these components are configured starting from a backbone network, like ResNet-50, to create a classification network.\nResNet-50 as a backbone network\nUsing a ResNet-50 backbone network as an example, let’s first examine the structure of the model:\n\n\n\n\n\n\nCode snippet to load a pre-trained ResNet-50 model\n\n\n\n\n\nimport torch\nimport torchvision.models as models\n\n# Load the pre-trained ResNet-50 model\nmodel = models.resnet50(pretrained=True)\n\n# Print the model architecture\nprint(model)\n\n\n\nThe output of the code snippet above provides the architecture of the ResNet-50 model, which includes a series of convolutional and batch normalization layers, typically used for feature extraction:\n\n\n\n\n\n\nOutput: ResNet-50 architecture\n\n\n\n\n\nResNet(\n(conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n(bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n(layer1): Sequential(\n(0): BasicBlock(\n(conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n(1): BasicBlock(\n(conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n)\n(layer2): Sequential(\n(0): BasicBlock(\n(conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(downsample): Sequential(\n(0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n(1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n)\n(1): BasicBlock(\n(conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n(layer3): Sequential(\n(0): BasicBlock(\n(conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(downsample): Sequential(\n(0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n(1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n)\n(1): BasicBlock(\n(conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n)\n(layer4): Sequential(\n(0): BasicBlock(\n(conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(downsample): Sequential(\n(0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n(1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n)\n(1): BasicBlock(\n(conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n)\n(avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n(fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n)\n\n\n\nThe output above is a liitle verbose, so let’s summarize the key components of the ResNet-50 model:\n\n\n\n\n\n\nSummary: ResNet-50 architecture\n\n\n\n\n\nResNet(\n(conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n(bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n(layer1): Sequential()\n(layer2): Sequential()\n(layer3): Sequential()\n(layer4): Sequential()\n(avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n(fc): Linear(in_features=512, out_features=1000, bias=True)\n)\nwhere layerx represents the convolutional blocks in the network.\n\n\n\nModifying the ResNet-50 model to perform classification\nTo modify the ResNet-50 model to perform classification, we first need to define the task. For this example, we will classify images into 10 classes, e.g., 10 different types of food. So, for the data, we have\n\ninput: an image\noutput: a 10-dimensional vector of class probabilities\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nHow would you modify the ResNet-50 model to perform classification?\n\n\nLet’s examine the three components of the model and see how we can modify them to perform classification.\n\n\n\n\n\n\nInput Adaptation\n\n\n\n\n\nNo changes needed. The ResNet-50 model is designed to accept images as input, which is suitable for our task.\n\n\n\n\n\n\n\n\n\nFeature Extractor\n\n\n\n\n\nNo changes needed. The pre-trained ResNet-50 model’s layers are retained for feature extraction, leveraging its ability to learn complex patterns from images.\n\n\n\n\n\n\n\n\n\nOutput Adaptation\n\n\n\n\n\nFrom the summary of the ResNet-50 architecture, we can see that the output layer is a linear layer with 1000 output features.\n(fc): Linear(in_features=512, out_features=1000, bias=True)\nIt is designed to output 1000 class probabilities, and we need to modify it to output 10 class probabilities for our task. To do this, we can replace the linear layer with a new linear layer that has 10 output features.\nimport torch.nn as nn\n\n# Replace the last linear layer with a new linear layer that has 10 output features\nmodel.fc = nn.Linear(512, 10)\n\n\n\nDone! We have modified the ResNet-50 model to perform classification. Here is the complete code:\n\n\n\n\n\n\nComplete code\n\n\n\n\n\nimport torch\nimport torchvision.models as models\nimport torch.nn as nn\n\n# Load the pre-trained ResNet-50 model\nmodel = models.resnet50(pretrained=True)\n\n# Replace the last linear layer with a new linear layer that has 10 output features\nmodel.fc = nn.Linear(512, 10)\n\n# Print the model architecture\nprint(model)\n\n\n\n\n\n\n\n\n\nQuick Thought\n\n\n\n\nUsing the ResNet-50 model as a backbone network, is there other way to modify the model to perform classification?\nWith the classification task as an example, try to think of how to modify the model to perform instance segmentation.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#a-step-by-step-guide-to-instance-segmentation",
    "href": "sections/exploring-advanced-neural-networks.html#a-step-by-step-guide-to-instance-segmentation",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "12.4 A step-by-step guide to instance segmentation",
    "text": "12.4 A step-by-step guide to instance segmentation\nTo achieve instance segmentation, let’s begin from object detection and progress to instance segmentation since object detection is like a simplified version of instance segmentation. Below, we list the input and output of object detection and instance segmentation.\n\n\n\n\n\n\n\n\nTask\nInput\nOutput\n\n\n\n\nObject Detection\nImage\nBounding boxes and class labels for each instance\n\n\nInstance Segmentation\nImage\nBounding boxes, masks and class labels for each instance\n\n\n\n\n12.4.1 The First Step: R-CNN\nOur journey begins with the introduction of Region-based Convolutional Neural Networks (R-CNN).\nImagine you are tasked with finding specific items in a crowded room (the image). The first step you need is a strategy to pinpoint potential areas where the items might be located. This is where Selective Search 9 comes into play. It acts like a diligent assistant that systematically scans the image, suggesting potential regions that might contain the items. Once you have these potential regions, the next step is to understand what each region contains. In our example, we use ResNet-50 to extract features from each region. With the features in hand, we then use a linear Support Vector Machine (SVM) to classify each region into one of the 10 classes. But recognizing objects is just part of the story. To ensure precision in locating these objects, we employ a linear regression model to refine the coordinates of the bounding boxes.\nSummary of R-CNN components:\n\nRegion proposal (selective search): Generate potential object regions.\nFeature extraction (ResNet-50): Extract features from each region.\nClassification (linear SVM): Classify each region into one of the 10 classes.\nBounding box regression (linear regression): Refine the coordinates of the bounding boxes.\n\nLimitations of R-CNN:\n\nSlow inference: Processes each region independently, leading to inefficiency.\nComplex training: Requires multiple models and stages.\nInefficient feature extraction: Redundant computations for overlapping regions.\n\n\n\n12.4.2 Evolution to Fast R-CNN\nContinuing our journey towards instance segmentation, we move from the foundational R-CNN to a more advanced and efficient model: Fast R-CNN. This evolution is necessary to overcome the challenges faced by R-CNN.\nIn our quest for faster and more efficient object detection, Fast R-CNN addresses the limitations of its predecessor, R-CNN. Imagine now that instead of examining each potential object region individually, you first take a comprehensive view of the entire scene. This panoramic approach forms the basis of Fast R-CNN.\n\nFeature extraction (single pass): Fast R-CNN starts by processing the entire image in a single forward pass through the network, using a shared convolutional feature map. This method not only accelerates the process but also eliminates the need to repeatedly extract features for overlapping regions, directly addressing the inefficiency of R-CNN.\nRegion proposal (selective search): Although it continues to use Selective Search to suggest possible object regions, the way these proposals are handled is integrated into a more streamlined workflow.\nRoI pooling: To efficiently zoom into specific regions of interest (RoIs), Fast R-CNN introduces RoI Pooling. This component crops and resizes the shared feature map for each proposed region, much like focusing a camera lens on the areas of interest. RoI Pooling ensures that each region is presented in a consistent shape to the classifier, optimizing the use of features extracted in the initial pass.\nClassification and bounding box regression (fully connected layers): Fast R-CNN employs fully connected layers to perform both classification and bounding box regression simultaneously. This integration simplifies the training pipeline and ensures that both tasks benefit from the shared feature representation, in contrast to the separate models used in R-CNN.\n\nSummary of Fast R-CNN Components:\n\nFeature extraction (single pass): Processes the entire image once, reducing redundancy and speeding up inference.\nRegion proposal (selective search): Still used, but proposals are more efficiently integrated into the workflow.\nRoI pooling: Efficiently refines regions of interest for classification and bounding box adjustments.\nClassification and bounding box regression (fully connected layers): Conducts these tasks concurrently, simplifying the model architecture.\n\nSolving R-CNN Limitations:\n\nSlow inference: Addressed by processing the entire image in a single pass, reducing computational load.\nComplex training pipeline: Simplified by integrating classification and regression within a unified model.\nInefficient feature extraction: Eliminated by using shared feature maps, reducing redundant processing.\n\nLimitations of Fast R-CNN:\n\nDependence on slow region proposal methods: The reliance on Selective Search for region proposals limits the speed of the overall system.\nSeparate proposal and detection stages: Although integrated more efficiently than R-CNN, the use of Selective Search still represents a bottleneck.\nPotential for further speed improvements: While faster than R-CNN, Fast R-CNN does not fully utilize potential speed optimizations through end-to-end training of the proposal mechanism.\n\n\n\n12.4.3 Introducing Faster R-CNN\nAs we progress in our journey towards instance segmentation, we arrive at Faster R-CNN, which further refines the processes established by Fast R-CNN. This model addresses the bottlenecks that persisted in previous approaches, particularly regarding region proposal efficiency.\n\n\n12.4.4 The Innovation of Faster R-CNN\nFaster R-CNN represents a significant leap forward in object detection by introducing a component that eliminates the dependence on slow external region proposal methods. This innovation stems from the integration of a Region Proposal Network (RPN), which transforms the object detection process into a unified, end-to-end trainable system.\n\nFeature extraction (single pass): Like Fast R-CNN, Faster R-CNN begins by processing the entire image in a single pass through a convolutional network to produce a shared feature map. This ensures efficient feature extraction and serves as the foundation for both region proposal and object detection tasks.\nRegion proposal network (RPN): The key innovation of Faster R-CNN is the RPN, which is integrated directly into the convolutional feature map. The RPN efficiently generates region proposals by sliding a small network over the shared feature map. This network outputs a set of objectness scores and refined bounding box coordinates for anchors at each spatial location, effectively replacing the slow Selective Search process used in Fast R-CNN.\nRoI pooling: After generating region proposals, Faster R-CNN employs RoI Pooling to extract fixed-size feature maps from the shared feature map for each proposal. This ensures that each region of interest is consistently prepared for the subsequent classification and bounding box refinement stages.\nClassification and bounding box regression (fully connected layers): The proposals are then fed into fully connected layers for simultaneous classification and bounding box regression, refining both the class labels and bounding box coordinates.\n\nSummary of Faster R-CNN Components:\n\nFeature extraction (single pass): Processes the entire image once, providing a shared feature map for all subsequent tasks.\nRegion proposal network (RPN): Generates high-quality region proposals directly from the feature map, replacing slower methods.\nRoI pooling: Standardizes proposal feature maps for further processing.\nClassification and bounding box regression (fully connected layers): Conducts these tasks effectively and efficiently, using the refined proposals.\n\nAddressing Fast R-CNN Limitations:\n\nDependence on slow region proposal methods: Solved by integrating the RPN, which provides fast and efficient region proposals.\nSeparate proposal and detection stages: Unified into a single, end-to-end trainable model.\nPotential for further speed improvements: Realized through the RPN, which allows for significant speed gains and paves the way for real-time object detection capabilities.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#transition-to-instance-segmentation-mask-r-cnn",
    "href": "sections/exploring-advanced-neural-networks.html#transition-to-instance-segmentation-mask-r-cnn",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "12.5 Transition to Instance Segmentation: Mask R-CNN",
    "text": "12.5 Transition to Instance Segmentation: Mask R-CNN\nHaving refined the processes of object detection with Faster R-CNN, we now transition to the realm of instance segmentation with Mask R-CNN. This model not only detects objects but also delineates each instance with precise pixel-level masks, taking the capabilities of Faster R-CNN a step further.\nMask R-CNN builds upon the robust framework of Faster R-CNN, adding a crucial component for instance segmentation. It retains the speed and efficiency of Faster R-CNN while introducing new functionality without compromising performance.\n\nFeature extraction (single pass): Mask R-CNN, like its predecessor, begins by processing the entire image in a single pass through a convolutional network to produce a shared feature map. This feature map serves as the foundation for both region proposal and instance segmentation tasks.\nRegion proposal network (RPN): The RPN continues to play a pivotal role in generating high-quality region proposals directly from the shared feature map. This component ensures that only the most promising regions are considered for further processing.\nRoI Align: To address the spatial misalignment introduced by RoI Pooling in Faster R-CNN, Mask R-CNN introduces RoI Align. This new component improves the precision of feature extraction by avoiding quantization errors, ensuring that the extracted feature maps for each region of interest are perfectly aligned with the input image.\nClassification, bounding box regression, and mask prediction (parallel heads): Mask R-CNN extends the two-head structure of Faster R-CNN by adding a third branch specifically for mask prediction. This additional branch predicts a binary mask for each region of interest, allowing the model to not only classify objects and refine bounding boxes but also generate accurate pixel-level masks.\n\nSummary of Mask R-CNN Components:\n\nFeature extraction (single pass): Processes the entire image once, ensuring efficient use of computational resources.\nRegion proposal network (RPN): Continues to provide fast and efficient region proposals.\nRoI Align: Refines feature extraction for precise spatial alignment.\nClassification, bounding box regression, and mask prediction (parallel heads): Conducts these tasks simultaneously, enabling comprehensive instance segmentation.\n\nEnhancements from Faster R-CNN:\n\nPixel-level precision: Achieved through RoI Align, improving spatial alignment and mask accuracy.\nInstance segmentation capability: Added through the mask prediction branch, enabling the model to generate detailed masks for each object instance.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#conclusion",
    "href": "sections/exploring-advanced-neural-networks.html#conclusion",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "12.6 Conclusion",
    "text": "12.6 Conclusion\nWe’ve explored the evolution from R-CNN to Mask R-CNN, showing how each step builds on the last to improve performance. This process demonstrates how you can apply similar improvements to tackle a wide range of advanced tasks, not just instance segmentation. By understanding these developments, you’re equipped to enhance existing models and create innovative solutions for various challenges in computer vision.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#footnotes",
    "href": "sections/exploring-advanced-neural-networks.html#footnotes",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "",
    "text": "R. Girshick, J. Donahue, T. Darrell, J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” CVPR 2014.↩︎\nR. Girshick, “Fast R-CNN,” ICCV 2015.↩︎\nS. Ren, K. He, R. Girshick, J. Sun, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” NIPS 2015.↩︎\nK. He, G. Gkioxari, P. Dollár, R. Girshick, “Mask R-CNN,” ICCV 2017.↩︎\nhttps://www.linkedin.com/pulse/medical-image-diagnosis-roles-object-detection-segmentation-egvcc↩︎\nhttps://www.youtube.com/watch?v=tNLtXb04i3w↩︎\nZhang, W., Witharana, C., Liljedahl, A. K., & Kanevskiy, M. (2018). Deep convolutional neural networks for automated characterization of arctic ice-wedge polygons in very high spatial resolution aerial imagery. Remote Sensing, 10(9), 1487.↩︎\nLi, W., Hsu, C. Y., Wang, S., Witharana, C., & Liljedahl, A. (2022, November). Real-time GeoAI for high-resolution mapping and segmentation of arctic permafrost features: the case of ice-wedge polygons. In Proceedings of the 5th ACM SIGSPATIAL international workshop on AI for geographic knowledge discovery (pp. 62-65).↩︎\nJ. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders, “Selective search for object recognition,” International Journal of Computer Vision, vol. 104, no. 2, pp. 154-171, 2013.↩︎",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "",
    "text": "Goal\nThis session introduces participants to Detectron2, a cutting-edge open-source library developed by Meta AI Research (FAIR) for state-of-the-art object detection and segmentation tasks. Built on the PyTorch deep learning framework, Detectron2 offers a flexible and modular design that makes it easier for researchers and developers to implement and experiment with advanced computer vision models.\nIn this session, participants will be introduced to the fundamentals of Detectron2 and its application in analyzing visual data specific to Arctic research. We will cover the installation process, basic usage, and explore how to train custom models to detect and segment objects relevant to Arctic studies. By the end of this tutorial, participants will have hands-on experience with Detectron2 and understand how to apply it to their own research projects.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#why-use-a-deep-learning-framework",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#why-use-a-deep-learning-framework",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.1 Why use a deep learning framework?",
    "text": "13.1 Why use a deep learning framework?\nDeep learning frameworks like Detectron2 or MMSegmentation provide advantages over core libraries like PyTorch directly. Here are some of the key benefits:\n\nEase of use and rapid prototyping: These frameworks offer pre-built, state-of-the-art model architectures and components that simplify the process of developing complex models. This allows researchers to quickly prototype and experiment with different models without having to worry about the underlying implementation details.\nModular and extensible design: These frameworks are designed to be highly modular, enabling researchers to easily customize and extend various components such as model architectures, training pipelines, evaluation metrics, and more. This flexibility allows for tailored solutions that can be adapted to specific research questions or datasets.\nOptimized performance: These frameworks often include optimized implementations of popular algorithms and techniques, which can lead to faster and more efficient model training and inference. Also, these frameworks are designed to be scalable, allowing researchers to train models on large datasets and scale up to high-performance hardware, e.g., multi-GPU systems.\nCommunity and support: Detectron2 has a large community and extensive documentation, making it easier to find resources and support.\nFocus on research: By abstracting many of the low-level details, these frameworks allow researchers to focus on the core research questions and innovations, enabling faster iteration on new ideas and research directions.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#configuration-system",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#configuration-system",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.2 Configuration system",
    "text": "13.2 Configuration system\nDetectron2’s configuration system provides a flexible and modular way to manage training and evaluation settings. It supports both YAML-based configuration files and Python-based lazy configurations, allowing users to define model architectures, datasets, training hyperparameters, and more.\n\nWhy use the configuration system?\n\nReproducibility: Configs make it easy to share and reproduce experiments.\nModularity: Different components (e.g., dataset, model, solver) are easily interchangeable.\nEase of Use: Instead of defining hyperparameters in scripts, you can manage them in a structured format.\n\n\n\nLazyConfig vs. Default Config\nDetectron2 offers two types of configuration systems:\n\nDefault Config (YAML-based): The traditional configuration system that loads from .yaml files.\nLazyConfig (Python-based): A more flexible system that supports Python expressions and function calls.\n\n\n\nBenefits of LazyConfig\n\nAllows defining configurations programmatically in Python.\nSupports dynamic configurations with function calls.\nReduces redundancy and improves maintainability.\n\n\n\nUsing LazyConfig\n\nLoading a base configuration\n\nLazyConfig organizes configurations as Python modules. You can start by loading a base configuration:\nfrom detectron2.config import LazyConfig\ncfg = LazyConfig.load(\"detectron2/projects/ViTDet/configs/mae_finetune.py\")\n\nModifying configurations\n\nSince LazyConfig is Python-based, modifications can be made directly:\ncfg.train.init_checkpoint = \"path/to/checkpoint.pth\"\ncfg.dataloader.train.dataset.names = \"coco_2017_train\"\ncfg.train.max_iter = 50000\n\nRegistering a new configuration\n\nYou can create your own configuration by defining a Python script:\nfrom detectron2.config import LazyCall\nfrom detectron2.modeling import build_model\n\nconfig = dict(\n    model=LazyCall(build_model)(\n        backbone=dict(type=\"ResNet\", depth=50),\n        num_classes=80,\n    ),\n    solver=dict(\n        base_lr=0.002,\n        max_iter=10000,\n    ),\n)\nSave this file as my_config.py and load it using:\ncfg = LazyConfig.load(\"my_config.py\")\n\nRunning training with LazyConfig\n\nTo train a model using a LazyConfig setup, use:\npython train_net.py --config-file my_config.py\n\n\nBest practices\n\nOrganize configs in a structured manner: Keep different configurations (model, dataset, solver) separate for better maintainability.\nUse function calls: Leverage Python functions to make dynamic changes to configurations.\nExperiment tracking: Store modified configs alongside experiment logs to ensure reproducibility.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#data-system",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#data-system",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.3 Data system",
    "text": "13.3 Data system\n\nDataset registration\nDetectron2 does not assume a fixed dataset format; instead, it requires datasets to be registered before use. Dataset registration involves providing metadata and a function that loads dataset samples.\nRegistering a custom dataset\nDetectron2 uses the DatasetCatalog and MetadataCatalog to manage datasets. To utilize a custom dataset, you need to register it so that Detectron2 knows how to access and interpret your data.\n\nImplement a Function to Load Your Dataset: Create a function that returns your dataset in the format of a list of dictionaries. Each dictionary should contain information about an image and its annotations.\n\ndef my_dataset_function():\n    # Load your dataset and return it as a list of dictionaries\n    return dataset_dicts\n\nRegister the dataset: Use DatasetCatalog.register() to associate your dataset with the function you’ve implemented.\n\nfrom detectron2.data import DatasetCatalog\n\nDatasetCatalog.register(\"my_dataset\", my_dataset_function)\nThis registration allows Detectron2 to access your dataset using the name my_dataset.\nBuilt-in datasets\nDetectron2 includes several built-in datasets such as COCO and LVIS. To use them, install the required dataset and enable it in the config.\nfrom detectron2.data.datasets import register_coco_instances\nregister_coco_instances(\"my_coco\", {}, \"path/to/annotations.json\", \"path/to/images\")\n\n\nData loading\nDetectron2 uses DatasetMapper for loading datasets efficiently. The dataset loader transforms raw dataset samples into a format suitable for training.\nCustom DatasetMapper\nA custom dataset mapper allows applying preprocessing steps before training.\nfrom detectron2.data import DatasetMapper\nfrom detectron2.data import detection_utils as utils\nimport torch\n\nclass MyDatasetMapper(DatasetMapper):\n    def __call__(self, dataset_dict):\n        dataset_dict = dataset_dict.copy()\n        image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n        dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1))\n        return dataset_dict\n\n\nConfiguring DataLoader\nDetectron2 provides a flexible data loader that can be modified based on batch size, augmentations, and transformations.\n\nBuild a data loader: Use build_detection_train_loader() for training and build_detection_test_loader() for evaluation.\n\nfrom detectron2.data import build_detection_train_loader\n\ndata_loader = build_detection_train_loader(cfg)\nfor batch in data_loader:\n    print(batch)  # Process batch\nThese functions handle batching, shuffling, and other data loading operations.\n\n\nData Augmentation\nData augmentation is a technique to improve model generalization by applying random transformations to the input data during training. Detectron2 integrates detectron2.data.transforms for efficient data augmentation.\n\nDefine augmentations: Create a list of augmentation operations.\n\nfrom detectron2.data import transforms as T\n\naugmentations = [\n    T.RandomBrightness(0.9, 1.1),\n    T.RandomFlip(prob=0.5),\n    T.RandomCrop(\"absolute\", (640, 640))\n]\n\nApply augmentations: Integrate these augmentations into your data loading pipeline.\n\nfrom detectron2.data import DatasetMapper\n\nmapper = DatasetMapper(cfg, is_train=True, augmentations=augmentations)\ndata_loader = build_detection_train_loader(cfg, mapper=mapper)",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#model-system",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#model-system",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.4 Model system",
    "text": "13.4 Model system\n\nBuilding Models from Configuration\nDetectron2 employs configuration files to define model architectures and parameters. To construct a model from a configuration, you can use the build_model function:\nfrom detectron2.modeling import build_model\nmodel = build_model(cfg)  # cfg is a configuration object\nThis function initializes the model structure with random parameters. To load pre-trained weights or previously saved parameters, utilize the DetectionCheckpointer:\nfrom detectron2.checkpoint import DetectionCheckpointer\ncheckpointer = DetectionCheckpointer(model)\ncheckpointer.load(file_path_or_url)  # Load weights from a file or URL\n\n\nModel Input and Output Formats\nDetectron2 models accept inputs as a list of dictionaries, each corresponding to an image. The required keys in these dictionaries depend on the model type and its mode (training or evaluation). Typically, for inference, the dictionary includes:\n\n“image”`: A tensor representing the image in (C, H, W) format.\n“height”and“width”` (optional): Desired output dimensions.\n\nDuring training, additional keys like \"instances\" (which contains ground truth annotations) are necessary. The model outputs are also structured as dictionaries, with formats varying based on the specific task (e.g., bounding box detection, segmentation).\n\n\nCustomizing and Extending Models\nDetectron2’s modular design allows for extensive customization. You can modify existing components or add new ones to tailor the models to your requirements. A common approach is to register new components, such as a custom backbone:\nfrom detectron2.modeling import BACKBONE_REGISTRY, Backbone, ShapeSpec\nimport torch.nn as nn\n\n@BACKBONE_REGISTRY.register()\nclass CustomBackbone(Backbone):\n    def __init__(self, cfg, input_shape):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=16, padding=3)\n\n    def forward(self, image):\n        return {\"conv1\": self.conv1(image)}\n\n    def output_shape(self):\n        return {\"conv1\": ShapeSpec(channels=64, stride=16)}\nAfter defining and registering your custom component, you can specify it in your configuration:\ncfg.MODEL.BACKBONE.NAME = \"CustomBackbone\"\nmodel = build_model(cfg)\nThis method allows you to integrate new architectures or functionalities seamlessly. For more detailed instructions on writing and registering new model components, see the official tutorial on writing models.\n\n\nConstructing Models with Explicit Arguments\nWhile configuration files offer a convenient way to build models, there are scenarios where you might need more control. In such cases, you can construct model components with explicit arguments in your code. For example, to use a custom ROI head:\nfrom detectron2.modeling.roi_heads import StandardROIHeads\n\nclass CustomROIHeads(StandardROIHeads):\n    def __init__(self, cfg, input_shape):\n        super().__init__(cfg, input_shape)\n        # Customize as needed\nThen, integrate it into your model:\nfrom detectron2.modeling import build_model\n\ncfg = ...  # your configuration\nmodel = build_model(cfg, roi_heads=CustomROIHeads(cfg, input_shape))",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#training-system",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#training-system",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.5 Training system",
    "text": "13.5 Training system\n\nHow Training Works in Detectron2\nAt its core, training a model in Detectron2 involves:\n\nLoading a dataset: Preparing images and annotations.\n\nConfiguring the model: Defining architecture, hyperparameters, and other settings.\n\nTraining with a Trainer: Using built-in tools or writing a custom loop.\n\nEvaluating performance: Running inference and analyzing metrics.\n\nDetectron2 provides two main ways to handle training:\n\nUsing a pre-built Trainer (Recommended for most users)\n\nWriting a custom training loop (For advanced customization)\n\n\n\nUsing the Default Trainer (Easy & Standard Approach)\nIf you want to train a model with minimal setup, Detectron2 offers DefaultTrainer, which automates most of the process.\nSteps to Train a Model with DefaultTrainer\nStep 1: Prepare Your Configuration\nModify a base config file and set paths, batch sizes, and hyperparameters.\nfrom detectron2.config import get_cfg\n\ncfg = get_cfg()\ncfg.merge_from_file(\"path/to/config.py\")\ncfg.DATASETS.TRAIN = (\"my_dataset_train\",)\ncfg.DATASETS.TEST = (\"my_dataset_val\",)\ncfg.OUTPUT_DIR = \"./output\"\ncfg.SOLVER.MAX_ITER = 5000  # Number of training iterations\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # Adjust for your dataset\nStep 2: Train Using DefaultTrainer\nfrom detectron2.engine import DefaultTrainer\n\ntrainer = DefaultTrainer(cfg)\ntrainer.resume_or_load(resume=False)\ntrainer.train()\nThis will handle data loading, logging, checkpointing, and evaluation automatically.\nStep 3: Evaluate the Model\nTo test your model on a validation dataset:\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\n\nevaluator = COCOEvaluator(\"my_dataset_val\", cfg, False, output_dir=\"./output/\")\nval_loader = build_detection_test_loader(cfg, \"my_dataset_val\")\nprint(inference_on_dataset(trainer.model, val_loader, evaluator))\n\n\nCustomizing Training (For Advanced Users)\nIf DefaultTrainer doesn’t fit your needs, you can modify it or write a fully custom training loop.\nOption 1: Overriding DefaultTrainer Methods\nFor example, if you need a custom evaluation function:\nclass MyTrainer(DefaultTrainer):\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n\ntrainer = MyTrainer(cfg)\ntrainer.train()\nOption 2: Using Hooks for Extra Functionality\nHooks allow you to add logic at specific points during training. For example, printing a message every 100 iterations:\nfrom detectron2.engine import HookBase\n\nclass PrintIterationHook(HookBase):\n    def after_step(self):\n        if self.trainer.iter % 100 == 0:\n            print(f\"Iteration {self.trainer.iter}\")\n\ntrainer = DefaultTrainer(cfg)\ntrainer.register_hooks([PrintIterationHook()])\ntrainer.train()\nOption 3: Writing a Fully Custom Training Loop\nFor full control, you can write your own loop instead of using DefaultTrainer:\nfrom detectron2.engine import SimpleTrainer\nfrom detectron2.solver import build_optimizer\n\nmodel = build_model(cfg)\noptimizer = build_optimizer(cfg, model)\ndata_loader = build_detection_train_loader(cfg)\n\ntrainer = SimpleTrainer(model, data_loader, optimizer)\ntrainer.train()\nThis is useful when experimenting with novel training strategies.\n\n\nLogging and Monitoring Training Progress\nDetectron2 provides event storage to track training metrics like loss and accuracy. You can log custom metrics inside your model:\nfrom detectron2.utils.events import get_event_storage\n\nstorage = get_event_storage()\nstorage.put_scalar(\"my_custom_metric\", value)\nFor visualization, you can use TensorBoard:\ntensorboard --logdir ./output\nThis helps you track training progress interactively.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#evaluation-system",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#evaluation-system",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.6 Evaluation system",
    "text": "13.6 Evaluation system\nIn Detectron2, evaluation is managed through the DatasetEvaluator interface. This interface processes pairs of inputs and outputs, aggregating results to compute performance metrics. Detectron2 offers several built-in evaluators tailored to standard datasets like COCO and LVIS. For instance, the COCOEvaluator computes metrics such as Average Precision (AP) for object detection, instance segmentation, and keypoint detection. Similarly, the SemSegEvaluator is designed for semantic segmentation tasks.\n\nUtilizing Built-in Evaluators\nTo evaluate a model using Detectron2’s built-in evaluators, you can employ the inference_on_dataset function. This function runs the model on all inputs from a specified data loader and processes the outputs using the chosen evaluators.\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\n\n# Initialize the evaluator\nevaluator = COCOEvaluator(\"your_dataset_name\", cfg, False, output_dir=\"./output/\")\n\n# Create a data loader for the test dataset\nval_loader = build_detection_test_loader(cfg, \"your_dataset_name\")\n\n# Perform inference and evaluation\neval_results = inference_on_dataset(model, val_loader, evaluator)\nIn this script, replace \"your_dataset_name\" with the name of your dataset as registered in Detectron2. The COCOEvaluator is initialized with the dataset name and configuration (cfg). The build_detection_test_loader function creates a data loader for the test dataset. Finally, inference_on_dataset runs the model on the test data and evaluates the results using the evaluator.\n\n\nCreating Custom Evaluators\nWhile Detectron2’s built-in evaluators cover many standard scenarios, you might encounter situations where custom evaluation logic is necessary. In such cases, you can implement your own evaluator by extending the DatasetEvaluator class. This involves defining methods to reset the evaluator, process each batch of inputs and outputs, and compute the final evaluation metrics. For example, to create an evaluator that counts the total number of detected instances across the validation set:\nfrom detectron2.evaluation import DatasetEvaluator\n\nclass InstanceCounter(DatasetEvaluator):\n    def reset(self):\n        self.count = 0\n\n    def process(self, inputs, outputs):\n        for output in outputs:\n            self.count += len(output[\"instances\"])\n\n    def evaluate(self):\n        return {\"total_instances\": self.count}\nIn this custom evaluator, the reset method initializes the count, the process method updates the count based on the number of instances in each output, and the evaluate method returns the total count. You can integrate this custom evaluator into your evaluation pipeline alongside built-in evaluators:\nfrom detectron2.evaluation import DatasetEvaluators\n\n# Combine multiple evaluators\nevaluator = DatasetEvaluators([COCOEvaluator(\"your_dataset_name\", cfg, False), InstanceCounter()])\n\n# Perform inference and evaluation\neval_results = inference_on_dataset(model, val_loader, evaluator)\nBy combining evaluators, you can perform comprehensive evaluations in a single pass over the dataset, which is efficient and convenient.\nEvaluating on Custom Datasets\nWhen working with custom datasets, it’s essential to ensure they adhere to Detectron2’s standard dataset format. This compatibility allows you to leverage existing evaluators like COCOEvaluator for your custom data. If your dataset follows the COCO format, you can use the COCOEvaluator directly. Otherwise, you might need to implement a custom evaluator tailored to your dataset’s structure and evaluation criteria.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html",
    "href": "sections/ai-workflows-and-mlops.html",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "",
    "text": "Instructors\nBen Galewsky, Sr. Research Software Engineer National Center for Supercomputing Applications (NCSA) University of Illinois Urbana-Champaign",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#overview",
    "href": "sections/ai-workflows-and-mlops.html#overview",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "Overview",
    "text": "Overview\nMachine learning models have become a vital tool for most branches of science. The process and tools for training these models on the lab’s desktop is often fragile, slow, and not reproducible. In this workshop, we will introduce the concept of MLOps, which is a set of practices that aims to streamline the process of developing, training, and deploying machine learning models. We will use the popular open source MLOps tool, MLflow, to demonstrate how to track experiments, package code, and deploy models. We will also introduce Garden, a tool that allows researchers to publish ML Models as citable objects.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#outline",
    "href": "sections/ai-workflows-and-mlops.html#outline",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to MLOps\nIntroduction to MLflow\nTracking experiments with MLflow\nPackaging code with MLflow\nDeploying models with MLflow\nPublishing models with Garden",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#three-challenges-for-ml-in-research",
    "href": "sections/ai-workflows-and-mlops.html#three-challenges-for-ml-in-research",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.1 Three Challenges for ML in Research",
    "text": "14.1 Three Challenges for ML in Research\n\nTraining productivity\nTraining reproducibility\nModel citability",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#introcution-to-mlops",
    "href": "sections/ai-workflows-and-mlops.html#introcution-to-mlops",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.2 Introcution to MLOps",
    "text": "14.2 Introcution to MLOps\nMachine Learning Operations (MLOps) is a set of practices that combines Machine Learning, DevOps, and Data Engineering to reliably and efficiently deploy and maintain ML models in production. Just as DevOps revolutionized software development by streamlining the bridge between development and operations, MLOps brings similar principles to machine learning systems.\nThink of MLOps as the infrastructure and practices that transform ML projects from experimental notebooks into robust, production-ready systems that deliver real reproducible scientific value.\n\nWhy Researchers Need MLOps\nAs a researcher, you’ve likely experienced the following challenges:\n\nInability to harness computing resources to robustly search hyperparameter space\nDifficulty reproducing results from six months ago\nRetraining is too painful to consider creating better models after new data becomes available\nTracking experiments becomes unwieldy as projects grow\nCollaboration among researchers in your group is difficult\nAdvisors have little visibility into students’ work\n\nMLOps addresses these pain points by providing:\n\n1. Reproducibility\n\nVersion control for data, code, and models\nAutomated documentation of experiments\nContainerization for consistent environments\n\n\n\n2. Automation\n\nAutomated training pipelines\nContinuous integration and deployment (CI/CD)\nAutomated testing and validation\n\n\n\n3. Production Monitoring\n\nReal-time performance monitoring\nData drift detection\nAutomated model retraining triggers\n\n\n\n4. Governance\n\nModel lineage tracking\n\n\nMLOps isn’t just another buzzword—it’s a crucial evolution in how we develop and deploy ML systems. As models become more complex and requirements for reliability increase, MLOps practices become essential for successful ML implementations.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#mlflow-a-comprehensive-platform-for-the-ml-lifecycle",
    "href": "sections/ai-workflows-and-mlops.html#mlflow-a-comprehensive-platform-for-the-ml-lifecycle",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.3 MLflow: A Comprehensive Platform for the ML Lifecycle",
    "text": "14.3 MLflow: A Comprehensive Platform for the ML Lifecycle\n\nWhat is MLflow?\nMLflow is an open-source platform designed to manage the end-to-end machine learning lifecycle. Created by Databricks, it provides a unified set of tools that address the core challenges in developing, training, and deploying machine learning models. MLflow is language-agnostic and can work with any ML library, algorithm, or deployment tool.\n\n\nThe PDG MLFlow Tracking Server\nThe Permafrost Discovery Gateway project has a shared MLFlow instance that you can use for your arctic research:\nhttps://pdg.mflow.software.ncsa.illinois.edu\n\n\nKey Components of MLflow\nMLFlow consists of three main components:\n\n\n\n\n\n\nTracking\n\n\n\n\n\nThe tracking server of MLFlow allows researchers to log and compare model parameters, metrics, and artifacts across multiple runs. With MLFlow’s tracking features, users can record hyperparameters, evaluation metrics, model versions, and even source code, making it easier to reproduce results and collaborate with team members. The platform provides a user-friendly interface to visualize and compare different experiments, helping practitioners identify the most promising models and configurations. Additionally, MLFlow’s tracking capabilities integrate seamlessly with popular ML frameworks, enabling users to incorporate experiment logging into their existing workflows with minimal code changes. This comprehensive approach to tracking enhances model development efficiency and facilitates better decision-making throughout the machine learning process.\n\nKey Concepts in Tracking:\n\nParameters: key-value inputs to your code\nMetrics: numeric values (can update over time)\nArtifacts: arbitrary files, including data, models and plots\nSource: training code that ran\nVersion: version of the training code\nTags and Notes: any additional information\n\nimport mlflow\n\nwith mlflow.start_run():\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_metric(\"accuracy\", 0.85)\n    mlflow.log_artifact(\"model.pkl\")\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\n\n\n\nMLFlow Projects provide a standardized format for packaging and organizing machine learning code to make it reproducible and reusable across different environments. A Project in MLFlow is essentially a directory containing code, dependencies, and an MLProject file that specifies the project’s entry points and environment requirements. This structure enables data scientists to share their work with teammates who can reliably execute the same code, regardless of their local setup. The MLProject file can define multiple entry points, each specifying its parameters and command to run, making it flexible for different use cases within the same project. MLFlow supports various environments for project execution, including conda environments, Docker containers, and system environments, ensuring consistency across different platforms. This standardization not only improves collaboration but also simplifies the deployment process, as projects can be easily versioned and moved between development and production environments.\n\nKey Concepts in Projects\n\nPackaging format for reproducible ML runs\n\nAny code folder or GitHub repository\nOptional MLproject file with project configuration\n\nDefines dependencies for reproducibility\n\nConda (+ R, Docker, …) dependencies can be specified in MLproject\nReproducible in (almost) any environment\n\nExecution API for running projects\n\nCLI / Python / R / Java\nSupports local and remote execution\n\n\nname: myproject\n\npython_env: python_env.yaml\n\nentry_points:\n  main:\n    parameters:\n      learning_rate: {type: float, default: 0.01}\n    command: \"python train.py --lr {learning_rate}\"\n\n\n\n\n\n\n\n\n\n\nModels\n\n\n\n\n\nThe Models component of MLFlow provides a standardized way to package and deploy machine learning models across different platforms and serving environments. MLFlow’s model format includes all the code and dependencies required to run the model, making it highly portable and easy to share. The platform supports a variety of popular ML frameworks like scikit-learn, TensorFlow, and PyTorch, allowing models to be saved in a framework-agnostic format using the MLFlow Model Registry. This registry acts as a centralized repository where teams can version their models, transition them through different stages (like staging and production), and maintain a clear lineage of model iterations. MLFlow also provides built-in deployment capabilities to various serving platforms such as Kubernetes, Amazon SageMaker, and Azure ML, streamlining the process of moving models from experimentation to production. Additionally, MLFlow’s model serving feature allows for quick local deployment of models as REST endpoints, enabling easy testing and integration with other applications.\n\nKey Concepts in Models\n\nPackaging format for ML Models\n\nAny directory with MLmodel file\n\nDefines dependencies for reproducibility\n\nConda environment can be specified in MLmodel configuration\n\nModel creation utilities\n\nSave models from any framework in MLflow format\n\nDeployment APIs\n\nCLI / Python / R / Java\n\nModel Versioning and Lifecycle -Model repository",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#scaling-up-training-with-mlflow-slurm",
    "href": "sections/ai-workflows-and-mlops.html#scaling-up-training-with-mlflow-slurm",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.4 Scaling Up Training With MLFlow-Slurm",
    "text": "14.4 Scaling Up Training With MLFlow-Slurm\nWith your project defined in MLProject file, it’s easy to scale up your workflows by launching them onto a cluster.\nThere is a plugin for MLFlow developed by NCSA called mlflow-slurm\nTo use, you have to create a json file that tells the plugin how to configure slurm jobs.\n{\n  \"partition\": \"cpu\",\n  \"account\": \"bbmi-delta-cpu\",\n  \"mem\": \"16g\",\n  \"modules\": [\"anaconda3_cpu\"]\n}\nWith this in place you can launch a training run on your cluster with the command\nmlflow run --backend slurm \\\n          --backend-config slurm_config.json \\\n          examples/sklearn_elasticnet_wine",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#how-mlflow-solves-common-mlops-challenges",
    "href": "sections/ai-workflows-and-mlops.html#how-mlflow-solves-common-mlops-challenges",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.5 How MLflow Solves Common MLOps Challenges",
    "text": "14.5 How MLflow Solves Common MLOps Challenges\n\nTraining productivity\n\nTrack impact of hyperparameter and code changes on model quality\nRun hyperparameter sweeps and find best run\nSwitch between desktop to supercomputer\n\n\n\nTraining reproducibility\n\nEnforced use of reproducible runtime environments\nTrace models back to specific runs\n\n\n\nModel citeability\n\nPublish models to repository\nVersioning and lifecycle events",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#hands-on-tutorial",
    "href": "sections/ai-workflows-and-mlops.html#hands-on-tutorial",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.6 Hands-On Tutorial",
    "text": "14.6 Hands-On Tutorial\nWe will be using a GPU powered JuypterHub provided by NCSA. Connection instructions will be provided in the classroom.\nThe example code is in the Cyber2A-RTS-ToyModel repo. It has a notebook along with some libraries to keep the notebook focused on the MLOps aspects of the code.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#sharing-models",
    "href": "sections/ai-workflows-and-mlops.html#sharing-models",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.7 Sharing Models",
    "text": "14.7 Sharing Models\nNow that we have trained and validated a model we will want to first of all, share it with other members of our research group.\n\nMLFlow Model Repository\nExpert users within our research group will have access to the MLFlow tracking server and model repository. You can test the model as an artifact from an existing run. Once you are satisfied with its performance, you can publish it to the MLFlow model repository with the Register Model button on the tracking server.\nPublished models are given sequential version numbers so colleauges can rely on a stable model for their research. Models in the repository can also follow a lifecycle with MLFlow model aliases. Members of the research group who are not activly involved in model development may just want to use the current best model. The researcher who is training the model can decide which version others should use. MLFlow allows you to pull down the model symbolically.\nmlflow.pyfunc.load_model(\"models:/rts@prod\")\n\n\nModels as Citable Objects\nPublishing your MLProject and training code to a Git repo and making the data publicly readable through a data repository is a way for others to reproduce your models. However, to make your work truly reusable it is better to publish the weights of your trained model in a way that is findable, citable, and usable.\nAt a minimum, you should publish your model on Hugging Face. You can include a README and an notebook demonstrating how to use the model. HuggingFace allows you to mint a DOI that you can cite in your publications.\nHere’s an example with the RTS model:\n\nhttps://huggingface.co/bengal1/RTS/tree/main\n\nA new facility called Garden takes this a step further. Your model is hosted as an endpoint which is a hosted function-as-a-service which allows anyone to perform inference with your model without needing to install anything.\nOur example model is hosted at 10.26311/x49j-2v19\nYou can run a remote inference:\nfrom garden_ai import GardenClient\ngarden_client = GardenClient()\n\nrts_garden = garden_client.get_garden('10.26311/5fb6-f950')\nimage_url=\"https://github.com/cyber2a/Cyber2A-RTS-ToyModel/blob/main/data/images/valtest_yg_055.jpg?raw=true\"\npred = rts_garden.identify_rts(image_url)",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#reference",
    "href": "sections/ai-workflows-and-mlops.html#reference",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.8 Reference",
    "text": "14.8 Reference\n\nMLflow\nGarden",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html",
    "href": "sections/hands-on-lab-ai-workflows.html",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "",
    "text": "Goal\nThis lab section will give students experience publishing MLModels as citeable objects that can be easily found and reused by other researchers. Students will learn how to package a model up as an executable function and publish it to garden.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#prerequisites",
    "href": "sections/hands-on-lab-ai-workflows.html#prerequisites",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Prerequisites",
    "text": "Prerequisites\nWe will be using the Modal service to create and host executable python functions. In order to use this you will need on your laptop:\n\nPython 3.9 or higher\nA free Globus account (https://www.globus.org/)\nA free modal account (https://www.modal.com/)\n\nYou will need to let the instructor know the email address associated with your globus account to be added to the garden publishers group.\nOnce you are added to the garden publishers group, you should be able to see the Garden Model Upload Form",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#assumptions",
    "href": "sections/hands-on-lab-ai-workflows.html#assumptions",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Assumptions",
    "text": "Assumptions\nDuring this lab we will use some pre-trained models to demonstrate the process of packaging and publishing a model. You are also welcome to use your own models if you have them.\nIn order for your model to be published to garden, it must be:\n\nOpen and already trained\n\nThe code is open source and in a package or a public repository\nYour model weights are stored somewhere public (or you can put them somewhere public)\n\nYour model isn’t massive\n\nYour model can fit in one GPU’s memory\nYour model weights are on the order of 10 GB or less",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#step-1-hello-garden",
    "href": "sections/hands-on-lab-ai-workflows.html#step-1-hello-garden",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Step 1: Hello, Garden!",
    "text": "Step 1: Hello, Garden!\nAll of the examples in this lab are available in the workshop repository. Clone the repository to your local machine:\ngit clone https://github.com/Garden-AI/uploadathon.git\nIn this step we will upload an existing trivial model to garden and execute it from a Google Colab notebook.\nUse the hello_garden.py from the example repo and upload it from the garden form.\nKeep it a “test garden” and don’t worry too hard about the metadata\nOnce you’ve created it, go make sure you can invoke it remotely. Try out this google colab notebook to see it in action.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#step-2-learn-how-to-iterate-on-modal-apps",
    "href": "sections/hands-on-lab-ai-workflows.html#step-2-learn-how-to-iterate-on-modal-apps",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Step 2: Learn how to iterate on Modal apps",
    "text": "Step 2: Learn how to iterate on Modal apps\nIn this step we will learn more about creating and debugging a modal app.\nInstall the modal package:\npip install modal\nDownload needs_a_tweak.py and try running it with\nmodal run needs_a_tweak.py\nEdit and re-run til it works the way you want it to.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#step-3-stage-and-execute-a-model",
    "href": "sections/hands-on-lab-ai-workflows.html#step-3-stage-and-execute-a-model",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Step 3: Stage and execute a model",
    "text": "Step 3: Stage and execute a model\nNow we will create a modal function that has your model’s weights baked into the container so that our functions don’t need to download the model every time they run.\nYour model weights will need to live somewhere publicly accessible. Garden can’t pull them from your machine.\n\nSee git_model_staging.py for an example of how to use weights hosted in GitHub or Hugging Face.\nSee figshare_model_staging.py for an example of how to pull weights from a FigShare download link.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#step-4-run-a-real-model",
    "href": "sections/hands-on-lab-ai-workflows.html#step-4-run-a-real-model",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Step 4: Run a “Real Model”",
    "text": "Step 4: Run a “Real Model”\nTake a look at the Retrograde Thaw Slump model. You can execute the model from the google colab notebook to see it in action.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#next-steps",
    "href": "sections/hands-on-lab-ai-workflows.html#next-steps",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Next Steps",
    "text": "Next Steps\nNow that you have successfully published a model to garden, you can share the link with your colleagues and collaborators. You can also use the model in your own research projects by invoking it from a Google Colab notebook or other python environment.\nThe Garden project provides $5/month in free compute credits to all users. This can go a surprisingly long way, however if you are planning to do high throughput infrence, then you may need to BYOM (Bring Your Own Modal). You can take the python function you uploaded to garden and directly uplaod to Modal and run it from your own account.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html",
    "href": "sections/foundation-models.html",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "",
    "text": "16.1 Overview\nFoundation models (FM) are deep learning models trained on massive raw unlabelled datasets usually through self-supervised learning. FMs enable today’s data scientists to use them as the base and fine-tune using domain specific data to obtain models that can handle a wide range of tasks (language, vision, reasoning etc.). In this chapter, we provide an introduction to FMs, its history, evolution, and go through its key features and categories, and a few examples. We also briefly discuss how foundation models work. This chapter will be a precursor to the hands-on session that follows on the same topic.\nIn this session, we take a closer look at what constitutes a foundation model, a few examples, and some basic principles around how it works.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#overview",
    "href": "sections/foundation-models.html#overview",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "",
    "text": "Fig : Image source- 2021 paper on foundation models by Stanford researchers [1]",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#outline",
    "href": "sections/foundation-models.html#outline",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "16.2 Outline",
    "text": "16.2 Outline\n\nOverview of foundation models\nTypes of foundation models\nArchitecture\nSegment Anything Model (SAM 2)\nRetrieval Augmented Generation",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#introduction",
    "href": "sections/foundation-models.html#introduction",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "16.3 Introduction",
    "text": "16.3 Introduction\n\n16.3.1 Traditional ML vs Deep Learning vs Foundation Models\nTraditional machine learning involves algorithms that learn patterns from structured data. Techniques like decision trees, support vector machines, and linear regression fall under this category. These methods often require feature engineering, where domain knowledge is used to select and transform input features to improve model performance. Traditional machine learning excels in scenarios with limited data and interpretable results.\nDeep learning is a subset of machine learning that employs neural networks with multiple layers (hence “deep”). These models automatically learn features from raw data, making them particularly powerful for complex tasks like image and speech recognition. Deep learning excels with large datasets and can capture intricate patterns but often requires significant computational resources and can be harder to interpret compared to traditional methods.\nFoundation models, such as GPT and BERT, represent a new paradigm in AI. These large-scale models are pre-trained on vast amounts of data and can be fine-tuned for specific tasks with minimal additional training. Earlier neural networks were narrowly tuned for specific tasks. With a little fine-tuning, foundation models can handle jobs from translating text to analyzing medical images. Foundation models generally learn from unlabeled datasets, saving the time and expense of manually describing each item in massive collections. Foundation models leverage transfer learning, allowing them to generalize across different tasks more effectively than traditional machine learning and deep learning models.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#foundation-models",
    "href": "sections/foundation-models.html#foundation-models",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "16.4 Foundation Models",
    "text": "16.4 Foundation Models\nFoundation models, introduced in 2021 by Standford Researchers [1], are characterized by their enormous neural networks trained on vast datasets through self-supervised learning. These models serves as a “foundation” on which many task-specific models can be built by adaptation. Their capabilities improves with more data, requiring substantial computational power for training. These models can be adapted to various downstream tasks and are designed for reuse, leveraging transfer learning to enhance performance across different applications.\n\n\n\n\n\nFig : 2021 paper on foundation models by Stanford researchers [1]\n\n\nWith the start of availability of big data for training, evidence showed that performance improves with size. The field came to the conclusion that scale matters, and with the right model architecture, intelligence comes with large-scale data.\nHere’s a few examples of foundation models and their parameter count:\n\nCLIP [2] - 63 million parameters\nBERT [3] - 345 million parameters\nGPT-3 [3] - 175 billion parameters\n\nWikipedia consists of only 3% of its training data\n\nGPT-4 [4] - 1.8 trillion parameters\n\n\n\n\nFig : Growth in compute power. (Source: GPT-3 paper [3])",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#types-of-foundation-models",
    "href": "sections/foundation-models.html#types-of-foundation-models",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "16.5 Types of foundation models",
    "text": "16.5 Types of foundation models\nFoundation models can be classified on the basis of its modality and its underlying architecture.\n\n\n\nCriteria: Modality\nCriteria: Architecture\n\n\n\n\nLanguage Models\nTransformer Models\n\n\nVision Models\nGenerative Models\n\n\nMultimodal Models\nDiffusion Models\n\n\n\n\n16.5.1 Types of foundation models (Modality)\n\n16.5.1.1 Language models\nLanguage models are trained for natural language processing tasks. The primary training objective for LLMs is often next-token prediction, where the model learns to predict the next word in a sequence given the preceding context. This is achieved through a vast amount of text data, enabling the model to learn grammar, facts, and even some reasoning patterns. LLMs tend to be good at various NLP related tasks, like translation, conversational AI, sentiment analysis, content summarization etc., to name a few.\nHere’s some examples of language models:\n\nGPT-3\nGPT-4\nLlama 3.2 [5]\n\n\n\n16.5.1.2 Vision models\nVision models are trained for computer vision tasks. The primary training objective of vision models is to effectively learn representations that enable accurate predictions or useful transformations based on visual data. Vision models tend to be good at tasks like object detection, segmentation, facial recognition, etc.\nHere’s some examples of vision models:\n\nGPT-4-turbo\nSAM [6]\nCLIP [5]\nSwin-transformer [7]\n\n\n\n16.5.1.3 Multimodal models\nMultimodal models are designed to process and understand multiple types of data modalities, such as text, images, audio, and more. These models can handle various data types simultaneously, allowing them to learn relationships and correlations between different forms of information (e.g., associating text descriptions with images). By training on datasets that include multiple modalities, multimodal foundation models learn to create a unified representation space where different types of data can be compared and processed together. This often involves shared architectures for encoding different modalities. These models can often perform well on tasks they haven’t been specifically trained on, thanks to their ability to leverage learned relationships across modalities. This makes them versatile for applications in various domains. Many multimodal models, like CLIP and DALL-E, use contrastive learning to improve their understanding of how different modalities relate. They aim to maximize similarity between paired data (e.g., an image and its caption) while minimizing similarity between unrelated pairs. These models can often perform well on tasks they haven’t been specifically trained on, thanks to their ability to leverage learned relationships across modalities. This makes them versatile for applications in various domains. Multimodal foundation models are used in diverse areas such as image and video captioning, visual question answering, cross-modal retrieval, and interactive AI systems that require understanding and generating multiple types of content.\nHere’s some examples of multimodal foundation models:\n\nGPT-4o\nDALL-E [8]\nCLIP [5]\nSora [9]\nGemini [10]\n\n\n\n\n16.5.2 Types of foundation models (Architecture)\n\n16.5.2.1 Transformer models\nIntroduced in 2017 by the paper “Attention is all you need” [11], the transformer architecture revolutionized NLP by enabling models to efficiently capture complex relationships in data without the limitations of recurrence. This architecture is known for its ability to handle sequential data efficiently. Its parallel processing capabilities and scalability have made it a foundational model for many state-of-the-art systems in various domains, including image processing and speech recognition. Checkout “The Illustrated Transformer” (blog post)[https://jalammar.github.io/illustrated-transformer/] for a detailed overview of the transformer architecture.\n\n\n\nFig : Transformer architecture\n\n\n\n16.5.2.1.1 Attention Mechanism\nAttention is, to some extent, motivated by how we pay visual attention to different regions of an image or correlate words in one sentence [12]. We can explain the relationship between words in one sentence or close context. When we see “eating”, we expect to encounter a food word very soon. The color term describes the food, but probably not so much with “eating” directly.\n\n\n\nFig : One word attends to other words in the same sentence differently\n\n\nCheck out Lilian Weng’s blog post [12] and MIT class on deep learning for detailed overview of attention mechanism.\n\n\n16.5.2.1.2 Key components of transformer architecture:\n\nSelf-Attention Mechanism:\n\n\nPurpose: Allows the model to weigh the importance of different words in a sequence relative to each other, capturing dependencies regardless of their distance.\nFunction: For each input token, self-attention computes a set of attention scores that determine how much focus to place on other tokens. This is done using three vectors: Query (Q), Key (K), and Value (V).\nCalculation: The attention score is computed as a dot product of Q and K, followed by a softmax operation to normalize it. The output is a weighted sum of the V vectors based on these scores.\n\nIn the example below, the self-attention mechanism enables us to learn the correlation between the current words and the previous part of the sentence.\n\n\n\nFig : The current word is in red and the size of the blue shade indicates the activation level [13]\n\n\n\nPositional Encoding:\n\n\nPurpose: Since transformers do not have a built-in notion of sequential order, positional encodings are added to the input embeddings to provide information about the position of tokens in the sequence.\nImplementation: Positional encodings use sine and cosine functions of different frequencies to generate unique values for each position.\n\n\nMulti-Head Attention:\n\n\nFunction: Instead of having a single set of attention weights, the transformer employs multiple attention heads, each learning different aspects of the relationships between tokens.\nProcess: The input is split into multiple sub-vectors, and self-attention is applied in parallel. The outputs of each head are concatenated and linearly transformed.\n\n\nFeed-Forward Networks:\n\n\nPurpose: After the multi-head attention step, each token’s representation is passed through a feed-forward neural network, which applies transformations independently to each position.\nStructure: Typically consists of two linear transformations with a ReLU activation in between.\n\n\nLayer Normalization and Residual Connections:\n\n\nLayer Normalization: Applied to stabilize and speed up training by normalizing the outputs of each layer.\nResidual Connections: Shortcuts are added around sub-layers (e.g., attention and feed-forward) to facilitate the flow of gradients during training, helping to mitigate the vanishing gradient problem.\n\n\nStacking Layers:\n\n\nTransformers consist of multiple layers of multi-head attention and feed-forward networks, allowing for deep representations of the input data.\n\n\nOutput Layer:\n\n\nFor tasks like language modeling or translation, the final layer typically uses a linear transformation followed by a softmax activation to predict the next token or class.\n\nThere are more than 50 major transformer models [14]. The transformer architecture is versatile and can be configured in different ways. The transformer architecture can support both auto-regressive and non-auto-regressive configurations depending on how the self-attention mechanism is applied and how the model is trained.\n\nAuto-Regressive Models: In an auto-regressive setup, like the original GPT (Generative Pre-trained Transformer), the model generates text one token at a time. During training, it predicts the next token in a sequence based on the previously generated tokens, conditioning on all prior context. This means that at each step, the model only attends to the tokens that come before the current position, ensuring that future tokens do not influence the prediction.\nNon-Auto-Regressive Models: Other models, like BERT (Bidirectional Encoder Representations from Transformers) [3], are designed to be non-auto-regressive. BERT processes the entire input sequence simultaneously and is trained using masked language modeling, where some tokens in the input are masked, and the model learns to predict them based on the surrounding context.\n\nGPT-3 and CLIP models utilize transformers as the underlying architecture.\n\n\n\n16.5.2.2 Generative-Adversarial models\nIntroduced in 2014, Generative Adversarial Networks (GANs) [15] involves two neural networks (generator-discriminator network pair) contest with each other in the form of a zero-sum game, where one agent’s gain is another agent’s loss. Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics.\n\n\n\nFig : GAN basic architecture\n\n\nIn a GAN,\n\nthe generator learns to generate plausible data. The generated instances become negative training examples for the discriminator.\nThe discriminator learns to distinguish the generator’s fake data from real data. The discriminator penalizes the generator for producing implausible results.\n\nWhen training begins, the generator produces obviously fake data, and the discriminator quickly learns to tell that it’s fake:\n\n\n\nFig : GAN training - early phase. Image source: Google developers blog\n\n\nAs training progresses, the generator gets closer to producing output that can fool the discriminator:\n\n\n\nFig : GAN training - mid phase\n\n\nFinally, if generator training goes well, the discriminator gets worse at telling the difference between real and fake. It starts to classify fake data as real, and its accuracy decreases. The training procedure for generator is to maximise the probability of discriminator making a mistake.\n\n\n\nFig : GAN training complete\n\n\nHere’s a picture of the whole system:\n\n\n\nFig : GAN architecture\n\n\nA disadvantage of GAN is potentially unstable training and less diversity in generation due to their adversarial training nature. StyleGAN [16] and BigGAN [17] are example of models that utilize GAN as the underlying architecture.\n\n\n16.5.2.3 Diffusion models\nDiffusion models, introduced in 2020 [18], are inspired by non-equilibrium thermodynamics. They define a Markov chain of diffusion steps to slowly add random noise to data and then learn to reverse the diffusion process to construct desired data samples from the noise [19].\n\n16.5.2.3.1 Key components of diffusion models\n\nForward Diffusion Process:\n\n\nThe forward process gradually adds Gaussian noise to the training data over a series of time steps. This process effectively transforms the original data into pure noise.\n\n\nReverse Diffusion Process:\n\n\nThe reverse process aims to denoise the noisy data back into a sample from the data distribution. This process is learned through a neural network.\nAt each time step, the model predicts the mean and variance of the distribution of the previous step conditioned on the current noisy data. The network outputs parameters that help in gradually removing the noise.\n\n\nTraining Objective:\n\n\nThe model is trained to minimize the difference between the predicted clean data and the actual data at each step of the diffusion process. This is often done using a mean squared error (MSE) loss between the predicted noise and the actual noise added during the forward process.\n\n\nSampling:\n\n\nTo generate new samples, the process starts with pure noise and applies the learned reverse diffusion process iteratively. Over multiple time steps, the model denoises the input until it resembles a sample from the training distribution.\n\n\n\n\nFig : Training a diffusion model. Image source : Lilweng’s blog\n\n\nDiffusion models can generate high-resolution and diverse images, often outperforming GANs in certain tasks. They are generally more stable to train compared to GANs, as they do not rely on adversarial training dynamics.\nStable-diffusion [20], DALL-E [8], Sora are some of the most common models utilizing diffusion architecture.\n\n\n\n\n16.5.3 Foundation Models - Applications\nHaving explored the foundational principles and capabilities of foundation models, we can now delve into specific applications that leverage their power. Two prominent techniques that build upon the capabilities of these models are Segment Anything Model (SAM) and Retrieval-Augmented Generation (RAG).",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#segment-anything-model",
    "href": "sections/foundation-models.html#segment-anything-model",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "16.6 Segment Anything Model",
    "text": "16.6 Segment Anything Model\nSegment Anything Model (SAM) is a foundation model for the Promptable Visual Segmentation (PVS) task. PVS inspired from prompt engineering in NLP that user prompts can be a powerful tool for pre-training foundation models and downstream tasks. It is developed by the Fundamental AI Research (FAIR) team at Meta [6]. SAM is a simple and flexible framework that can segment any object in an image by providing a combination of one or more prompts - namely points, bounding boxes, or other segmentation masks. SAM is based on the transformer architecture and is trained on a large-scale dataset of images and their corresponding segmentation masks.\nThe latest version of SAM, SAM 2, can segment images and videos and uses a unified architecture for both tasks [21]. It is designed to handle complex scenes with multiple objects and can generate high-quality segmentations with minimal user input. The model can be used for various applications, including image editing, object detection, and video analysis.\nSince large-scale datasets for segmentation are unavailable, the research team created a data engine to generate segmentation masks, which were then manually annotated when developing SAM. The model was trained on diverse images to improve its generalization capabilities. This model-in-the-loop self-supervised training created two datasets: SA-1B containing 1B segmentation masks from about 11M privacy preserving images and SA-V dataset containing 642.6K masklets (spatio-temporal segmentation masks) from 50.9K videos.\n\n\n\n\n\n\nFigure 16.1: SAM 2 Architecture. Image source: [21]",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#retrieval-augmented-generation-rag",
    "href": "sections/foundation-models.html#retrieval-augmented-generation-rag",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "16.7 Retrieval-Augmented Generation (RAG)",
    "text": "16.7 Retrieval-Augmented Generation (RAG)\nLarge pre-trained Language Models (LLMs) have revolutionized natural language processing, but they come with inherent limitations that necessitate the development of techniques like Retrieval-Augmented Generation (RAG). This chapter explores the motivations behind RAG by examining the constraints of traditional LLMs.\n\n\n\n\n\nFig : A typical user interaction with LLM\n\n\n\n16.7.1 Limitations of Large Language Models\n\nLack of Specific Knowledge Access\n\nLLMs, despite their vast training data, cannot access specific knowledge bases or datasets that weren’t part of their original training. This limitation becomes apparent when dealing with specialized domains or recent information.\n\nAbsence of User-Specific Data\n\nLLM has not seen “your” data - the unique, often proprietary information that organizations and individuals possess. This gap can lead to generic responses that lack the nuance and specificity required in many real-world applications.\n\nDomain-Specific Knowledge Challenges\n\nWhen specific domain knowledge is required, the traditional approach has been to fine-tune the LLM. However, this process can be resource-intensive and may not always yield optimal results, especially for niche or rapidly evolving fields. For example, an LLM fine-tuned on chemistry domain might not be suitable for a researcher in a physics lab. Hence a particle-physics scientist will have to fine-tune a model on the lab-specific data, which might not be useful for a quantum physics lab.\n\n\n\n\n\nFig : Fine-tuning LLMs. Image source : datacamp blong\n\n\n\nLack of Source Attribution\n\nLLMs generate responses based on patterns learned during training, but they don’t provide sources for their information. This lack of attribution can be problematic in contexts where verifiability and credibility are crucial.\n\nHallucinations\n\nOne of the most significant issues with LLMs is their tendency to produce “hallucinations” - plausible-sounding but factually incorrect or nonsensical information. This phenomenon can undermine the reliability of the model’s outputs. See Lilweng’s blog post [22] on hallucinations for detailed information.\n\n\n\n\n\n\n\nFig : LLM Hallucination examples\n\n\n\n\nOutdated Information\n\nThe knowledge of an LLM is static, frozen at the time of its training. This leads to the problem of outdated information, where the model cannot account for recent events, discoveries, or changes in the world.\nRetrieval-Augmented Generation emerges as a solution to these limitations. By combining the generative capabilities of LLMs with the ability to retrieve and incorporate external, up-to-date information, RAG offers a path to more accurate, current, and verifiable AI-generated content. In the following sections, we will explore how RAG works, its advantages, and its potential applications in various domains.\n\n\n16.7.2 Introduction to RAG\nIntroduced in 2020 [23], RAG framework can be thought of as combining two techniques -\n\nGeneration\n\nDone by LLMs.\nLLM models used are typically tuned for question-answering\nLLM responds to a user query.\n\nRetrieval-Augmented\n\nUse an external database to store specific knowledge\nRetrieve the required information from the provided knowledge base\nProvide this retrieved information to the LLMs as context to answer user question.\n\n\nLet’s now compare the traditional LLM and RAG approaches\n\n16.7.2.1 Traditional LLM approach\n\n\n\nFig : Traditional LLM approach\n\n\n\nUser Input: The process begins with the user submitting a question.\nPrompt Engineering: The user’s question is combined with a pre-defined prompt.\nLLM Processing: The combined prompt and question are fed into the LLM.\nResponse Generation: The LLM generates and returns a response based on its training.\n\n\n\n16.7.2.2 RAG approach\n\n\n\nFig : RAG approach\n\n\n\nUser Input: As before, the user submits a question.\nKnowledge Base Query: The question is used to query a knowledge base.\nDocument Retrieval: Relevant documents are retrieved from the knowledge base.\nPrompt Construction: A prompt is constructed using:\n\nThe original question\nRetrieved relevant documents\nAny additional context or instructions\n\nLLM Processing: The comprehensive prompt is fed into the LLM.\nResponse Generation: The LLM generates a response based on both its pre-trained knowledge and the provided context.\n\n\n\n\n\n\n\n\nWithout RAG\nWith RAG\n\n\n\n\nNo ability to access a specific knowledge/domain\nPoint to a knowledge base\n\n\nNo sources\nSources cited in LLM response\n\n\nHallucinations\nLLM response is grounded by relevant information from knowledge base\n\n\nOut-of-date information\nUpdate the knowledge base with new information\n\n\n\nRAG has multiple use-cases. One of the most common usecase is a chatbot that can answer specific questions. For example, lets say we have a chatbot that has the knowledge base of documentation on supercomputers. This chatbot could help users write scripts/jobs(SLURM scripts) to be submitted to supercomputers, can help guide users to specific section in the documentation if they are stuck, and might even help in debugging errors once their script runs on a supercomputer. If a new SLURM package is introduced, the supercomputer maintainers just need to update the SLURM documentation, which gets pulled into the knowledge base. Hence the chatbot will always have access to the latest information.\nMore detailed information about RAG and its implementation will be discussed in detail in the hands-on part.\n\n\n\n\n[1] R. Bommasani et al., “On the opportunities and risks of foundation models,” ArXiv, 2021, Available: https://crfm.stanford.edu/assets/report.pdf\n\n\n[2] A. Radford et al., “Learning transferable visual models from natural language supervision,” CoRR, vol. abs/2103.00020, 2021, Available: https://arxiv.org/abs/2103.00020\n\n\n[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” CoRR, vol. abs/1810.04805, 2018, Available: http://arxiv.org/abs/1810.04805\n\n\n[4] OpenAI et al., “GPT-4 technical report.” 2024. Available: https://arxiv.org/abs/2303.08774\n\n\n[5] A. Dubey et al., “The llama 3 herd of models.” 2024. Available: https://arxiv.org/abs/2407.21783\n\n\n[6] A. Kirillov et al., “Segment anything.” 2023. Available: https://arxiv.org/abs/2304.02643\n\n\n[7] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using shifted windows.” 2021. Available: https://arxiv.org/abs/2103.14030\n\n\n[8] A. Ramesh et al., “Zero-shot text-to-image generation,” CoRR, vol. abs/2102.12092, 2021, Available: https://arxiv.org/abs/2102.12092\n\n\n[9] Y. Liu et al., “Sora: A review on background, technology, limitations, and opportunities of large vision models.” 2024. Available: https://arxiv.org/abs/2402.17177\n\n\n[10] G. Team et al., “Gemini: A family of highly capable multimodal models.” 2024. Available: https://arxiv.org/abs/2312.11805\n\n\n[11] A. Vaswani et al., “Attention is all you need,” CoRR, vol. abs/1706.03762, 2017, Available: http://arxiv.org/abs/1706.03762\n\n\n[12] L. Weng, “Attention? attention!” lilianweng.github.io, 2018, Available: https://lilianweng.github.io/posts/2018-06-24-attention/\n\n\n[13] J. Cheng, L. Dong, and M. Lapata, “Long short-term memory-networks for machine reading,” CoRR, vol. abs/1601.06733, 2016, Available: http://arxiv.org/abs/1601.06733\n\n\n[14] X. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and M. Kazi, “Transformer models: An introduction and catalog.” 2024. Available: https://arxiv.org/abs/2302.07730\n\n\n[15] I. J. Goodfellow et al., “Generative adversarial networks.” 2014. Available: https://arxiv.org/abs/1406.2661\n\n\n[16] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for generative adversarial networks,” CoRR, vol. abs/1812.04948, 2018, Available: http://arxiv.org/abs/1812.04948\n\n\n[17] A. Brock, J. Donahue, and K. Simonyan, “Large scale GAN training for high fidelity natural image synthesis,” CoRR, vol. abs/1809.11096, 2018, Available: http://arxiv.org/abs/1809.11096\n\n\n[18] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” CoRR, vol. abs/2006.11239, 2020, Available: https://arxiv.org/abs/2006.11239\n\n\n[19] L. Weng, “What are diffusion models?” lilianweng.github.io, Jul. 2021, Available: https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\n\n\n[20] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” CoRR, vol. abs/2112.10752, 2021, Available: https://arxiv.org/abs/2112.10752\n\n\n[21] N. Ravi et al., “SAM 2: Segment anything in images and videos,” arXiv preprint arXiv:2408.00714, 2024, Available: https://arxiv.org/abs/2408.00714\n\n\n[22] L. Weng, “Extrinsic hallucinations in LLMs.” lilianweng.github.io, Jul. 2024, Available: https://lilianweng.github.io/posts/2024-07-07-hallucination/\n\n\n[23] P. S. H. Lewis et al., “Retrieval-augmented generation for knowledge-intensive NLP tasks,” CoRR, vol. abs/2005.11401, 2020, Available: https://arxiv.org/abs/2005.11401",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html",
    "href": "sections/hands-on-lab-foundation-models.html",
    "title": "17  Hands-On Lab: Foundation Models",
    "section": "",
    "text": "17.1 Overview\nThe hands-on lab on foundation models will focus on building and applying foundation models for some example use cases. The main goal of this session is to get more familiarized with foundation models and in interacting with them.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#source-code",
    "href": "sections/hands-on-lab-foundation-models.html#source-code",
    "title": "17  Hands-On Lab: Foundation Models",
    "section": "17.2 Source Code",
    "text": "17.2 Source Code\nVisit https://github.com/ncsa/cyber2a-workshop and follow the instructions in the README file to set up and run the Jupyter Notebooks used in this hands-on lab.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#image-segmentation-using-segment-anything-model-2-sam-2",
    "href": "sections/hands-on-lab-foundation-models.html#image-segmentation-using-segment-anything-model-2-sam-2",
    "title": "17  Hands-On Lab: Foundation Models",
    "section": "17.3 Image Segmentation using Segment Anything Model 2 (SAM 2)",
    "text": "17.3 Image Segmentation using Segment Anything Model 2 (SAM 2)\n\n17.3.1 Background\nImage segmentation is a fundamental computer vision technique of dividing an image into one or more regions or objects [1]. Promptable Visual Segmentation (PVS) is a new type of segmentation that combines the flexibility of prompts with the power of computer vision models to enable users to segment images interactively based on prompts.\nFor this section of the foundational model hands-on session, we will use the [2]. SAM 2 is a foundation model for the PVS task trained on large-scale generic data that can predict object segmentation masks based on input prompts. These prompts include points, bounding boxes (e.g., rectangles), masks, or combinations. The model converts the image into an image embedding (a dense vector representation of the image), which it then uses to predict segmentation masks based on a user prompt.\nOne prominent class in the SAM 2 source code is SAM2ImagePredictor, which provides an easy interface to the model. Users can attach an input image to the model using its set_image method, which calculates the image embeddings. Then, the users can use the predict method to share prompts (user inputs) that help with the segmentation mask prediction.\n\n\n\n\n\n\nNote\n\n\n\nThe Jupyter Notebook for this hands-on session is available within the https://github.com/ncsa/cyber2a-workshop repository here. You can clone or download this repository directly from GitHub.\nThis notebook reuses some code segments (e.g., helper methods, imports, loading the model, etc.) from the image predictor example initially published in the SAM 2 source code repository. SAM 2 source code is released under the Apache License, Version 2.0, and Meta Platforms, Inc. and affiliates hold the copyright for the image_predictor_example.ipynb notebook. We have adapted and modified the image predictor example notebook to use data files from Arctic datasets and included specific activities for the Cyber2A Workshop.\n\n\n\n\n17.3.2 Data\nImages used in this hands-on lab section are from the LeConte Glacier Unmanned Aerial Vehicle (UAV) imagery dataset [3]. Specifically, we use a low-resolution version (640 x 427) of images with IDs 20180917-112527 and 20180917-115018 from the zip file located at this URL.\nBefore continuing with the rest of the sections, open the segmentation.ipynb notebook from the running Jupyter Notebook server.\n\n\n17.3.3 Environment Setup\nFirst, we import the necessary packages, download SAM 2 model checkpoints, and define methods for visualizing the results. Here, model checkpoints are files containing model weights and architecture saved after a certain number of iterations of model training. These checkpoints are used to load the model and make predictions.\n\n\nShow code\n\nimport os\n# if using Apple MPS, fall back to CPU for unsupported ops\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n\n\nShow code\n\nos.chdir(\"SAM_checkpoints\")\n!sh download_checkpoints.sh\nos.chdir(\"..\")\n\nNow, we use the code to select the device for computation. Depending on the machine running the segmentation notebook, you could choose between a CPU, GPU, or Metal Performance Shaders (MPS) for computation. The code snippet below shows how to select the device for computation.\n\n\nShow code\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# Select the device for computation. We will be using CUDA to run this notebook. Other options are provided for running this notebook in different environments.\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\nprint(f\"using device: {device}\")\n\nif device.type == \"cuda\":\n    # use bfloat16 for the entire notebook\n    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n    if torch.cuda.get_device_properties(0).major &gt;= 8:\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\nelif device.type == \"mps\":\n    print(\n        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n    )\n\nNext, we use the code to define methods for visualizing the results. The show_mask method displays a single segmentation mask, the show_points method displays the points, the show_box method displays the bounding box, and the show_masks method displays the image with the segmentation masks, points, and bounding boxes.\n\n\nShow code\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\nnp.random.seed(3)\n\ndef show_mask(mask, ax, random_color=False, borders = True):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask = mask.astype(np.uint8)\n    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    if borders:\n        import cv2\n        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n        # Try to smooth contours\n        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n    ax.imshow(mask_image)\n\ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n\ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n\ndef show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n    for i, (mask, score) in enumerate(zip(masks, scores)):\n        plt.figure(figsize=(10, 10))\n        plt.imshow(image)\n        show_mask(mask, plt.gca(), borders=borders)\n        if point_coords is not None:\n            assert input_labels is not None\n            show_points(point_coords, input_labels, plt.gca())\n        if box_coords is not None:\n            # boxes\n            show_box(box_coords, plt.gca())\n        if len(scores) &gt; 1:\n            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n        plt.axis('off')\n        plt.show()\n\n\n\n17.3.4 Example Image 1\nNow, we read the first example image (data/images/20180917-112527-reduced.jpg), create an object, and display it with a grid for estimating point and box coordinates.\n\n\nShow code\n\nimage = Image.open('data/images/20180917-112527-reduced.jpg')\nimage = np.array(image.convert(\"RGB\"))\n\nplt.figure(figsize=(10, 10))\nplt.imshow(image)\nplt.grid(visible=True)\nplt.axis('on')\nplt.show()\nOutput: \n\n\n\n17.3.5 Loading the SAM 2 model and configuration\nNow, let’s load the SAM 2 model and configuration file. We load the model from the SAM_checkpoints directory and the configuration file from the configs directory. We use the sam2.1_hiera_large model checkpoint and the sam2.1_hiera_l.yaml configuration file. Other model versions and their corresponding configuration files can also be used, but the accuracy of the segmentation outputs may vary.\n\n\nShow code\n\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\nsam2_checkpoint = \"SAM_checkpoints/sam2.1_hiera_large.pt\"\nmodel_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n\nsam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\npredictor = SAM2ImagePredictor(sam2_model)\n\nNow, we process the image to produce an image embedding by calling the SAM2ImagePredictor.set_image method. The SAM2ImagePredictor object stores this embedding and stores the subsequent mask prediction.\n\n\nShow code\n\npredictor.set_image(image)\n\n\n\n17.3.6 Specifying an object or region using a single point\nIn this example image, to prompt for the glacier region, let’s choose a point on it.\nPoints are a type of input to the model. They’re represented in (x,y) format and have corresponding labels 1 or 0, which represent the foreground and background, respectively. As we will see later, we can input multiple points, but here, we use only one. The show_points method displays the selected point using a star icon.\n\n\nShow code\n\ninput_point = np.array([[600, 400]])\ninput_label = np.array([1])\n\nplt.figure(figsize=(10, 10))\nplt.imshow(image)\nshow_points(input_point, input_label, plt.gca())\nplt.axis('on')\nplt.show()\n\nOutput: \n\n\n17.3.7 Predicting the segmentation mask\nNow, we predict the segmentation mask using the selected point as input. The predict method of the SAM2ImagePredictor object predicts the segmentation mask based on the input point. The show_masks utility method displays the segmentation mask on the image.\n\n\nShow code\n\nmasks, scores, logits = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=True,\n)\nsorted_ind = np.argsort(scores)[::-1] # Sorting the scores in decreasing order\nmasks = masks[sorted_ind]\nscores = scores[sorted_ind]\nlogits = logits[sorted_ind]\n\nshow_masks(image, masks, scores, point_coords=input_point, input_labels=input_label, borders=True)\n\nOutput:   \n\n\n17.3.8 Activity 1: Specifying an object or region using multiple points\nWe can see that the single input point can be ambiguous, and the model has returned multiple sub-regions within the glacier image. We can alleviate this by providing multiple points as input. We can also provide a mask from a previous model iteration to help improve the prediction. When specifying a single object with multiple prompts, we can ask the model to generate a single mask by setting multimask_output=False.\n\n\nShow code\n\n# E.g., input format for specifying two points\n# input_point = np.array([[x1, y1], [x2, y2]])\n# input_label = np.array([1, 1])\n\n# TODO: In the below piece of code, replace \"None\" with your two input points. You can specify more points if needed, but please make sure to increase the labels as well.\n\ninput_point = np.array([[600, 400], [500, 200]])\ninput_label = np.array([1, 1])\n\nFor example, we can use [600, 400] and [500, 200] as the two points to specify the glacier region.\n\n\nShow code\n\ninput_point = np.array([[600, 400], [500, 200]])\ninput_label = np.array([1, 1])\n\nmask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask from previous iteration\n\nmasks, scores, _ = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    mask_input=mask_input[None, :, :],\n    multimask_output=False,\n)\n\nshow_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)\n\nOutput: \nWe can see that providing multiple points as input has helped the model to predict a more accurate segmentation mask for the glacier region.\n\n\n17.3.9 Example Image 2\nNow, let’s read the second example image, create an object, and display it with grid for estimating point and box coordinates.\n\n\nShow code\n\nimage = Image.open('data/images/20180917-115018-reduced.jpg')\nimage = np.array(image.convert(\"RGB\"))\n\nplt.figure(figsize=(10, 10))\nplt.imshow(image)\nplt.grid(visible=True)\nplt.axis('on')\nplt.show()\n\n# Replace image in the predictor with the new image\npredictor.set_image(image)\n\nOutput: \n\n\n17.3.10 Activity 2: Specifying an object or region using multiple points (foreground and background)\nA background point (with label 0) can be supplied to exclude the glacier and water surrounding it and just include the glacial discharge.\n\n\nShow code\n\n# E.g., input format for specifying three points\n# input_point = np.array([[x1, y1], [x2, y2], [x3, y3]])\n# input_label = np.array([1, 1, 0])\n\n# TODO: In the below piece of code, use two or three input points, of which at least one needs to be a background point (water). You can specify more points if needed, but please make sure to increase the labels as well.\ninput_point = None\ninput_label = None\n\nFor example, we can use [600, 200] and [600, 300] as the foreground points and [600, 250] as the background point to specify the glacial discharge region.\n\n\nShow code\n\ninput_point = np.array([[600, 200], [600, 300], [600, 250]])\ninput_label = np.array([1, 1, 0])\n\nmask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask from previous iteration\n\nmasks, scores, _ = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=False,\n)\n\nshow_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)\nOutput: \nWe can see that with three points, including one background point, the model has predicted a large section of the glacial discharge region. With additional points (foreground and/or background), users can further guide the model to predict more accurate segmentation masks.\n\n\n17.3.11 Activity 3: Specifying a specific object with a box\nWe will specify the glacial discharge region using a bounding box in this activity. The bounding box is represented as [x1, y1, x2, y2], where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner of the box.\n\n\nShow code\n\n# E.g., input format for specifying a box\n# input_box = np.array([x1, y1, x2, y2])\n\n# TODO: In the below piece of code, replace \"None\" with a box coordinate.\ninput_box = None\nFor example, we can use [400, 150, 640, 400] as the bounding box to specify the glacial discharge region.\n\n\nShow code\n\ninput_box = np.array([400, 150, 640, 400])\n\nmasks, scores, _ = predictor.predict(\n    box_coords=input_box,\n    multimask_output=False,\n)\n\nshow_masks(image, masks, scores, box_coords=input_box)\nOutput: \nThe model has predicted the glacial discharge region based on the bounding box input, but the results are not perfect. Additional prompts, along with the bounding box, can improve this.\nWe have provided three optional activities for you to try out in the segmentation notebook, including one that does automatic segment generation without prompts. Feel free to experiment with different prompts and see how the model responds.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#retrieval-augmented-generation-rag-hands-on",
    "href": "sections/hands-on-lab-foundation-models.html#retrieval-augmented-generation-rag-hands-on",
    "title": "17  Hands-On Lab: Foundation Models",
    "section": "17.4 Retrieval Augmented Generation (RAG) Hands-On",
    "text": "17.4 Retrieval Augmented Generation (RAG) Hands-On\nWe will use Langchain framework for this section of the hands-on session. Langchain is a framework that provides tools and libraries for building and deploying AI models. It is built on top of PyTorch and HuggingFace transformers.\nSuggested code references: - Langchain RAG from scratch github - Langchain RAG tutorial\nSession hands-on code in github.com/ncsa/cyber2a-workshop\nSession technical details in course book : cyber2a.github.io/cyber2a-course/sections/foundation-models.html\nIn this section, we will build a chatbot using the RAG system, i.e., a chatbot that has access to your specific knowledge base and answers questions related to that knowledge base.\n\n17.4.1 RAG Recap\n\n\n\n\n\n\n\nWithout RAG\nWith RAG\n\n\n\n\nNo ability to access a specific knowledge/domain\nPoint to a knowledge base\n\n\nNo sources\nSources cited in LLM response\n\n\nHallucinations\nLLM response is grounded by relevant information from knowledge base\n\n\nOut-of-date information\nUpdate the knowledge base with new information\n\n\n\n\n\n\nRAG approach\n\n\n\n\n17.4.2 RAG\nWe can think of the RAG system as combining two techniques: 1. Retrieval 2. Generation , where the Generation step is augmented or improved by the Retrieval step.\nA specialized database typically retrieves the data; an LLM normally does the generation part.\n\nRetrieval\n\n\nSetup a knowledge base\nRetrieve documents relevant to the user query\n\n\nGeneration\n\n\nUsing LLMs\nUse the retrieved documents as context\n\n\n\n17.4.3 Hands-on Environment Setup\n\nLLM For the hands-on session on RAG, we will use OpenAI GPT-4o-mini and a Llama3.2:8b model hosted on an Ollama instance. You will need an OpenAI API key to access the OpenAI models. To access the Llama model, you must set up an Ollama instance and have an API key associated with that instance. Details on setting up an Ollama instance are available here. Users can also download a Llama model on their local machine and run this hands-on code without an Ollama instance. However, we recommend using an Ollama instance as this would enable api calls (curl requests) to access the LLM.\nCompute requirements There are no GPU requirements for this hands-on. However, we recommend some memory on your local system, as we will be using your device’s local memory for a small database. We recommend testing out the code for this hands-on session in a Jupyter notebook. For instructions on launching a Jupyter notebook, see here.\nCode The code is available at github.com/ncsa/cyber2a-workshop. Feel free to clone or download this repo directly from GitHub. Steps to clone the repo and access the rag.ipynb file:\n\nOpen a terminal\nRun command: git clone https://github.com/ncsa/cyber2a-workshop\nNavigate to the foundation_models/hands_on directory in the cloned repo.\n\nData requirements If you clone or download the repo, a data directory will be created. Feel free to add your favorite documents, such as TXT, PDF, CSV, or docx files, to the data/docs directory. We will insert these documents into our specialized database.\nEnvironment variables You will find an env.example file in the GitHub repo. This file will provide you with the environment variables required for this course. Add your OpenAI API key, Ollama API key, your data folder (currently data/docs), and a data collection name (e.g., data-collection). The OpenAI API key will be used to access the OpenAI models. Ollama API key will be used to access your Ollama instance. The collection name is a way to recognize a collection/table in the specialized database, that we will be setting up soon. Edit the env.example file to contain your values and save the file. Rename env.example to env.txt.\nPackage requirements This tutorial will require the following list of packages.\n\njupyter\npandas\npython-dotenv==1.0\nqdrant-client==1.12\n# langchain\nlangchain==0.3\nlangchain-community==0.3\nlangchain-core==0.3\nlangchain-openai==0.2\nlangchainhub==0.1\n# openai\nopenai==1.54\n# rst file loaders\n# pandoc\npypandoc==1.14\nunstructured==0.16\nInstructions for installing the required packages:\n\nCreate a virtual environment with python version not less than 3.10.\n\nUse conda/miniconda to set up a virtual environment. If you have a virtual environment with python&gt;=3.10 with a jupyter kernel, use that env/kernel and skip the below steps.\n\n\nDownload the latest Miniconda3 installer from the Miniconda web page.\n\n\nFrom the Terminal (Mac/Linux) or Command Prompt (Windows) add conda-forge package repository/channel to your environment:\n\n\nconda config --add channels conda-forge\n\n\nCreate the python environment (for this example we choose name rag-python3.9):\n\n\nconda create -n rag-python3.10 python=3.10\n\n\nActivate the environment: python conda activate rag-python3.10\n\n\n\nCreate a requirements.txt file and copy-paste the above packages as-is. You could also use the requirements.txt file from the cloned Github repository cyber2a-workshop\nType pip install -r requirements.txt\n\nLet’s start by importing some basic python packages.\n\n\nShow code\n\n# basic imports\nimport os\nimport json\nimport logging\nimport sys\nimport pandas as pd\n\nfrom dotenv import load_dotenv\nload_dotenv('env.txt', override=True)\n\n# create and configure logger\nlogging.basicConfig(level=logging.INFO, datefmt='%Y-%m-%dT%H:%M:%S',\n                    format='%(asctime)-15s.%(msecs)03dZ %(levelname)-7s : %(name)s - %(message)s',\n                    handlers=[logging.StreamHandler(sys.stdout)]\n                    )\n# create log object with current module name\nlog = logging.getLogger(__name__)\n\n\n\n17.4.4 RAG - Retrieval-Augmented Generation\nWe will focus on the “retrieval” part of RAG for this section.\nRAG - Retrieval Steps\n\nPrepare data\nCreate a database and insert data\nSearch the database and retrieve relevant documents according to the search query.\n\nAs mentioned earlier, the RAG system gives the LLM access to our knowledge base, which has specific information for our use case.\n\n17.4.4.1 Data Preparation\nLet’s consider that our knowledge base contain only textual data. The data present in the GitHub repo contains proceedings from the Arctic Data Symposium 2023\n\n17.4.4.1.1 Load data\nSince we have different file types, we will need different types of data loaders to read these different data formats. - Langchain provides different data loaders for different file types - Eg: Langchain CSVLoader is essentially a wrapper for Python csv.DictReader class - Data is loaded into Langchain Document object format - The Document class has page_content and metadata attributes. - The page_content is the textual content parsed from the document files. - The metadata can be user-defined or default (class defined) key-value pairs. These key-value pairs can be used for filtering the documents retrieved from the database. - By default, each file type has its own metadata content. Eg: PDF file has source and page. - Filtering methods are not shown in this course. These methods will be well-documented in the database tool that you choose (explained later in the vectorDB section).\n\n\n\nLangchain document class\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nFor details on langchain packages, please refer to their documentation and source-code.\nIf using an IDE (PyCharm, VSCode, etc), Ctrl+click, or Command+click on the package and it should open-up its source code.\n\n\n\n\nNow, let’s load some data. This code loads all files in a directory. For now, we have only PDF files.\n\n\nShow code\n\n# data loaders\nfrom langchain_community.document_loaders import CSVLoader, DataFrameLoader, PyPDFLoader, Docx2txtLoader, UnstructuredRSTLoader, DirectoryLoader\n\n# Defining a class for data loaders. All data loaders are defined in this class\nclass DataLoaders:\n    \"\"\"\n    various data loaders\n    \"\"\"\n    def __init__(self, data_dir_path):\n        self.data_dir_path = data_dir_path\n    \n    def csv_loader(self):\n        csv_loader_kwargs = {\n                            \"csv_args\":{\n                                \"delimiter\": \",\",\n                                \"quotechar\": '\"',\n                                },\n                            }\n        dir_csv_loader = DirectoryLoader(self.data_dir_path, glob=\"**/*.csv\", use_multithreading=True,\n                                    loader_cls=CSVLoader, \n                                    loader_kwargs=csv_loader_kwargs,\n                                    )\n        return dir_csv_loader\n    \n    def pdf_loader(self):\n        dir_pdf_loader = DirectoryLoader(self.data_dir_path, glob=\"**/*.pdf\",\n                                    loader_cls=PyPDFLoader,\n                                    )\n        return dir_pdf_loader\n    \n    def word_loader(self):\n        dir_word_loader = DirectoryLoader(self.data_dir_path, glob=\"**/*.docx\",\n                                    loader_cls=Docx2txtLoader,\n                                    )\n        return dir_word_loader\n    \n    def rst_loader(self):\n        rst_loader_kwargs = {\n                        \"mode\":\"single\"\n                        }\n        dir_rst_loader = DirectoryLoader(self.data_dir_path, glob=\"**/*.rst\",\n                                    loader_cls=UnstructuredRSTLoader, \n                                    loader_kwargs=rst_loader_kwargs\n                                    )\n        return dir_rst_loader\n\nLoad the data\n\n\nShow code\n\n# load data\ndata_dir_path = os.getenv('DATA_DIR_PATH', \"data/docs\")\ndata_loader = DataLoaders(data_dir_path=data_dir_path)\nlog.info(\"Loading files from directory %s\", data_dir_path)\n# instantiate loaders\ndir_csv_loader = data_loader.csv_loader()\ndir_word_loader = data_loader.word_loader()\ndir_pdf_loader = data_loader.pdf_loader()\ndir_rst_loader = data_loader.rst_loader()\n# call load method\ncsv_data = dir_csv_loader.load()\nword_data = dir_word_loader.load()\npdf_data = dir_pdf_loader.load()\nrst_data = dir_rst_loader.load()\n\nSince our test data only has pdf documents, only the pdf_data will have values. Let’s see how the first document looks like :\n# only printing the first document in pdf_data\nfor doc in pdf_data:\n    print(doc)\n    break\nThe above code will display the first document. The page_content will be the text content from the document and metadata gives you the page number and source file. The metadata field currently has default values, set by the class the document is loaded from (in this case PDFLoader class). For other classes, metadata would differ. For example, if loading a CSV file using CSVLoader, the metadata will have row number instead of page number. Users have the option to customize metadata as required by simple code changes.\nAs seen from the page_content and metadata value, the first document only has the text data from the first page. Langchain PDFLoader loads pdf documents in pages. Each document will be one pdf page.\n\n\n17.4.4.1.2 Format the data\nAs the previous code block shows, each document is in a Document class with attributes page_content and metadata. The LLM can only access the textual content (page_content), so let’s reformat the documents accordingly. However, we still need metadata, which is helpful for filtering purposes.\nUsers could also customize metadata to have similar key-value pairs across different documents. This would be helpful if several types of documents are inserted into one database and the metadata is used to filter across them.\nSteps implemented in the below code block: - Convert data to a list of texts and metadata - Custom metadata is set so that metadata is same for all different data sources.\n\n\nShow code\n\n# get text and metadata from the data\ndef get_text_metadatas(csv_data=None, pdf_data=None, word_data=None, rst_data=None):\n    \"\"\"\n    Each document class has page_content and metadata properties\n    Separate text and metadata content from Document class\n    Have custom metadata if needed\n    \"\"\"\n    csv_texts = [doc.page_content for doc in csv_data]\n    # custom metadata\n    csv_metadatas = [{'source': doc.metadata['source'], 'row_page': doc.metadata['row']} for doc in csv_data]   # default metadata={'source': 'filename.csv', 'row': 0}\n    pdf_texts = [doc.page_content for doc in pdf_data]\n    pdf_metadatas = [{'source': doc.metadata['source'], 'row_page': doc.metadata['page']} for doc in pdf_data]  # default metadata={'source': 'data/filename.pdf', 'page': 8}\n    word_texts = [doc.page_content for doc in word_data]\n    word_metadatas = [{'source': doc.metadata['source'], 'row_page': ''} for doc in word_data] \n    rst_texts = [doc.page_content for doc in rst_data]\n    rst_metadatas = [{'source': doc.metadata['source'], 'row_page': ''} for doc in rst_data]         # default metadata={'source': 'docs/images/architecture/index.rst'}\n\n    texts = csv_texts + pdf_texts + word_texts + rst_texts\n    metadatas = csv_metadatas + pdf_metadatas + word_metadatas + rst_metadatas\n    return texts, metadatas\n\ntexts , metadatas = get_text_metadatas(csv_data, pdf_data, word_data, rst_data)\n\nLet’s print the number of texts and metadata\nprint(\"Number of PDF texts: \", len(texts))\nprint(\"Number of PDF metadata: \", len(metadatas))\n\n\n17.4.4.1.3 Chunking\nChunking involves breaking large amounts of data into smaller, more manageable pieces. LLMs have a limited context window and cannot take in the entire dataset at once. For example, GPT-4 has a token limit of 128k. The Langchain document class also chunks PDF documents by page. However, we will need to make sure that all the content fits within the LLM token limit of 128k, so we will try splitting the documents meaningfully. We first split by pages, then by sections, then by paragraphs, then by new lines, then by sentences, by words, and lastly by characters (e.g., what if there’s a word with more than 128k characters!). This is just to ensure that no content is lost.\n\n\n\nChunking\n\n\nSteps implemented in the below code block: - Split texts into chunks - Return a list of document chunks (list of langchain document class) - Here, we select a chunk size of 1000 and an overlap of 200 tokens. There is no set rule for this choice. However, this is a recommended pattern. - Chunk sizes determine the granularity of information being searched for. - The chunk size should be smaller if very granular information is required. If the chunk size is larger, the overall content of the documents is returned.\n\n\nShow code\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\nfrom typing import List\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=1000,\n        chunk_overlap=200,\n        separators=[\n            \"\\n\\n\", \"\\n\", \". \", \" \", \"\"\n        ]  # try to split on paragraphs... fallback to sentences, then chars, ensure we always fit in context window\n    )\n\ndocs: List[Document] = text_splitter.create_documents(texts=texts, metadatas=metadatas)\n\nNow, let’s see if the first document changed and how many documents are available after chunking.\nprint(docs[0])\nprint(\"Number of documents: \", len(docs))\n\n\n17.4.4.1.4 Vector embeddings\nNeural networks do not understand characters or texts. However, they understand numbers and are really good at numerical computation. Hence, textual data is converted to vectors of real-valued numbers.\n\n\n Image source: MIT Deep Learning course slides\nVector embeddings are mathematical representations of data points in a high-dimensional space. In the context of natural language processing:\n\nWord Embeddings: Individual words are represented as real-valued vectors in a multi-dimensional space.\nSemantic Capture: These embeddings capture the semantic meaning and relationships of the text.\nSimilarity Principle: Words with similar meanings tend to have similar vector representations.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe words ‘word embeddings’, ‘vector embeddings’, and ‘embeddings’ will be used interchangeably throughout this course.\n\n\n\n\n\n\nVectors\n\n\nIn the above vector example, “King” and “Queen” have the same relationship as “man” and “woman.” The “King” is at a similar distance from “man” and “queen” from “woman.”\nThese word embeddings are learned by feeding a model vast amounts of text. Models specialized in generating these text embeddings are called embedding models. Word2Vec is one of the first (very basic) embedding models. The GloVE model was one of the more popular models that learned word embeddings, i.e., one of the first models that was trained as a word embedding model. There are many open-source embedding models, e.g., Text embedding models from HuggingFace. Check out the HuggingFace Embedding models leaderboard to compare different embedding models. We will be using the OpenAI text embedding model, which, according to OpenAI documentation, has a maximum token limit of 8191.\n# embeddings \nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\nAnd now, we have completed step 1 of RAG RAG - Retrieval Steps\n1. Prepare data\n\nCreate a knowledge base and insert data\nSearch the database and retrieve relevant documents according to the search query.\n\n\n\n\n17.4.4.2 Knowledge Database\nIn the age of burgeoning data complexity and high-dimensional information, traditional databases often fail to efficiently handle and extract meaning from intricate datasets. Enter vector databases, a technological innovation that has emerged as a solution to the challenges posed by the ever-expanding data landscape. (Source: beginner’s blog post on vector DB)\n\n17.4.4.2.1 Vector database\nVector databases have gained significant importance in various fields due to their unique ability to efficiently store, index, and search high-dimensional data points, often referred to as vectors. These databases are designed to handle data where each entry is represented as a vector in a multidimensional space. Vectors can represent a wide range of information, such as numerical features, embeddings from text or images, and even complex data like molecular structures.\nVector databases store data as vector embeddings and are optimized for fast retrieval and similarity search. Vector database records are vectors, and the distance between them corresponds to whether the vectors are similar or not. Vectors that are closer are more similar than vectors that are farther apart. \n\n17.4.4.2.1.1 How vector databases work\nLet’s start with a simple example of dealing with an LLM such as ChatGPT. The model has large volumes of data with a lot of content, and they provide us with the ChatGPT application.\n\n\n\nVectorDB within RAG. Source: KDnuggets blog post\n\n\nSo let’s go through the steps of retrieval using vectorDB.\n\nWe first partition the data (to be used in the knowledge base) into chunks\nUse embedding model to create vector embeddings for the data (create indexes)\nInsert data vector embeddings into the database, with some reference to the original content (metadata).\nUser query is converted to vector embeddings using the same embedding model used for data.\nVectorDB searches the knowledgebase for vector embeddings similar to the user query.\nVectorDB returns similar document chunks and sends it back to the user.\n\nNow, let’s see how it works in the vector database.\n\n\n\nVectorDB pipeline. Source: pinecone blog post\n\n\nThe three main stages that a vector database query goes through are:\n\nIndexing\n\nAs explained in the example above, once the data vector embedding moves into the vector database, it then uses a variety of algorithms to map the vector embedding to data structures for faster searching.\n\nQuerying\n\nOnce it has gone through its search, the vector database compares the queried vector (user query) to indexed vectors, applying a similarity metric to find the nearest neighbor.\n\nPost Processing\n\nDepending on the vector database you use, the vector database will post-process the final nearest neighbor to produce a final output to the query. We could also possibly re-rank the nearest neighbors for future reference.\n\n\n17.4.4.2.1.2 Inserting documents into VectorDB\n\n\n\nInserting into VectorDB. Source : Blog.demir\n\n\n\n\n\n17.4.4.2.2 Vector Store\n\nWe will use Qdrant vector store for this example\nFor this tutorial, we will utilize local memory for storage\nQdrant has a docker image that can be used to create a vector store and hosted remotely\nOne can configure a Qdrant docker image to run locally and have a Qdrant client that makes API requests.\nQdrant creates a collection from the inserted documents (similar to a table in SQL databases)\nBlog post on vector stores link\n\nLet’s create a Qdrant vector store in local memory\n\n\nShow code\n\n# creating a Qdrant vector store in local memory\n\nfrom langchain_community.vectorstores import Qdrant\n\n# qdrant collection name\ncollection_name = os.getenv('QDRANT_COLLECTION_NAME', \"data-collection\")\n\n# create vector store in local memory\nvectorstore = Qdrant.from_documents(\n    documents=docs, # pass in the chunked docs\n    embedding=embeddings,  # use this embedding model\n    location=\":memory:\",  # Local mode with in-memory storage only\n    collection_name=collection_name,  # give a collection name\n    )\n\nAnd now, we have completed step 2 of RAG retrieval RAG - Retrieval Steps\n1. Prepare data\n2. Create a knowledge base and insert data\n\nSearch the database and retrieve relevant documents according to the search query.\n\n\n\n\n17.4.4.3 Retrieve relevant documents\nCreate a retriever from the vector store. This retriever performs similarity search and retrieves similar document chunks from the Qdrant vector store.\n# Retriever to retrieve relevant chunks\nretriever = vectorstore.as_retriever()\nAnd now, we have completed all 3 steps of RAG retrieval\n1. Prepare data\n2. Create a vector store and insert data\n3. Search the vector store and retrieve relevant documents\n\n\n\n17.4.5 RAG - Retrieval-Augmented Generation\nWe will now move on to the “Generation” part of RAG. Here, the LLMs do most of the heavy lifting.\n\n17.4.5.1 LLM\n\nLLMs are pre-trained large language models\nTrained to predict the next word (token), given some input text.\nOpen-source models - HuggingFace leaderboard\nFor this HandsOn, we will use OpenAI GPT-4o-mini and the Ollama Llama3.2:3.2B model hosted by NCSA.\n\nLet’s see how best to communicate/prompt these LLM models for RAG.\n\n\n17.4.5.2 Prompting\nPrompting is a crucial technique in effectively communicating with Large Language Models (LLMs) to achieve desired outcomes without modifying the underlying model. As LLMs become more sophisticated, the art of crafting effective prompts has emerged as a key skill in natural language processing and AI applications. Check out LilianWeng’s blog post [4], medium blog post on prompt engineering.\nPrompting is often an iterative process. It typically requires multiple trial-and-error attempts to achieve the desired effect. Each iteration can provide insights into how the model interprets and responds to different input structures.\n\n17.4.5.2.1 Key Elements of Effective Prompting\n\nDefining a Persona\n\nAssigning the LLM a specific role or behavior can significantly influence its responses. By giving it a defined persona, the model will attempt to respond in a manner that aligns with that role. This can improve the quality and relevance of its answers.\nExample: “You are a helpful research assistant.”\nThis prompt frames the model’s responses to be in line with the behavior expected of a research assistant, such as providing accurate information and being resourceful.\n\nSetting Guardrails\n\nGuardrails provide boundaries or conditions within which the model should operate. This is particularly useful to avoid misleading or incorrect information. You can ask the model to refrain from answering if it’s unsure of the response.\nExample: “If you don’t know the final answer, just say ‘I don’t know’.”\nThis instructs the LLM to admit uncertainty instead of generating a potentially incorrect answer, thereby increasing reliability.\n\nProviding Clear Instructions\n\nGiving the LLM specific actions to perform before generating responses ensures that it processes the necessary information correctly. This is important when dealing with tasks like reviewing files or using external data.\nExample: “Read the data file before answering any questions.”\nThis directs the LLM to review relevant materials, improving the quality of the subsequent answers.\n\nSpecifying Response Formats\n\nYou can enhance the usefulness of responses by specifying the desired output format. By doing this, you ensure the model delivers information in a form that aligns with your needs.\nExample: “Respond using markdowns.”\nThis ensures the LLM outputs text in Markdown format, which can be helpful for structured documents or technical writing.\n\n\n17.4.5.2.2 Prompt template\n\nUse Langchain hub to pull prompts\n\neasy to share and reuse prompts\ncan see what are the popular prompts for specific use cases\nEg: rlm/rag-prompt \n\nUse a prompt template Langchain PromptTemplate to generate custom prompts\n\nincludes input parameters that can be dynamically changed\ncan include instructions and other prompting patterns\n\n\nqa_prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n    1. If the question has some initial findings, use that as context.\n    2. If you don't know the answer, don't try to make up an answer. Just say **I can't find the final answer but you may want to check the following sources** and add the source documents as a list.\n    3. If you find the answer, write the answer in a concise way and add the list of sources that are **directly** used to derive the answer. Exclude the sources that are irrelevant to the final answer.\n\n    {context}\n\n    Question: {question}\n    Helpful Answer:\"\"\"\n\nrag_chain_prompt = PromptTemplate.from_template(qa_prompt_template) \nLet’s use the rlm/rag-prompt from Langchain hub.\n# prompting\n\nfrom langchain import hub\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n\n\n17.4.5.3 Call LLM\n\nWe will use\n\nOpenAI GPT-4o-mini and\nOllama llama3.2 model (hosted by NCSA)\n\nEach model has its own formats and parameters\nFormat the documents as string to pass on to the LLM\n\n# formatting the documents as a string before calling the LLM\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n17.4.5.3.1 Call LLM - without RAG\n\n\nShow code\n\n# call open ai GPT-4o-mini\nfrom langchain_openai import ChatOpenAI\n\n# create a chat openai model\nllm: ChatOpenAI = ChatOpenAI(\n            temperature=0,\n            model=\"gpt-4o-mini\",\n            max_retries=500,\n        )\n\n# call GPT4o-mini. \n# No RAG. Not giving any instructions/context to the LLM.\n\nllm.invoke(\"What is the capital of the world?\")\n\n# Notice the OpenAI LLM response format: content , metadata\n\n\nShow code\n\n# call ollama llama3:latest\n\nfrom langchain_community.llms import Ollama\n\nollama_api_key = os.getenv('OLLAMA_API_KEY')\nollama_jwt_token = os.getenv('OLLAMA_JWT_TOKEN')\nollama_headers = {\"Authorization\": f\"Bearer {ollama_api_key}\"}\n\n# create a ollama model\nollamallm: Ollama = Ollama(\n    base_url=\"https://ollama.software.ncsa.illinois.edu/ollama\",\n    model=\"llama3.2:latest\",\n    headers=ollama_headers\n    )\n\n# call llama3 model\n# No RAG. Not giving any prompt/specific instructions to the LLM\nollamallm.invoke(\"What is the capital of the world?\")\n\n# Notice the Llama LLM response format: plain text\n\n\n\n\n17.4.6 RAG System\nLet’s bring it all together\n\n\n\nFig : RAG system. Image source : blog.demir\n\n\n\nUser Submits Query: The user inputs a query into the system. This is the initial step where the user’s request is captured.\nRAG System Query Relevant Documents: The RAG system processes the user’s query and searches for relevant documents.\nDocument Database Returns Documents: The document database receives the request for relevant documents and returns the documents it finds to the RAG system.\nCombine The Query & The Documents: The RAG system takes the documents provided by the document database and combines them with the original query.\nLLM Returns Answer: The combined query and documents are sent to a Large Language Model (LLM), which generates an answer based on the information provided.\nRAG System Return Answer to User: Finally, the answer generated by the LLM is sent back through the RAG system.\n\n\n17.4.6.1 RAG chain with OpenAI\nLet’s code the RAG chain with OpenAI LLM\n\n\nShow code\n\n# rag chain\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\nopenai_rag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt  # change to custom prompt here\n    | llm  # change openAI llm model here\n    | StrOutputParser()\n)\n\nThe above code implements the RAG system - Context is the retrieved docs from the retriever/vector db - RunnablePassthrough() is used to pass the user query as is to the chain - format_docs is used to format the documents as a string - prompt is the prompt used to call LLM with - llm is used to call the LLM - StrOutputParser() is used to parse the output from the LLM\nLet’s call the OpenAI RAG chain\n# call openai rag chain\nopenai_rag_chain.invoke(\"What were the goals of the symposium?\")  # change the user query in the text here\n# call openai rag chain\n# This should ideally give \"I dont know\" - different from the llm.invoke() method where we do not give a custom prompt\nopenai_rag_chain.invoke(\"What is the capital of the world?\")  # change the user query in the text here\nFeel free to try out other queries and test out other prompts.\n\n\n17.4.6.2 RAG chain with Llama model\nLet’s code the RAG chain with Llama\n\n\nShow code\n\n# ollama rag chain\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\nollama_rag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | ollamallm\n    | StrOutputParser()\n)\n\n# call ollama rag chain\nollama_rag_chain.invoke(\"Who is the president of USA?\")\n# This should ideally give \"I dont know\" since the question asked is outside of the context in the vector store. \n# None of the document chunks in the vector store will have good similarity with the user query\n# Notice that Llama model does not give \"I dont know\" answer. However, it does say that the information is outside of the context provided.\n# Removes hallucinations and grounds the answer. \n\n\n\n\n\n\nTip\n\n\n\n\n\nGPT models are generally good at following instructions. OpenAI had an “InstructGPT” model which was specifically trained to follow instructions\n\n\n\n\n\n17.4.6.3 Adding sources to RAG\nNow that we have used RAG to control hallucinations and ground the LLM responses, let’s add source citations to the LLM generated response.\n\n\nShow code\n\n## adding sources to openai rag chain\n\nfrom langchain_core.runnables import RunnableParallel\n\nopenai_rag_chain_from_docs = (\n    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nopenai_rag_chain_with_source = RunnableParallel(\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n).assign(answer=openai_rag_chain_from_docs)\n\n# call openai rag chain with source\n# this will return the answer and the sources (context)\nopenai_rag_chain_with_source.invoke(\"What were the goals of the symposium?\")\nopenai_rag_chain_with_source.invoke(\"Why is tundra restoration and rehabilitation important\")\nopenai_rag_chain_with_source.invoke(\"Who is Bernadette Adams?\")\n\n\n17.4.6.4 RAG Steps\nThat concludes the RAG implementation. We have completed all the steps for Retrieval-Augmented Generation (RAG)\n\nPrepare data\nCreate a vector store and insert into db\nSearch the vector store and retrieve relevant documents\nCall LLM with the user query and the retrieved documents\nReturn the LLM response to the user",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#conclusion",
    "href": "sections/hands-on-lab-foundation-models.html#conclusion",
    "title": "17  Hands-On Lab: Foundation Models",
    "section": "17.5 Conclusion",
    "text": "17.5 Conclusion\nIn this hands-on lab on Foundation Models, we learned how to set up and use the Segment Anything Model 2 (SAM 2) for image segmentation tasks and implement a chatbot using the RAG system. We delved into many details related to both use cases and learned how to interact with the foundation models used in them. We hope this hands-on lab has given you a practical understanding of foundation models and how to interact with them.\n\n\n\n\n[1] IBM, “What Is Image Segmentation?  IBM.” Sep. 2023. Accessed: Dec. 21, 2024. [Online]. Available: https://www.ibm.com/think/topics/image-segmentation\n\n\n[2] N. Ravi et al., “SAM 2: Segment Anything in Images and Videos.” arXiv, Oct. 2024. doi: 10.48550/arXiv.2408.00714.\n\n\n[3] J. Amundson, “LeConte Glacier Unmanned Aerial Vehicle (UAV) imagery, LeConte Glacier, Alaska, 2018,” 2019, doi: 10.18739/A2445HC19.\n\n\n[4] L. Weng, “Prompt engineering,” lilianweng.github.io, Mar. 2023, Available: https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html",
    "href": "sections/reproducibility.html",
    "title": "18  Reproducibility",
    "section": "",
    "text": "18.1 Goal\nDownload Reproducibility slides\nThis session aims to highlight the importance of reproducibility in AI-driven Arctic research. Participants will learn about the challenges and best practices for ensuring that AI models and their results can be reproduced by other researchers, a cornerstone for building trust and advancing the field. The discussion will cover strategies for documenting experiments, sharing data and code, and using version control systems.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#introduction",
    "href": "sections/reproducibility.html#introduction",
    "title": "18  Reproducibility",
    "section": "18.2 Introduction",
    "text": "18.2 Introduction\nReproducibility is not a new topic when it comes to artificial intelligence and machine learning in science, but is more important than ever as AI research is often criticized for not being reproducible. This becomes particularly problematic when validation of a model requires reproducing it.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#why-is-reproducibility-important",
    "href": "sections/reproducibility.html#why-is-reproducibility-important",
    "title": "18  Reproducibility",
    "section": "18.3 Why is Reproducibility Important?",
    "text": "18.3 Why is Reproducibility Important?\n\n18.3.1 Reproducible vs. Replicable\nReproducibility is important in science because it allows other researchers to validate the results of a study and/or use the same analysis for processing their data, promoting open science and collaboration.\n\nReproducible means that other researchers can take the same data, run the same analysis, and get the same result.\nReplicable means other researchers can take different data, run the same analysis, and get their own result without errors.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#the-reproducibility-checklist",
    "href": "sections/reproducibility.html#the-reproducibility-checklist",
    "title": "18  Reproducibility",
    "section": "18.4 The Reproducibility Checklist",
    "text": "18.4 The Reproducibility Checklist\nThe Reproducibility Checklist was created by Canadian computer scientist, Joelle Pineau, with the goal of facilitating reproducible machine learning algorithms that can be tested and replicated. When publishing your model, it is beneficial to work through this checklist and ensure that you’re including the items on this checklist. The checklist is as follows:\nFor all models and algorithms, check that you include:\n\nA clear description of the mathematical setting, algorithm, and/or model\nAn analysis of the complexity (time, space, sample size) of any algorithm\nA link to a downloadable source code*, with specification of all dependencies, including external libraries\n\nFor any theoretical claim, check that you include:\n\nA statement of the result\nA clear explanation of each assumption\nA complete proof of the claim\n\nFor all figures and tables that include empirical results, check that you include:\n\nA complete description of the data collection process, including sample size\nA link to a downloadable version of the dataset or simulation environment\nAn explanation of any data that was excluded and a description of any preprocessing step\nAn explanation of how samples were allocated for training, validation, and testing\nThe range of hyperparameters considered, method to select the best hyperparameter configuration, and specification of each hyperparameter used to generate results\nThe exact number of evaluation runs\nA description of how experiments were run\nA clear definition of the specific measure of statistics used to report results\nClearly defined error bars\nA description of results with central tendency (e.g., mean) and variation (e.g., standard deviation)\nA description of the computing infrastructure used\n\n*With sensitive data or proprietary code, scientists may not wish to release all of their code and data. In this case, data can be anonymized and/or partial code can be released that won’t run but can be read and reproduced.\nConsider the sensitivity of your data/code when publishing.\n\nSensitive data should be anonymized before publishing\nResearchers or organizations may only release partial code if their code is proprietary\nBe sure that the partial code released can still be read and reproduced",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#sharing-code",
    "href": "sections/reproducibility.html#sharing-code",
    "title": "18  Reproducibility",
    "section": "18.5 Sharing Code",
    "text": "18.5 Sharing Code\nThe first step to solving the problem of reproducibility is sharing the code that was used to generate the model. This allows other researchers to:\n\nValidate the model\nTrack code construction and see any author annotations\nExpand on published work\n\nDespite this, sharing code does not always mean that models are fully reproducible. Many machine learning models are trained on restricted datasets or require extensive computing power for training the model. Because of this, there are a few additional criteria that improve reproducibility including:\n\nData and metadata availability (must be included without question)\nTransparency of the code you’re using and dependencies needed to run the code\nEasily installable computational analysis tools and pipelines\nInstalled software should behave the same on every machine and should have the same runtime\n\n\n18.5.1 Trips and Tricks to Sharing Code\n\nAvoid using absolute file paths when reading in data (and in general the use of slashes, as these differ between operating systems)\n\n https://keytodatascience.com/python-read-csv-txt-file/\n\nClean your data within your code\nAvoid copy/pasting in a spreadsheet\nAlways keep an unedited version of your raw data\n\nA general guide to publishing reproducible work:  https://medium.com/data-science/scientific-data-analysis-pipelines-and-reproducibility-75ff9df5b4c5",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#model-repositories",
    "href": "sections/reproducibility.html#model-repositories",
    "title": "18  Reproducibility",
    "section": "18.6 Model Repositories",
    "text": "18.6 Model Repositories\nPyTorch Hub is a pre-trained model repository designed to facilitate reproducibility and enable new research. It is easily usable with Colab and Papers with Code, but models must be trained on openly accessible data.\n\nPapers with Code is an open source hub for publications that include direct links to GitHub code, no account needed to access datasets.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#version-control",
    "href": "sections/reproducibility.html#version-control",
    "title": "18  Reproducibility",
    "section": "18.7 Version Control",
    "text": "18.7 Version Control\n\nVersion control is the process of keeping track of every individual change by each contributor that’s saved in a version control framework, or a special database. Keeping a history of these changes to track model performance relative to model parameters saves the time you’d spend retraining the model.\nThe three components of version control in machine learning are:\n\nCode: We recommend writing and storing your model code in the same language as your implementation code to make it easier to maintain all code and dependencies\nData: Versioning should link the data to the appropriate metadata and note any changes in either\nModel: The model connects your code and data with your model parameters and analysis\n\nUsing a version control system ensures easier:\n\nCollaboration\n\nCollaborators can easily pull changes from a shared repository, push their own changes, annotate their code, and revert back to previous versions of their model\n\nVersioning\n\nIf your model breaks, you’ll have a log of any changes that were made, allowing you or others to revert back to a stable version\n\nDependency tracking\n\nYou can test more than one model on different branches or repositories, tune the model parameters and hyperparameters, and monitor the accuracy of each implemented change\n\nModel updates\n\nVersion control allows for incrementally released versions while continuing the development of the next release",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#summary",
    "href": "sections/reproducibility.html#summary",
    "title": "18  Reproducibility",
    "section": "18.8 Summary",
    "text": "18.8 Summary\nConsider the following to ensure your model is reproducible:\n\nUse the reproducibility checklist for algorithms, theoretical claims, and figures/tables.\nAnonymize any sensitive data and remove proprietary code before publishing\n\nBUT still provide training data and enough code for others to replicate your model\n\nShare data and metadata, be transparent in any dependencies needed to run your model, use easily installable computational analysis tools and pipelines, and ensure installed software behaves the same on every machine (i.e. runtime)\nUse a pre-trained model repository (ex. PyTorch Hub) and publish to open-source journals/websites (ex. Papers with Code)\nPractice efficient version control (recommend GitHub if working with collaborators)",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#hands-on-activity",
    "href": "sections/reproducibility.html#hands-on-activity",
    "title": "18  Reproducibility",
    "section": "18.9 Hands-On Activity",
    "text": "18.9 Hands-On Activity\nLEGO Metadata: In groups of 3-5 people, take ~15 minutes to create a structure out of LEGO bricks and write instructions for a group who will recreate your structure based on these instructions.\nGroups will then be rotated and given instructions and LEGO pieces from another group where they will have ~15 minutes to attempt to recreate that group’s structure.\nWe will have a closing group discussion about this activity. Some questions include:\n\nWhat were some assumptions you made while writing your instructions?\nWere there any unexpected hurdles you encountered when writing your instructions or trying to replicate another group’s structure?\nWhat did you find most difficult about this activity?\nNow that you see how successful or unsuccessful the other group was in recreating your structure, is there anything you would do differently?\n\nThis activity was adapted from the Lego Metadata for Reproducibility Game Pack (doi: 10.36399/gla.pubs.196477) developed by Mary Donaldson and Matt Mahon at the University of Glasgow.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#references-resources",
    "href": "sections/reproducibility.html#references-resources",
    "title": "18  Reproducibility",
    "section": "18.10 References & Resources",
    "text": "18.10 References & Resources\n\nGundersen, Odd Erik, and Sigbjørn Kjensmo. 2018. “State of the Art: Reproducibility in Artificial Intelligence”. Proceedings of the AAAI Conference on Artificial Intelligence 32 (1).\nGundersen, Odd Erik, Yolanda Gil, and David W. Aha. “On Reproducible AI: Towards Reproducible Research, Open Science, and Digital Scholarship in AI Publications.” AI Magazine 39, no. 3 (September 28, 2018): 56–68.\n“How the AI Community Can Get Serious about Reproducibility.” Accessed September 18, 2024.\nAbid, Areeba. “Addressing ML’s Reproducibility Crisis.” Medium, January 7, 2021.\nPyTorch. “Towards Reproducible Research with PyTorch Hub.” Accessed September 18, 2024.\nStojnic, Robert. “ML Code Completeness Checklist.” PapersWithCode (blog), April 8, 2020.\nAkalin, Altuna. “Scientific Data Analysis Pipelines and Reproducibility.” Medium, July 5, 2021.\nHashesh, Ahmed. “Version Control for ML Models: What It Is and How To Implement It.” neptune.ai, July 22, 2022.\nNCEAS Learning Hub\nDonaldson, M. and Mahon, M. 2019. LEGO® Metadata for Reproducibility game pack. Documentation. University of Glasgow.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/the-fun-and-frontiers-of-ai.html",
    "href": "sections/the-fun-and-frontiers-of-ai.html",
    "title": "19  The Fun and Frontiers of AI: Innovation, Imagination, Interaction",
    "section": "",
    "text": "Overview\nThis presentation, “The Fun and Frontiers of AI in Arctic Science,” explores how artificial intelligence (AI) is transforming Arctic research. It introduces key AI subfields, including computer vision and natural language processing, and highlights their applications in environmental science, such as mapping permafrost thaw, forecasting weather patterns, and analyzing vast datasets. The slides showcase real-world projects like PolarHub and PolarGlobe, which leverage AI for data discovery and climate visualization. Additionally, the presentation discusses AI-driven search and recommendation systems, deep learning for sea ice monitoring, and the challenges of using large models in scientific research. Through these examples, it illustrates how AI can enhance scientific discovery while emphasizing the need for responsible and informed AI applications in Arctic studies.",
    "crumbs": [
      "<b>Day 5: AI Frontiers</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>The Fun and Frontiers of AI: Innovation, Imagination, Interaction</span>"
    ]
  },
  {
    "objectID": "sections/the-fun-and-frontiers-of-ai.html#slides",
    "href": "sections/the-fun-and-frontiers-of-ai.html#slides",
    "title": "19  The Fun and Frontiers of AI: Innovation, Imagination, Interaction",
    "section": "Slides",
    "text": "Slides\nDownload Slides",
    "crumbs": [
      "<b>Day 5: AI Frontiers</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>The Fun and Frontiers of AI: Innovation, Imagination, Interaction</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] A.\nK. Liljedahl et al., “Pan-Arctic ice-wedge\ndegradation in warming permafrost and its influence on tundra\nhydrology,” Nature Geoscience, vol. 9, no. 4, pp.\n312–318, Apr. 2016, doi: 10.1038/ngeo2674.\n\n\n[2] A.\nA. Vasiliev, D. S. Drozdov, A. G. Gravis, G. V. Malkova, K. E. Nyland,\nand D. A. Streletskiy, “Permafrost degradation in the\nWestern Russian Arctic,”\nEnvironmental Research Letters, vol. 15, no. 4, p. 045001, Apr.\n2020, doi: 10.1088/1748-9326/ab6f12.\n\n\n[3] S.\nL. Smith, H. B. O’Neill, K. Isaksen, J. Noetzli, and V. E. Romanovsky,\n“The changing thermal state of permafrost,” Nature\nReviews Earth & Environment, vol. 3, no. 1, pp. 10–23, Jan.\n2022, doi: 10.1038/s43017-021-00240-1.\n\n\n[4] T.\nA. Douglas, M. R. Turetsky, and C. D. Koven, “Increased rainfall\nstimulates permafrost thaw across a variety of Interior\nAlaskan boreal ecosystems,” npj Climate and\nAtmospheric Science, vol. 3, no. 1, pp. 1–7, Jul. 2020, doi: 10.1038/s41612-020-0130-4.\n\n\n[5] R.\nÍ. Magnússon et al., “Extremely wet summer events enhance\npermafrost thaw for multiple years in Siberian\ntundra,” Nature Communications, vol. 13, no. 1, p. 1556,\nMar. 2022, doi: 10.1038/s41467-022-29248-x.\n\n\n[6] L.\nM. Farquharson, V. E. Romanovsky, W. L. Cable, D. A. Walker, S. V.\nKokelj, and D. Nicolsky, “Climate Change\nDrives Widespread and Rapid\nThermokarst Development in Very\nCold Permafrost in the Canadian\nHigh Arctic,” Geophysical Research\nLetters, vol. 46, no. 12, pp. 6681–6689, 2019, doi: 10.1029/2019GL082187.\n\n\n[7] D.\nNotz and J. Stroeve, “Observed Arctic sea-ice loss\ndirectly follows anthropogenic CO2 emission,”\nScience, vol. 354, no. 6313, pp. 747–750, Nov. 2016, doi: 10.1126/science.aag2345.\n\n\n[8] D.\nM. Nielsen, M. Dobrynin, J. Baehr, S. Razumov, and M. Grigoriev,\n“Coastal Erosion Variability at the\nSouthern Laptev Sea\nLinked to Winter Sea\nIce and the Arctic\nOscillation,” Geophysical Research Letters,\nvol. 47, no. 5, p. e2019GL086876, 2020, doi: 10.1029/2019GL086876.\n\n\n[9] L.\nBruhwiler, F.-J. W. Parmentier, P. Crill, M. Leonard, and P. I. Palmer,\n“The Arctic Carbon Cycle\nand Its Response to Changing\nClimate,” Current Climate Change Reports,\nvol. 7, no. 1, pp. 14–34, Mar. 2021, doi: 10.1007/s40641-020-00169-5.\n\n\n[10] T.\nK. F. Campbell, T. C. Lantz, R. H. Fraser, and D. Hogan, “High\nArctic Vegetation Change\nMediated by Hydrological\nConditions,” Ecosystems, vol. 24, no. 1,\npp. 106–121, Jan. 2021, doi: 10.1007/s10021-020-00506-7.\n\n\n[11] S.\nC. Davidson et al., “Ecological insights from three\ndecades of animal movement tracking across a changing\nArctic,” Science, vol. 370, no. 6517, pp.\n712–715, Nov. 2020, doi: 10.1126/science.abb7080.\n\n\n[12] L.\nSuter, D. Streletskiy, and N. Shiklomanov, “Assessment of the cost\nof climate change impacts on critical infrastructure in the circumpolar\nArctic,” Polar Geography, vol. 42, no. 4,\npp. 267–286, Oct. 2019, doi: 10.1080/1088937X.2019.1686082.\n\n\n[13] M.\nL. Druckenmiller et al., “The\nArctic,” Bulletin of the American Meteorological\nSociety, vol. 102, no. 8, pp. S263–S316, Aug. 2021, doi: 10.1175/BAMS-D-21-0086.1.\n\n\n[14] M.\nPhilipp, A. Dietz, S. Buchelt, and C. Kuenzer, “Trends in\nSatellite Earth Observation for\nPermafrost Related\nAnalyses—A Review,”\nRemote Sensing, vol. 13, no. 6, p. 1217, Jan. 2021, doi: 10.3390/rs13061217.\n\n\n[15] “Changing state of Arctic\nsea ice across all seasons - IOPscience.” Accessed:\nOct. 18, 2024. [Online]. Available: https://iopscience.iop.org/article/10.1088/1748-9326/aade56\n\n\n[16] “AI in\nAnalytics: Top Use\nCases and Tools.” Accessed: Oct. 18,\n2024. [Online]. Available: https://www.marketingaiinstitute.com/blog/how-to-use-artificial-intelligence-for-analytics\n\n\n[17] M.\nI. Jordan and T. M. Mitchell, “Machine learning:\nTrends, perspectives, and prospects,”\nScience, vol. 349, no. 6245, pp. 255–260, Jul. 2015, doi: 10.1126/science.aaa8415.\n\n\n[18] A.\nVaswani et al., “Attention is all you need,” in\nAdvances in neural information processing systems, I. Guyon, U.\nV. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R.\nGarnett, Eds., Curran Associates, Inc., 2017. Available: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n\n\n[19] C.\nWitharana et al., “An\nObject-Based Approach for\nMapping Tundra\nIce-Wedge Polygon\nTroughs from Very High\nSpatial Resolution Optical\nSatellite Imagery,” Remote\nSensing, vol. 13, no. 4, p. 558, Jan. 2021, doi: 10.3390/rs13040558.\n\n\n[20] C.\nWitharana et al., “Ice-wedge polygon detection in\nsatellite imagery from pan-Arctic regions,\nPermafrost Discovery Gateway,\n2001-2021,” 2023, doi: 10.18739/A2KW57K57.\n\n\n[21] L.\nEdwards and M. Veale, “Enslaving the Algorithm:\nFrom a ‘Right to an\nExplanation’ to a ‘Right to\nBetter Decisions’?” Social\nScience Research Network, Rochester, NY, 2018. doi: 10.2139/ssrn.3052831.\n\n\n[22] S.\nFink, “This High-Tech\nSolution to Disaster Response\nMay Be Too Good to\nBe True,” The New York Times,\nAug. 2019, Accessed: Oct. 19, 2024. [Online]. Available: https://www.nytimes.com/2019/08/09/us/emergency-response-disaster-technology.html\n\n\n[23] D.\nLeslie, “Understanding artificial intelligence ethics and safety:\nA guide for the responsible design and implementation of\nAI systems in the public sector,” Zenodo, Jun. 2019.\ndoi: 10.5281/zenodo.3240529.\n\n\n[24] S.\nLo Piano, “Ethical principles in machine learning and artificial\nintelligence: Cases from the field and possible ways forward,”\nHumanities and Social Sciences Communications, vol. 7, no. 1,\npp. 1–7, Jun. 2020, doi: 10.1057/s41599-020-0501-9.\n\n\n[25] A.\nMcGovern, I. Ebert-Uphoff, D. J. G. Ii, and A. Bostrom, “Why we\nneed to focus on developing ethical, responsible, and trustworthy\nartificial intelligence approaches for environmental science,”\nEnvironmental Data Science, vol. 1, p. e6, Jan. 2022, doi: 10.1017/eds.2022.5.\n\n\n[26] T.\nShepherd, “Indigenous rangers to use SpaceCows\nprogram to protect sacred sites and rock art from feral herds,”\nThe Guardian, Sep. 2021, Accessed: Oct. 19, 2024. [Online].\nAvailable: https://www.theguardian.com/australia-news/2021/sep/15/indigenous-rangers-to-use-spacecows-program-to-protect-sacred-sites-and-rock-art-from-feral-herds\n\n\n[27] CSIRO, “SpaceCows:\nUsing AI to tackle feral herds in the\nTop End.” Accessed: Oct. 19, 2024.\n[Online]. Available: https://www.csiro.au/en/news/All/News/2021/September/SpaceCows-Using-AI-to-tackle-feral-herds-in-the-Top-End\n\n\n[28] A.\nD. S. A. (ADSA), “The Data Science\nEthos - Operationalizing Ethics\nin Data Science,” The Data Science\nEthos. Accessed: Oct. 19, 2024. [Online]. Available: https://ethos.academicdatascience.org/\n\n\n[29] W.\nChen and A. Quan-Haase, “Big Data Ethics\nand Politics: Toward New\nUnderstandings,” Social Science Computer\nReview, vol. 38, no. 1, pp. 3–9, Feb. 2020, doi: 10.1177/0894439318810734.\n\n\n[30] “Excavating AI.”\nAccessed: Oct. 19, 2024. [Online]. Available: https://excavating.ai/\n\n\n[31] J.\nGray and A. Witt, “A feminist data ethics of care for machine\nlearning: The what, why, who and how,” First\nMonday, Dec. 2021, doi: 10.5210/fm.v26i12.11833.\n\n\n[32] “Checklist to Examine\nAI-readiness for Open\nEnvironmental Datasets,”\nfigshare. Jun. 2022. doi: 10.6084/m9.figshare.19983722.v1.\n\n\n[33] S.\nLong and T. Romanoff, “AI-Ready\nOpen Data.”\nAI-Ready Open Data\n Bipartisan Policy\nCenter, 2023. Accessed: Oct. 19, 2024. [Online]. Available:\nhttps://bipartisanpolicy.org/explainer/ai-ready-open-data/\n\n\n[34] O.\nBenjelloun et al., “Croissant Format\nSpecification,” Croissant site. 2024.\nAccessed: Oct. 20, 2024. [Online]. Available: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n\n\n[35] M.\nD. Mahecha et al., “Earth system data cubes unravel\nglobal multivariate dynamics,” Earth System Dynamics,\nvol. 11, no. 1, pp. 201–234, Feb. 2020, doi: 10.5194/esd-11-201-2020.\n\n\n[36] “ERA5 hourly data on single\nlevels from 1940 to present.” doi: https://doi.org/10.24381/cds.adbb2d47.\n\n\n[37] O.\nJ. Reichman, M. B. Jones, and M. P. Schildhauer, “Challenges and\nopportunities of open data in ecology.” Science (New York,\nN.Y.), vol. 331, no. 6018, pp. 703–5, Feb. 2011, doi: 10.1126/science.1197962.\n\n\n[38] I.\nNitze et al., “DARTS:\nMulti-year database of AI detected\nretrogressive thaw slumps (RTS) and active layer detachment\nslides (ALD) in hotspots of the circum-arctic permafrost\nregion - v1,” 2024, doi: 10.18739/A2RR1PP44.\n\n\n[39] A.\nD. of F. Game, D. of C. and Fisheries, and A.-Y.-K. Region,\n“Salmon age, sex, and length data from\nArctic-Yukon-Kuskokwim\nRegion of Alaska, 1960-2017,” 2018,\ndoi: 10.5063/SN07CZ.\n\n\n[40] M.\nD. Wilkinson et al., “The FAIR\nGuiding Principles for scientific data\nmanagement and stewardship,” Scientific Data, vol. 3, p.\n160018, Mar. 2016, doi: 10.1038/sdata.2016.18.\n\n\n[41] M.\nD. Wilkinson, S.-A. Sansone, E. Schultes, P. Doorn, L. O. Bonino da\nSilva Santos, and M. Dumontier, “A design framework and exemplar\nmetrics for FAIRness,” Scientific Data,\nvol. 5, p. 180118, Jun. 2018, doi: 10.1038/sdata.2018.118.\n\n\n[42] G.\nPeng et al., “Harmonizing quality measures of\nFAIRness assessment towards machine-actionable quality\ninformation,” International Journal of Digital Earth,\nvol. 17, no. 1, p. 2390431, Dec. 2024, doi: 10.1080/17538947.2024.2390431.\n\n\n[43] M.\nJones et al., “MetaDIG:\nEngaging Scientists in the\nImprovement of Metadata and\nData,” Figshare, 2016, doi: 10.6084/m9.figshare.4055808.v1.\n\n\n[44] M.\nJones, P. Slaughter, and T. Habermann, “Quantifying\nFAIR: Metadata improvement and guidance in the\nDataONE repository network.” 2019. doi: https://doi.org/10.5063/f1kp80gx.\n\n\n[45] S.\nS. Chong, M. Schildhauer, M. O’Brien, B. Mecum, and M. B. Jones,\n“Enhancing the FAIRness of Arctic\nResearch Data Through\nSemantic Annotation,” Data Science\nJournal, vol. 23, no. 1, Jan. 2024, doi: 10.5334/dsj-2024-002.\n\n\n[46] R.\nBommasani et al., “On the opportunities and risks of\nfoundation models,” ArXiv, 2021, Available: https://crfm.stanford.edu/assets/report.pdf\n\n\n[47] A.\nRadford et al., “Learning transferable visual models from\nnatural language supervision,” CoRR, vol.\nabs/2103.00020, 2021, Available: https://arxiv.org/abs/2103.00020\n\n\n[48] J.\nDevlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of deep bidirectional transformers for language\nunderstanding,” CoRR, vol. abs/1810.04805, 2018,\nAvailable: http://arxiv.org/abs/1810.04805\n\n\n[49] OpenAI et al., “GPT-4 technical\nreport.” 2024. Available: https://arxiv.org/abs/2303.08774\n\n\n[50] A.\nDubey et al., “The llama 3 herd of models.” 2024.\nAvailable: https://arxiv.org/abs/2407.21783\n\n\n[51] A.\nKirillov et al., “Segment anything.” 2023.\nAvailable: https://arxiv.org/abs/2304.02643\n\n\n[52] Z.\nLiu et al., “Swin transformer: Hierarchical vision\ntransformer using shifted windows.” 2021. Available: https://arxiv.org/abs/2103.14030\n\n\n[53] A.\nRamesh et al., “Zero-shot text-to-image\ngeneration,” CoRR, vol. abs/2102.12092, 2021, Available:\nhttps://arxiv.org/abs/2102.12092\n\n\n[54] Y.\nLiu et al., “Sora: A review on background, technology,\nlimitations, and opportunities of large vision models.” 2024.\nAvailable: https://arxiv.org/abs/2402.17177\n\n\n[55] G.\nTeam et al., “Gemini: A family of highly capable\nmultimodal models.” 2024. Available: https://arxiv.org/abs/2312.11805\n\n\n[56] A.\nVaswani et al., “Attention is all you need,”\nCoRR, vol. abs/1706.03762, 2017, Available: http://arxiv.org/abs/1706.03762\n\n\n[57] L.\nWeng, “Attention? attention!”\nlilianweng.github.io, 2018, Available: https://lilianweng.github.io/posts/2018-06-24-attention/\n\n\n[58] J.\nCheng, L. Dong, and M. Lapata, “Long short-term memory-networks\nfor machine reading,” CoRR, vol. abs/1601.06733, 2016,\nAvailable: http://arxiv.org/abs/1601.06733\n\n\n[59] X.\nAmatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and M.\nKazi, “Transformer models: An introduction and catalog.”\n2024. Available: https://arxiv.org/abs/2302.07730\n\n\n[60] I.\nJ. Goodfellow et al., “Generative adversarial\nnetworks.” 2014. Available: https://arxiv.org/abs/1406.2661\n\n\n[61] T.\nKarras, S. Laine, and T. Aila, “A style-based generator\narchitecture for generative adversarial networks,” CoRR,\nvol. abs/1812.04948, 2018, Available: http://arxiv.org/abs/1812.04948\n\n\n[62] A.\nBrock, J. Donahue, and K. Simonyan, “Large scale GAN\ntraining for high fidelity natural image synthesis,”\nCoRR, vol. abs/1809.11096, 2018, Available: http://arxiv.org/abs/1809.11096\n\n\n[63] L.\nWeng, “What are diffusion models?”\nlilianweng.github.io, Jul. 2021, Available: https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\n\n\n[64] J.\nHo, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic\nmodels,” CoRR, vol. abs/2006.11239, 2020, Available: https://arxiv.org/abs/2006.11239\n\n\n[65] R.\nRombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n“High-resolution image synthesis with latent diffusion\nmodels,” CoRR, vol. abs/2112.10752, 2021, Available: https://arxiv.org/abs/2112.10752\n\n\n[66] N.\nRavi et al., “SAM 2: Segment anything in images and\nvideos,” arXiv preprint arXiv:2408.00714, 2024,\nAvailable: https://arxiv.org/abs/2408.00714\n\n\n[67] P.\nS. H. Lewis et al., “Retrieval-augmented generation for\nknowledge-intensive NLP tasks,” CoRR, vol.\nabs/2005.11401, 2020, Available: https://arxiv.org/abs/2005.11401\n\n\n[68] L.\nWeng, “Extrinsic hallucinations in LLMs.”\nlilianweng.github.io, Jul. 2024, Available: https://lilianweng.github.io/posts/2024-07-07-hallucination/\n\n\n[69] L.\nWeng, “Prompt engineering,” lilianweng.github.io,\nMar. 2023, Available: https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n\n\n[70] P.\nNorvig and S. J. Russell, Artificial intelligence: A modern\napproach, 3rd ed. Pearson, 2004. Available: https://books.google.com/books/about/Artificial_Intelligence.html?id=8jZBksh-bUMC\n\n\n[71] D.\nO. Hebb, The organization of behavior: A neuropsychological\ntheory. New York: Wiley, 1949. Available: https://en.wikipedia.org/wiki/The_Organization_of_Behavior\n\n\n[72] F.\nRosenblatt, “The perceptron: A probabilistic model for information\nstorage and organization in the brain,” Psychological\nReview, vol. 65, no. 6, pp. 386–408, 1958, doi: 10.1037/H0042519.\n\n\n[73] M.\nBennett, A brief history of intelligence: Evolution, AI, and the\nfive breakthroughs that made our brains, Hardcover. Harper,\n2023.\n\n\n[74] Inc. PitchBook Data, “Artificial\nintelligence & machine learning report, Q2 2024,” PitchBook,\n2024. Available: https://pitchbook.com/news/reports/q2-2024-artificial-intelligence-machine-learning-report\n\n\n[75] D.\nKawahara, S. Ozeki, and I. Mizuuchi, “A curiosity algorithm for\nrobots based on the free energy principle,” pp. 53–59, 2022, doi:\n10.1109/SII52469.2022.9708819.\n\n\n[76] T.\nWang, F. Wang, Z. Xie, and F. Qin, “Curiosity model policy\noptimization for robotic manipulator tracking control with input\nsaturation in uncertain environment,” Frontiers in\nNeurorobotics, vol. 18, 2024, doi: 10.3389/fnbot.2024.1376215.\n\n\n[77] IBM, “What Is\nImage Segmentation? \nIBM.” Sep. 2023. Accessed: Dec. 21, 2024. [Online].\nAvailable: https://www.ibm.com/think/topics/image-segmentation\n\n\n[78] N.\nRavi et al., “SAM 2: Segment\nAnything in Images and\nVideos.” arXiv, Oct. 2024. doi: 10.48550/arXiv.2408.00714.\n\n\n[79] J.\nAmundson, “LeConte Glacier\nUnmanned Aerial Vehicle\n(UAV) imagery, LeConte Glacier,\nAlaska, 2018,” 2019, doi: 10.18739/A2445HC19.",
    "crumbs": [
      "References"
    ]
  }
]