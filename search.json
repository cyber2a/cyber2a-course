[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "",
    "text": "Course Overview\nAI for Arctic Research represents an introduction to Artificial Intelligence (AI) techniques produced by the Cyber2A project, an innovative training program to empower the Arctic science community with advanced AI-driven data analytics and cyberinfrastructure (CI) skills to tackle the pressing challenges facing the Arctic and thus our planet. Today, Artificial Intelligence has become one of the most powerful tools to analyze Arctic big data and enable new ways of data-driven discovery. However, training on these emerging topics is often missing in current undergraduate and graduate curricula, particularly for active Arctic researchers. This project aims to fill this skills gap in order to foster the growth of an Arctic science workforce with strong data science skills through a series of complementary and mutually reinforcing training activities.\nThe week-long course is designed with a modular curriculum, where each module can be incorporated into learning activities across Universities and other organizations. The curriculum is free to be re-used, licensed under a CC-BY Attribution license and covers 5 main topical areas:\nThe sections presented here fit into a one-week workshop as follows, but the modules can be also used individually:",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nPlease note that by participating in this activity you agree to abide by the NCEAS Code of Conduct.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#session-structure-and-content",
    "href": "index.html#session-structure-and-content",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "Session Structure and Content",
    "text": "Session Structure and Content\nThe course has been designed with several guiding principals in mind.\n\nBalance Theory and Hands-On Work: Spend about one-third of the time on theory and the other two-thirds on hands-on activities.\nBuild Gradually: Start with the basics and build up gradually. Expand both the theory and hands-on tasks as you go along.\nChoose Your Tools: You can use either Jupyter Notebook or VSCode for the hands-on parts of your session. Choose whichever one you’re more comfortable with.\nOpen Data Licensing: Use open data for examples that can be ethically shared and re-used, both within the workshop and when the course materials are used and incorporated into other courses.\nOffer Support: Make sure participants know how to ask for help if they get stuck. Regularly ask participants if they’re keeping up and adjust the pace if needed.\nExtra Resources: Provide additional materials like readings or videos for participants who want to learn more.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "About this book",
    "text": "About this book\nCitation:\n\nWenwen Li, Anna Liljedahl, Matthew B. Jones, Chia-Yu Hsu, Alyona Kosobokova, Jim Regetz, Chandi Witharana, Yili Yang, Ben Galewsky, Minu Mathew, Sandeep Satheesan, Nicole Greco, Kenton McHenry, Carmen Galaz García, Kate Holman Billmeier. 2024. AI for Arctic Research. Arctic Data Center. doi:10.18739/A2222R77V\n\nThe materials in this book are licensed for reuse, and are available from the cyber2a-course github repository. The book is written in Quarto, a cross platform markdown-based platform for writing books and technical materials that works with Python, R, and other languages.\n\nGetting Started: You can find a guide to getting started with Quarto, including editing and previewing content locally with various tools, here.\nQuarto Guide: You can find a comprehensive guide to Quarto here.\nFormat: You can choose to write your content in either Jupyter Notebook (.ipynb) or Markdown files (.qmd). Quarto can render both formats. And both formats can be easily included in other teaching materials.\nEmbed Notebooks: If you choose to write your content in Markdown but have a separate Jupyter Notebook for the hands-on part, you can embed the notebook. Follow the guide here to learn how to do this.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese course materials were developed with funding for Cyber2A from the National Science Foundation under award # 2230034 to W. Li and M. Jones and award # 2230035 to A. Liljedahl and K. McHenry",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "License",
    "text": "License\n\nCyber2A: AI for Arctic Research is licensed under CC BY 4.0",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "",
    "text": "1.1 The changing Arctic\nThe Arctic is one of the Earth’s remaining frontiers that is critical to the Earth’s climate system. Climate warming and change have pushed the Arctic ecosystem to a tipping point: the frozen is becoming unfrozen with subsequent dramatic impact to its terrestrial and coastal landscapes. Permafrost warming and degradation are documented across the Arctic[1], [2], [3], and are coupled with long-term global warming and extremes in air temperature and precipitation [4], [5], [6]. Further, Arctic sea ice is decreasing rapidly [7], which increases coastal erosion rates across the globe [8]. The Arctic region is remote and is experiencing dramatic changes with local and global implications due to the shift from ice to water: altered soil carbon fluxes [9], changes in vegetation cover [10], shifts in animal behavior [11], and challenges to infrastructure [12]. Accordingly, the transformation of ice to water through degrading permafrost and melting sea and lake ice reverberates through the entire Arctic ecosystem and, therefore, enlists the interest of a broad range of earth, engineering, and social science disciplines [13]. Remote sensing of satellite imagery is an important approach in developing Arctic baseline information, monitoring change, and exploring physical processes [14], [15]. Today, there exist important climatic, geological, biological and sociological data that are yet to be exploited by the Arctic science community. To make the best possible use of these data to address the pressing challenges facing the Arctic environment and Arctic people, the more advanced methods and tools that are available need to be applied. AI-driven analytics, especially those incorporating deep machine learning, can process Arctic big data, automatically detect hidden patterns, and derive new knowledge to enable a new wave of data-driven discovery [16].",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#the-changing-arctic",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#the-changing-arctic",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "",
    "text": "Arctic mountains",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#ai-for-arctic-challenges",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#ai-for-arctic-challenges",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.2 AI for Arctic Challenges",
    "text": "1.2 AI for Arctic Challenges\n\n\n\n\n\n\n“AI will be the most transformative technology since electricity.” – Eric Schmidt\n\n\n\n\n\n\n\n\n\n“AI is just another model.” – Unknown\n\n\n\n\n\n\n\n\n\nArtificial Intelligence\n\n\n\n\n\nArtificial Intelligence (AI) in its broadest sense describes the focus on computing systems that exhibit forms of intelligence. Multiple approaches towards AI have been identified, including:\n\nKnowledge Representation and Reasoning to gain a semantic, logical model of a system\nMachine Learning in which statistical models are used for pattern recognition and prediction\nNatural Language Processing for communication in human languages such as English\nExpert Systems using rule-based logical systems for decision-making\nLarge Language Models for filtering and generating language\n…\n\n\n\n\nThe pursuit of AI as a field has been around since the 1956 with the Dartmouth Workshop, but really took a leap forward in the 2010’s with rising performance of computing hardware and new techniques in machine learning, particularly in the field of deep learning. More recently, AI has entered the public consciousness with the promotion of large language models (LLMs) such as the GPT-3 transformer model and related generative AI systems that are based on foundation models and can quickly generate new outputs [17].\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\nMachine learning (ML) is the subfield of AI concerned with pattern detection using statistical models, which then can be applied to unseen data for prediction and extrapolation without explicit instructions [18]. This mechanistic view of ‘learning’ supports robust evaluation of error and has applications in computer vision, image recognition, speech recognition, text processing and filtering, and many more areas.\n\n\n\nTechniques for machine learning are often divided into three types (supervised, unsupervised, and reinforcement learning). These techiques differ based on the feedback provided to the learning system:\n\nSupervised learning: Training input data are labeled (often manually) by a human, and the algorithm learns by generalizing from these inputs to predict correct outputs\nUnsupervised learning: Without labels, the ML algorithm is designed to detect patterns and structure in the input, often using techniques like gradient descent, clustering, and classification algorithms.\nReinforcement learning: A ML algorithm learns dynamically from interactive input to solve a problem or learn a goal, where correct responses are rewarded (weighted) higher than less correct responses. Learning then becomes an optimization/hill-climbing problem.\n\nThese general approaches all have strengths and weaknesses, and are often used in combination to tackle different aspects of a learning problem.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#geospatial-ai",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#geospatial-ai",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.3 Geospatial AI",
    "text": "1.3 Geospatial AI\nIn this course, we will more narrowly focus on geospatial applications of AI, and particularly on the use of deep learning techniques that employ, for example, convolutional neural networks for feature recognition tasks across massive image datasets such as satellite imagery. As we’ll see during the course, advances in computing hardware, and particularly in available Graphical Processing Unit (GPU) performance have enabled massive growth in the scale of models that can be generated. Today, we can train deep learning models on high-resolution, sub-meter scale satellite imagery (e.g., pan-Arctic, 50cm Maxar imagery), and apply the generated models across the Arctic to better understand change at Arctic scales.\nFor one example, Witharana et al. [19] trained a convolutional neural network model on Maxar imagery, and used the trained model to detect permafrost ice-wedges across the entire Arctic at sub-meter scale [20], producing a map of over a billion vector features, and the first-ever permafrost map at this scale.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#welcome-and-introductions",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#welcome-and-introductions",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.4 Welcome and Introductions",
    "text": "1.4 Welcome and Introductions\nLet’s kick the week off with a warm welcome and round of introductions. We’ll start with our Cyber2A project instructors and speakers, and then introduce each of our participants. Everyone is here due to a deep interest in finding solutions to challenges in Arctic science, and everyone is on their own personal journey through data and science. To learn a little about one another, let’s share:\n\nName and affiliation\nYour data science background (be brief!)\nOne! thing you’d like to get out of the course\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#cyber2a-project",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#cyber2a-project",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.5 Cyber2A project",
    "text": "1.5 Cyber2A project\nDespite the power of these machine learning techniques for Arctic research, the Arctic community has been somewhat delayed compared to other geoscience disciplines in adopting these techniques.\n\nThe Cyber2A project aims to build an Arctic learning community to stimulate the use of GeoAI through data science education. This short-course represents a first pass at a survey of relevant AI techniques that would be useful across Arctic regions and disciplines. The goal is to produce an online curriculum and materials that can be used for self-paced learning by Arctic researchers, and can be included in University graduate and undergraduate courses. While there are many online tutorials on machine learning and AI, these materials will specifically target the types of data and challenges typically found in Arctic research, and focus in on the techniques that will make data science learning more approachable.\nThis course is also a starting point, and not an endpoint. We welcome feedback, suggestions, revisions, and edits to the materials. We want people to adopt, adapt, and revise the materials, and, importantly, contribute those changes back so that others can benefit from these curricular advances. Look for more from Cyber2A as we continue to engage in promoting the use of GeoAI across the Arctic.\n\n\n\n\n\n[1] A. K. Liljedahl et al., “Pan-Arctic ice-wedge degradation in warming permafrost and its influence on tundra hydrology,” Nature Geoscience, vol. 9, no. 4, pp. 312–318, Apr. 2016, doi: 10.1038/ngeo2674.\n\n\n[2] A. A. Vasiliev, D. S. Drozdov, A. G. Gravis, G. V. Malkova, K. E. Nyland, and D. A. Streletskiy, “Permafrost degradation in the Western Russian Arctic,” Environmental Research Letters, vol. 15, no. 4, p. 045001, Apr. 2020, doi: 10.1088/1748-9326/ab6f12.\n\n\n[3] S. L. Smith, H. B. O’Neill, K. Isaksen, J. Noetzli, and V. E. Romanovsky, “The changing thermal state of permafrost,” Nature Reviews Earth & Environment, vol. 3, no. 1, pp. 10–23, Jan. 2022, doi: 10.1038/s43017-021-00240-1.\n\n\n[4] T. A. Douglas, M. R. Turetsky, and C. D. Koven, “Increased rainfall stimulates permafrost thaw across a variety of Interior Alaskan boreal ecosystems,” npj Climate and Atmospheric Science, vol. 3, no. 1, pp. 1–7, Jul. 2020, doi: 10.1038/s41612-020-0130-4.\n\n\n[5] R. Í. Magnússon et al., “Extremely wet summer events enhance permafrost thaw for multiple years in Siberian tundra,” Nature Communications, vol. 13, no. 1, p. 1556, Mar. 2022, doi: 10.1038/s41467-022-29248-x.\n\n\n[6] L. M. Farquharson, V. E. Romanovsky, W. L. Cable, D. A. Walker, S. V. Kokelj, and D. Nicolsky, “Climate Change Drives Widespread and Rapid Thermokarst Development in Very Cold Permafrost in the Canadian High Arctic,” Geophysical Research Letters, vol. 46, no. 12, pp. 6681–6689, 2019, doi: 10.1029/2019GL082187.\n\n\n[7] D. Notz and J. Stroeve, “Observed Arctic sea-ice loss directly follows anthropogenic CO2 emission,” Science, vol. 354, no. 6313, pp. 747–750, Nov. 2016, doi: 10.1126/science.aag2345.\n\n\n[8] D. M. Nielsen, M. Dobrynin, J. Baehr, S. Razumov, and M. Grigoriev, “Coastal Erosion Variability at the Southern Laptev Sea Linked to Winter Sea Ice and the Arctic Oscillation,” Geophysical Research Letters, vol. 47, no. 5, p. e2019GL086876, 2020, doi: 10.1029/2019GL086876.\n\n\n[9] L. Bruhwiler, F.-J. W. Parmentier, P. Crill, M. Leonard, and P. I. Palmer, “The Arctic Carbon Cycle and Its Response to Changing Climate,” Current Climate Change Reports, vol. 7, no. 1, pp. 14–34, Mar. 2021, doi: 10.1007/s40641-020-00169-5.\n\n\n[10] T. K. F. Campbell, T. C. Lantz, R. H. Fraser, and D. Hogan, “High Arctic Vegetation Change Mediated by Hydrological Conditions,” Ecosystems, vol. 24, no. 1, pp. 106–121, Jan. 2021, doi: 10.1007/s10021-020-00506-7.\n\n\n[11] S. C. Davidson et al., “Ecological insights from three decades of animal movement tracking across a changing Arctic,” Science, vol. 370, no. 6517, pp. 712–715, Nov. 2020, doi: 10.1126/science.abb7080.\n\n\n[12] L. Suter, D. Streletskiy, and N. Shiklomanov, “Assessment of the cost of climate change impacts on critical infrastructure in the circumpolar Arctic,” Polar Geography, vol. 42, no. 4, pp. 267–286, Oct. 2019, doi: 10.1080/1088937X.2019.1686082.\n\n\n[13] M. L. Druckenmiller et al., “The Arctic,” Bulletin of the American Meteorological Society, vol. 102, no. 8, pp. S263–S316, Aug. 2021, doi: 10.1175/BAMS-D-21-0086.1.\n\n\n[14] M. Philipp, A. Dietz, S. Buchelt, and C. Kuenzer, “Trends in Satellite Earth Observation for Permafrost Related Analyses—A Review,” Remote Sensing, vol. 13, no. 6, p. 1217, Jan. 2021, doi: 10.3390/rs13061217.\n\n\n[15] “Changing state of Arctic sea ice across all seasons - IOPscience.” Accessed: Oct. 18, 2024. [Online]. Available: https://iopscience.iop.org/article/10.1088/1748-9326/aade56\n\n\n[16] “AI in Analytics: Top Use Cases and Tools.” Accessed: Oct. 18, 2024. [Online]. Available: https://www.marketingaiinstitute.com/blog/how-to-use-artificial-intelligence-for-analytics\n\n\n[17] A. Vaswani et al., “Attention is all you need,” in Advances in neural information processing systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., Curran Associates, Inc., 2017. Available: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n\n\n[18] M. I. Jordan and T. M. Mitchell, “Machine learning: Trends, perspectives, and prospects,” Science, vol. 349, no. 6245, pp. 255–260, Jul. 2015, doi: 10.1126/science.aaa8415.\n\n\n[19] C. Witharana et al., “An Object-Based Approach for Mapping Tundra Ice-Wedge Polygon Troughs from Very High Spatial Resolution Optical Satellite Imagery,” Remote Sensing, vol. 13, no. 4, p. 558, Jan. 2021, doi: 10.3390/rs13040558.\n\n\n[20] C. Witharana et al., “Ice-wedge polygon detection in satellite imagery from pan-Arctic regions, Permafrost Discovery Gateway, 2001-2021,” 2023, doi: 10.18739/A2KW57K57.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html",
    "href": "sections/ai-for-everyone.html",
    "title": "2  AI for Everyone",
    "section": "",
    "text": "2.1 Learning Objectives\nThis session introduces AI to a non-specialist audience, ensuring participants can understand essential concepts. The focus is on key terminology and principles of machine learning (ML), deep learning (DL), and neural networks (NN). By the end of this session, participants will have foundational knowledge of AI concepts to engage with more advanced topics.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#table-of-contents",
    "href": "sections/ai-for-everyone.html#table-of-contents",
    "title": "2  AI for Everyone",
    "section": "2.2 Table of Contents",
    "text": "2.2 Table of Contents\n\nWhat is AI? History and Challenges\nTypes and Techniques\nWorking with Data\nThe Role of AI in Modern Research\nThe Future of AI & Science\nHands-On: Setting up the Coding Environment",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#what-is-ai",
    "href": "sections/ai-for-everyone.html#what-is-ai",
    "title": "2  AI for Everyone",
    "section": "2.3 What is AI?",
    "text": "2.3 What is AI?\nArtificial Intelligence (AI) refers to computer systems capable of performing tasks requiring cognitive functions, such as recognizing patterns, learning from data, and making predictions.\nBut what is human intelligence? Well, we don’t really know…\nLet’s focus on a simpler question. \nWe think experientially or in other words, probabilistically.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#ai-types-and-techniques",
    "href": "sections/ai-for-everyone.html#ai-types-and-techniques",
    "title": "2  AI for Everyone",
    "section": "2.4 AI Types and Techniques",
    "text": "2.4 AI Types and Techniques\n\n2.4.1 Machine Learning (ML)\n\n\nSupervised Learning: Training on labeled data.\nUnsupervised Learning: Discovering patterns within unlabeled data.\nSemi-Supervised: Training on autogenerated labeles.\nReinforcement Learning (RL): An agent learns by interacting with an environment, commonly used in robotics.\n\n\n\n2.4.2 Neural Networks (NN)\nNeural Networks are loosely inspired by the brain’s structure, NNs consist of interconnected nodes (neurons) that process information. NN Playground:\n   \n\n\n2.4.3 Deep Learning\n   \n\nConvolutional Neural Networks (CNN) are specialized for processing structured data, like images.\nLarge Language Models (LLM) are neural networks that understand and generate human language (e.g., GPT-3).\nRecurrent Neural Networks (RNN) handle sequential data by remembering previous inputs,. They are used for tasks like speech recognition and time series.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#data",
    "href": "sections/ai-for-everyone.html#data",
    "title": "2  AI for Everyone",
    "section": "2.5 Data",
    "text": "2.5 Data\n“Garbage in, garbage out”, or quality data leads to accurate models.\n\nStructured Data: Clearly defined types (e.g., temperature readings).\nUnstructured Data: Complex formats (e.g., satellite imagery), requiring preprocessing.\n\nKey takeaway: Data is the foundation for accurate predictions and insights in AI.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#rapid-surge-in-ai-talk-why-now",
    "href": "sections/ai-for-everyone.html#rapid-surge-in-ai-talk-why-now",
    "title": "2  AI for Everyone",
    "section": "2.6 Rapid Surge in AI Talk: Why Now?",
    "text": "2.6 Rapid Surge in AI Talk: Why Now?\n\n2.6.1 Brief History:\n\n1986: Backpropagation was introduced by Geoffrey Hinton and colleagues, revolutionizing neural networks by enabling them to adjust weights and improve through learning. This breakthrough laid the foundation for modern deep learning.\n2017: The Transformer architecture was introduced, which transformed natural language processing (NLP) using an attention mechanism. It allowed models to weigh the importance of words in a sequence and process entire sentences in parallel, significantly improving efficiency and accuracy.\n2018-2020: The combination of backpropagation, advanced computing power (GPUs, cloud technology), and the availability of large datasets led to the rise of Large Language Models (LLMs) like GPT-3, with 175 billion parameters, capable of generating human-like text and answering questions.\n\nKey takeaway: The transformer model paved the way for advanced LLMs like GPT-3, driving the current AI surge. Advancements in computing power allowed these models to process massive datasets quickly, enabling them to generate highly accurate predictions, perform complex tasks, and revolutionize fields such as language processing, automation, and research.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#the-future-of-ai-in-science",
    "href": "sections/ai-for-everyone.html#the-future-of-ai-in-science",
    "title": "2  AI for Everyone",
    "section": "2.7 The Future of AI in Science",
    "text": "2.7 The Future of AI in Science\n\nScientific Method and AI:\n\nObservation: AI aids data collection (e.g., computer vision).\nHypothesis: Unsupervised learning clusters data.\nExperiment: Reinforcement learning simulates environments.\nConclusion: AI can analyze data and validate hypotheses. AI helps identify patterns in data and can support every step of the scientific method, from observation to conclusion.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#additional-resources",
    "href": "sections/ai-for-everyone.html#additional-resources",
    "title": "2  AI for Everyone",
    "section": "2.8 Additional Resources",
    "text": "2.8 Additional Resources\nBooks: - Ray Kurzweil, How to Create a Mind (2012) - Max Bennett, A Brief History of Intelligence (2023)    Videos: - 3Blue1Brown’s Neural Networks Playlist\nPapers: - Jakob Uszkoreit, The Transformer (2017)",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#do-you-have-any-questions",
    "href": "sections/ai-for-everyone.html#do-you-have-any-questions",
    "title": "2  AI for Everyone",
    "section": "2.9 Do You Have Any Questions?",
    "text": "2.9 Do You Have Any Questions?\nFeel free to reach out! /n - Email: alyonak@nceas.ucsb.edu - Website: alonakosobokova.com - YouTube: Dork Matter Girl",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html",
    "href": "sections/ai-ready-data-in-arctic-research.html",
    "title": "3  AI-Ready Data in Arctic Research: Principles and Practices",
    "section": "",
    "text": "Goal\nThis session provides an understanding of ‘AI-ready data’ in Arctic science and geoscience, highlighting the importance of suitable data for AI applications. Participants will learn about creating and managing metadata and organizing data repositories. We’ll discuss best practices for data preparation and structuring for AI processing. By the end, participants will clearly understand AI-ready data characteristics and the steps to transform raw data for AI applications.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research: Principles and Practices</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#key-elements",
    "href": "sections/ai-ready-data-in-arctic-research.html#key-elements",
    "title": "3  AI-Ready Data in Arctic Research: Principles and Practices",
    "section": "Key Elements",
    "text": "Key Elements\nAI-ready data definition, data suitability importance, metadata management, data repository organization, data preparation practices, raw data transformation",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research: Principles and Practices</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html",
    "href": "sections/data-annotation.html",
    "title": "Data Annotation: The Foundation of Deep Learning Models",
    "section": "",
    "text": "Goals\nThis session explores the critical role of training data in deep learning, focusing on data annotation methods, tools, and strategies for acquiring high-quality data. Participants will learn how well-annotated data supports effective deep learning models, understanding the challenges and best practices in data annotation. By the end, participants will be equipped to prepare their datasets for deep learning.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#key-elements",
    "href": "sections/data-annotation.html#key-elements",
    "title": "Data Annotation: The Foundation of Deep Learning Models",
    "section": "Key Elements",
    "text": "Key Elements\nTraining data’s role, annotation methods/tools, annotated data’s importance, annotation challenges, annotation best practices, dataset preparation",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#annotation-fundamentals",
    "href": "sections/data-annotation.html#annotation-fundamentals",
    "title": "Data Annotation: The Foundation of Deep Learning Models",
    "section": "4.1 Annotation Fundamentals",
    "text": "4.1 Annotation Fundamentals\n\n\n\n\n\n\nHighlights\n\n\n\n\nReiterate ideas related to supervised learning, and the core idea of learning from examples\nDiscuss key role of labeling/annotation in general for generating examples to learn from\nTake a quick tour of label/annotation examples across various ML applications (structured data, text, audio, image, video, etc)\nTalk about some general challenges of procuring/producing labeled data for Machine Learning\n\n\n\n\n4.1.1 Fueling intelligence: It’s All About the Data!\nThe modern AI renaissance is driven by the synergistic combination of Computing advances, more & better data for training, and Algorithmic innovations.\n\n\n\nSource: OECD.ai\n\n\nEach of these is critical, but you really can’t overstate the importance of massively upscaling training and validation data. Indeed, to a large extent, the most important advances in algorithms and compute are indeed the ones that are allowing us to efficiently use the large amount of data. The more data available, the better the model can learn.\nRemember that in Machine Learning:\n\nYou are building a model to produce some desired output for a given input. Imagine handing a model an aerial photo that contains a water body, or a camera trap video that contains a bear, or an audio recording that captures a the song of a particular bird species. In each case, you want the model to correctly detect, recognize, and report the relevant feature.\nTo achieve this, you do not build this model by instructing the computer how to detect the water body or the bear or the bird species. Instead, you assemble many (often many, many!) good examples of the phenomena of interest, and feed them to an algorithm that allows the model to adaptively learn from these examples. Now, in practice there may be rule-based guardrails, but we can talk about that separately later in the course.\n\n\n\n\n.\n\n\nMuch of this course is about understanding what kinds of model structures and learning algorithms allow this seemingly magical learning to happen inside the computer, and what the end-to-end process looks like. But for the sake of these next couple of sections, what is important is that this core concept makes sense to you:\nFor any given project, you will quite likely be pulling a generic AI tool off the shelf that has basically no particular knowledge of your particular application area, and the way you will adapt it to apply to your project is not by hand-tweaking parameters or choosing functional forms or anything like that, but rather by (again) exposing the algorithm to many examples.\nBottom line, much like vehicles without fuel, even the best training algorithms in the world will just sit and gather dust if they don’t have sufficient data to learn from!\n\n\n\nSource: Walking Dead Fandom\n\n\nTherefore although a lot of this week is about the models and how to operationalize them on compute platforms, your success in applying AI (especially if you are training and/or fine-tuning models, not simply applying pre-trained models) will depend on having a robust and effective data pipeline, from data collection methods to data annotation to data curation.\n\n\n\n\n\nSource: DZone\n\n\nIn this module, we focus on data annotation.\n\n\n4.1.2 What is annotation?\nData annotation is the process of labeling or marking up data with information that is not already explicit in the data itself.\nIn general, we do this to provide important and relevant context or meaning to the data. As humans, especially in knowledge work, we do this all the time for the purpose of sharing information with others.\n\n\n\nSource: PowerPoint Tricks\n\n\nIn the context of Machine Learning and AI, our objective is to teach a model how to create accurate and useful annotations itself when it encounters new, unannotated data. In order to do this, we need to provide the model with annotated examples that it can train on.\nTo put it a different way, annotation is the process of taking some data just like the kind of data you will eventually feed into the model, and attaching to it the correct answer to whatever question you will be asking the model about that data.\nSimply put, annotation refers to labeling data with information that a model needs to learn, and is not already inherently present in the data.\n\nNote: The term “annotation” is synonymous with “labeling”\n\n\n4.1.2.1 Examples\n\n\n\n\n\n\n\nTabular Data Annotation\n\n\n\n\n\n\n\nLabel (aka Target) column: Species\n\nWhen working with tabular data, we don’t usually talk about “annotating” the data. Nevertheless, the concept of labeling for supervised learning tasks (such as classification and regression) still applies, and indeed it’s common practice to refer to the data used for classification and regression model training as “labeled data”. Labeled tabular data contains a column designated as the target for learning, i.e. the column containing the value that a model learns to predict. Depending on the context (and background of the writer/speaker), you might also hear this referred to as the label, outcome variable, dependent variable, or even just y variable. If this is not already inherently present in the dataset, it must be added by an annotator before proceeding with modeling.\n\n\n\n\n\n\n\n\n\nText Annotation\n\n\n\n\n\n\n\nSentiment: Positive\nParts of speech: most::adv, beautiful::adj\nNamed entity: Alaska\n\n\n\n\n\n\n\n\n\n\nAudio Annotation\n\n\n\n\n\n\n\nVoice recognition\nSpeech to text\n\n\n\n\n\n\n\n\n\n\nImage Annotation\n\n\n\n\n\n\n… our focus today and this week! See details below.\n\n\n\n\n\n\n\n\n\n\nVideo Annotation\n\n\n\n\n\n\nLike image annotation, but with many frames! The focus is often on tracking movement of objects, detecting change, and recognizing activities.\n\n\n\n\n\n\n4.1.3 Why is annotation so important?\nWe’ve already talked about the critical role of data overall in enabling supervised learning, and the role of annotation in explicitly adding or revealing the information in the data.\nMore specifically, the annotated data will be used at training time, when a specific learning algorithm will use the information in your annotated data to update internal parameters to yield a specific parameterized (aka “trained”) version of the model that can do a sufficiently good job at getting the right answer when exposed to new data that it hasn’t seen before, and doesn’t have labels.\nThe overall volume and quality of the annotations will have a huge impact on the following characteristics of a model trained on those data:\n\nAccuracy\nPrecision\nGeneralizability\n\nObviously there is a bit of tension here! The point of training the model is do something for you. But in order for the AI to be able to do this, you have to first teach it how, which means doing the very thing that you want it to do.\nThink of it like hiring a large team of interns. Yes, it takes extra time up front to get them trained up. But once you do that, you’re able to scale up operations far beyond what you could do on your own.\nThis raises a few questions that we’ll touch on as we proceed through the course:\n\nIs there a model out there that already knows at least something about what I’m trying to do, so I’m not training it from scratch? Maybe yes! This is a benefit that foundation models (and more generally, transfer learning) offer. To build on the human intern analogy, if you can hire undergrad researchers studying in a field relevant to the task, you’re likely to move much faster than if you hired a 1st grader!\nHow much annotated data do I need? Unfortunately, there is no simple answer. It depends on complexity of task, the clarity of the information, etc. So as we’ll discuss, best practice is to proceed iteratively.\n\n\n\n4.1.4 Annotation challenges\nBy now it should be clear that your goal in the data annotation phase is to quickly and correctly annotate a large enough corpus of inputs that collectively provide an adequate representation of information you want the model to learn.\nHere are some of the key challenges to this activity:\n\n\n\n\n\n\nScalability\n\n\n\n\n\nSimply put, annotating large datasets can be time-consuming!\nThis is especially the case for more complex annotation tasks. Identifying a penguin standing on a rock is one thing, but comprehensively identifying and label all land cover types present in a satellite image is much more time-consuming. Multiply this task by hundreds or thousands, and you’ve quite a scaling challenge!\n\n\n\n\n\n\n\n\n\nCost\n\n\n\n\n\nCosts become important in conjunction with the scalability challenge.\nYou may find you need to pay for:\n\nAnnotators’ time, whether they are directly employed or used via a service\nAnnotation software or services, if you go with a commercial tool vendor\nData storage, if you are leveraging your own hardware and/or cloud providers like AWS to store large amounts of data\nCPU/GPU cycles, if you are leveraging your own hardware or cloud services to run annotation software, especially if you are using AI-assisted annotation capabilities\n\n\n\n\n\n\n\n\n\n\nQuality control\n\n\n\n\n\nAnnotation is not always straightforward and easy, but as we’ve discussed, effective model training depends on producing sufficiently high quality annotations of sufficiently high quality training data.\nSome factors to consider:\n\nSource data quality. Is the information signal clear in the data? And does the input dataset include a sufficiently diverse set of examples that are representive of what the model will encountered when deployed?\nAnnotation consistency. Do the annotations capture information in the same way across images? This becomes an even bigger factors when multiple annotators are involved. Clear annotation guidelines and tracking various consistency metrics can help here.\nAnnotation quality. Are the annotations accurate, precise, and complete? Have annotators introduced bias?\n\nIn the end, you will likely need to strike balance between speed and quality. Determining the right goalposts for “good enough” will require experimentation and iterative model training/testing.\n\n\n\n\n\n\n\n\n\nSubjectivity\n\n\n\n\n\nIn some applications, there is no clear correct answer! In that case, especially without clear guidelines and training, different annotators can interpret data differently. This can leading to inconsistent labels, which in turn will negatively impact model training and lead to degraded model performance.\n\n\n\n\n\n\n\n\n\nData and annotation management\n\n\n\n\n\nOn a practical front, effectively managing a large-scale annotation activity also requires managing and organizing all associated annotation artifacts, including both the input data and the generated annotations.\nIf you are performing annotation across a team of people, and now you also need to likely need to keep track of multiple annotations per data object (performed across multiple annotators), metadata associated with those annotations (e.g., how long the annotator took to complete the task), and various metrics for monitoring annotation and annotator performance over time.\n\n\n\n\n\n\n\n\n\nData privacy & security\n\n\n\n\n\nThis is especially important if you use a cloud-based tool for annotation. What is their data privacy and security policy, and is it sufficient to meet your needs?\n\n\n\n\n\n\n\n\n\nBias & Ethics\n\n\n\n\n\nManaging bias and ethics is not an annotation-specific problem, and we’ll discuss this later in the cousre. However, bear in mind that annotation can be a major factor, because it is a step in the modeling process when some specific human knowledge (i.e., what the annotators know) is attached to the input data, and will very directly exposed to the model during training. This creates an opportunity for injecting bias, exposing sensitive or private information, among other things.\n\n\n\n\n\n\n\n\n\nCallout: Annotating satellite imagery\n\n\n\n\nLabeling of satellite imagery brings its own specific challenges. Consider:\n\nScenes are often highly complex and rich in detail\nGeographic distortion: Angle of sensor\nAtmospheric distortion: Haze, fog, clouds\nVariability over time:\n\nWhat time of day? Angle of the sun affects visible characteristics\nWhat time of year? Many features change seasonality (e.g. deciduous forest, grasslands in seasonally arid environments, snow cover, etc)\nFeatures change! Forests are cut, etc. Be mindful of the difference between labeling an image and labeling a patch of the earth’s surface.\n\nIt’s often desirable to maintain the correspondence between pixels and their geospatial location, for cross-reference with maps and/or other imagery\n\n\n\n\n\n4.1.5 Annotation best practices\n\n\n\n\n\n\nDevelop a thorough annotation protocol\n\n\n\n\n\nCreate and maintain clear labeling instructions.\n\n\n\n\n\n\n\n\n\nProvide annotator training\n\n\n\n\n\n\nWork with annotators to make sure they understand the domain, use cases, and overall purpose of the project.\nProvide specific guidance about what to do in ambiguous or difficult cases, in order to help standardize annotations.\nConsider having new annotators apply annotations on a set of sample inputs, assess those annotations, and provide clear feedback with reference to what they could or should do better.\n\n\n\n\n\n\n\n\n\n\nHave a quality control process\n\n\n\n\n\nTo ensure sufficient quality, plan on doing regular checks, running cross-validations, and having feedback loops.\nFirst, periodically conduct manual annotation reviews to ensure compliance with instructions. This might include having a recognized expert on the team randomly selecting a subset of annotated images to assess.\nSecond, identify and calculate quality metrics on an ongoing basis, targeting each of the following:\nConsensus. To measure the degree to which different annotators on the team are providing similar annotations, have multiple annotors annotate some of the same images, and calculate a consensus measure like Inter-annotator agreement (IAA). Several flavors of this metric exist, such as Cohen’s kappa (to compare 2 labelers) and Fleiss’ kappa (to compare &gt;2 labelers).\nAccuracy. In cases where there’s a known “correct” answer, either for all images or some subset thereof, calculate annotation performance metrics. Here are a couple of examples: - For bounding boxes, calculate a metric like Intersection over union (IoU): Take the area of overlap between the ground truth box and the annotated box, and divide by total area of the (unioned) boxes. - For detected objects overall, calculate standard metrics like precision (proportion of labeled objects that are correctly labeled) and recall (proportion of all objects that were correctly labeled)\nCompleteness. Keep track of annotation completeness overall. For example, when doing bounding box annotation for an object detection task, ensure that all drawn boxes are associated with a valid label.\n\n\n\n\n\n\n\n\n\nProceed iteratively!\n\n\n\n\n\nIn a nutshell: Start small, refine, and scale gradually.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#image-annotation-methodology",
    "href": "sections/data-annotation.html#image-annotation-methodology",
    "title": "Data Annotation: The Foundation of Deep Learning Models",
    "section": "4.2 Image Annotation Methodology",
    "text": "4.2 Image Annotation Methodology\n\n\n\n\n\n\nHighlights\n\n\n\n\nDiscuss the primary types of image annotations\nDiscuss the common image-related AI/ML tasks requiring annotation\nDiscuss different methods for annotating images\nDescribe a high level annotation workflow\n\n\n\nIt’s important to understand and recognize the difference between image annotation types, tasks, and methods. Note that this isn’t universal or standardized terminology, but it’s pretty widespread.\nIn this context:\n\nAn annotation type describes the specific format or structure of the annotation used to convey information about the data critical for supporting the task.\nAn annotation task is the specific objective that the annotations are meant to support, i.e. the job you want your AI application to do. In the computer vision context, this typically means identifying or understanding something about an image, and conveying that information in some specific form.\nAn annotation method refers to the process or approach used to create the annotations.\n\n\n4.2.1 Image Annotation Types\nThe type of annotation you apply will depend partly on the task (see next section), as different annotation types are naturally suited for different tasks. However, the decision will also be driven in part by time, cost, and accuracy considerations.\n\n\n\n\n\n\nImage tags\n\n\n\n\n\nTags are categorical labels, words, or phrases associated with the image as a whole, without explicit linkage to any localized portion of the image. \n\nLabel: beach\nCaption: “Embracing the serenity of the shore, where the sky meets the ocean #outdoor #beachlife #nature”\n\n\n\n\n\n\n\n\n\n\nBounding boxes\n\n\n\n\n\nBounding boxes are rectangles drawn around objects to localize them within an image. \nTypically they are axis-aligned, meaning two sides are parallel with the image top/bottom, and two sides are parallel with the image sides, but sometimes rotation is supported.\n\n\n\n\n\n\n\n\n\nPolygons\n\n\n\n\n\nGeneralizing the bounding box concept, polygons are a series of 3 or more connected line segments (each with definable end coordinates) that form a closed shape (i.e. the end of the last segment is the beginning of the first segment), used to more precisely localize objects or areas by outlining their shape. \n\n\n\n\n\n\n\n\n\nSegmentations\n\n\n\n\n\nSegmentations involve assigning a class label to individual pixels (or collectively, to regions of individual pixels) in an image. Segmentation may be done either fully for all pixels, or partially only for pixels associated with phenomena of interest.\nIn practice, pixel segmentations can be produced either by drawing a polygon or using a brush tool.\n\n\n\n\n\n\n\n\n\n\nKeypoints\n\n\n\n\n\nKeypoints are simply points, used for denoting specific landmarks or features (e.g., skeletal points in human pose estimation). \n\n\n\n\n\n\n\n\n\nPolylines\n\n\n\n\n\nPolylines are conceptually similar to polygons, but they do not form a closed shape. Instead, the lines are used to mark linear features such as roads, rivers, powerlines, or boundaries. \n\n\n\n\n\n\n\n\n\n3D Cuboids\n\n\n\n\n\n3D cuboids are bounding boxes extended to three dimensions. These are often used in LiDAR data which is represented as a 3-dimensional point cloud, but can also be used to indicate depth of field in a 2D image when the modeling task involves understanding position in three dimensions. \n\n\n\n\n\n4.2.2 Image Annotation Tasks\nThe task you choose will depend on the type of information you want the model to extract from the images. Here are the key types of annotation tasks in computer vision:\n\n\n\n\n\n\nImage Classification\n\n\n\n\n\nImage classification is the task of assigning an entire image to a category.\nThe classification typically refers to some singular dominant object or feature (e.g., “Polar bear”) within the image, or some defining characteristic of the image (e.g., “Grassland”), but the details depend on the specific use case motivating the modeling exercise.\n\n\n\n\n\n\n\n\n\nImage Captioning\n\n\n\n\n\nImage captioning is the task of generating textual descriptions of the image. It is conceptually similar to image classification, but involves producing freeform text for each image rather than assigning the image to one of a set of pre-defined categorical classifications.\n\n\n\n\n\n\n\n\n\nObject Detection\n\n\n\n\n\nObject detection is the task of identifying one or more objects or discrete entities within an image.\nNote that object detection involves two distinct sub-tasks:\n\nLocalization: Where is the object within the image?\nClassification: What is the localized object?\n\n\n\n\n\n\n\n\n\n\nImage Segmentation\n\n\n\n\n\nSegmentation is the task of associating individual pixels annotation for detailed image analysis (e.g., land-use segmentation). In some sense, you can think of it as object detection reported at the pixel level.\nThere are three distinct kinds of segmentation, illustrated below for the following image: \nSemantic Segmentation assigns a class label to each pixel in the image, without differentiating individual instances of that class. It is best for amorphous and uncountable “stuff”. In the image below, notice the segmentation and separation of the foreground grass from the background trees from the water in the middle. Also notice that the bears are all lumped together in one segment.\n\nInstance Segmentation separately detects and segments each object instance. It’s therefore similar to semantic segmentation, but identifies the existence, location, shape, and count of objects. It is best for distinct and countable “things”. Notice the separately identified four bears in the image below:\n\nPanoptic Segmentation) combines semantic segmentation + instance segmentation by labeling all pixels, including differentiation of discrete and separately objects within categories. Notice the complete segmentation in the image below, including both the various background types as well as the four distinct bears.\n\nFor more on Panoptic Segmentation, check out the research publication.\n\n\n\n\n\n\n\n\n\nTemporal Annotation\n\n\n\n\n\nTemporal annotation is the task of labeling satellite images over time to track changes in environmental features.\n\n\n\n\n\n4.2.3 Image Annotation Methods\nThe annotation method largely boils down to whether annotations are done manually versus with some level of supporting automation. Ultimately, the choice involves project-specific determination of the cost, speed, and quality of human annotation relative to what can be achieved with available AI assistance.\n\n\n\n\n\n\nManual Annotation\n\n\n\n\n\nWith purely manual annotation, all labeling is done by human annotators.\nNote that good tooling may help make this process easier and more efficient, but ultimately it is up to the human annotator to fully apply the annotation to an unlabeled input.\n\n\n\n\n\n\n\n\n\nSemi-Automated Annotation\n\n\n\n\n\nWith semi-automated annotation, machines assist humans in generating annotations, but humans are still heavily involved in real time with labeling decisions, ranging from actually applying the annotations to refining AI-generated annotations.\nNote that this can take many forms. For example:\n\nModel-based filtering: A model is trained to recognize images with any candidate objects (as compared to empty scenes), and is used to reduce the number of images passed to the human annotator.\nModel-suggested labels: A pre-trained model provides hints, e.g. multiple options of likely object classes, and the human makes the decision.\nModel-assisted labeling: A pre-trained model generates a candidate annotation, which the human can accept, reject, or modify in some way (e.g., size, position, category).\nActive Learning: A model is learning how to annotate the images alongside the human, and actively decides which images the human should label to accelarate model training the fastest.\n\n\n\n\n\n\n\n\n\n\nAutomated Annotation with Human Validation\n\n\n\n\n\nAt the level of automated annotation with human validation, AI models generate most annotations autonomously. Humans only review the results after the fact, typically checking accuracy metrics at a high level and perhaps inspecting a random sample of annotations, rather than reviewing every annotation.\nExample: A pre-trained model processes satellite images and automatically labels roads, rivers, and forests across thousands of images. A human reviewer then inspects a small percentage of these results to confirm the annotations are accurate, fixing any errors and perhaps fine-tuning the model before the dataset is finalized.\nAt first glance, it might seem illogical that this scenario could exist! If you already have a model that can do the annotation, then don’t you already have a model to do the actual task you want to do?\nIn practice, however, there are some cases where this might be applicable:\n\nOne scenario involves model distillation. Imagine there exists a big, expensive, and/or proprietary (i.e., hidden behind an API) model that does the task you want, and perhaps a lot more. You can use this model to generate labels for a dataset that you use to train your own more compact model or economical model that you control. In the end, you have effectively distilled the source model’s capability into your own model, through the annotated training data set.\nA second scenario is when your goal is not simply to train a model that can annotate data, but to use that model to annotate vasts amounts of data that you then use as inputs to some other machine learning or analysis pipeline. Indeed, in research settings, this is usually the objective! In that case, arguably the application of the trained annotator model will be carried out as automated annotation with human validation.\n\n\n\n\n\n\n\n\n\n\nFully Automated Annotation\n\n\n\n\n\nRare in practice! Under fully automated annotation, trained models generate annotations with no human involvement, and the quality is deemed sufficient without review.\nThis is typically only relevant in very specific settings, namely in environments where the image data is very highly controlled. For example, consider images that were produced in a lab setting where the composition of the images is highly controlled, or images that were generated synthetically by some known computational agent (e.g., in video games). A related approach with synthetic data involves using trained AI models to generate both the images and their corresponding annotations, in which case the annotation ground truth for each image.\n\n\n\n\n\n4.2.4 Data Annotation Workflow\n\n\n\n\n\n\n1 - Data collection\n\n\n\n\n\nFirst step: Get a sufficiently large and diverse set of data to annotate and subsequently train on.\nYou may already have a set of images from your own research, e.g. from a set of camera traps or aerial flights. Or perhaps you already have a clear use case around detecting features in a particular satellite dataset, and have already procured the imagery. If so, great.\nIf you don’t have your own imagery – and maybe even if you do – you may want to consider augmenting it with additional images if you don’t have enough diversity or content in your own imagery. Depending on your use cases, you may want to poke around public mage datasets like ImageNet.\n\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n2 - Tool Selection\n\n\n\n\n\nTime to choose your annotation tool/platform!\nThere are many options, and lots of factors to consider. See the next section for plenty more detail.\n\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n3 - Data preprocessing\n\n\n\n\n\nBefore proceeding, it’s almost always useful (some sometimes essential) to apply various preprocessing tasks to your data to make it easeir to annotatate and/or eventually train on.\n\n\n\nSource: Medium\n\n\nHere are some categories of common preprocessing tasks:\nReformatting. If relevant, you may need to convert your source images into a better file format for your task. Beyond this, it may be useful to rotate, crop, rescale, and/or reproject your images to get them into a consistent structural format.\nBasic data cleaning. - For example, with satellite or aerial imagery, you may find it useful to apply pre-processing stesp such as filtering to removing noise, correcting for atmospheric conditions, correcting other distortion, adjusting brightness/contrast/color.\nFeature enhancement. Other context-specific transformations may be useful for “bringing out” information for the model (and human annotators) to use, leading to faster and/or better model outcomes. As an example, you can hear about how careful transformations of Sentinel 2 imagery provided a huge boost in the detection of field boundaries as part of the UKFields project.\n\n\n\n\n\n\n\n\n\n4 - Guideline Development\n\n\n\n\n\nAs we discussed earlier, before you begin in earnest, it’s critical that you develop specific guidelines for annotators to follow when doing the annotation using the selected tool.\nNote: These should be written down! Some annotation platforms provide a way to document instructions within the tool, but if yours doesn’t (and probably even if it does), you should create and maintain your own written documentation\n\n\n\nSource: Acquiro\n\n\nOften this will be based on a combination of prior knowledge and task familiarity. To the extent that nobody on the project has extensive experience with the task at hand, it’s often helpful to do some prototyping to inform development of the guidelines.\n\n\n\n\n\n\n\n\n\n5 - Annotation\n\n\n\n\n\nIt’s time to annotate!\n\n\n\nSource: shaip\n\n\nKeep in mind the following image annotation best practices. They may not always hold, but in general:\n\nKeeping bounding boxes and polygons “tight” to the object:\nFor occluded objects, annotate as if the entire object were in view\nIn general, label partial objects cut off at the edge\nLabel all relevant objects in the image. Otherwise, “negative” labels will hamper model learning.\n\nAbove all else, remember, consistency is critical!\n\n\n\n\n\n\n\n\n\n6 - Quality Assurance\n\n\n\n\n\nReview the annotations for quality, and if needed, refine by returning to an earlier step in the workflow.\n\nNote that although QA is identified here as a discrete stage in the workflow, in practice quality is achieved through deliberate attention at multiple stages in the process, including:\n\nInitial annotator workforce training before any annotation is done\nContinuous monitoring during the annotation process\nFinal post-annotation review\n\n\n\n\n\n\n\n\n\n\n7 - Data Export\n\n\n\n\n\nFinalize and output the annotated data for model training.\n\nTypically you will need to get the data into some particular format before proceeding with model training. If your annotation tool can export in this format, you’re all set. If not, you’ll need to export in some other format and then use a conversion tool that you either find or create yourself.\n\n\n\nFrom here, presumably you’ll move on to model training!\nRemember this key best practice: Iterate! You will almost certain not proceed through the annotation workflow in one straight shot. Plan to do some annotations, train, test, fix annotations, figure out whether/how to do more and/or better annotations, refine your annotation approaches, etc.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#annotation-tools-platforms",
    "href": "sections/data-annotation.html#annotation-tools-platforms",
    "title": "Data Annotation: The Foundation of Deep Learning Models",
    "section": "4.3 Annotation Tools & Platforms",
    "text": "4.3 Annotation Tools & Platforms\n\n\n\n\n\n\nHighlights\n\n\n\n\nGet a sense of what kind of tools are out there today!\nDiscuss high level considerations for choosing a tool\nReview some specific tools out there today\nHighlight how fast things are changing!\n\n\n\n\n4.3.1 High level considerations\nHere are some questions you should be asking…\n\n\n\n\n\n\nWhat annotation types are supported?\n\n\n\n\n\nDoes the tooling allow you do create the kinds of annotations necessary for your task? This probably the first and most fundamental question you should be asking!\n\n\n\n\n\n\n\n\n\nWhat import image formats are supported?\n\n\n\n\n\nCan the tool read images in the right format?\nFortunately, most tools can automatically take a wide range standard image formats including JPG, PNG, BMP, and TIF, and more.\nHowever, if you are working with spatial imagery, including GeoTIFFs, most tools will not natively read in your data. You will need to convert between formats, or choose a tool that is explicitly designed to handle that kind of data.\n\n\n\n\n\n\n\n\n\nWhat output annotation formats are supported?\n\n\n\n\n\nWhile image formats are reasonably standardized, image annotation formats are more diverse. In general, the format you need will be dictated by the constraints of whatever modeling tasks and tooling you will be using to train and validate a model with your annotated data.\nSome annotation software, especially the major players and cloud-based offerings, support a diverse set of output formats, whereas others output only a limited number of formats – or even just their own idiosyncratic format! In that case, you may need to do a conversion to get your annotations in the right format. Fortunately, there’s a good chance that somebody else has already been down this path, and if you search around, you may find a script or package that can do it for you.\nExample formats (not exhaustive!):\n\nVarious JSON formats\n\nCOCO JSON\nVGG Image Annotator JSON\nLabelMe JSON\n\nYOLO TXT\nPascal VOC XML\nTensorFlow TFRecord\n… and lots more …\n\nSee this great page for exploring many different formats.\n\n\n\n\n\n\n\n\n\nWho will be doing the annotation?\n\n\n\n\n\n\nIn-house: You and your team.\nCrowdsource: The broader community.\nOutsource: External people with whom you contract, either directly or through a 3rd party annotation services company. Yes, these do exist!\n\n\n\n\n\n\n\n\n\n\nHow can I assess annotation quality?\n\n\n\n\n\nWe’ve discussed the importance of having high quality annotations, and briefly covered various types of quality assessment. Some tools leave it entirely up to you to handle this, but others have features that help in this area. This can include:\n\nAutomatic calculation of various quality metrics\nConfigurable mechanisms for distributing images among annotators, and choosing how many annotators will see each image\nVarious other forms of annotation process metadata and analytics\n\n\n\n\n\n\n\n\n\n\nIs the tool easy to use?\n\n\n\n\n\nAs with any category of software, some options will be easier to use than others. For image annotation, where you are likely going to want to scale up to a large number of images, small speed-ups in the annotation process will really start to add up over time.\nConsider:\n\nIs the software easy to navigate in general?\nDoes the annotation interface have responsive, reliable, and easy-to-use UI elements for creating, modifying, and deleting image annotations?\nAre there effective keyboard shortcuts to help speed up manual annotations?\nDoes the tool offer effective model-assisted or other “smart” annotation capabilities?\nAre there well-designed features for managing your images, annotations, and ovearll workflow?\nIs there any useful API support to enable programmatic upload, download, or other automation?\n\n\n\n\n\n\n\n\n\n\nHow much am I willing to pay for tooling?\n\n\n\n\n\nIn short, some software options are free, wherease others are commercial offerings with varying costs and prices tiers. As you compare features, consider what you’re willing (and able) to pay for.\n\n\n\n\n\n\n\n\n\nHow is the software licensed?\n\n\n\n\n\nSome annotation software apps and libraries are open source, whereas others are proprietary. You may want to lean toward the open source options if you want to be able to review the source code and understand how it works, and/or (perhaps more importantly) have the option of modifying it to better meet your needs. Of course, general speaking, the open source options will typically also be free, whereas proprietary software is more likely to come with costs.\n\n\n\n\n\n\n\n\n\nWhere does the software run?\n\n\n\n\n\nDo you care if the software runs on your local computer? Do you want it to be something that you deploy and run on your own managed server, either locally or on a VM hosted in a public cloud? Or would you prefer to use a pure cloud-based annotation platform (i.e., a SaaS offering) that somebody else maintains and you access via a browser and/or API?\nAs with any software decision, there are pros and cons to each option.\nBear in mind that with image annotation, any cloud-based offering raises security and privacy considerations, as your images and annotations will reside on somebody else’s server. Consider whether this is a concern for you.\n\n\n\n\n\n\n\n\n\nWhat collaboration features are there?\n\n\n\n\n\n\nWhat collaborative features are offered?\n\n\n\n\n\n\n4.3.2 Tools & services galore\nNote that for geospatial image data annotation in particular, historically there’s been a divide between these two approaches:\n\nMature GIS platforms (QGIS, ArcGIS, etc) -\n\nFirst-class geospatial data and imagery support\nNative capabilities for drawing and editing features like points, lines, and polygons\nBut all of menus and heavyweight UI around robust spatial feature management can impede fast & efficient annotation\nLimited or no support for the broader annotation workflow and lifecycle\n\nImage annotation software and platforms (LabelBox, RoboFlow)\n\nReally nice and constantly improving\nMostly generic with respect to supporting annotation for Computer Vision tasks, not full-featured around environmental research applications, especially with respect to Remote Sensing imagery with spatial component, multispectral bands, etc\n\n\nIn between, you’ll find a few dedicated software packages for environmental and/or spatial image annotation. However, because this is a small niche, you’ll find that they’re often rough around the edges, and likely have a very focused (i.e., limited) set of features addressing only the specific use cases of relevance to the development team. On the plus side, usually they are developed as open source projects, so if you’re up for the investment, you may want to consider contributing or extending these tools to meet your needs.\n\n4.3.2.1 Open-Source Tools for Image Annotation\n\nLabelImg\n\nHigh level: An open-source tool for creating bounding boxes.\nUsed for object detection mainly, maybe??\nOnly supports bounding boxes for annotation\n“Graphical image annotation tool and label object bounding boxes in images”\nIt is written in Python and uses Qt for its graphical interface.\nAnnotations are saved as XML files in PASCAL VOC format, the format used by ImageNet. Besides, it also supports YOLO and CreateML formats\nSee this third-party video tutorial\n\nVGG Image Annotator (VIA)\n\nHigh level: A flexible (but manual) tool for image, video, and audio annotation.\nServerless web application, runs locally and self-contained in a browser, with no network connection required\nReleased in 2016, still maintained, based out of Oxford\nSee demo\n\nLabelMe\n\nOriginally built as an online annotation tool, now distributed\nNow distributed as a deployable web application that you can ran on a local web server\nNot to be confused with this independent Python/QT port of labelme\nWait and what about this labelme GitHub repo??\n\nIRIS (Intelligently Reinforced Image Segmentation)\n\nProvides semi-automated annotation for image segmentation, geared toward multi-band satellite imagery\n\n\n\n\n4.3.2.2 GIS platforms with annotation plugins\n\nQGIS\nArcGIS\n\n\n\n4.3.2.3 Hybrid solutions with both desktop and hosted options\n\nCVAT (Computer Vision Annotation Tool):\n\nOpen-source tool for video and image annotation, widely used in computer vision projects.\nUses pre-trained models to assist annotation?\nSee GitHub repository\nAlso has cloud-based offering and offers annotation services\nSupports:\n\nlabeling images\ndrawing bounding boxes\nmodel assisted labeling using models like YOLO \nmanual semantic segmentation \nautomatic semantic segmentation with SAM \n\n\n🔥 Label Studio\n\nMulti-type data labeling and annotation tool with standardized output format\nWorks on various data types (text, image, audio)\nHas both open source option and paid cloud service\nSee online playground\n\nMicrosoft’s Spatial imagely labeling toolkit\nimglab\n\n\n\n4.3.2.4 Commercial apps\n\nRectLabel\n\nOffline image annotation tool for object detection and segmentation\nHas regular and Pro version\nBuilt for Mac\nSee support page\n\n\n\n\n4.3.2.5 Commercial services\n\nLabelbox\n\nCloud-based commercial platform, albeit with possible free options for academic researchers\n\nRoboflow annotate\n\nOnline platform, with limited free tier\nFree tier does not offer any privacy\n\nSuperAnnotate\n\nHigh level: Full-featured collaborative annotation and modeling platform\nCommercial offering with free tier\n\nMakeSense.ai\n\nIncludes AI models!\nGitHub0\n\nSupervise.ly (commercial with free version)\nLabelerr (commercial with free researcher tier)\nRMSI annotation tools & services\nKili annotation platform (see geoannotation docs)\nSegments.ai labeling platform\nSama\nScaleAI\nDiffgram (see tech docs and GiHub) – commercial but locally installed? Hard to tell!\nDarkLabel\nGroundwork professional labeling services\n\n\n\n4.3.2.6 Fully managed AI & annotation services\n\nAlegion\nManthano\n\n\n\n4.3.2.7 Other platforms\n\nZooniverse? Crowd-sourcing annotation platform\n\nE.g. The Arctic Bears Project\n\nDeepForest\n\nFrom the Weecology lab\nPython package for training and predicting ecological objects in airborne imagery\nComes with a tree crown object detection model and a bird detection model\nSee GitHub repo\n\n\n\n\n\n4.3.3 Miscellaneous links\n\nSatellite image deep learning (Robin Cole’s site)\nOpen Source Data Annotation & Labeling Tools",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html",
    "href": "sections/hands-on-lab-data-annotation.html",
    "title": "Hands-On Lab: Data Annotation",
    "section": "",
    "text": "Goal\nThis hands-on lab session is designed to give participants practical experience in data annotation for deep learning. Participants will apply the methods, tools, and best practices discussed in the previous session, working directly with datasets to annotate data effectively.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#key-elements",
    "href": "sections/hands-on-lab-data-annotation.html#key-elements",
    "title": "Hands-On Lab: Data Annotation",
    "section": "Key Elements",
    "text": "Key Elements\nUse of annotation methods and tools, direct dataset interaction",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#choose-your-own-adventures",
    "href": "sections/hands-on-lab-data-annotation.html#choose-your-own-adventures",
    "title": "Hands-On Lab: Data Annotation",
    "section": "Choose your own adventure(s)",
    "text": "Choose your own adventure(s)\nIn this section, we’ll provide some links, basic information, and suggested starter activities for variety of annotation tools available today. Have a look and get your hands dirty!\nNote: You’ll need some images to annotate in each case. Feel free to use any relevant images you might already have, or just do a web search and find something interesting. Of course, when experimeting with the web-based annotation platforms, be sure not to use upload anything personal, private, or otherwise sensitive.\nIdeally you’ll cover:\n\nSimple bounding box annotation\nPolygon, line, and point annotation\nInteractive model-assisted segmentation\nInspecting annotation output files in various formats, including COCO JSON\nOne or more cloud (web-based) tools\n(For the even more adventurous) One or more locally installed tools",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-make-sense",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-make-sense",
    "title": "Hands-On Lab: Data Annotation",
    "section": "5.1 Adventure: Make Sense",
    "text": "5.1 Adventure: Make Sense\n\n\n\n.\n\n\n\nMakeSense.ai is a simple, single-user, browser-based image annotation app\nSupports annotation via bounding boxes, poylgons, points, and lines\nUpload one or more images, apply/edit annotations, then export annotations\nOffers model-based semi-automated annotation with an accept/reject interface\nIf you prefer, you can also grab the source code and run it locally using npm or Docker\n\nThings to try\n\nUpload one or more images\nPlay around with manually creating various annotations of various classes. What is the experience?\nUse Actions to edit label names, colors, etc\nUse Actions to export annotations. What formats are offered?\nTry exporting polygon annotations in both VGG and COCO format. How do they compare?\nUse Actions to run the COCO SSD model locally to suggest boxes. How well does it work?",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-roboflow",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-roboflow",
    "title": "Hands-On Lab: Data Annotation",
    "section": "5.2 Adventure: Roboflow",
    "text": "5.2 Adventure: Roboflow\n\n\n\n.\n\n\n\nRoboflow offers a cloud-hosted, web-based platform for computer vision, including tooling for data annotation along with model training and deployment\nThey offer a limited free tier, which does not offer any privacy (project and images are automatically public)\nNice interface for doing annotations, managing artifacts, managing team\n\nThings to try\n\nCreate an account and test project\nUpload one or more images\nGo to the Annotate interface and experiment with different annotation types. How easy is it to create, edit, and delete annotations?\nUse the Smart Polygon tool to create polygons by clicking on an object, then refining by adding more clicks inside and outside the object. What is the experience like? Does this speed up your annotations?\nGo back to the main Annotate menu and note how it is organized to support a coherent, team-based annotation workflow. Check out their collaboration documentation. Imagine how you might use this for a multi-person project.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-cvat",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-cvat",
    "title": "Hands-On Lab: Data Annotation",
    "section": "5.3 Adventure: CVAT",
    "text": "5.3 Adventure: CVAT\n\n\n\n.\n\n\n\nCVAT can be used as a desktop application that you install & run on your own local computer or server.\nHowever, for today, consider creating your own (free) account for annotating using their hosted platform\nThe V7 cvat guide might be helpful\n\nThings to try\n\nCreate a free account\nLog in and create a test Project. At this stage, you’ll need to define at least one relevant label under the Constructor tab (you can edit these later)\nCreate a Task (i.e., a collection of images to annotate) under your Project, and upload one or more images.\nStart an annotation Job within the task. What do think of the interface? Is the documentation helpful?\nUsing the menu bar on the left, try creating box, polygon, line, and point annotations. Note: Click the Shape button to start each annotation. How is the experience?\nAlso try creating a 3D cuboid annotation. Figure out how to resize and orient the cube. What do you think?\nLastly, try doing brush-based segmentations.\nAfter doing some annotations, go to Jobs, use the 3-dots selector on your job to open the action menu, and export annotations in a couple different formats. How do they compare?\nAs a another Jobs action, you can do click on View analytics and run a performance report.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-zooniverse",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-zooniverse",
    "title": "Hands-On Lab: Data Annotation",
    "section": "5.4 Adventure: Zooniverse",
    "text": "5.4 Adventure: Zooniverse\n\n\n\n.\n\n\nZooniverse is a cool community crowdsourcing platform on the web, for data annotation and digitization.\nThings to try\n\nCheck out the Penguin Watch project.\n\nVisit the About, Talk, and Collect pages. Imagine how you might set up your own project to encourage and support a crowdsourced annotation community\nVisit the Classify page, go through the Tutorial, and then see how the Task works.\n\nAlso check out the Arctic Bears image classification and interpretation project\nFeel free to search the site for other projects",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-segment-geospatial-samgeo",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-segment-geospatial-samgeo",
    "title": "Hands-On Lab: Data Annotation",
    "section": "5.5 Adventure: Segment-Geospatial (samgeo)",
    "text": "5.5 Adventure: Segment-Geospatial (samgeo)\nThis is an open source tool that you can either install locally or run in JupyterLab (or Google colab).\nFirst check out the online Segment Anything Model (SAM) demo. SAM was developed by Meta AI. It is trained as a generalized segmentation model that is able to segment (but not label) arbitrary objects in an image. It is designed as a promptable tool, which means a user can provide initial point(s) or box(es) that roughly localize an object within an image, and SAM will try to fully segment that object. Alternatively, it can automatically segment an entier image, effectively by self-promtping with a complete grid of points, and then intelligently merging the corresponding segments.\nToday, SAM is used by numerous image annotation tools to provide interactive, AI-assisted segmentation capabilities.\nOne such tool is the segment-geospatial Python package, which provides some base functionality for applying SAM to geospatial data, either programatically or interactively.\n\nsudo apt install libgdal-dev gdal-bin\nMy copy of the Google Colab notebook (related to this workshop)\n\nNote that in addition to using segment-geospatial directly using Python in a notebook or other environment, you can also play with SAM-assisted segmentation in QGIS and ArcGIS.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-label-studio",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-label-studio",
    "title": "Hands-On Lab: Data Annotation",
    "section": "5.6 Adventure: Label Studio",
    "text": "5.6 Adventure: Label Studio\n\nMulti-type data labeling and annotation tool with standardized output format\nWorks on various data types (text, image, audio)\nHas both open source option and paid cloud service\nSee online playground",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-iris-intelligently-reinforced-image-segmentation",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-iris-intelligently-reinforced-image-segmentation",
    "title": "Hands-On Lab: Data Annotation",
    "section": "5.7 Adventure: IRIS (Intelligently Reinforced Image Segmentation)",
    "text": "5.7 Adventure: IRIS (Intelligently Reinforced Image Segmentation)\n\n\n\n.\n\n\n\nTool for manual image segmentation of satellite imagery (or images in general).\nSemi-automated annotation for image segmentation\nSee YouTube video with the main creator Alistar Francis\nMain premise:\n\nIn each image, there is a lot of correlation between the pixels\nIn one scene, might only be a few types of pixels\n\nRuns as a JS application on the frontend with Python in the backend\nDesigned to accelerate the creation of ML training datasets for Earth Observation.\nFlask app which can be run locally\nSupport by AI (gradient boosted decision tree) when doing image segmentation\nMultiple and configurable views for multispectral imagery\nSimple setup with pip and one configuration file\nPlatform independent app (runs on Linux, Windows and Mac OS)\nMulti-user support: work in a team on your dataset and merge the results",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#other-things-to-try",
    "href": "sections/hands-on-lab-data-annotation.html#other-things-to-try",
    "title": "Hands-On Lab: Data Annotation",
    "section": "5.8 Other things to try",
    "text": "5.8 Other things to try\n\nVGG Image Annotator (VIA)\n\nTry local installation?\n\nSAM demo for understanding how this can be used for interactive segmentation\n\ntwo-polar-bears.jpg",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html",
    "href": "sections/the-building-blocks-of-nn-and-dl.html",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "",
    "text": "6.1 Overview\nThis session aims to provide a comprehensive introduction to the fundamental components of neural networks and deep learning. Participants will explore the architecture of neural networks, including layers, neurons, weights, and activation functions, as well as the principles behind training models, such as loss functions and optimizers. The goal is to equip participants with a solid understanding of how neural networks are constructed and how they learn, paving the way for deeper dives into specific neural network models and applications in future sessions.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#outline",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#outline",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.2 Outline",
    "text": "6.2 Outline\n\nFundamentals of neural network: history and evolution\nCore components: neurons, layers, and weights\nArchitecture of neural networks: layers and activation functions\nTraining neural networks: loss functions and optimizers\nConclusion and Q&A",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#reference",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#reference",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.3 Reference",
    "text": "6.3 Reference\n\nhttps://cs231n.stanford.edu",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html",
    "href": "sections/intro-to-pytorch.html",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "",
    "text": "7.1 Goal\nThis session introduces PyTorch, one of the most popular deep learning frameworks, known for its flexibility and ease of use. Participants will gain hands-on experience with PyTorch’s core functionalities and apply them to a sample dataset, setting the stage for a subsequent hands-on session. The goal is to arm participants with the essential knowledge and confidence required to begin utilizing PyTorch in their deep learning endeavors, ensuring they’re well-prepared for practical application and further exploration.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#outline",
    "href": "sections/intro-to-pytorch.html#outline",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.2 Outline",
    "text": "7.2 Outline\n\nIntroduction to PyTorch\nUnderstanding PyTorch’s core functionalities\nWorking with data in PyTorch\nBuilding a simple neural network in PyTorch\nTraining a model\nEvaluating a model\nConclusion and preparing for hands-on session",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#reference",
    "href": "sections/intro-to-pytorch.html#reference",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.3 Reference",
    "text": "7.3 Reference\n\nhttps://pytorch.org\nhttps://pytorch.org/tutorials/",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html",
    "href": "sections/hands-on-lab-pytorch.html",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "",
    "text": "8.1 Overview\nThis hands-on lab session is designed to provide participants with practical experience using PyTorch to build, train, and evaluate neural network models. Participants will work through guided exercises that reinforce the concepts introduced in the previous session, applying PyTorch to real-world datasets relevant to Arctic research. By the end of this session, participants will have a solid understanding of how to implement deep learning models using PyTorch, empowering them to tackle their own projects with confidence.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#outline",
    "href": "sections/hands-on-lab-pytorch.html#outline",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.2 Outline",
    "text": "8.2 Outline\n\nRecap of PyTorch core functionalities\nGuided exercise 1: working with real-world datasets\nGuided exercise 2: building a simple neural network\nGuided exercise 3: training and evaluating the model\nTroubleshooting and optimization tips\nConclusion and Q&A",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#reference",
    "href": "sections/hands-on-lab-pytorch.html#reference",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.3 Reference",
    "text": "8.3 Reference\n\nhttps://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\nhttps://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\nhttps://pytorch.org/tutorials/beginner/vt_tutorial.html",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/permafrost-discovery-gateway.html",
    "href": "sections/permafrost-discovery-gateway.html",
    "title": "9  Permafrost Discovery Gateway",
    "section": "",
    "text": "We have more satellite imagery data than what we know what to do with. There are many people with different knowledges and common passions for permafrost-affected landscapes. At the same time, Alaska and the Arctic at large are starving for basic geospatial information about it’s permafrost-affected landscape and infrastructure and people may have a hard time finding others to help make visions come true. The Permafrost Discovery Gateway (PDG) is an online free tool meant to empower a) researchers to produce and do science with big geospatial data (think sub-meter resolution maps across Alaska and the entire Arctic) and b) agencies and community leaders in land and infrastructure management that involves permafrost. PDG currently offers easy visual exploration via a regular web-browser of datasets that otherwise crush traditional GIS software due to the file size. In the works are, for example, new big datasets of permafrost thaw features and infrastructure, partial dataset download tool, the incorporation of statistical summaries and AI tools that help the user find interesting stories in the big data products, plug-and-play workflows to help you develop your own big dataset, and the monitoring of permafrost thaw near-real time. The PDG can become a gateway where data and people can connect, where technology enables anyone with an internet connection, no matter your technical skills and resources, to connect, explore, and together create.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Permafrost Discovery Gateway</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html",
    "href": "sections/ai-ethics.html",
    "title": "10  AI Ethics",
    "section": "",
    "text": "Goal\nReview FAIR and CARE Principles, and their relevance to data ethics. Examine how ethical considerations are shared and considered at the Arctic Data Center. Discuss ethical considerations in machine learning.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#introduction",
    "href": "sections/ai-ethics.html#introduction",
    "title": "10  AI Ethics",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nArtificial Intelligence (AI) can be thought of as the development of computer systems that can perform tasks we usually think require human intelligence, such as image recognition, language translation, or autonomous movement. The rapid development and adoption of AI tools in the past years, particularly machine learning algorithms, has revolutionized how big datasets are analyzed, transforming decision-making in all sectors of society. However, frameworks to examine the ethical considerations of AI are just emerging, and careful consideration of how to best develop and apply AI systems is essential to the responsible use of these new, rapidly changing tools. In this section, we will give an overview of the FAST Principles put forward by the Alan Turing Institute in their guide for the responsible design and implementation of AI systems [1].",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#the-fast-principles",
    "href": "sections/ai-ethics.html#the-fast-principles",
    "title": "10  AI Ethics",
    "section": "10.2 The FAST Principles",
    "text": "10.2 The FAST Principles\nFAST stands for Fairness, Accountability, Sustainability, and Transparency. The FAST principles aim to guide the ethical development of AI projects from their inception to deployment. The continuous involvement and commitment of software developers, domain experts, technical leads, project managers, rightsholders, and collaborators involved in the AI project is crucial to implement these principles successfully. The following is a brief overview of each of the FAST principles, we greatly encourage you to read through the Alan Turing Institute guide to learn more!\n\n\n\nLeslie, 2019",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#fairness",
    "href": "sections/ai-ethics.html#fairness",
    "title": "10  AI Ethics",
    "section": "10.3 Fairness",
    "text": "10.3 Fairness\nBias can enter at any point of a research project, from data collection and preprocessing, to model design and implementation. This is because AI projects, as any other, are created by human beings who (even with the best of intentions) can introduce error, prejudice, or misjudgement into a system. Fairness refers to the active minimization of bias and commitment to not harm others through the outcomes of an AI system. The FAST principles [1] suggest the following baseline for fairness:\n\nThe designers and users of AI systems ensure that the decisions and behaviours of their models do not generate discriminatory or inequitable impacts on affected individuals and communities. This entails that these designers and users ensure that the AI systems they are developing and deploying:\n\nAre trained and tested on properly representative, relevant, accurate, and generalisable datasets (Data Fairness)\nHave model architectures that do not include target variables, features, processes, or analytical structures (correlations, interactions, and inferences) which are unreasonable, morally objectionable, or unjustifiable (Design Fairness)\nDo not have discriminatory or inequitable impacts on the lives of the people they affect (Outcome Fairness)\nAre deployed by users sufficiently trained to implement them responsibly and without bias (Implementation Fairness)\n\n\n\n\n\n\n\n\nReal-life Example : Insufficient Radar Network\n\n\n\n\n\nThe following figure [2] shows coverage of the national Doppler weather network (green and yellow circles) over a demographic map of the Black population in the southeast US. This would be an example of an issue in data fairness, since radar coverage does not represent the population uniformly, leaving out areas with higher Black population. Problems with outcome fairness could ensue if this non-representative biases an AI model to under-predict weather impacts in such populations.\n\n\n\nMcGovern et al., 2022 by courtesy of Jack Sillin (CC BY 4.0).",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#accountability",
    "href": "sections/ai-ethics.html#accountability",
    "title": "10  AI Ethics",
    "section": "10.4 Accountability",
    "text": "10.4 Accountability\nAccountability in AI projects stems from the shared view that isolated AI models used to automate decisions are not morally responsible in the same way as a decision-making human. As outputs from AI models are increasingly used to make decisions that affect the environment and human lives, there is a critical need for competent human authorities to offer explanations and justifications for the development process, outputs, and ensuing decisions made by AI systems. Such answerability assignments can be challenging, as AI implementations are often the product of big development teams where the responsibility to answer for a project’s outcome may not be delineated, creating an issue known as “the problem of many hands.” The FAST principles encourage the following accountability implementation:\nAccountability by Design: All AI systems must be designed to facilitate end-to-end answerability and auditability. This requires both responsible humans-in-the-loop across the entire design and implementation chain as well as activity monitoring protocols that enable end-to-end oversight and review.\n\n\n\n\n\n\nReal-life Example: AI for natural disasters response\n\n\n\n\n\nAccountability and the ability to audit AI methods can be crucial when model outputs support critical decision-making, such as in natural disasters. In 2021, a New York Times investigation [3]] covered a private company’s premature release of outputs about neighborhoods most affected by potential earthquakes in Seattle. While the initial release erroneously did not show threats for non-residential areas, ensuing updated versions showed non-compatible predictions again. Although the company acknowledged that its AI models would not replace the first responder’s judgment, the lack of audibility and opacity in the model development hindered accountability for any party, ultimately eroding the public confidence in the tools and leading to a loss of public resources.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#sustainability",
    "href": "sections/ai-ethics.html#sustainability",
    "title": "10  AI Ethics",
    "section": "10.5 Sustainability",
    "text": "10.5 Sustainability\nSustainability in the FAST principles includes continuous assessment of the social impacts of an AI system and technical sustainability of the AI model. In the first consideration, the FAST principles advocate for performing a Stakeholder Impact Assessment (SIA) at different stages to help build confidence in the project and uncover unexpected risks or biases, among other benefits. The Alan Turing Institute guide shares a prototype of an SIA [1]. The core of technical sustainability is creating safe, accurate, reliable, secure, and robust AI systems. To achieve these technical goals, teams must implement thorough testing, performance metrics, uncertainty quantification, and be aware of changes to the underlying distribution of data, among other essential practices.\n\n\n\n\n\n\nReal-life Example: SpaceCows\n\n\n\n\n\nThe SpaceCows project [4], [5] in northern Australia is a collaboration between scientists, industry leaders, and local indigenous communities developing AI centered platforms to analyze GPS tracking data collected from feral cows alongside satellite imagery and weather data. Indigenous knowledge and traditional land owners have been at the center of the development, providing guidance and ultimately benefiting from the AI tools to protect their land and cultural sites.\n\n\n\nImportant indigenous cultural sites can be damaged by feral cattle. Image from CSIRO, SpaceCows: Using AI to tackle feral herds in the Top End.\n\n\nVideos with more information on SpaceCows:\nCSIRO rolls out world’s largest remote ‘space cows’ herd management system\nSpaceCows: Using AI to tackle feral herds in the Top End",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#transparency",
    "href": "sections/ai-ethics.html#transparency",
    "title": "10  AI Ethics",
    "section": "10.6 Transparency",
    "text": "10.6 Transparency\nUnder the FAST principles, transparency in AI projects refers to transparency about how an AI project was designed and implemented and the content and justification of the outcome produced by the AI model. To ensure process transparency, the project should show how the design and implementation included ethical, safety, and fairness considerations throughout the project. To clarify the content and explain the outcomes of an AI system, the project should offer plain language, non-technical explanations accessible to non-specialists that convey how and why a model performed the way it did. In this direction, it is essential to avoid a ‘mathematical glass box’ where the code and mathematics behind the algorithm are openly available, but there is a lack of rationale about how or why the model goes from input to output. Finally, the explanations about how the outcomes were produced should become the basis to justify the outcomes in terms of ethical permissibility, fairness, and trustworthiness. A careful consideration of the balance between the sustainability and transparency principles is necessary when dealing with protected or private data.\n\n\n\n\n\n\nReal-life Example: France’s Digital Republic Act\n\n\n\n\n\nThe concern for transparency in using personal data is an active space for debate. In 2018, the French government passed a law to protect citizens’ privacy, establishing the citizen’s “right to an explanation” regarding, among other things, how an algorithm contributed to decisions on their persona and which data was processed [6], [7]. Overall, this legislation aims to create a fairer and more transparent digital environment where everyone can enjoy equal opportunities.\n\n\n\nPhoto by Google DeepMind",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#conclusion",
    "href": "sections/ai-ethics.html#conclusion",
    "title": "10  AI Ethics",
    "section": "10.7 Conclusion",
    "text": "10.7 Conclusion\nAs new AI developments and applications rapidly emerge and transform everyday life, we need to pause and ensure these technologies are fair, sustainable, and transparent. We must acknowledge human responsibility in designing and implementing AI systems to use these novel tools fairly and with accountability. Finally, we acknowledge that the information covered here is a lightning introduction to AI’s ethical considerations and implications. Whether you are a researcher interested in using AI for the first time or a seasoned ML practitioner, we urge you to dive into the necessary and ever-expanding AI ethics work to learn how to best incorporate these concepts into your work.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#further-reading",
    "href": "sections/ai-ethics.html#further-reading",
    "title": "10  AI Ethics",
    "section": "10.8 Further Reading",
    "text": "10.8 Further Reading\nAcademic Data Science Alliance (ADSA) (2024) The Data Sciene Ethos https://ethos.academicdatascience.org [8]\nChen, W., & Quan-Haase, A. (2020) Big Data Ethics and Politics: Towards New Understandings. Social Science Computer Review. [9]\nCrawford, K., & Paglen, T. (2019) Excavating AI: The Politics of Training Sets for Machine Learning. [10]\nGray, J., & Witt, A. (2021) A feminist data ethics of care framework for machine learning: The what, why, who and how. First Monday, 26(12), Article number: 11833 [11]\n\n\n\n\n[1] D. Leslie, “Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector,” Zenodo, Jun. 2019. doi: 10.5281/zenodo.3240529.\n\n\n[2] A. McGovern, I. Ebert-Uphoff, D. J. G. Ii, and A. Bostrom, “Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science,” Environmental Data Science, vol. 1, p. e6, Jan. 2022, doi: 10.1017/eds.2022.5.\n\n\n[3] S. Fink, “This High-Tech Solution to Disaster Response May Be Too Good to Be True,” The New York Times, Aug. 2019, Accessed: Oct. 19, 2024. [Online]. Available: https://www.nytimes.com/2019/08/09/us/emergency-response-disaster-technology.html\n\n\n[4] T. Shepherd, “Indigenous rangers to use SpaceCows program to protect sacred sites and rock art from feral herds,” The Guardian, Sep. 2021, Accessed: Oct. 19, 2024. [Online]. Available: https://www.theguardian.com/australia-news/2021/sep/15/indigenous-rangers-to-use-spacecows-program-to-protect-sacred-sites-and-rock-art-from-feral-herds\n\n\n[5] CSIRO, “SpaceCows: Using AI to tackle feral herds in the Top End.” Accessed: Oct. 19, 2024. [Online]. Available: https://www.csiro.au/en/news/All/News/2021/September/SpaceCows-Using-AI-to-tackle-feral-herds-in-the-Top-End\n\n\n[6] L. Edwards and M. Veale, “Enslaving the Algorithm: From a ‘Right to an Explanation’ to a ‘Right to Better Decisions’?” Social Science Research Network, Rochester, NY, 2018. doi: 10.2139/ssrn.3052831.\n\n\n[7] S. Lo Piano, “Ethical principles in machine learning and artificial intelligence: Cases from the field and possible ways forward,” Humanities and Social Sciences Communications, vol. 7, no. 1, pp. 1–7, Jun. 2020, doi: 10.1057/s41599-020-0501-9.\n\n\n[8] A. D. S. A. (ADSA), “The Data Science Ethos - Operationalizing Ethics in Data Science,” The Data Science Ethos. Accessed: Oct. 19, 2024. [Online]. Available: https://ethos.academicdatascience.org/\n\n\n[9] W. Chen and A. Quan-Haase, “Big Data Ethics and Politics: Toward New Understandings,” Social Science Computer Review, vol. 38, no. 1, pp. 3–9, Feb. 2020, doi: 10.1177/0894439318810734.\n\n\n[10] “Excavating AI.” Accessed: Oct. 19, 2024. [Online]. Available: https://excavating.ai/\n\n\n[11] J. Gray and A. Witt, “A feminist data ethics of care for machine learning: The what, why, who and how,” First Monday, Dec. 2021, doi: 10.5210/fm.v26i12.11833.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html",
    "href": "sections/guest-lecture-yili-arts-dataset.html",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "",
    "text": "11.1 Overview\nIn this session, we will introduce and explore the Arctic Retrogressive Thaw Slump (ARTS) dataset. We aim to illuminate the background and motivation behind the ARTS dataset, detail its design elements including functions, metadata, and usage, and underscore its defining features such as scalability, scientific integrity, and the potential for community contribution.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html#outline",
    "href": "sections/guest-lecture-yili-arts-dataset.html#outline",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "11.2 Outline",
    "text": "11.2 Outline\n\nBackground and motivation of the Arctic\nRetrogressive Thaw Slump (ARTS) data set.\nSource data for the ARTS data set\nDesign of the data set - functions, metadata, usage\nFeatures of the data set - scalable, scientific, contributable\nData Curation Framework - standards, protocols\nThe ARTS repository - user and contributor guideline\nQuestions and discussions",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html#reference",
    "href": "sections/guest-lecture-yili-arts-dataset.html#reference",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "11.3 Reference",
    "text": "11.3 Reference\n\nhttps://github.com/whrc/ARTS",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html",
    "href": "sections/exploring-advanced-neural-networks.html",
    "title": "12  Exploring Advanced Neural Networks: Semantic Segmentation",
    "section": "",
    "text": "12.1 Overview\nThis session focuses on advanced neural networks, specifically targeting semantic segmentation. Participants will delve into models such as Fully Convolutional Networks (FCNs) and U-Net, learning how these networks are structured, how they function, and how they can be applied to accurately segment and label each pixel of an image according to the object it represents. The goal is to deepen participants’ understanding of the technical foundations and practical applications of semantic segmentation, equipping them with the skills needed for hands-on implementation and exploration of its real-world utility, particularly in the context of Arctic research.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Semantic Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#outline",
    "href": "sections/exploring-advanced-neural-networks.html#outline",
    "title": "12  Exploring Advanced Neural Networks: Semantic Segmentation",
    "section": "12.2 Outline",
    "text": "12.2 Outline\n\nIntroduction to semantic segmentation\nOverview of key models: Fully Convolutional Networks (FCNs) and U-Net\nDetailed architecture and functionality\nApplications in Arctic research: case studies\nConclusion and Q&A",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Semantic Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#reference",
    "href": "sections/exploring-advanced-neural-networks.html#reference",
    "title": "12  Exploring Advanced Neural Networks: Semantic Segmentation",
    "section": "12.3 Reference",
    "text": "12.3 Reference\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks for biomedical image segmentation.” Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer International Publishing, 2015. https://arxiv.org/abs/1505.04597\nMinaee, Shervin, et al. “Image segmentation using deep learning: A survey.” IEEE transactions on pattern analysis and machine intelligence 44.7 (2021): 3523-3542. http://www.arxiv.org/abs/2001.05566",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Semantic Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "",
    "text": "13.1 Overview\nThis session introduces participants to MMSegmentation, a specialized deep learning library designed for semantic segmentation tasks. Participants will explore the unique capabilities of MMSegmentation in handling sophisticated image analysis projects. The session will cover how to navigate the library, implement advanced features, and apply them to real-world datasets, particularly in the context of Arctic research. While the focus will be on MMSegmentation, Detectron2 will also be mentioned as another powerful tool for image analysis. By the end of this session, participants will have a theoretical understanding of MMSegmentation and be prepared for a more extensive hands-on lab session.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#outline",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#outline",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.2 Outline",
    "text": "13.2 Outline\n\nIntroduction to MMSegmentation: features and capabilities\nNavigating MMSegmentation: tools and techniques\nImplementing semantic segmentation with MMSegmentation\nPractical applications in Arctic research\nBrief overview of Detectron2 for comparison\nConclusion and Q&A",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#reference",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#reference",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.3 Reference",
    "text": "13.3 Reference\n\nhttps://github.com/open-mmlab/mmsegmentation\nhttps://github.com/facebookresearch/detectron2 (for further exploration)",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-mmsegmentation.html",
    "href": "sections/hands-on-lab-mmsegmentation.html",
    "title": "14  Hands-On Lab: MMSegmentation",
    "section": "",
    "text": "14.1 Overview\nThis hands-on lab session provides participants with practical experience using MMSegmentation to perform semantic segmentation tasks. Participants will engage in guided exercises that build on the concepts introduced in the previous session, applying MMSegmentation to real-world datasets relevant to Arctic research. By the end of this session, participants will have gained the practical skills necessary to implement and fine-tune semantic segmentation models using MMSegmentation, enabling them to effectively apply these techniques in their own research projects.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hands-On Lab: MMSegmentation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-mmsegmentation.html#outline",
    "href": "sections/hands-on-lab-mmsegmentation.html#outline",
    "title": "14  Hands-On Lab: MMSegmentation",
    "section": "14.2 Outline",
    "text": "14.2 Outline\n\nRecap of MMSegmentation core functionalities\nGuided exercise 1: preparing and loading data\nGuided exercise 2: building and training a semantic segmentation model\nGuided exercise 3: evaluating and fine-tuning the model\nWorking with real-world datasets: Arctic research applications\nTroubleshooting and optimization tips\nConclusion and Q&A",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hands-On Lab: MMSegmentation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-mmsegmentation.html#reference",
    "href": "sections/hands-on-lab-mmsegmentation.html#reference",
    "title": "14  Hands-On Lab: MMSegmentation",
    "section": "14.3 Reference",
    "text": "14.3 Reference\n\nhttps://mmsegmentation.readthedocs.io/en/latest/user_guides/index.html",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hands-On Lab: MMSegmentation</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html",
    "href": "sections/ai-workflows-and-mlops.html",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "",
    "text": "15.1 Instructors\nBen Galewsky, Sr. Research Software Engineer National Center for Supercomputing Applications (NCSA) University of Illinois Urbana-Champaign",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#overview",
    "href": "sections/ai-workflows-and-mlops.html#overview",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "15.2 Overview",
    "text": "15.2 Overview\nMachine learning models have become a vital tool for most branches of science. The process and tools for training these models on the lab’s desktop is often fragile, slow, and not reproducible. In this workshop, we will introduce the concept of MLOps, which is a set of practices that aims to streamline the process of developing, training, and deploying machine learning models. We will use the popular open source MLOps tool, MLflow, to demonstrate how to track experiments, package code, and deploy models. We will also introduce Garden, a tool that allows researchers to publish ML Models as citable objects.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#outline",
    "href": "sections/ai-workflows-and-mlops.html#outline",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "15.3 Outline",
    "text": "15.3 Outline\n\nIntroduction to MLOps\nIntroduction to MLflow\nTracking experiments with MLflow\nPackaging code with MLflow\nDeploying models with MLflow\nPublishing models with Garden",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#reference",
    "href": "sections/ai-workflows-and-mlops.html#reference",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "15.4 Reference",
    "text": "15.4 Reference\n\nMLflow\nGarden",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html",
    "href": "sections/foundation-models.html",
    "title": "17  Foundation Models: The Cornerstones of Modern AI",
    "section": "",
    "text": "17.1 Overview\nFoundation models (FM) are deep learning models trained on massive raw unlabelled datasets usually through self-supervised learning. FMs enable today’s data scientists to use them as the base and fine-tune using domain specific data to obtain models that can handle a wide range of tasks [1, 6, 7]. In this talk, we provide an introduction to FMs, its history, evolution, and go through its key features and categories, and a few examples. We also briefly discuss how foundation models work. This talk will be a precursor to the hands-on session that follows on the same topic.\nImage source: 2021 paper on foundation models by Stanford researchers [1].\nIn this session, we take a closer look at what constitutes a foundation model, a few examples, and some basic principles around how it works.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#outline",
    "href": "sections/foundation-models.html#outline",
    "title": "17  Foundation Models: The Cornerstones of Modern AI",
    "section": "17.2 Outline",
    "text": "17.2 Outline\n\nIntroduction to foundation models, its history and evolution\nKey features of foundation models\nTypes of foundation models: Language, Vision, Generative, and Multimodal\nExamples of foundation models: BERT [3], GPT [4], YOLO [2], SAM [5], DALLE-2\nHow do foundation models work?",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#reference",
    "href": "sections/foundation-models.html#reference",
    "title": "17  Foundation Models: The Cornerstones of Modern AI",
    "section": "17.3 Reference",
    "text": "17.3 Reference\n\nOn the opportunities and risk of Foundation models\nYou Only Look Once\nBERT\nGPT3\nSegment Anything Model\nNVIDIA blog post on foundation models\nWhat are Foundation Models? - Generative AI",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html",
    "href": "sections/hands-on-lab-foundation-models.html",
    "title": "18  Hands-On Lab: Foundation Models",
    "section": "",
    "text": "18.1 Overview\nThe hands-on lab on foundation models will focus on building and applying small-scale foundation models for some example use cases. The main goal of this 1-hour session will be to get more familiarized with foundation models and in interacting with them.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#outline",
    "href": "sections/hands-on-lab-foundation-models.html#outline",
    "title": "18  Hands-On Lab: Foundation Models",
    "section": "18.2 Outline",
    "text": "18.2 Outline\n\nImage Segmentation using Segment Anything Model (SAM)\nChatbot using LLMs + RAG",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#references",
    "href": "sections/hands-on-lab-foundation-models.html#references",
    "title": "18  Hands-On Lab: Foundation Models",
    "section": "18.3 References",
    "text": "18.3 References\n\nSegment Anything\nSegment Anything Notebook",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html",
    "href": "sections/reproducibility.html",
    "title": "19  Reproducibility",
    "section": "",
    "text": "19.1 Goal\nThis session aims to highlight the importance of reproducibility in AI-driven Arctic research. Participants will learn about the challenges and best practices for ensuring that AI models and their results can be reproduced by other researchers, a cornerstone for building trust and advancing the field. The discussion will cover strategies for documenting experiments, sharing data and code, and using version control systems.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#introduction",
    "href": "sections/reproducibility.html#introduction",
    "title": "19  Reproducibility",
    "section": "19.2 Introduction",
    "text": "19.2 Introduction\nReproducibility is not a new topic when it comes to artificial intelligence and machine learning in science, but is more important than ever as AI research is often criticized for not being reproducible. This becomes particularly problematic when validation of a model requires reproducing it.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#why-is-reproducibility-important",
    "href": "sections/reproducibility.html#why-is-reproducibility-important",
    "title": "19  Reproducibility",
    "section": "19.3 Why is Reproducibility Important?",
    "text": "19.3 Why is Reproducibility Important?\n\n19.3.1 Reproducible vs. Replicable\nReproducibility is important in science because it allows other researchers to validate the results of a study and/or use the same analysis for processing their data, promoting open science and collaboration.\n\nReproducible means that other researchers can take the same data, run the same analysis, and get the same result.\nReplicable means other researchers can take different data, run the same analysis, and get their own result without errors.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#the-reproducibility-checklist",
    "href": "sections/reproducibility.html#the-reproducibility-checklist",
    "title": "19  Reproducibility",
    "section": "19.4 The Reproducibility Checklist",
    "text": "19.4 The Reproducibility Checklist\nThe Reproducibility Checklist was created by Canadian computer scientist, Joelle Pineau, with the goal of facilitating reproducible machine learning algorithms that can be tested and replicated. When publishing your model, it is beneficial to work through this checklist and ensure that you’re including the items on this checklist. The checklist is as follows:\nFor all models and algorithms, check that you include:\n\nA clear description of the mathematical setting, algorithm, and/or model\nAn analysis of the complexity (time, space, sample size) of any algorithm\nA link to a downloadable source code*, with specification of all dependencies, including external libraries\n\nFor any theoretical claim, check that you include:\n\nA statement of the result\nA clear explanation of each assumption\nA complete proof of the claim\n\nFor all figures and tables that include empirical results, check that you include:\n\nA complete description of the data collection process, including sample size\nA link to a downloadable version of the dataset or simulation environment\nAn explanation of any data that was excluded and a description of any preprocessing step\nAn explanation of how samples were allocated for training, validation, and testing\nThe range of hyperparameters considered, method to select the best hyperparameter configuration, and specification of each hyperparameter used to generate results\nThe exact number of evaluation runs\nA description of how experiments were run\nA clear definition of the specific measure of statistics used to report results\nClearly defined error bars\nA description of results with central tendency (e.g., mean) and variation (e.g., standard deviation)\nA description of the computing infrastructure used\n\n*With sensitive data or proprietary code, scientists may not wish to release all of their code and data. In this case, data can be anonymized and/or partial code can be released that won’t run but can be read and reproduced.\nConsider the sensitivity of your data/code when publishing.\n\nSensitive data should be anonymized before publishing\nResearchers or organizations may only release partial code if their code is proprietary\nBe sure that the partial code released can still be read and reproduced",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#sharing-code",
    "href": "sections/reproducibility.html#sharing-code",
    "title": "19  Reproducibility",
    "section": "19.5 Sharing Code",
    "text": "19.5 Sharing Code\nThe first step to solving the problem of reproducibility is sharing the code that was used to generate the model. This allows other researchers to:\n\nValidate the model\nTrack code construction and see any author annotations\nExpand on published work\n\nDespite this, sharing code does not always mean that models are fully reproducible. Many machine learning models are trained on restricted datasets or require extensive computing power for training the model. Because of this, there are a few additional criteria that improve reproducibility including:\n\nData and metadata availability (must be included without question)\nTransparency of the code you’re using and dependencies needed to run the code\nEasily installable computational analysis tools and pipelines\nInstalled software should behave the same on every machine and should have the same runtime\n\n\n19.5.1 Trips and Tricks to Sharing Code\n\nAvoid using absolute file paths when reading in data (and in general the use of slashes, as these differ between operating systems)\n\n\n\n\nAvoid using an absolute path to read in your data as shown here\n\n\n\nClean your data within your code\nAvoid copy/pasting in a spreadsheet\nAlways keep an unedited version of your raw data\n\nA general guide to publishing reproducible work:",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#model-repositories",
    "href": "sections/reproducibility.html#model-repositories",
    "title": "19  Reproducibility",
    "section": "19.6 Model Repositories",
    "text": "19.6 Model Repositories\nPyTorch Hub is a pre-trained model repository designed to facilitate reproducibility and enable new research. It is easily usable with Colab and Papers with Code, but models must be trained on openly accessible data.\n\nPapers with Code is an open source hub for publications that include direct links to GitHub code, no account needed to access datasets.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#version-control",
    "href": "sections/reproducibility.html#version-control",
    "title": "19  Reproducibility",
    "section": "19.7 Version Control",
    "text": "19.7 Version Control\n\nVersion control is the process of keeping track of every individual change by each contributor that’s saved in a version control framework, or a special database. Keeping a history of these changes to track model performance relative to model parameters saves the time you’d spend retraining the model.\nThe three components of version control in machine learning are:\n\nCode: We recommend writing and storing your model code in the same language as your implementation code to make it easier to maintain all code and dependencies\nData: Versioning should link the data to the appropriate metadata and note any changes in either\nModel: The model connects your code and data with your model parameters and analysis\n\nUsing a version control system ensures easier:\n\nCollaboration\n\nCollaborators can easily pull changes from a shared repository, push their own changes, annotate their code, and revert back to previous versions of their model\n\nVersioning\n\nIf your model breaks, you’ll have a log of any changes that were made, allowing you or others to revert back to a stable version\n\nDependency tracking\n\nYou can test more than one model on different branches or repositories, tune the model parameters and hyperparameters, and monitor the accuracy of each implemented change\n\nModel updates\n\nVersion control allows for incrementally released versions while continuing the development of the next release",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#summary",
    "href": "sections/reproducibility.html#summary",
    "title": "19  Reproducibility",
    "section": "19.8 Summary",
    "text": "19.8 Summary\nConsider the following to ensure your model is reproducible:\n\nUse the reproducibility checklist for algorithms, theoretical claims, and figures/tables.\nAnonymize any sensitive data and remove proprietary code before publishing\n\nBUT still provide training data and enough code for others to replicate your model\n\nShare data and metadata, be transparent in any dependencies needed to run your model, use easily installable computational analysis tools and pipelines, and ensure installed software behaves the same on every machine (i.e. runtime)\nUse a pre-trained model repository (ex. PyTorch Hub) and publish to open-source journals/websites (ex. Papers with Code)\nPractice efficient version control (recommend GitHub if working with collaborators)",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#hands-on-activity",
    "href": "sections/reproducibility.html#hands-on-activity",
    "title": "19  Reproducibility",
    "section": "19.9 Hands-On Activity",
    "text": "19.9 Hands-On Activity\nLEGO Metadata: In groups of 3-5 people, take ~15 minutes to create a structure out of LEGO bricks and write instructions for a group who will recreate your structure based on these instructions.\nGroups will then be rotated and given instructions and LEGO pieces from another group where they will have ~15 minutes to attempt to recreate that group’s structure.\nWe will have a closing group discussion about this activity. Some questions include:\n\nWhat were some assumptions you made while writing your instructions?\nWere there any unexpected hurdles you encountered when writing your instructions or trying to replicate another group’s structure?\nWhat did you find most difficult about this activity?\nNow that you see how successful or unsuccessful the other group was in recreating your structure, is there anything you would do differently?\n\nThis activity was adapted from the Lego Metadata for Reproducibility Game Pack (doi: 10.36399/gla.pubs.196477) developed by Mary Donaldson and Matt Mahon at the University of Glasgow.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#references-resources",
    "href": "sections/reproducibility.html#references-resources",
    "title": "19  Reproducibility",
    "section": "19.10 References & Resources",
    "text": "19.10 References & Resources\n\nGundersen, Odd Erik, and Sigbjørn Kjensmo. 2018. “State of the Art: Reproducibility in Artificial Intelligence”. Proceedings of the AAAI Conference on Artificial Intelligence 32 (1).\nGundersen, Odd Erik, Yolanda Gil, and David W. Aha. “On Reproducible AI: Towards Reproducible Research, Open Science, and Digital Scholarship in AI Publications.” AI Magazine 39, no. 3 (September 28, 2018): 56–68.\n“How the AI Community Can Get Serious about Reproducibility.” Accessed September 18, 2024.\nAbid, Areeba. “Addressing ML’s Reproducibility Crisis.” Medium, January 7, 2021.\nPyTorch. “Towards Reproducible Research with PyTorch Hub.” Accessed September 18, 2024.\nStojnic, Robert. “ML Code Completeness Checklist.” PapersWithCode (blog), April 8, 2020.\nAkalin, Altuna. “Scientific Data Analysis Pipelines and Reproducibility.” Medium, July 5, 2021.\nHashesh, Ahmed. “Version Control for ML Models: What It Is and How To Implement It.” neptune.ai, July 22, 2022.\nNCEAS Learning Hub\nDonaldson, M. and Mahon, M. 2019. LEGO® Metadata for Reproducibility game pack. Documentation. University of Glasgow.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] A.\nK. Liljedahl et al., “Pan-Arctic ice-wedge\ndegradation in warming permafrost and its influence on tundra\nhydrology,” Nature Geoscience, vol. 9, no. 4, pp.\n312–318, Apr. 2016, doi: 10.1038/ngeo2674.\n\n\n[2] A.\nA. Vasiliev, D. S. Drozdov, A. G. Gravis, G. V. Malkova, K. E. Nyland,\nand D. A. Streletskiy, “Permafrost degradation in the\nWestern Russian Arctic,”\nEnvironmental Research Letters, vol. 15, no. 4, p. 045001, Apr.\n2020, doi: 10.1088/1748-9326/ab6f12.\n\n\n[3] S.\nL. Smith, H. B. O’Neill, K. Isaksen, J. Noetzli, and V. E. Romanovsky,\n“The changing thermal state of permafrost,” Nature\nReviews Earth & Environment, vol. 3, no. 1, pp. 10–23, Jan.\n2022, doi: 10.1038/s43017-021-00240-1.\n\n\n[4] T.\nA. Douglas, M. R. Turetsky, and C. D. Koven, “Increased rainfall\nstimulates permafrost thaw across a variety of Interior\nAlaskan boreal ecosystems,” npj Climate and\nAtmospheric Science, vol. 3, no. 1, pp. 1–7, Jul. 2020, doi: 10.1038/s41612-020-0130-4.\n\n\n[5] R.\nÍ. Magnússon et al., “Extremely wet summer events enhance\npermafrost thaw for multiple years in Siberian\ntundra,” Nature Communications, vol. 13, no. 1, p. 1556,\nMar. 2022, doi: 10.1038/s41467-022-29248-x.\n\n\n[6] L.\nM. Farquharson, V. E. Romanovsky, W. L. Cable, D. A. Walker, S. V.\nKokelj, and D. Nicolsky, “Climate Change\nDrives Widespread and Rapid\nThermokarst Development in Very\nCold Permafrost in the Canadian\nHigh Arctic,” Geophysical Research\nLetters, vol. 46, no. 12, pp. 6681–6689, 2019, doi: 10.1029/2019GL082187.\n\n\n[7] D.\nNotz and J. Stroeve, “Observed Arctic sea-ice loss\ndirectly follows anthropogenic CO2 emission,”\nScience, vol. 354, no. 6313, pp. 747–750, Nov. 2016, doi: 10.1126/science.aag2345.\n\n\n[8] D.\nM. Nielsen, M. Dobrynin, J. Baehr, S. Razumov, and M. Grigoriev,\n“Coastal Erosion Variability at the\nSouthern Laptev Sea\nLinked to Winter Sea\nIce and the Arctic\nOscillation,” Geophysical Research Letters,\nvol. 47, no. 5, p. e2019GL086876, 2020, doi: 10.1029/2019GL086876.\n\n\n[9] L.\nBruhwiler, F.-J. W. Parmentier, P. Crill, M. Leonard, and P. I. Palmer,\n“The Arctic Carbon Cycle\nand Its Response to Changing\nClimate,” Current Climate Change Reports,\nvol. 7, no. 1, pp. 14–34, Mar. 2021, doi: 10.1007/s40641-020-00169-5.\n\n\n[10] T.\nK. F. Campbell, T. C. Lantz, R. H. Fraser, and D. Hogan, “High\nArctic Vegetation Change\nMediated by Hydrological\nConditions,” Ecosystems, vol. 24, no. 1,\npp. 106–121, Jan. 2021, doi: 10.1007/s10021-020-00506-7.\n\n\n[11] S.\nC. Davidson et al., “Ecological insights from three\ndecades of animal movement tracking across a changing\nArctic,” Science, vol. 370, no. 6517, pp.\n712–715, Nov. 2020, doi: 10.1126/science.abb7080.\n\n\n[12] L.\nSuter, D. Streletskiy, and N. Shiklomanov, “Assessment of the cost\nof climate change impacts on critical infrastructure in the circumpolar\nArctic,” Polar Geography, vol. 42, no. 4,\npp. 267–286, Oct. 2019, doi: 10.1080/1088937X.2019.1686082.\n\n\n[13] M.\nL. Druckenmiller et al., “The\nArctic,” Bulletin of the American Meteorological\nSociety, vol. 102, no. 8, pp. S263–S316, Aug. 2021, doi: 10.1175/BAMS-D-21-0086.1.\n\n\n[14] M.\nPhilipp, A. Dietz, S. Buchelt, and C. Kuenzer, “Trends in\nSatellite Earth Observation for\nPermafrost Related\nAnalyses—A Review,”\nRemote Sensing, vol. 13, no. 6, p. 1217, Jan. 2021, doi: 10.3390/rs13061217.\n\n\n[15] “Changing state of Arctic\nsea ice across all seasons - IOPscience.” Accessed:\nOct. 18, 2024. [Online]. Available: https://iopscience.iop.org/article/10.1088/1748-9326/aade56\n\n\n[16] “AI in\nAnalytics: Top Use\nCases and Tools.” Accessed: Oct. 18,\n2024. [Online]. Available: https://www.marketingaiinstitute.com/blog/how-to-use-artificial-intelligence-for-analytics\n\n\n[17] M.\nI. Jordan and T. M. Mitchell, “Machine learning:\nTrends, perspectives, and prospects,”\nScience, vol. 349, no. 6245, pp. 255–260, Jul. 2015, doi: 10.1126/science.aaa8415.\n\n\n[18] A.\nVaswani et al., “Attention is all you need,” in\nAdvances in neural information processing systems, I. Guyon, U.\nV. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R.\nGarnett, Eds., Curran Associates, Inc., 2017. Available: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n\n\n[19] C.\nWitharana et al., “An\nObject-Based Approach for\nMapping Tundra\nIce-Wedge Polygon\nTroughs from Very High\nSpatial Resolution Optical\nSatellite Imagery,” Remote\nSensing, vol. 13, no. 4, p. 558, Jan. 2021, doi: 10.3390/rs13040558.\n\n\n[20] C.\nWitharana et al., “Ice-wedge polygon detection in\nsatellite imagery from pan-Arctic regions,\nPermafrost Discovery Gateway,\n2001-2021,” 2023, doi: 10.18739/A2KW57K57.\n\n\n[21] L.\nEdwards and M. Veale, “Enslaving the Algorithm:\nFrom a ‘Right to an\nExplanation’ to a ‘Right to\nBetter Decisions’?” Social\nScience Research Network, Rochester, NY, 2018. doi: 10.2139/ssrn.3052831.\n\n\n[22] S.\nFink, “This High-Tech\nSolution to Disaster Response\nMay Be Too Good to\nBe True,” The New York Times,\nAug. 2019, Accessed: Oct. 19, 2024. [Online]. Available: https://www.nytimes.com/2019/08/09/us/emergency-response-disaster-technology.html\n\n\n[23] D.\nLeslie, “Understanding artificial intelligence ethics and safety:\nA guide for the responsible design and implementation of\nAI systems in the public sector,” Zenodo, Jun. 2019.\ndoi: 10.5281/zenodo.3240529.\n\n\n[24] S.\nLo Piano, “Ethical principles in machine learning and artificial\nintelligence: Cases from the field and possible ways forward,”\nHumanities and Social Sciences Communications, vol. 7, no. 1,\npp. 1–7, Jun. 2020, doi: 10.1057/s41599-020-0501-9.\n\n\n[25] A.\nMcGovern, I. Ebert-Uphoff, D. J. G. Ii, and A. Bostrom, “Why we\nneed to focus on developing ethical, responsible, and trustworthy\nartificial intelligence approaches for environmental science,”\nEnvironmental Data Science, vol. 1, p. e6, Jan. 2022, doi: 10.1017/eds.2022.5.\n\n\n[26] T.\nShepherd, “Indigenous rangers to use SpaceCows\nprogram to protect sacred sites and rock art from feral herds,”\nThe Guardian, Sep. 2021, Accessed: Oct. 19, 2024. [Online].\nAvailable: https://www.theguardian.com/australia-news/2021/sep/15/indigenous-rangers-to-use-spacecows-program-to-protect-sacred-sites-and-rock-art-from-feral-herds\n\n\n[27] CSIRO, “SpaceCows:\nUsing AI to tackle feral herds in the\nTop End.” Accessed: Oct. 19, 2024.\n[Online]. Available: https://www.csiro.au/en/news/All/News/2021/September/SpaceCows-Using-AI-to-tackle-feral-herds-in-the-Top-End\n\n\n[28] A.\nD. S. A. (ADSA), “The Data Science\nEthos - Operationalizing Ethics\nin Data Science,” The Data Science\nEthos. Accessed: Oct. 19, 2024. [Online]. Available: https://ethos.academicdatascience.org/\n\n\n[29] W.\nChen and A. Quan-Haase, “Big Data Ethics\nand Politics: Toward New\nUnderstandings,” Social Science Computer\nReview, vol. 38, no. 1, pp. 3–9, Feb. 2020, doi: 10.1177/0894439318810734.\n\n\n[30] “Excavating AI.”\nAccessed: Oct. 19, 2024. [Online]. Available: https://excavating.ai/\n\n\n[31] J.\nGray and A. Witt, “A feminist data ethics of care for machine\nlearning: The what, why, who and how,” First\nMonday, Dec. 2021, doi: 10.5210/fm.v26i12.11833.",
    "crumbs": [
      "References"
    ]
  }
]