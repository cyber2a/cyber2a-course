[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "",
    "text": "Course Overview\nAI for Arctic Research represents an introduction to Artificial Intelligence (AI) techniques produced by the Cyber2A project, an innovative training program to empower the Arctic science community with advanced AI-driven data analytics and cyberinfrastructure (CI) skills to tackle the pressing challenges facing the Arctic and thus our planet. Today, Artificial Intelligence has become one of the most powerful tools to analyze Arctic big data and enable new ways of data-driven discovery. However, training on these emerging topics is often missing in current undergraduate and graduate curricula, particularly for active Arctic researchers. This project aims to fill this skills gap in order to foster the growth of an Arctic science workforce with strong data science skills through a series of complementary and mutually reinforcing training activities.\nThe week-long course is designed with a modular curriculum, where each module can be incorporated into learning activities across Universities and other organizations. The curriculum is free to be re-used, licensed under a CC-BY Attribution license and covers 5 main topical areas:\nThe sections presented here fit into a one-week workshop as follows, but the modules can be also used individually:",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nPlease note that by participating in this activity you agree to abide by the NCEAS Code of Conduct.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#session-structure-and-content",
    "href": "index.html#session-structure-and-content",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "Session Structure and Content",
    "text": "Session Structure and Content\nThe course has been designed with several guiding principals in mind.\n\nBalance Theory and Hands-On Work: Spend about one-third of the time on theory and the other two-thirds on hands-on activities.\nBuild Gradually: Start with the basics and build up gradually. Expand both the theory and hands-on tasks as you go along.\nChoose Your Tools: You can use either Jupyter Notebook or VSCode for the hands-on parts of your session. Choose whichever one you’re more comfortable with.\nOpen Data Licensing: Use open data for examples that can be ethically shared and re-used, both within the workshop and when the course materials are used and incorporated into other courses.\nOffer Support: Make sure participants know how to ask for help if they get stuck. Regularly ask participants if they’re keeping up and adjust the pace if needed.\nExtra Resources: Provide additional materials like readings or videos for participants who want to learn more.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "About this book",
    "text": "About this book\nCitation:\n\nWenwen Li, Anna Liljedahl, Matthew B. Jones, Chia-Yu Hsu, Alyona Kosobokova, Jim Regetz, Chandi Witharana, Yili Yang, Ben Galewsky, Minu Mathew, Sandeep Puthanveetil Satheesan, Nicole Greco, Kenton McHenry, Carmen Galaz García, Kate Holman Billmeier. 2024. AI for Arctic Research. Arctic Data Center. doi:10.18739/A2222R77V\n\nThe materials in this book are licensed for reuse, and are available from the cyber2a-course github repository. The book is written in Quarto, a cross platform markdown-based platform for writing books and technical materials that works with Python, R, and other languages.\n\nGetting Started: You can find a guide to getting started with Quarto, including editing and previewing content locally with various tools, here.\nQuarto Guide: You can find a comprehensive guide to Quarto here.\nFormat: You can choose to write your content in either Jupyter Notebook (.ipynb) or Markdown files (.qmd). Quarto can render both formats. And both formats can be easily included in other teaching materials.\nEmbed Notebooks: If you choose to write your content in Markdown but have a separate Jupyter Notebook for the hands-on part, you can embed the notebook. Follow the guide here to learn how to do this.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese course materials were developed with funding for Cyber2A from the National Science Foundation under award # 2230034 to W. Li and M. Jones and award # 2230035 to A. Liljedahl and K. McHenry",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "License",
    "text": "License\n\nCyber2A: AI for Arctic Research is licensed under CC BY 4.0",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "",
    "text": "1.1 The changing Arctic\nThe Arctic is one of the Earth’s remaining frontiers that is critical to the Earth’s climate system. Climate warming and change have pushed the Arctic ecosystem to a tipping point: the frozen is becoming unfrozen with subsequent dramatic impact to its terrestrial and coastal landscapes. Permafrost warming and degradation are documented across the Arctic[1], [2], [3], and are coupled with long-term global warming and extremes in air temperature and precipitation [4], [5], [6]. Further, Arctic sea ice is decreasing rapidly [7], which increases coastal erosion rates across the globe [8]. The Arctic region is remote and is experiencing dramatic changes with local and global implications due to the shift from ice to water: altered soil carbon fluxes [9], changes in vegetation cover [10], shifts in animal behavior [11], and challenges to infrastructure [12]. Accordingly, the transformation of ice to water through degrading permafrost and melting sea and lake ice reverberates through the entire Arctic ecosystem and, therefore, enlists the interest of a broad range of earth, engineering, and social science disciplines [13]. Remote sensing of satellite imagery is an important approach in developing Arctic baseline information, monitoring change, and exploring physical processes [14], [15]. Today, there exist important climatic, geological, biological and sociological data that are yet to be exploited by the Arctic science community. To make the best possible use of these data to address the pressing challenges facing the Arctic environment and Arctic people, the more advanced methods and tools that are available need to be applied. AI-driven analytics, especially those incorporating deep machine learning, can process Arctic big data, automatically detect hidden patterns, and derive new knowledge to enable a new wave of data-driven discovery [16].",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#the-changing-arctic",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#the-changing-arctic",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "",
    "text": "Arctic mountains",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#ai-for-arctic-challenges",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#ai-for-arctic-challenges",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.2 AI for Arctic Challenges",
    "text": "1.2 AI for Arctic Challenges\n\n\n\n\n\n\n“AI will be the most transformative technology since electricity.” – Eric Schmidt\n\n\n\n\n\n\n\n\n\n“AI is just another model.” – Unknown\n\n\n\n\n\n\n\n\n\nArtificial Intelligence\n\n\n\n\n\nArtificial Intelligence (AI) in its broadest sense describes the focus on computing systems that exhibit forms of intelligence. Multiple approaches towards AI have been identified, including:\n\nKnowledge Representation and Reasoning to gain a semantic, logical model of a system\nMachine Learning in which statistical models are used for pattern recognition and prediction\nNatural Language Processing for communication in human languages such as English\nExpert Systems using rule-based logical systems for decision-making\nLarge Language Models for filtering and generating language\n…\n\n\n\n\nThe pursuit of AI as a field has been around since the 1956 with the Dartmouth Workshop, but really took a leap forward in the 2010’s with rising performance of computing hardware and new techniques in machine learning, particularly in the field of deep learning. More recently, AI has entered the public consciousness with the promotion of large language models (LLMs) such as the GPT-3 transformer model and related generative AI systems that are based on foundation models and can quickly generate new outputs [17].\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\nMachine learning (ML) is the subfield of AI concerned with pattern detection using statistical models, which then can be applied to unseen data for prediction and extrapolation without explicit instructions [18]. This mechanistic view of ‘learning’ supports robust evaluation of error and has applications in computer vision, image recognition, speech recognition, text processing and filtering, and many more areas.\n\n\n\nTechniques for machine learning are often divided into three types (supervised, unsupervised, and reinforcement learning). These techiques differ based on the feedback provided to the learning system:\n\nSupervised learning: Training input data are labeled (often manually) by a human, and the algorithm learns by generalizing from these inputs to predict correct outputs\nUnsupervised learning: Without labels, the ML algorithm is designed to detect patterns and structure in the input, often using techniques like gradient descent, clustering, and classification algorithms.\nReinforcement learning: A ML algorithm learns dynamically from interactive input to solve a problem or learn a goal, where correct responses are rewarded (weighted) higher than less correct responses. Learning then becomes an optimization/hill-climbing problem.\n\nThese general approaches all have strengths and weaknesses, and are often used in combination to tackle different aspects of a learning problem.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#geospatial-ai",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#geospatial-ai",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.3 Geospatial AI",
    "text": "1.3 Geospatial AI\nIn this course, we will more narrowly focus on geospatial applications of AI, and particularly on the use of deep learning techniques that employ, for example, convolutional neural networks for feature recognition tasks across massive image datasets such as satellite imagery. As we’ll see during the course, advances in computing hardware, and particularly in available Graphical Processing Unit (GPU) performance have enabled massive growth in the scale of models that can be generated. Today, we can train deep learning models on high-resolution, sub-meter scale satellite imagery (e.g., pan-Arctic, 50cm Maxar imagery), and apply the generated models across the Arctic to better understand change at Arctic scales.\nFor one example, Witharana et al. [19] trained a convolutional neural network model on Maxar imagery, and used the trained model to detect permafrost ice-wedges across the entire Arctic at sub-meter scale [20], producing a map of over a billion vector features, and the first-ever permafrost map at this scale.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#welcome-and-introductions",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#welcome-and-introductions",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.4 Welcome and Introductions",
    "text": "1.4 Welcome and Introductions\nLet’s kick the week off with a warm welcome and round of introductions. We’ll start with our Cyber2A project instructors and speakers, and then introduce each of our participants. Everyone is here due to a deep interest in finding solutions to challenges in Arctic science, and everyone is on their own personal journey through data and science. To learn a little about one another, let’s share:\n\nName and affiliation\nYour data science background (be brief!)\nOne! thing you’d like to get out of the course\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#cyber2a-project",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#cyber2a-project",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.5 Cyber2A project",
    "text": "1.5 Cyber2A project\nDespite the power of these machine learning techniques for Arctic research, the Arctic community has been somewhat delayed compared to other geoscience disciplines in adopting these techniques.\n\nThe Cyber2A project aims to build an Arctic learning community to stimulate the use of GeoAI through data science education. This short-course represents a first pass at a survey of relevant AI techniques that would be useful across Arctic regions and disciplines. The goal is to produce an online curriculum and materials that can be used for self-paced learning by Arctic researchers, and can be included in University graduate and undergraduate courses. While there are many online tutorials on machine learning and AI, these materials will specifically target the types of data and challenges typically found in Arctic research, and focus in on the techniques that will make data science learning more approachable.\nThis course is also a starting point, and not an endpoint. We welcome feedback, suggestions, revisions, and edits to the materials. We want people to adopt, adapt, and revise the materials, and, importantly, contribute those changes back so that others can benefit from these curricular advances. Look for more from Cyber2A as we continue to engage in promoting the use of GeoAI across the Arctic.\n\n\n\n\n\n[1] A. K. Liljedahl et al., “Pan-Arctic ice-wedge degradation in warming permafrost and its influence on tundra hydrology,” Nature Geoscience, vol. 9, no. 4, pp. 312–318, Apr. 2016, doi: 10.1038/ngeo2674.\n\n\n[2] A. A. Vasiliev, D. S. Drozdov, A. G. Gravis, G. V. Malkova, K. E. Nyland, and D. A. Streletskiy, “Permafrost degradation in the Western Russian Arctic,” Environmental Research Letters, vol. 15, no. 4, p. 045001, Apr. 2020, doi: 10.1088/1748-9326/ab6f12.\n\n\n[3] S. L. Smith, H. B. O’Neill, K. Isaksen, J. Noetzli, and V. E. Romanovsky, “The changing thermal state of permafrost,” Nature Reviews Earth & Environment, vol. 3, no. 1, pp. 10–23, Jan. 2022, doi: 10.1038/s43017-021-00240-1.\n\n\n[4] T. A. Douglas, M. R. Turetsky, and C. D. Koven, “Increased rainfall stimulates permafrost thaw across a variety of Interior Alaskan boreal ecosystems,” npj Climate and Atmospheric Science, vol. 3, no. 1, pp. 1–7, Jul. 2020, doi: 10.1038/s41612-020-0130-4.\n\n\n[5] R. Í. Magnússon et al., “Extremely wet summer events enhance permafrost thaw for multiple years in Siberian tundra,” Nature Communications, vol. 13, no. 1, p. 1556, Mar. 2022, doi: 10.1038/s41467-022-29248-x.\n\n\n[6] L. M. Farquharson, V. E. Romanovsky, W. L. Cable, D. A. Walker, S. V. Kokelj, and D. Nicolsky, “Climate Change Drives Widespread and Rapid Thermokarst Development in Very Cold Permafrost in the Canadian High Arctic,” Geophysical Research Letters, vol. 46, no. 12, pp. 6681–6689, 2019, doi: 10.1029/2019GL082187.\n\n\n[7] D. Notz and J. Stroeve, “Observed Arctic sea-ice loss directly follows anthropogenic CO2 emission,” Science, vol. 354, no. 6313, pp. 747–750, Nov. 2016, doi: 10.1126/science.aag2345.\n\n\n[8] D. M. Nielsen, M. Dobrynin, J. Baehr, S. Razumov, and M. Grigoriev, “Coastal Erosion Variability at the Southern Laptev Sea Linked to Winter Sea Ice and the Arctic Oscillation,” Geophysical Research Letters, vol. 47, no. 5, p. e2019GL086876, 2020, doi: 10.1029/2019GL086876.\n\n\n[9] L. Bruhwiler, F.-J. W. Parmentier, P. Crill, M. Leonard, and P. I. Palmer, “The Arctic Carbon Cycle and Its Response to Changing Climate,” Current Climate Change Reports, vol. 7, no. 1, pp. 14–34, Mar. 2021, doi: 10.1007/s40641-020-00169-5.\n\n\n[10] T. K. F. Campbell, T. C. Lantz, R. H. Fraser, and D. Hogan, “High Arctic Vegetation Change Mediated by Hydrological Conditions,” Ecosystems, vol. 24, no. 1, pp. 106–121, Jan. 2021, doi: 10.1007/s10021-020-00506-7.\n\n\n[11] S. C. Davidson et al., “Ecological insights from three decades of animal movement tracking across a changing Arctic,” Science, vol. 370, no. 6517, pp. 712–715, Nov. 2020, doi: 10.1126/science.abb7080.\n\n\n[12] L. Suter, D. Streletskiy, and N. Shiklomanov, “Assessment of the cost of climate change impacts on critical infrastructure in the circumpolar Arctic,” Polar Geography, vol. 42, no. 4, pp. 267–286, Oct. 2019, doi: 10.1080/1088937X.2019.1686082.\n\n\n[13] M. L. Druckenmiller et al., “The Arctic,” Bulletin of the American Meteorological Society, vol. 102, no. 8, pp. S263–S316, Aug. 2021, doi: 10.1175/BAMS-D-21-0086.1.\n\n\n[14] M. Philipp, A. Dietz, S. Buchelt, and C. Kuenzer, “Trends in Satellite Earth Observation for Permafrost Related Analyses—A Review,” Remote Sensing, vol. 13, no. 6, p. 1217, Jan. 2021, doi: 10.3390/rs13061217.\n\n\n[15] “Changing state of Arctic sea ice across all seasons - IOPscience.” Accessed: Oct. 18, 2024. [Online]. Available: https://iopscience.iop.org/article/10.1088/1748-9326/aade56\n\n\n[16] “AI in Analytics: Top Use Cases and Tools.” Accessed: Oct. 18, 2024. [Online]. Available: https://www.marketingaiinstitute.com/blog/how-to-use-artificial-intelligence-for-analytics\n\n\n[17] A. Vaswani et al., “Attention is all you need,” in Advances in neural information processing systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., Curran Associates, Inc., 2017. Available: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n\n\n[18] M. I. Jordan and T. M. Mitchell, “Machine learning: Trends, perspectives, and prospects,” Science, vol. 349, no. 6245, pp. 255–260, Jul. 2015, doi: 10.1126/science.aaa8415.\n\n\n[19] C. Witharana et al., “An Object-Based Approach for Mapping Tundra Ice-Wedge Polygon Troughs from Very High Spatial Resolution Optical Satellite Imagery,” Remote Sensing, vol. 13, no. 4, p. 558, Jan. 2021, doi: 10.3390/rs13040558.\n\n\n[20] C. Witharana et al., “Ice-wedge polygon detection in satellite imagery from pan-Arctic regions, Permafrost Discovery Gateway, 2001-2021,” 2023, doi: 10.18739/A2KW57K57.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html",
    "href": "sections/ai-for-everyone.html",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "",
    "text": "Goals\nIn this chapter, we introduce artificial intelligence to a non-specialist audience. We cover key terminology and the basic principles of intelligence, artificial intelligence, and machine learning. By the end of the chapter, participants will have a solid foundation for the subsequent chapters.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#the-foundations-of-intelligence",
    "href": "sections/ai-for-everyone.html#the-foundations-of-intelligence",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.1 The Foundations of Intelligence",
    "text": "2.1 The Foundations of Intelligence\nBefore reading the definition, take a moment to consider: In your own words, what is AI?\n\n\n\n\n\n\nWhat is AI?\n\n\n\nArtificial Intelligence (AI) is the field concerned with building computer systems that perform tasks typically requiring cognitive functions associated with human intelligence, such as pattern recognition, learning from data, and making predictions.\n\n\nFor a time, one of the goals of AI was to pass the Turing test. The first program often considered to pass a variation of the Turing test was Eugene Goostman, which simulated a 13-year-old boy from Odessa, Ukraine. It passed the Turing test in 2014. Modern chatbots, such as ChatGPT-4, arguably can easily pass the Turing test. However, as early as the 1980s, philosophers like John Searle argued that passing the Turing test does not prove that a machine is intelligent. This concern is increasingly relevant today. Searle often illustrated his argument with the Chinese Room thought experiment.\n\nImagine a person who knows no Chinese locked in a room.\nOutside, native Chinese speakers pass written questions into the room.\nInside, the person uses a large rulebook to match the input characters with appropriate responses.\nThe person inside then passes the answers back out.\n\nTo outsiders, the room appears to “understand” Chinese, yet the person inside has no comprehension of the language’s meaning. Searle argued that a computer running a program is analogous to the person following the rulebook: it manipulates syntax without understanding semantics. Thus, behavioral imitation alone is not evidence of true intelligence.\n\n\n\nFigure 2.1.1: Source [1] The Chinese Room Argument.\n\n\nWhether one finds Searle convincing, the example highlights a gap between performance and actual intelligence.\nArtificial General Intelligence\nMost modern AIs are narrow, or “weak” AIs. The opposite concept, originally envisioned as “strong AI,” is now referred to as Artificial General Intelligence (AGI). AGI is often described as human-like intelligence.\nBut that raises a big question! What exactly is human intelligence?\nGenerally speaking, human intelligence encompasses the capacity to learn from experience, identify patterns, handle abstract concepts, and apply knowledge to shape the environment. Yet, these are only functional descriptions. The core, or what many consider a distinguishing feature is the presence of self-awareness. The reason for its existence remains controversial: why does self-awareness matter for cognition, and what evolutionary advantage does subjective experience provide?\n\n\n\nFigure 2.1.2: The Hard Problem of Consciousness.\n\n\nIn the philosophy of mind, this phenomenon is referred to as qualia, and there is still no definitive scientific answer to why qualia exist. Some argue that consciousness or self-awareness is an emergent property, and it appears when the system becomes complex enough; others propose that it is fundamental. These questions are all part of the Hard Problem of Consciousness. Interested readers can explore scientific theories of consciousness.\n\n\n\n\n\n\nWhat is an Intelligent Agent?\n\n\n\nIntelligent Agent is an entity that can perceive and interact with its environment autonomously.\n\n\nHumans can be seen as one type of intelligent agent, maintaining internal bodily processes and responding to environmental changes. Several attempts have been made to define general intelligence in terms of cognitive abilities. One such theory is the Cattell-Horn-Carroll (CHC) theory, which divides general intelligence into four core abilities: motor control, sensory perception, focused attention, and knowledge. Cattell-Horn-Carroll\n\n\n\n\n\nflowchart TB\n    Title([\"   **Cattell–Horn–Carroll &lt;br/&gt; Theory**  \"])\n\n    Title --&gt; GI(\"**General**&lt;br&gt;**Intelligence**\")\n    GI(\"**General&lt;br&gt;Intelligence**\"):::main\n\n    GI --&gt; MC(\"**Motor Control**\")\n    GI --&gt; SP(\"**Sensory Perception**\")\n    GI --&gt; FA[\"**Focused Attention**\"]\n    GI --&gt; K(\"**Knowledge**\")\n\n\n\n\n\n\nStudying intelligence and consciousness presents a fascinating yet complex set of problems. For now, let’s narrow our focus and ask a simpler question:\nHow do humans think?\nBehaviorism\nBefore the 1950s, psychology was dominated by behaviorism, which held that internal mental processes were either unknowable or scientifically irrelevant. Instead, behaviorists focused on mapping observable stimuli to observable responses.\nCognitive Revolution\nDuring the cognitive revolution of the 1950s–1970s, researchers inspired by digital computers began proposing explicit rule-based models of thinking, often using if/then representations—a so-called symbolic approach (e.g., “If I drink more coffee, I’ll be jittery”).\nProbabilistic Reasoning\nStarting in the late 1970s, a new perspective emerged: psychologists noticed that humans often think experientially. For example, based on today’s cloud cover and similar past experiences, one might infer a high probability of rain and decide to carry an umbrella. Geoffrey Hinton, who won the Nobel Prize in 2024, was a pioneer in this field. He demonstrated that such experiential reasoning can be modeled using probabilistic computations, laying the foundation for modern machine learning algorithms.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#types-of-artificial-intelligence",
    "href": "sections/ai-for-everyone.html#types-of-artificial-intelligence",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.2 Types of Artificial Intelligence",
    "text": "2.2 Types of Artificial Intelligence\nBefore discussing machine learning in depth, it is important to highlight that there are several distinct approaches to developing AI. In this course, we will focus primarily on neural networks, as they currently dominate the field. Neural networks aim to emulate the brain’s structure at the cellular level, forming the basis of an approach known as deep learning. However, not all AI techniques rely on machine learning principles. Just as human reasoning includes multiple modes, such as probabilistic, inference, and symbolic logic, the development of AGI will require combining a variety of paradigms.\n\n\n\n\n\n\n\nAI Technique\nDescription\n\n\n\n\nSymbolic (Logic-Based)\nUses logical rules and symbolic representations to encode and manipulate knowledge. Focuses on deductive reasoning.\n\n\nGenetic (Evolutionary)\nOptimization algorithms inspired by natural selection.\n\n\nFuzzy Logic\nA form of logic that works with “degrees of truth”, making it useful for uncertain or ambiguous scenarios.\n\n\nKnowledge Representation and Reasoning (KR&R)\nStructures information using ontologies, semantic networks, and formal logic to support reasoning tasks.\n\n\nBayesian Networks\nProbabilistic graphical models that capture dependencies between variables, enabling inference under uncertainty.\n\n\n\n Real-world systems are typically hybrids. Modern AI applications, such as self-driving cars and voice assistants, integrate deep learning for perception, symbolic planning, and probabilistic filtering to meet requirements for safety, robustness, and interpretability.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#machine-learning",
    "href": "sections/ai-for-everyone.html#machine-learning",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.3 Machine Learning",
    "text": "2.3 Machine Learning\n\n\n\n\n\n\nWhat is ML?\n\n\n\nMachine Learning (ML) is a subfield of AI focused on algorithms that enable computers to learn patterns from data and build probabilistic models.\n\n\n\n\n\nFigure 2.3.1: Organogram of AI algorithms.\n\n\nSupervised Learning involves learning from labeled data, where models directly learn from input-output pairs. These models are generally simpler in terms of training and achieve high performance. With a labelled dataset, you already know the correct output for every input, so you can optimise model parameters to fit the answer. We will explore them in-depth through the course.\nSemi-Supervised Learning combines a small amount of labeled data with a large amount of unlabeled data, often using auto-labeling techniques. Examples include Self-training models, where a model iteratively labels data to improve, and Graph Neural Networks (GNNs), which are useful for understanding relationships between data points.\nUnsupervised Learning relies on unlabeled data, focusing on identifying patterns or structures. Popular models include Autoencoders, Generative Adversarial Networks (GANs), and Restricted Boltzmann Machines (RBMs).\n\n\n\nFigure 2.3.2: Types of Machine Learning.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#neural-networks",
    "href": "sections/ai-for-everyone.html#neural-networks",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.4 Neural Networks",
    "text": "2.4 Neural Networks\n\n\n\n\n\n\nWhat is ANN?\n\n\n\nArtificial Neural Network (ANN) is a computational model that transforms and interprets input data through layers. Analogous to biological neural networks composed of interconnected neurons, an ANN consists of nodes (basic processing units) arranged in connected layers.\n\n\n\n\n\n\n\nFigure 2.4.1: Source [2] The parts of a neuron: a cell body with a nucleus, branching dendrites, and a long axon connecting with thousands of other neurons at synapses.\n\n\n\n\n\n\nFigure 2.4.2: Structure of a neural network: Ramón y Cajal’s drawing of the cells of the chick cerebellum, from Estructura de los centros nerviosos de las aves, Madrid, 1905\n\n\n\n\nNeural Networks Elements\nThe principle “neurons that fire together, wire together” [3] captures the idea that the strength of neuronal connections adjusts based on experience. Artificial neural networks mimic this by assigning each connection a weight that training continually adjusts. Larger weights reinforce patterns the network finds useful.\n\nEach node multiplies its inputs by their weights, adds the results, and feeds that sum into an activation function. The activation function decides if, and how strongly, the signal moves on to the next layer. When it does, we say the neuron is “activated.”\n\n\n\n\n\n\nWeights\n\n\n\n\n\nWeights are parameters that transform input data as it passes through the network. They set the strength of connections between nodes, with each weight controlling how much influence one node exerts on another. During training, the network adjusts these weights to reduce prediction errors.\n\n\n\n\n\n\n\n\n\nActivation Function\n\n\n\n\n\nThe activation function computes the output of each neuron. It does the non-linear transformation to the input, making it capable to learn and performing more complex tasks.\n\n\n\n\n\n\n\n\n\nInput Layer\n\n\n\n\n\nInformation from the outside world enters the artificial neural network from the input layer. Input nodes provide a connection between the input data and the hidden layers.\n\n\n\n\n\n\n\n\n\nOutput Layer\n\n\n\n\n\nThe output layer produces the network’s final prediction. Every ANN has at minimum an input layer and an output layer. Adding hidden layers in between usually makes the model much more powerful, because they introduce non-linear transformations.\n\n\n\n\n\n\n\n\n\nHidden Layer\n\n\n\n\n\nHidden layers are all the layers between the input and the output. Each one takes the previous layer’s output vector (its “representation”) and converts it into a new vector. More hidden layers mean a deeper neural network. If there is only one or a few we call the network shallow.\n\n\n\nThe Perceptron [4], one of the earliest and simplest neural network models, was invented in 1957 by psychologist Frank Rosenblatt. Rosenblatt’s Perceptron was a physical machine with retina-like sensors as inputs, wires acting as the hidden layers, and a binary output system.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#backpropagation",
    "href": "sections/ai-for-everyone.html#backpropagation",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.5 Backpropagation",
    "text": "2.5 Backpropagation\nInitially, neural networks were quite shallow feed-forward networks. Adding more hidden layers made training them difficult. However, in the 1980s—often referred to as the rebirth of AI—the invention of the backpropagation algorithm revolutionized the field.\nIt allowed for efficient error correction across layers, making it possible to train much deeper networks than before.\n\n\n\n\n\n\nWhat is backpropagation?\n\n\n\nBackpropagation is an algorithm that calculates the error at the output layer of a neural network and then “back propagates” this error through the network, layer by layer. It updates the connections (weights) between neurons to reduce the error, allowing the model to improve its accuracy during training.\n\n\n\n\n\n\n\nFigure 2.5.1: Backpropagation flow. A two-layer network showing the forward pass (blue arrows) that produces the output and the back-propagation pass (green/red arrows) that carries the error signal back through the layers to update each weight\n\n\n\n\n\n\nFigure 2.5.2: A gif visualization of a 784-input, three-hidden-layer perceptron as it processes a single example: bright lines represent strong positive weights, darker/red lines strong negative weights, giving an intuitive sense of how information flows through the network’s layers. Source (3Blue1Brown)\n\n\n\n\nThus, the backpropagation algorithm enabled the training of neural networks with multiple layers, laying the foundation for the field of deep learning.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#deep-learning",
    "href": "sections/ai-for-everyone.html#deep-learning",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.6 Deep Learning",
    "text": "2.6 Deep Learning\n\n\n\n\n\n\nDeep Learning\n\n\n\nDeep Learning (DL) is a subset of ML that uses multilayered neural networks, called deep neural networks.\n\n\n\n\n\nFigure 2.6.1: (a) A shallow model, such as linear regression, has short computation paths between input and output. (b) A decision list network has some long paths for some possible input values, but most paths are short. (c) A deep learning network has longer computation paths, allowing each variable to interact with all the others. Source: Artificial Intelligence - A Modern Approach. [2]\n\n\nDespite advances in backpropagation, deep learning, computing power, and optimization, neural networks still face the problem known as catastrophic forgetting — losing old knowledge when trained on new tasks. Current AI models are often “frozen” and specialized, needing complete retraining for updates, unlike even simple animals that can continuously learn without forgetting [5]. This limitation is one of the reasons that led to the development of specialized deep learning models, each with unique architectures tailored to specific tasks. Let’s explore how each of these models can be applied in scientific research!\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\n\n\n\nConvolutional Neural Networks (CNN) are artificial neural networks designed to process structured data like images. Originally inspired by the mammalian visual cortex, CNNs attempt to mimic how our brains process visual information in layers. First, brains interpret basic shapes like lines and edges and then move to more complex structures, like recognizing a dog ear or the whole animal. CNNs are typically trained using a supervised learning approach.\nCNNs simulate this by using small, repeated filters, or kernels, that scan parts of an image to find basic shapes, edges, and textures, regardless of their location. This scanning process, called convolution, enables early CNN layers to detect simple patterns (like lines) and deeper layers to identify more complex shapes or objects.\nApplication: CNNs are highly effective for image-related tasks, making them ideal for analyzing satellite or drone imagery in ecology and arctic science, identifying structures in biomedical imaging, and classifying galaxies in astrophysics. In permafrost mapping, CNNs help with segmenting thaw features and permafrost extent from satellite imagery, producing pixel‑wise probability maps.\n\n\n\nFigure 2.6.2: Source (Bennett, 2023) [5] Convolutional Neural Networks\n\n\nLimitations of CNNs\nIronically, though CNNs were inspired by the mammalian visual system, they struggle with tasks that even simpler animals like fish handle easily. CNNs have trouble with rotated or differently angled objects. The current workaround is to have variations of object images in training data with all kinds of different angles [5].\nWhile CNNs follow a layered structure, recent research reveals that the brain’s visual processing is more flexible and not as hierarchical as once believed. Our visual system can “skip” layers or process information in parallel, allowing simultaneous handling of different types of visual input across various brain regions.\n\n\n\n\n\n\n\n\n\nRecurrent Neural Networks\n\n\n\n\n\nRecurrent Neural Networks (RNN) are artificial neural networks designed to process sequential data. By incorporating cycles in their computation graph, RNNs can “remember” previous inputs, making them especially useful for tasks where context is important.\nApplication: These models are commonly used for time series data, such as weather forecasting, monitoring ecological changes over time, and analyzing temporal patterns in genomic data.\n\n\n\nFigure 2.6.3: Recurrent Neural Network. Source: dataaspirant.com\n\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning\n\n\n\n\n\nReinforcement Learning (RL) is a learning technique in which an agent interacts with the environment and periodically receives rewards (reinforcements) or penalties to achieve a goal.\nWith supervised learning, an agent learns by passively observing example input/output pairs provided by a “teacher.” Reinforcement Learning introduces AI agents that can actively learn from their own experience in a given environment.[2]\nThe schematic shown in the figure illustrates the basic RL loop. The agent interacts with the environment by taking actions, which affect the environment’s state. The environment responds by updating the state and producing a reward signal. An interpreter component observes the outcome and translates it into a state and reward signal for the agent. This feedback loop allows the agent to learn and adapt its behavior over time.\n\n\n\nFigure 2.6.4: Reinforcement Learning\n\n\nApplications: RL is applied in robotics and can also assist with experiment simulation in science, environmental monitoring, autonomous driving, and creating AI opponents in gaming. Imagine an autonomous rover (or drone) tasked with surveying melt patterns on sea-ice floes.\nOne type of RL is the Actor-Critic model, which divides the learning process into two roles: the Actor, who explores the environment and makes decisions, and the Critic, who evaluates these actions. The Critic provides feedback on the quality of each action, helping the Actor balance exploration (trying new actions) with exploitation (choosing actions with known rewards). Recent research has explored various algorithms to model curiosity in artificial agents [6] [7].\n\n\n\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\nLarge Language Models (LLM) are a type of neural network that has revolutionized natural language processing (NLP). Trained on massive datasets, these models can generate human-like text, translate languages, create various forms of content, and answer questions informatively (e.g., GPT-3, Gemini, Llama).\n\n\n\nFigure 2.6.5: Large Language Models. Source: Artificial Intelligence - A Modern Approach. [2]\n\n\nTransformer is a neural-network architecture introduced in 2017 that relies on a mechanism called self-attention. It supports most modern large language models (LLMs). Unlike previous models like Recurrent Neural Networks (RNNs) that processed language sequentially, Transformers use self-attention to assess relationships between all words in a sentence simultaneously. This allows them to dynamically focus on different parts of the input text and weigh the importance of each word in relation to others. This allows them to understand context and meaning with much better accuracy.\n\n\n\nFigure 2.6.6: Evolution of Natural Languge Processing. Source[2]\n\n\nThe success of LLMs has driven AI’s recent surge in popularity and research. Between 2010 and 2022, the volume of AI-related publications nearly tripled, climbing from about 88,000 in 2010 to over 240,000 by 2022. Likewise, AI patent filings have skyrocketed, increasing from roughly 3,500 in 2010 to over 190,000 in 2022. In the first half of 2024 alone, AI and machine learning companies in the United States attracted $38.6 billion in investment out of a total of $93.4 billion. [8]",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#the-future-of-ai-in-arctic-science",
    "href": "sections/ai-for-everyone.html#the-future-of-ai-in-arctic-science",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.7 The Future of AI in Arctic Science",
    "text": "2.7 The Future of AI in Arctic Science\nAI is transforming the scientific method by supporting each step of scientific discovery. Let’s consider how various AI techniques can be applied at each stage of the scientific process:\n\nObservation: Using computer vision for data collection.\nHypothesis: Clustering data with unsupervised learning.\nExperiment: Simulating environments through reinforcement learning.\nData Analysis: Simplifying and classifying data using neural networks.\nConclusion: Combining LLMs with KR&R to generate complex findings and insights.\n\n\n\nAI already accelerates Arctic science: computer-vision models detect thaw features and permafrost extent; clustering algorithms flag anomalies; neural networks compress and classify multimodal data; and LLMs combined with knowledge graphs help researchers draw conclusions. These tools help turn petabytes of imagery and sensor data into testable hypotheses and valuable insights.\n\nIt’s important to note that, regardless of model type, high-quality data is essential for accurate AI predictions and insights. In the next sessions, we’ll explore practical tips for working with well-prepared, high-quality data.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#exercise-nn-playground",
    "href": "sections/ai-for-everyone.html#exercise-nn-playground",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.8 Exercise: NN Playground",
    "text": "2.8 Exercise: NN Playground\nLet’s build intuition by experimenting with a neural network simulator for approximately 10 minutes.\n\n\n\n\n\n\nWeb-based app, no setup or account required: playground.tensorflow.org\n\n\n\n\n\n\n.\n\n\nTensorFlow Playground is an example of a feed-forward network, where data flows only from the input layer to the output layer without feedback loops. Training a neural network involves automatically adjusting its internal parameters so that the network maps inputs to desired outputs with minimal error. As you press the play button, you can see the number of epochs increase. In an Artificial Neural Network, an epoch represents one complete pass through the training dataset.\n\nAdjust the ratio of training to test data split. Does the quality of the output vary?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOrange indicates negative values, while blue represents positive values. Typically, an 80/20 split for training and testing data is used. Smaller datasets may need a 90% training portion for more examples, while larger datasets can reduce training data to increase test samples. Background colors illustrate the network’s predictions, with more intense colors representing higher confidence in its prediction.\n\n\n\n\nExperiment with noise and batch size parameters. How does the output change?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBackground colors illustrate the network’s predictions, with more intense colors representing higher confidence in its prediction. Adding noise to the training data simulates variability or imperfections often found in real-world datasets. This noise may include measurement errors or irrelevant features. By training with noisy data, the model becomes less sensitive to outliers and irrelevant fluctuations, improving generalization to new inputs.\n\n\n\n\nAdd or remove hidden layers. Notice how it affects the neural network’s performance?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nA deeper network can model more complex patterns, but it also increases the number of parameters and the training time. It raises the risk of overfitting, where training metrics improve while validation metrics stall or deteriorate. In general, you should choose the shallowest model that reaches your target performance without overfitting.\n\n\n\n\nChange the number of neurons in the hidden layers. Can you see any impact on model predictions?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nStart with one hidden layer and one or two neurons, observing predictions (orange vs. blue background) against actual data points (orange vs. blue dots). With very few neurons, the model cannot capture complex nonlinear relationships, resulting in overly simplistic predictions. Increasing the number of neurons and layers allows the network to represent more complex decision boundaries, improving the match between predictions and actual data.\n\n\n\n\nManually adjust the weight. Did you notice the thickness of the line changed?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nLine thickness shows the strength of the connection between nodes. The thinner the line, the less effect they have on each other, and vice versa. If the line turns red, that means the weight is negative; it pushes the next neuron down instead of up. Sort of suppresses the next neuron’s firing.\n\n\n\n\nChange the learning rate to observe its effect on training speed and accuracy.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe learning rate is a key setting or hyperparameter that controls how much a model adjusts its weights during training. A higher rate speeds up learning but risks overshooting the optimal solution. The optimal solution would be the best set of weights that makes the model’s error as small as possible, and the predictions that are accurate. At the same time, a lower rate makes learning more precise but slower. It’s one of the most crucial settings when building a neural network. Because every weight update in a neural-network optimizer is scaled by the learning rate, so that a single number effectively sets how “faster” models learns\n\n\n\n\nTry various activation functions to see how they influence model performance.\nExperiment with different problem types (e.g., classification vs. regression) and analyze the outcomes.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#do-you-have-any-questions",
    "href": "sections/ai-for-everyone.html#do-you-have-any-questions",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "Do You Have Any Questions?",
    "text": "Do You Have Any Questions?\nEmail: alyonak@nceas.ucsb.edu\nWebsite: alyonakosobokova.com\nYouTube: Dork Matter Girl\n\n\n\n\n[1] T. Jackson, Artificial intelligence. New Burlington, 2025, p. 176.\n\n\n[2] P. Norvig and S. J. Russell, Artificial intelligence: A modern approach, 3rd ed. Pearson, 2004. Available: https://books.google.com/books/about/Artificial_Intelligence.html?id=8jZBksh-bUMC\n\n\n[3] D. O. Hebb, The organization of behavior: A neuropsychological theory. New York: Wiley, 1949. Available: https://en.wikipedia.org/wiki/The_Organization_of_Behavior\n\n\n[4] F. Rosenblatt, “The perceptron: A probabilistic model for information storage and organization in the brain,” Psychological Review, vol. 65, no. 6, pp. 386–408, 1958, doi: 10.1037/H0042519.\n\n\n[5] M. Bennett, A brief history of intelligence: Evolution, AI, and the five breakthroughs that made our brains, Hardcover. Harper, 2023.\n\n\n[6] D. Kawahara, S. Ozeki, and I. Mizuuchi, “A curiosity algorithm for robots based on the free energy principle,” pp. 53–59, 2022, doi: 10.1109/SII52469.2022.9708819.\n\n\n[7] T. Wang, F. Wang, Z. Xie, and F. Qin, “Curiosity model policy optimization for robotic manipulator tracking control with input saturation in uncertain environment,” Frontiers in Neurorobotics, vol. 18, 2024, doi: 10.3389/fnbot.2024.1376215.\n\n\n[8] Inc. PitchBook Data, “Artificial intelligence & machine learning report, Q2 2024,” PitchBook, 2024. Available: https://pitchbook.com/news/reports/q2-2024-artificial-intelligence-machine-learning-report",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html",
    "href": "sections/ai-ready-data-in-arctic-research.html",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "",
    "text": "Goal\nThis session dives into the concept of ‘AI-ready data’ in Arctic science and geoscience, highlighting the importance of suitable data for AI applications. Participants will learn about creating and managing metadata and organizing data repositories. We’ll discuss best practices for data preparation and structuring for AI processing. By the end, participants will clearly understand AI-ready data characteristics and the steps to transform raw data for AI applications.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#are-we-ready-for-ai",
    "href": "sections/ai-ready-data-in-arctic-research.html#are-we-ready-for-ai",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.1 Are we ready for AI?",
    "text": "3.1 Are we ready for AI?\n\n\n\n\n\n\nWhat is AI-Ready Data?\n\n\n\nData that are accessible, preferably open, and well-documented, making them easily interpretable and machine-readable to simplify reuse.\n\n\nThis is really a variant on Analysis Ready Data (ARD), or, more recently, “Analysis Ready, Cloud Optimized (ARCO)” data.\n\n\n\nMahecha et al. 2020. [1] Visualization of the implemented Earth system data cube. The figure shows from the top left to bottom right the variables sensible heat (H), latent heat (LE), gross primary production (GPP), surface moisture (SM), land surface temperature (LST), air temperature (Tair), cloudiness (C), precipitation (P), and water vapour (V). The resolution in space is 0.25° and 8 d in time, and we are inspecting the time from May 2008 to May 2010; the spatial range is from 15° S to 60° N, and 10° E to 65° W.\n\n\nWorking with xarray and zarr, one can access many multi-petabyte earth systems datasets like CMIP6 (Coupled Model Intercomparison Project Phase 6) and ERA5 (Earth Re). For an overview of Zarr, see the Arctic Data Center Scalable Computing course chapter on Zarr.\nTake, for example, the ERA5 reanalysis dataset [2], which is normally downloadable in bulk from the Copernicus Data Service. ARCO-ERA5 is an Analysis Ready, Cloud Optimized variant of ERA5 which has been reprocessed into a consistent 0.25° global grid, and chunked and saved in Zarr format with extensive metadata such that spatial and temporal subsets are easily extracted. Hosted on the Google Cloud Storage service in a public bucket (gcp-public-data-arco-era5), anyone can easily access slices of this massive multi-petabyte dataset from anywhere on the Internet, and can be doing analysis in seconds. Let’s take a quick peek at this massive dataset:\nimport xarray\n\nds = xarray.open_zarr(\n    'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3',\n    chunks=None,\n    storage_options=dict(token='anon')\n)\nds\n\nWith one line of code, we accessed 273 climate variables (e.g., 2m_temperature, evaporation, forecast_albedo) spanning 8 decades at hourly time scales. And while this dataset is massive, we can explore it from the comfort of our laptop (not all at once, for which we would need a bigger machine!).\nSo, there’s nothing really special about AI-Ready data, in that a lot of the core requirements for Analysis Ready Data are exactly what are needed for AI modeling as well. Labeling is probably the main difference. Neverthless, many groups have gotten motivated by the promise of AI, and particularly machine learning, across disciplines. For example, the federal government has been ramping up readiness for AI across many agencies. In 2019, the White House Office of Science Technology and Policy (OSTP) started an AI-Readiness matrix, which was followed shortly by the National AI Initiative Act in 2020 [3].\n\nFor example, management agencies have started entire new programs to prepare data and staff for the introduction of AI and machine learning into their processes. One such program with a focus on AI-Ready data is NOAA’s Center for Artificial Intelligence (NCAI).\n\n\n\nNOAA NCAI\n\n\nIn beginning to define AI-Ready data for NOAA, Christensen et al. 2020 defined several axes for evaluation, including data quality, data acess, and data documentation. We’ll be dinving into many of these today and over the course of the week.\n\n\n\n\n\n\n\n\n\nData Quality\n\n\n\n\nCompleteness\nConsistency\nLack of bias\nTimeliness\nProvenance and Integrity\n\n\n\n\n\n\n\n\n\n\n\nData Access\n\n\n\n\nFormats\nDelivery options\nUsage rights\nSecurity / privacy\n\n\n\n\n\n\n\n\n\n\n\nData Documentation\n\n\n\n\nDataset Metadata\nData dictionary\nIdentifier",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#open-data-foundations",
    "href": "sections/ai-ready-data-in-arctic-research.html#open-data-foundations",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.2 Open Data Foundations",
    "text": "3.2 Open Data Foundations\nPreservation and open data access are the foundation of Analysis-Ready and AI-ready data. While all modeling and analysis requires access to data, the ability for AI to encompass massive swaths of information and combine disparate data streams makes open data incredibly valuable. And while the open data movement has seen massive growth and adoption, it’s an unfortunate fact that most research data collected today are still not published and accessible, and challenges to the realization of open data outlined by Reichman et al. (2011) are still prominent today [4].",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#arctic-data-center",
    "href": "sections/ai-ready-data-in-arctic-research.html#arctic-data-center",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.3 Arctic Data Center",
    "text": "3.3 Arctic Data Center\nNevertheless, progress has been made. The National Science Foundation Office of Polar Programs Data, Code, and Sample Management Policy (DCL 22-106) embraces the need to preserve, document, and share the data and results from NSF-funded research, and since 2016 has funded the Arctic Data Center to provide services supporting reseach community data needs. The center provides data submission guidelines and data curation support to create well-documented, understandable, and reusable data from the myriad projects funded by NSF and globally each year. In short, the Arctic Data Center provides a long-term home for over 7000 open, Arctic datasets that are AI-Ready. Researchers increasingly deposit large datasets from remote sensing campaigns using unmanned aerial vehicles (UAV), field expeditions, and observing networks, all of which are prime content for AI.\n\n\n\n\n\n\n\nArctic Data Center Catalog\n\n\n\nIn addition to raw observational data and remote sensing imagery, the ADC also stores and distributes model output, labeled training data, and other derived data products. A recent example comes from the Permafrost Discovery Gateway project, in which Neitze et al. used machine learning on multispectral PlanetScope imagery to extract high-resolution geospatial footprints for retrogressive thaw slumps (RTS) and active layer detachment (ALD) slides across the circum-Arctic permafrost region [5]. In addition, the dataset includes human-generated training labels, processing code, and model checkpoints – just what is needed for further advances in this critical field of climate research.\n\n\n\n\n\n\nDARTS retrogressive thaw slump dataset doi:10.18739/A2RR1PP44\n\n\nWhile this and other valuable data for cross-cutting analysis are available from the Arctic Data Center, there are many other repositories that hold relevant data as well. Regardless of which repository a researher has chosen to share their data, the important thing to remember is to do so – data on your laptop or a University web server are rarely accessible and ready for reuse.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#dataone",
    "href": "sections/ai-ready-data-in-arctic-research.html#dataone",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.4 DataONE",
    "text": "3.4 DataONE\nDataONE is a network designed to connect over 60 global data repositories (and growing) to improve the discoverability and accessiblilty of data from across the world. DataONE provides global data search and discovery by harmonizing myriad metadata standards used across the world, and providing an interoperability API across repositories to make datasets findable and programatically accessible regardless of where they live.\n\nFor example, a query across DataONE in 2024 revealed over 4500 datasets held by 16 different repositories, most of which are not specifically tied to Greenland research, per se.\n\nLooking across the whole of the Arctic, we found over 98,000 datasets from 39 data repositories. It is notable that only 6 of those repositories are focused on Arctic research (like the Arctic Data Center), while the rest are either general repositories or discipline specific repositories. For example, Pangaea as a generalist repository has the most datasets with over 10,000, but there are also significant and important data sets on archeology (TDAR), hydrology (HydroShare) and geochemistry (EarthChem).\n\n\n\nGraph of Arctic Data across DataONE",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#metadata-harmonization",
    "href": "sections/ai-ready-data-in-arctic-research.html#metadata-harmonization",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.5 Metadata harmonization",
    "text": "3.5 Metadata harmonization\nOne of the main roles of DataONE is to promote interoperability and improve the quality and discoverability of global data holdings – all of direct benefit to AI Ready data. DataONE promotes the use of detailed, discipline-specific metadata standards that enable researchers to comprehensively document the structure, contents, context, and protocols used when collecting data. For example, a good metadata record records not only the bibliographic information about the Dataset creators, but also documents the spatial and temporal extent of the data, the methods used to collect it, the types of measured properties that were observed or modeled, and other details that are fundamental to the proper interpretation and reuse of the data. Different disciplines focus on different standards: in ecology and environmental science, where biological metadata on taxonomy are important, the Ecological Metadata Language (EML) is used extensively, whereas in geospatial science where time and space are critical, the emphasis is on the ISO 19115 family of metadata standards. Overall, DataONE supports more than a dozen metadata variants, and can be extended to support more. Across the Arctic, we find datasets that use many different metadata approaches.\n\nDataONE harmonizes these standards by cross-walking them conceptually and making the data available for search through an integrated discovery portal and API. And DataONE promotes semantic labeling of the data as well, particularly for measurement types (e.g., fork length for fish length meeasurements) and dataset classification [6]. These annotations are indexed against controlled, ontologically-precise term labels that are stored in queryable systems. For example, the Ecosystem Ontology (ECSO, the Environment Ontology (ENVO), and many others contain precisely defined terms that are useful for precise dataset labeling to differentiate subtly different terms and concepts.\n\n\n\nA sub-Arctic salmon-related dataset [7], showing annotations for each of the measured variables in the dataset. Each annotation is to a precisely defined concept or term from a controlled vocabulary, allowing subtle differences in methodology to be distinguished, which helps with both data discovery and proper reuse. The underlying metadata model is machine-readable, allowing search systems, and amchine learning harvesters to make use of this structured label data.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#croissant-metadata-for-machine-learning",
    "href": "sections/ai-ready-data-in-arctic-research.html#croissant-metadata-for-machine-learning",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.6 Croissant metadata for machine learning",
    "text": "3.6 Croissant metadata for machine learning\nWhile domain-specific metadata dialects continue to proliferate, an increasing number of data repositories support schema.org as a lingua franca to describe datasets on the web for discoverability. The Science on Schema.org (SOSO) project provides interoperability guidelines for using schema.org metadata in dataset landing pages, and DataONE supports search across repositories that produce schema.org. That said, the dialect is fairly lightweight, somewhat lossely defined, and therefore permits some ambiguity in usage. But it has the major advantage that, as a graph-based metadata dialect, it can be easily extended to support new terms and use cases.\nThe Croissant specification [8] extends schema.org with more precise and structured metadata to enable machine-interpretation and use of datasets across multiple tools. While the vocabulary is not as rich as, for example, the ISO 19115 metadata for geospatial metadata, it does provide a more strict structural definition of data types and contents that plain schema.org. A quote from the specification illustrates its intended scope [8]:\n\nThe Croissant metadata format simplifies how data is used by ML models. It provides a vocabulary for dataset attributes, streamlining how data is loaded across ML frameworks such as PyTorch, TensorFlow or JAX. In doing so, Croissant enables the interchange of datasets between ML frameworks and beyond, tackling a variety of discoverability, portability, reproducibility, and responsible AI (RAI) challenges.\n\nCroissant has also explicitly defined metadata to meet the needs of machine-learning tools and algorithms. For example, Crosissant supports the definition of categorical values, data splits for training, testing, and prediction, labels/annotations, specification of bounding boxes and segmentation masks.\nLabel Data. As an example, Crioissant has specific metadata fields designed to capture which fields with the data contain critical label data, which are used by supervised learning workflows. The Croissant metadata class cr:Label can be used in a RecordSet to indicate that a specific field contians labels that apply to the that record.\n{\n  \"@type\": \"cr:RecordSet\",\n  \"@id\": \"images\",\n  \"field\": [\n    {\n      \"@type\": \"cr:Field\",\n      \"@id\": \"images/image\"\n    },\n    {\n      \"@type\": \"cr:Field\",\n      \"@id\": \"images/label\",\n      \"dataType\": [\"sc:Text\", \"cr:Label\"]\n    }\n  ]\n}\nThe intention is that multiple tools, all supporting the Crosissant metadata model, will be able to exchange ML-related data seamlessly. A number of ML tools support Croissant out-of-the-box, but only a tiny fraction of the datasets available today use this nascent vocabulary. In addition, it lacks the sophistication of Analysis Ready, Cloud Optimized (ARCO) data standards like Zarr that permit seamless access to massive data with minimal overhead. But it has a lot of promise for streamlining AI-Ready data, and can be used on top of exiting standards like Zarr.\n\n\n\nCommon ML data providers like Hugging Face and Kaggle could use Croissant to produce ML-optimized datasets, which in turn can be seamlessly loaded and used with compatible ML libraries such as TensorFlow and PyTorch. Image credit: [8]",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#data-quality-1",
    "href": "sections/ai-ready-data-in-arctic-research.html#data-quality-1",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.7 Data Quality",
    "text": "3.7 Data Quality\nOne of the primary determinants of AI-Ready data is whether the data are of sufficient quality for the intended purpose. While the quality of a dataset may have been high for the iniital hypothesis for which it was generated, it might be quite low (e.g., due to biased or selective sampling) at the scales at which machine learning might operate. Consequently, it is fundamentally important to assess the quality of datasets during the planning and execution of an AI project. Questions such as the following would be of prime interest:\n\nDoes the dataset represent the complete population of interest?\nDoes training data reflect an unbiased sample of that population?\nAre the data well-documented, enabling methodological interpretation?\nDid data collection procedures follow standards for responsible AI and ethical reseatch practices?\nAre the data Tidy (normalized) and structured (see Tidy Data lesson)?\nAre geospatial data accessibly structured, with sufficient metadata (e.g., about the Coordinate Reference System)?\n…\n\n\nMany of these issues are encompassed by the FAIR Principles, which are intended to ensure that published data are Finadable, Accessible, Interoperable, and Reusable [9], [10]. While there are a huge variety of methods to assess data quality in use across disciplines, some groups have started to harmonize rubrics for data quality and how to represent data quality results in metadata records (see [11]).\nDataONE is one such group that has operationalized FAIR Assessment [11]. Within the DataONE network, all compatible datasets are evaluated using an automated FAIR rubric which rates the dataset on 52 FAIR checks [12], [13]. Each of these checks is atomic, and looks at a small facet of the dataset quality, but combined they give a powerful assessment of dataset readiness for various analytical purposes. And these suites of checks are extensible, so different groups can create suites of automated quality assessment checks that match their needs and recommendations as a community.\n\n\n\n\nDataONE FAIR assessment\n\n\n\nWe see a marked improvement in dataset quality across the FAIR axes as datasets go through our curation process at the Arctic Data Center.\nThe Arctic Data Center team is currently working to extend this quality assessment suite to deeper data quality checks. Most of the current checks are based on metadata, mainly because these are accessible through the centralized DataONE network, whereas data are distributed throughout the network, and are much larger. The ADC data quality suite will assess generic quality checks and domain-specific quality checks to produce both dataset-level and project-level quality reports. Some of the key types of checks that are posisble with the system include:\n\n\n\nGeneric Checks\nDomain/Discipline Checks\n\n\n\n\nDomain and range values\nTree diameters don’t get smaller\n\n\nData type conformance\nUnit and measurement semantics\n\n\nData model normal\nSpecies taxonomy consistent\n\n\nChecksum matches metadata\nOutlier flagging\n\n\nMalware scans\nAllometric relations among variables\n\n\nCharacters match encoding\nCalibration and Validation conformance\n\n\nPrimary keys unique\nTemporal autocorrelation\n\n\nForeign keys valid\nData / model agreement",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#esip-ai-readiness-checklist",
    "href": "sections/ai-ready-data-in-arctic-research.html#esip-ai-readiness-checklist",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.8 ESIP AI-Readiness Checklist",
    "text": "3.8 ESIP AI-Readiness Checklist\nTo round out our survey of AI-Readiness in data, let’s look at the AI-Readiness Checklist that has been developed by an inter-agency collaboration at the Earth Science Information Partners (ESIP) [14]. This checklist was designed as a way to evaluate the readiness of a data product for use in specific AI workflows, but it is general enough to apply to a wide variety of datasets. In general, the checklist asks questions about four major areas of readiness:\n\nData Preparation\nData Quality\nData Documentation\nData Access\n\nThe challenge with this checklist and others is that readiness is truly in the eye of the beholder. Each project has unique data needs, and what is a fine dataset for one analytical purpose (e.g., a regional model) may be entirely inadequate for another (e.g., a pan-Arctic model).\n\n\n\n\n\n\nExercise: Assess Dataset Readiness\n\n\n\nThere are a huge variety of datasets available from DataONE and the Arctic Data Center, and many other repositories. In this exercise we will do a quick assessment of AI Readiness for the ice wedge polygon permafrost dataset from the Permafrost Discovery Gateway project [[5]], using the ESIP AI-Readiness Checklist.\nLink to dataset: - doi:10.18739/A2KW57K57 - Visualize Permafrost Ice Wedge Polygon data on PDG:\nWe’re going to break into groups, and each group will work on a portion of the evaluation for the dataset. The groups are:\n\nGroup A (Preparation)\nGroup B (Data Quality)\nGroup C (Data Documentation)\nGroup D (Data Access)\n\nInstructions: - Make a copy of the AI-Readiness Checklist spreadsheet - Split into 4 groups and try to quickly answer the questions (we won’t really have time, so don’t get too bogged down)\nQuestions to consider as you are doing the assessment: - How much time would it take for you to do a true assessment? - How useful would this assessment be to you if it were available for most datasets? - Is there a correct answer to the checklist questions?\n\n\n\n\n\n\n\n[1] M. D. Mahecha et al., “Earth system data cubes unravel global multivariate dynamics,” Earth System Dynamics, vol. 11, no. 1, pp. 201–234, Feb. 2020, doi: 10.5194/esd-11-201-2020.\n\n\n[2] “ERA5 hourly data on single levels from 1940 to present.” doi: https://doi.org/10.24381/cds.adbb2d47.\n\n\n[3] S. Long and T. Romanoff, “AI-Ready Open Data.” AI-Ready Open Data  Bipartisan Policy Center, 2023. Accessed: Oct. 19, 2024. [Online]. Available: https://bipartisanpolicy.org/explainer/ai-ready-open-data/\n\n\n[4] O. J. Reichman, M. B. Jones, and M. P. Schildhauer, “Challenges and opportunities of open data in ecology.” Science (New York, N.Y.), vol. 331, no. 6018, pp. 703–5, Feb. 2011, doi: 10.1126/science.1197962.\n\n\n[5] I. Nitze et al., “DARTS: Multi-year database of AI detected retrogressive thaw slumps (RTS) and active layer detachment slides (ALD) in hotspots of the circum-arctic permafrost region - v1,” 2024, doi: 10.18739/A2RR1PP44.\n\n\n[6] S. S. Chong, M. Schildhauer, M. O’Brien, B. Mecum, and M. B. Jones, “Enhancing the FAIRness of Arctic Research Data Through Semantic Annotation,” Data Science Journal, vol. 23, no. 1, Jan. 2024, doi: 10.5334/dsj-2024-002.\n\n\n[7] A. D. of F. Game, D. of C. and Fisheries, and A.-Y.-K. Region, “Salmon age, sex, and length data from Arctic-Yukon-Kuskokwim Region of Alaska, 1960-2017,” 2018, doi: 10.5063/SN07CZ.\n\n\n[8] O. Benjelloun et al., “Croissant Format Specification,” Croissant site. 2024. Accessed: Oct. 20, 2024. [Online]. Available: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n\n\n[9] M. D. Wilkinson et al., “The FAIR Guiding Principles for scientific data management and stewardship,” Scientific Data, vol. 3, p. 160018, Mar. 2016, doi: 10.1038/sdata.2016.18.\n\n\n[10] M. D. Wilkinson, S.-A. Sansone, E. Schultes, P. Doorn, L. O. Bonino da Silva Santos, and M. Dumontier, “A design framework and exemplar metrics for FAIRness,” Scientific Data, vol. 5, p. 180118, Jun. 2018, doi: 10.1038/sdata.2018.118.\n\n\n[11] G. Peng et al., “Harmonizing quality measures of FAIRness assessment towards machine-actionable quality information,” International Journal of Digital Earth, vol. 17, no. 1, p. 2390431, Dec. 2024, doi: 10.1080/17538947.2024.2390431.\n\n\n[12] M. Jones, P. Slaughter, and T. Habermann, “Quantifying FAIR: Metadata improvement and guidance in the DataONE repository network.” 2019. doi: https://doi.org/10.5063/f1kp80gx.\n\n\n[13] M. Jones et al., “MetaDIG: Engaging Scientists in the Improvement of Metadata and Data,” Figshare, 2016, doi: 10.6084/m9.figshare.4055808.v1.\n\n\n[14] “Checklist to Examine AI-readiness for Open Environmental Datasets,” figshare. Jun. 2022. doi: 10.6084/m9.figshare.19983722.v1.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html",
    "href": "sections/data-annotation.html",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "",
    "text": "Goals\nThis session explores the critical role of training data in deep learning, focusing on data annotation methods, tools, and strategies for acquiring high-quality data. Participants will learn how well-annotated data supports effective deep learning models, understanding the challenges and best practices in data annotation. By the end, participants will be equipped to prepare their datasets for deep learning.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#key-elements",
    "href": "sections/data-annotation.html#key-elements",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "Key Elements",
    "text": "Key Elements\nTraining data’s role, annotation methods/tools, annotated data’s importance, annotation challenges, annotation best practices, dataset preparation",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#annotation-fundamentals",
    "href": "sections/data-annotation.html#annotation-fundamentals",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "4.1 Annotation Fundamentals",
    "text": "4.1 Annotation Fundamentals\n\n\n\n\n\n\nHighlights\n\n\n\n\nReiterate ideas related to supervised learning and the core idea of learning from examples\nDiscuss key role of labeling/annotation in general for generating examples to learn from\nTake a quick tour of label/annotation examples across various ML applications (structured data, text, audio, image, video, etc)\nTalk about some general challenges of procuring/producing labeled data for Machine Learning\n\n\n\n\n4.1.1 Fueling intelligence: It’s All About the Data!\nThe modern AI renaissance is driven by the synergistic combination of Computing advances, more & better data for training, and algorithmic innovations.\n\n\n\nSource: OECD.ai\n\n\nEach of these is critical, but you really can’t overstate the importance of massively upscaling training and validation data. Indeed, to a large extent, the most important recent advances in algorithms and computing have been those that allow us to efficiently use huge amounts of data. The more data available, the better the model can learn.\nRemember that in Machine Learning:\n\nYou are building a model to produce some desired output for a given input. Imagine you have an aerial photo that contains a water body, or a camera trap video that contains a bear, or an audio recording that captures the song of a particular bird species. In each case, you want the model to correctly detect, recognize, and report the relevant feature.\nTo achieve this, you do not build this model by instructing the computer how to detect the water body, bear, or bird species. Instead, you assemble many (often many, many!) good examples of the phenomena of interest, and feed them to an algorithm that allows the model to adaptively learn from these examples. In practice, there may be rule-based guardrails, but we can talk about that separately later in the course.\n\n\n\n\n.\n\n\nMuch of this course is about understanding what kinds of model structures and learning algorithms allow this seemingly magical learning to happen inside the computer, and what the end-to-end process looks like. But for now, we are going to focus on the input data. And as we embark, it is essential that this core concept makes sense to you:\nFor any project involving the development of an AI model, you will quite likely be starting with a generic algorithm that has limited or even zero specific knowledge of your particular application area. Unlike with “classical” modeling, the way you will adapt it to apply to your project is not by hand-tweaking parameters or choosing functional forms describing your phenomenon of interest, but rather by exposing this generalized algorithm to many relevant examples (positive and negative) to learn from.\nBottom line, much like vehicles without fuel, even the best training algorithms in the world will just sit and gather dust if they don’t have sufficient data to learn from!\n\n\n\nSource: Walking Dead Fandom\n\n\nUltimately, although you will need to have an understanding of algorithms and models, and learn how to operationalize them on compute platforms, your success in applying AI (especially if you are training and/or fine-tuning models, rather than simply applying pre-trained models) will depend on your ability to implement a robust and effective data pipeline, from data collection methods to data annotation to data curation.\n\n\n\n\n\nSource: DZone\n\n\nIn this module, we focus on data annotation.\n\n\n4.1.2 What is annotation?\nData annotation is the process of labeling or marking up data with information that is not already explicit in the data itself.\nIn general, we do this to provide important and relevant context or meaning to the data. As humans, especially in knowledge work, we do this all the time for the purpose of sharing information with others.\n\n\n\nSource: PowerPoint Tricks\n\n\nIn the context of Machine Learning and AI, our objective is to teach a model how to create accurate and useful annotations itself when it encounters new, unannotated data in the future. In order to do this, we need to provide the model with annotated examples that it can train on.\nTo put it a different way, annotation is the process of taking some data just like the kind of data you will eventually feed into the model, and attaching to it the correct answer to whatever question you will be asking the model about that data.\nSimply put, annotation refers to labeling data with information that a model needs to learn, and is not already inherently present in the data.\n\n\n\n\n\n\nThe term “annotation” is synonymous with “labeling.”\n\n\n\n\n4.1.2.1 Examples\n\n\n\n\n\n\n\nTabular Data Annotation\n\n\n\n\n\n\n\nLabel (aka Target) column: Species\n\nWhen working with tabular data, we don’t usually talk about “annotating” the data. Nevertheless, the concept of labeling for supervised learning tasks (such as classification and regression) still applies, and indeed it’s common practice to refer to the data used for classification and regression model training as “labeled data”. Labeled tabular data contains a column designated as the target for learning, i.e. the column containing the value that a model learns to predict. Depending on the context (and background of the writer/speaker), you might also hear this referred to as the label, outcome variable, dependent variable, or even just y variable. If this is not already inherently present in the dataset, it must be added by an annotator before proceeding with modeling.\n\n\n\n\n\n\n\n\n\nText Annotation\n\n\n\n\n\n\n\nSentiment: Positive\nParts of speech: most::adv, beautiful::adj\nNamed entity: Alaska\n\n\n\n\n\n\n\n\n\n\nAudio Annotation\n\n\n\n\n\n\n\nVoice recognition\nSpeech to text\n\n\n\n\n\n\n\n\n\n\nImage Annotation\n\n\n\n\n\n\n… our focus today and this week! See details below.\n\n\n\n\n\n\n\n\n\n\nVideo Annotation\n\n\n\n\n\n\nLike image annotation, but with many frames! The focus is often on tracking movement of objects, detecting change, and recognizing activities.\n\n\n\n\n\n\n4.1.3 Why is annotation so important?\nWe’ve already talked about the critical role of data overall in enabling supervised learning, and the role of annotation in explicitly adding or revealing the information in the data.\n\nMore specifically, the annotated data will be used at training time, when a specific learning algorithm will use the information in your annotated data to update internal parameters to yield a specific parameterized (aka “trained”) version of the model that can do a sufficiently good job at getting the right answer when exposed to new data that it hasn’t seen before, and doesn’t have labels.\nThe overall volume and quality of the annotations will have a huge impact on the following characteristics of a model trained on those data:\n\nAccuracy\nPrecision\nGeneralizability\n\nObviously, there is a bit of tension here! The point of training the model is do something for you. But in order for the AI to be able to do this, you have to first teach it how, which means doing the very thing that you want it to do.\nThink of it like hiring a large team of interns. Yes, it takes extra startup time to get them trained. But once you do that, you’re able to scale up operations far beyond what you could do on your own.\nThis raises a few questions that we’ll touch on as we proceed through the course:\n\nIs there a model out there that already knows at least something about what I’m trying to do, so I’m not training it from scratch? Maybe yes! This is a benefit that foundation models (and more generally, transfer learning) offer. To build on the human intern analogy, if you can hire undergrad researchers studying in a field relevant to the task, you’re likely to move much faster than if you hired a 1st grader!\nHow much annotated data do I need? Unfortunately, there is no simple answer. It depends on the complexity of the task, the clarity of the information, etc. So, as we’ll discuss, best practice is to proceed iteratively.\n\n\n\n4.1.4 Annotation challenges\nBy now, it should be clear that your goal in the data annotation phase is to quickly and correctly annotate a large enough corpus of inputs that collectively provide an adequate representation of the information you want the model to learn.\nHere are some of the key challenges to this activity:\n\n\n\n\n\n\nScalability\n\n\n\n\n\nSimply put, annotating large datasets can be time-consuming!\nThis is especially the case for more complex annotation tasks. Identifying a penguin standing on a rock is one thing, but comprehensively identifying and labeling all land cover types present in a satellite image is much more time-consuming. Multiply this task by hundreds or thousands, and you’ve quite a scaling challenge!\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nCost\n\n\n\n\n\nCosts become important in conjunction with the scalability challenge.\nYou may find you need to pay for:\n\nAnnotators’ time, whether they are directly employed or used via a service\nAnnotation software or services, if you go with a commercial tool vendor\nData storage, if you are leveraging your own hardware and/or cloud providers like AWS to store large amounts of data\nCPU/GPU cycles, if you are leveraging your own hardware or cloud services to run annotation software, especially if you are using AI-assisted annotation capabilities\n\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nQuality control\n\n\n\n\n\nAnnotation is not always straightforward, but as we’ve discussed, effective model training depends on producing sufficiently high-quality annotations of sufficiently high-quality training data.\nSome factors to consider:\n\nSource data quality. Is the information signal clear in the data? And does the input dataset include a sufficiently diverse set of examples that are representative of what the model will encounter when deployed?\nAnnotation consistency. Do the annotations capture information in the same way across images? This becomes an even bigger factor when multiple annotators are involved. Clear annotation guidelines and tracking various consistency metrics can help here.\nAnnotation quality. Are the annotations accurate, precise, and complete? Have annotators introduced bias?\n\nIn the end, you will likely need to strike a balance between speed and quality. Determining the right goalposts for “good enough” will require experimentation and iterative model training and testing.\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nSubjectivity\n\n\n\n\n\nIn some applications, there is no clear correct answer! In that case, especially without clear guidelines and training, different annotators can interpret data differently. This can lead to inconsistent labels, which in turn will negatively impact model training and lead to degraded model performance.\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nData and annotation management\n\n\n\n\n\nOn a practical front, effectively managing a large-scale annotation activity also requires managing and organizing all associated annotation artifacts, including both the input data and the generated annotations.\nIf you are performing annotation across a team of people, you also likely need to keep track of multiple annotations per data object (performed across multiple annotators), metadata associated with those annotations (e.g., how long each annotator took to complete the task), and various metrics for monitoring annotation and annotator performance over time.\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nData privacy & security\n\n\n\n\n\nThis is especially important if you use a cloud-based tool for annotation.\nAsk yourself: What is their data privacy and security policy, and is it sufficient to meet your needs?\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nBias & Ethics\n\n\n\n\n\nManaging bias and ethics is not an annotation-specific problem, and we’ll discuss this later in the course. However, bear in mind that annotation can be a major factor, because it is a step in the modeling process when some specific human knowledge (i.e., what the annotators know) is attached to the input data, and will be very directly exposed to the model during training. This creates an opportunity for injecting bias, exposing sensitive or private information, among other things.\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nCallout: Annotating satellite imagery\n\n\n\n\nLabeling of satellite imagery brings its own specific challenges. Consider:\n\nScenes are often highly complex and rich in detail\nGeographic distortion: Angle of sensor\nAtmospheric distortion: Haze, fog, clouds\nVariability over time:\n\nWhat time of day? The angle of the sun affects visible characteristics\nWhat time of year? Many features change seasonally (e.g., deciduous forest, grasslands in seasonally arid environments, snow cover, etc.)\nFeatures change! Forests are cut, etc. Be mindful of the difference between labeling an image and labeling a patch of the Earth’s surface.\n\nIt’s often desirable to maintain the correspondence between pixels and their geospatial location, for cross-reference with maps and/or other imagery\n\n\n\n\n\n4.1.5 Annotation best practices\nThis list could certainly be longer, but if you remember and apply these practices, you’ll start off on a good foot.\n\n\n\n\n\n\nDevelop a thorough annotation protocol\n\n\n\n\n\nCreate and maintain clear labeling instructions.\n\n\n\n\n\n\n\n\n\nProvide annotator training\n\n\n\n\n\n\nWork with annotators to make sure they understand the domain, use cases, and overall purpose of the project.\nProvide specific guidance about what to do in ambiguous or difficult cases, in order to help standardize annotations.\nConsider having new annotators apply annotations on a set of sample inputs, assess those annotations, and provide clear feedback with reference to what they could or should do better.\n\n\n\n\n\n\n\n\n\n\nHave a quality control process\n\n\n\n\n\nTo ensure sufficient quality, plan on doing regular checks, running cross-validations, and having feedback loops.\nFirst, periodically conduct manual annotation reviews to ensure compliance with instructions. This might include having a recognized expert on the team randomly selecting a subset of annotated images to assess.\nSecond, identify and calculate quality metrics on an ongoing basis, targeting each of the following:\nConsensus. To measure the degree to which different annotators on the team are providing similar annotations, have multiple annotators annotate some of the same images, and calculate a consensus measure like Inter-annotator agreement (IAA). Several flavors of this metric exist, such as Cohen’s kappa (to compare 2 labelers) and Fleiss’ kappa (to compare &gt;2 labelers).\nAccuracy. In cases where there’s a known “correct” answer, either for all images or some subset thereof, calculate annotation performance metrics. Here are a couple of examples: - For bounding boxes, calculate a metric like Intersection over union (IoU): Take the area of overlap between the ground truth box and the annotated box, and divide by total area of the (unioned) boxes. - For detected objects overall, calculate standard metrics like precision (proportion of labeled objects that are correctly labeled) and recall (proportion of all objects that were correctly labeled)\nCompleteness. Keep track of annotation completeness overall. For example, when doing bounding box annotation for an object detection task, ensure that all drawn boxes are associated with a valid label.\n\n\n\n\n\n\n\n\n\nProceed iteratively!\n\n\n\n\n\nIn a nutshell:\n\nStart small\nRefine and improve as you go\nScale gradually",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#image-annotation-methodology",
    "href": "sections/data-annotation.html#image-annotation-methodology",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "4.2 Image Annotation Methodology",
    "text": "4.2 Image Annotation Methodology\n\n\n\n\n\n\nHighlights\n\n\n\n\nDiscuss the primary types of image annotations\nDiscuss the common image-related AI/ML tasks requiring annotation\nDiscuss different methods for annotating images\nDescribe a high-level annotation workflow\n\n\n\nIt’s important to understand and recognize the difference between image annotation types, tasks, and methods. Note that this isn’t universal or standardized terminology, but it’s pretty widespread.\nIn this context:\n\nAn annotation type describes the specific format or structure of the annotation used to convey information about the data critical for supporting the task.\nAn annotation task is the specific objective that the annotations are meant to support, i.e., the job you want your AI application to do. In the computer vision context, this typically means identifying or understanding something about an image and conveying that information in some specific form.\nAn annotation method refers to the process or approach used to create the annotations.\n\n\n4.2.1 Image Annotation Types\nThe type of annotation you apply will depend partly on the task (see next section), as different annotation types are naturally suited for different tasks. However, the decision will also be driven in part by time, cost, and accuracy considerations.\n\n\n\n\n\n\nImage tags\n\n\n\n\n\nTags are categorical labels, words, or phrases associated with the image as a whole, without explicit linkage to any localized portion of the image. \n\nLabel: beach\nCaption: “Embracing the serenity of the shore, where the sky meets the ocean #outdoor #beachlife #nature”\n\n\n\n\n\n\n\n\n\n\nBounding boxes\n\n\n\n\n\nBounding boxes are rectangles drawn around objects to localize them within an image. \nTypically, they are axis-aligned, meaning two sides are parallel with the image top/bottom, and two sides are parallel with the image sides, but sometimes rotation is supported.\n\n\n\n\n\n\n\n\n\nPolygons\n\n\n\n\n\nGeneralizing the bounding box concept, polygons are a series of 3 or more connected line segments (each with definable end coordinates) that form a closed shape (i.e., the end of the last segment is the beginning of the first segment), used to more precisely localize objects or areas by outlining their shape. \n\n\n\n\n\n\n\n\n\nSegmentations\n\n\n\n\n\nSegmentations involve assigning a class label to individual pixels (or collectively, to regions of individual pixels) in an image. Segmentation may be done either fully for all pixels or partially only for pixels associated with phenomena of interest.\nIn practice, segmentations are produced either by drawing a polygon to circumscribe relevant pixels or using a brush tool to select them in entire swaths at a time\n\n\n\n\n\n\n\n\n\n\nKeypoints\n\n\n\n\n\nKeypoints are simply points used for denoting specific landmarks or features (e.g., skeletal points in human pose estimation). \n\n\n\n\n\n\n\n\n\nPolylines\n\n\n\n\n\nPolylines are conceptually similar to polygons, but they do not form a closed shape. Instead, the lines are used to mark linear features such as roads, rivers, powerlines, or boundaries. \n\n\n\n\n\n\n\n\n\n3D Cuboids\n\n\n\n\n\n3D cuboids are bounding boxes extended to three dimensions. These are often used in LiDAR data, which contain a 3-dimensional point cloud, but can also be used to indicate depth of field in a 2D image when the modeling task involves understanding position in three dimensions. \n\n\n\n\n\n4.2.2 Image Annotation Tasks\nThe task you choose will depend on the type of information you want the model to extract from the images. Here are the key types of annotation tasks in computer vision:\n\n\n\n\n\n\nImage Classification\n\n\n\n\n\nImage classification is the task of assigning an entire image to a category.\nThe classification typically refers to some singular dominant object or feature (e.g., “Polar bear”) within the image, or some defining characteristic of the image (e.g., “Grassland”), but the details depend on the specific use case motivating the modeling exercise.\n\n\n\n\n\n\n\n\n\nImage Captioning\n\n\n\n\n\nImage captioning is the task of generating textual descriptions of the image. It is conceptually similar to image classification, but involves producing freeform text for each image rather than assigning the image to one of a set of pre-defined categorical classifications.\n\n\n\n\n\n\n\n\n\nObject Detection\n\n\n\n\n\nObject detection is the task of identifying one or more objects or discrete entities within an image.\nNote that object detection involves two distinct sub-tasks:\n\nLocalization: Where is the object within the image?\nClassification: What is the localized object?\n\n\n\n\n\n\n\n\n\n\nImage Segmentation\n\n\n\n\n\nSegmentation is the task of associating individual pixels with labels for the purpose of enabling detailed image analysis (e.g., land-use segmentation). In some sense, you can think of it as object detection reported at the pixel level.\nThere are three distinct kinds of segmentation, illustrated below for the following image:\n\nSemantic Segmentation assigns a class label to each pixel in the image, without differentiating individual instances of that class. It is best for amorphous and uncountable “stuff”. In the image below, notice the segmentation and separation of the foreground grass from the background trees from the water in the middle. Also notice that the bears are all lumped together in one segment.\n\nInstance Segmentation separately detects and segments each object instance. It’s therefore similar to semantic segmentation, but identifies the existence, location, shape, and count of objects. It is best for distinct and countable “things”. Notice the four separately identified bears in the image below:\n\nPanoptic Segmentation) combines semantic segmentation + instance segmentation by labeling all pixels, including differentiation of discrete and separate objects within categories. Notice the complete segmentation in the image below, including both the various background types as well as the four distinct bears.\n\nFor more on Panoptic Segmentation, check out the research publication.\n\n\n\n\n\n\n\n\n\nTemporal Annotation\n\n\n\n\n\nTemporal annotation is the task of labeling satellite images over time to track changes in environmental features.\n\n\n\n\n\n4.2.3 Image Annotation Methods\nThe annotation method largely boils down to whether annotations are done manually versus with some level of supporting automation. Ultimately, the choice involves project-specific determination of the cost, speed, and quality of human annotation relative to what can be achieved with available AI assistance.\n\n\n\n\n\n\nManual Annotation\n\n\n\n\n\nWith purely manual annotation, all labeling is done by human annotators.\nNote that good tooling may help make this process easier and more efficient, but ultimately it is up to the human annotator to fully apply annotations to unlabeled inputs.\n\n\n\n\n\n\n\n\n\nSemi-Automated Annotation\n\n\n\n\n\nWith semi-automated annotation, machines assist humans in generating annotations, but humans are still heavily involved in real time with labeling decisions, ranging from actually applying the annotations to refining AI-generated annotations.\nThis can take a few different forms. For example:\n\nModel-based filtering: A model is trained to recognize images with any candidate objects (as compared to empty scenes), and is used to reduce the number of images passed to the human annotator.\nModel-assisted labeling: A pre-trained model generates a candidate annotation, which the human can accept, reject, or modify in some way (e.g., size, position, category).\nActive Learning: A model is learning how to annotate the images alongside the human, and actively decides which images the human should label to accelerate model training the fastest.\n\n\n\n\n\n\n\n\n\n\nAutomated Annotation with Human Validation\n\n\n\n\n\nAt the level of automated annotation with human validation, AI models generate most annotations autonomously. Humans only review the results after the fact, typically checking accuracy metrics at a high level and perhaps inspecting a random sample of annotations, rather than reviewing every annotation.\nExample: A pre-trained model processes satellite images and automatically labels roads, rivers, and forests across thousands of images. A human reviewer then inspects a small percentage of these results to confirm the annotations are accurate, fixing any errors and perhaps fine-tuning the model before the dataset is finalized.\nAt first glance, it might seem illogical that this scenario could exist! If you already have a model that can do the annotation, then don’t you already have a model to do the actual task you want to do?\nIn practice, however, there are some cases where this might be applicable:\n\nOne scenario involves model distillation. Imagine there exists a big, expensive, and/or proprietary (i.e., hidden behind an API) model that does the task you want, and perhaps a lot more. You can use this model to annotate a dataset that you use to train a more compact or economical model that you own and control. In the end, you have effectively distilled the source model’s capability into your own model through the annotated training dataset.\nA second scenario is when you do indeed already have a trained model to perform annotations, whether your own or someone else’s, and are now using it to automatically annotate vast amounts of data that will serve as inputs to some other machine learning or analysis pipeline. Indeed, in research settings, this is usually the end objective! When you reach this point in the process, you will effectively be doing automated annotation with human validation to ensure that the results are reasonable in aggregate.\n\n\n\n\n\n\n\n\n\n\nFully Automated Annotation\n\n\n\n\n\nRare in practice! Under fully automated annotation, trained models generate annotations with no human involvement, and the quality is deemed sufficient without review.\nThis is typically only relevant in very specific settings, namely in environments where the image data is very highly controlled. For example, consider images that were produced in a lab setting where the composition of the images is highly controlled, or images that were generated synthetically by some known computational agent (e.g., in video games). A related approach with synthetic data involves using trained AI models to generate both the images and their corresponding annotations, in which case the annotation ground truth for each image.\n\n\n\n\n\n4.2.4 Data Annotation Workflow\n\n\n\n\n\n\n1 - Data collection\n\n\n\n\n\nFirst step: Get a sufficiently large and diverse set of data to annotate and subsequently train on.\nYou may already have a set of images from your own research, e.g., from a set of camera traps or aerial flights. Or perhaps you already have a clear use case around detecting features in a particular satellite dataset, and have already procured the imagery. If so, great.\nIf you don’t have your own imagery – and maybe even if you do – you may want to consider augmenting it with additional images if you don’t have enough diversity or content in your own imagery. Depending on your use cases, you may want to poke around public mage datasets like ImageNet.\n\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n2 - Tool Selection\n\n\n\n\n\nTime to choose your annotation tool/platform!\nThere are many options, and lots of factors to consider. See the next section for plenty more details.\n\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n3 - Data preprocessing\n\n\n\n\n\nBefore proceeding, it’s almost always useful (some sometimes essential) to apply various preprocessing tasks to your data to make it easeir to annotatate and/or eventually train on.\n\n\n\nSource: Medium\n\n\nHere are some categories of common preprocessing tasks:\nReformatting. If relevant, you may need to convert your source images into a better file format for your task. Beyond this, it may be useful to rotate, crop, rescale, and/or reproject your images to get them into a consistent structural format.\nBasic data cleaning. - For example, with satellite or aerial imagery, you may find it useful to apply pre-processing steps such as filtering to remove noise, correcting for atmospheric conditions, correcting other distortion, adjusting brightness/contrast/color.\nFeature enhancement. Other context-specific transformations may be useful for “bringing out” information for the model (and human annotators) to use, leading to faster and/or better model outcomes. For an example, listen to this story about how careful transformations of Sentinel 2 imagery provided a huge boost in the detection of field boundaries as part of the UKFields project.\n\n\n\n\n\n\n\n\n\n4 - Guideline Development\n\n\n\n\n\nAs we discussed earlier, before you begin in earnest, it’s critical that you develop specific guidelines for annotators to follow when doing the annotation using the selected tool.\nNote: These should be written down! Some annotation platforms provide a way to document instructions within the tool, but if yours doesn’t (and probably even if it does), you should create and maintain your own written documentation\n\n\n\nSource: Acquiro\n\n\nOften this will be based on a combination of prior knowledge and task familiarity. To the extent that nobody on the project has extensive experience with the task at hand, it’s often helpful to do some prototyping to inform development of the guidelines.\n\n\n\n\n\n\n\n\n\n5 - Annotation\n\n\n\n\n\nIt’s time to annotate!\n\n\n\nSource: shaip\n\n\nKeep in mind the following image annotation best practices. They may not always hold, but in general:\n\nKeeping bounding boxes and polygons “tight” to the object:\nFor occluded objects, annotate as if the entire object were in view\nIn general, label partial objects cut off at the edge\nLabel all relevant objects in the image. Otherwise, “negative” labels will hamper model learning.\n\nAbove all else, remember, consistency is critical!\n\n\n\n\n\n\n\n\n\n6 - Quality Assurance\n\n\n\n\n\nReview the annotations for quality, and if needed, refine by returning to an earlier step in the workflow.\n\nNote that although QA is identified here as a discrete stage in the workflow, in practice quality is achieved through deliberate attention at multiple stages in the process, including:\n\nInitial annotator workforce training before any annotation is done\nContinuous monitoring during the annotation process\nFinal post-annotation review\n\n\n\n\n\n\n\n\n\n\n7 - Data Export\n\n\n\n\n\nFinalize and output the annotated data for model training.\n\nTypically, you will need to get the data into some particular format before proceeding with model training. If your annotation tool can export in this format, you’re all set. If not, you’ll need to export in some other format and then use a conversion tool that you either find or create yourself.\n\n\n\nFrom here, presumably, you’ll move on to model training!\nRemember this key best practice: Iterate! You will almost certainly not proceed through the annotation workflow in one straight shot. Plan to do some annotations, train, test, fix annotations, figure out whether/how to do more and/or better annotations, refine your annotation approaches, etc.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#annotation-tools-platforms",
    "href": "sections/data-annotation.html#annotation-tools-platforms",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "4.3 Annotation Tools & Platforms",
    "text": "4.3 Annotation Tools & Platforms\n\n\n\n\n\n\nHighlights\n\n\n\n\nGet a sense of what kind of tools are out there today!\nDiscuss high level considerations for choosing a tool\nReview some specific tools out there today\nHighlight how fast things are changing!\n\n\n\n\n4.3.1 High level considerations\nHere are some questions you should be asking…\n\n\n\n\n\n\nWhat annotation types are supported?\n\n\n\n\n\nDoes the tooling allow you to create the kinds of annotations necessary for your task? This is probably the first and most fundamental question you should be asking!\n\n\n\n\n\n\n\n\n\nWhat import image formats are supported?\n\n\n\n\n\nCan the tool read images in the right format?\nFortunately, most tools can automatically take a wide range of standard image formats including JPG, PNG, BMP, and TIF, and more.\nHowever, if you are working with spatial imagery, including GeoTIFFs, most tools will not natively read in your data. You will need to convert between formats, or choose a tool that is explicitly designed to handle that kind of data.\n\n\n\n\n\n\n\n\n\nWhat output annotation formats are supported?\n\n\n\n\n\nWhile image formats are reasonably standardized, image annotation formats are more diverse. In general, the format you need will be dictated by the constraints of whatever modeling tasks and tooling you will be using to train and validate a model with your annotated data.\nSome annotation software, especially the major players and cloud-based offerings, support a diverse set of output formats, whereas others output only a limited number of formats – or even just their own idiosyncratic format! In that case, you may need to do a conversion to get your annotations in the right format. Fortunately, there’s a good chance that somebody else has already been down this path, and if you search around, you may find a script or package that can do it for you.\nExample formats (not exhaustive!):\n\nVarious JSON formats\n\nCOCO JSON\nVGG Image Annotator JSON\nLabelMe JSON\n\nYOLO TXT\nPascal VOC XML\nTensorFlow TFRecord\n… and lots more …\n\nSee this great page for exploring many different formats.\n\n\n\n\n\n\n\n\n\nWho will be doing the annotation?\n\n\n\n\n\n\nIn-house: You and your team.\nCrowdsource: The broader community.\nOutsource: External people with whom you contract, either directly or through a 3rd party annotation services company. Yes, these do exist!\n\n\n\n\n\n\n\n\n\n\nHow can I assess annotation quality?\n\n\n\n\n\nWe’ve discussed the importance of having high quality annotations, and briefly covered various types of quality assessment. Some tools leave it entirely up to you to handle this, but others have features that help in this area. This can include:\n\nAutomatic calculation of various quality metrics\nConfigurable mechanisms for distributing images among annotators, and choosing how many annotators will see each image\nVarious other forms of annotation process metadata and analytics\n\n\n\n\n\n\n\n\n\n\nIs the tool easy to use?\n\n\n\n\n\nAs with any category of software, some options will be easier to use than others. For image annotation, where you are likely going to want to scale up to a large number of images, small speed-ups in the annotation process will really start to add up over time.\nConsider:\n\nIs the software easy to navigate in general?\nDoes the annotation interface have responsive, reliable, and easy-to-use UI elements for creating, modifying, and deleting image annotations?\nAre there effective keyboard shortcuts to help speed up manual annotations?\nDoes the tool offer effective model-assisted or other “smart” annotation capabilities?\nAre there well-designed features for managing your images, annotations, and overall workflow?\nIs there any useful API support to enable programmatic upload, download, or other automation?\n\n\n\n\n\n\n\n\n\n\nHow much am I willing to pay for tooling?\n\n\n\n\n\nIn short, some software options are free, whereas others are commercial offerings with varying costs and pricing tiers. As you compare features, consider what you’re willing (and able) to pay for.\n\n\n\n\n\n\n\n\n\nHow is the software licensed?\n\n\n\n\n\nSome annotation software apps and libraries are open source, whereas others are proprietary. You may want to lean toward the open source options if you want to be able to review the source code and understand how it works, and/or (perhaps more importantly) have the option of modifying it to better meet your needs. Of course, generally speaking, the open source options will typically also be free, whereas proprietary software is more likely to come with costs.\n\n\n\n\n\n\n\n\n\nWhere does the software run?\n\n\n\n\n\nDo you care if the software runs on your local computer? Do you want it to be something that you deploy and run on your own managed server, either locally or on a VM hosted in a public cloud? Or would you prefer to use a pure cloud-based annotation platform (i.e., a SaaS offering) that somebody else maintains and you access via a browser and/or API?\nAs with any software decision, there are pros and cons to each option.\nBear in mind that with image annotation, any cloud-based offering raises security and privacy considerations, as your images and annotations will reside on somebody else’s server. Consider whether this is a concern for you.\n\n\n\n\n\n\n\n\n\nWhat collaboration features are there?\n\n\n\n\n\n\nWhat collaborative features are offered?\n\n\n\n\n\n\n4.3.2 Tools & services galore\nNote that for geospatial image data annotation in particular, historically there’s been a divide between these two approaches:\n\nMature GIS platforms (QGIS, ArcGIS, etc) -\n\nFirst-class geospatial data and imagery support\nNative capabilities for drawing and editing features like points, lines, and polygons\nBut all of the menus and heavyweight UI around robust spatial feature management can impede fast & efficient annotation\nLimited or no support for the broader annotation workflow and lifecycle\n\nImage annotation software and platforms (LabelBox, RoboFlow)\n\nReally nice and constantly improving\nMostly generic with respect to supporting annotation for Computer Vision tasks, not full-featured around environmental research applications, especially with respect to Remote Sensing imagery with spatial component, multispectral bands, etc\n\n\nIn between, you’ll find a few dedicated software packages for environmental and/or spatial image annotation. However, because this is a small niche, you’ll find that they’re often rough around the edges, and likely have a very focused (i.e., limited) set of features addressing only the specific use cases of relevance to the development team. On the plus side, usually they are developed as open source projects, so if you’re up for the investment, you may want to consider contributing or extending these tools to meet your needs.\n\n4.3.2.1 Open-Source Tools for Image Annotation\n\nLabelImg\n\nHigh level: An open-source tool for creating bounding boxes.\nUsed for object detection mainly, maybe??\nOnly supports bounding boxes for annotation\n“Graphical image annotation tool and label object bounding boxes in images”\nIt is written in Python and uses Qt for its graphical interface.\nAnnotations are saved as XML files in PASCAL VOC format, the format used by ImageNet. Besides, it also supports YOLO and CreateML formats\nSee this third-party video tutorial\n\nVGG Image Annotator (VIA)\n\nHigh level: A flexible (but manual) tool for image, video, and audio annotation.\nA serverless web application, runs locally and self-contained in a browser, with no network connection required\nReleased in 2016, still maintained, based out of Oxford\nSee demo\n\nLabelme\n\nLocally installed application written in Python/QT and used for polygonal annotation of images\nSee GitHub repo\nDraws inspiration from an older Javscript-based LabelMe web application for image annotation\n\nIRIS (Intelligently Reinforced Image Segmentation)\n\nProvides semi-automated annotation for image segmentation, geared toward multi-band satellite imagery\n\n\n\n\n4.3.2.2 GIS platforms with annotation plugins\n\nQGIS\nArcGIS\n\n\n\n4.3.2.3 Hybrid solutions with both desktop and hosted options\n\nCVAT (Computer Vision Annotation Tool):\n\nOpen-source tool for video and image annotation, widely used in computer vision projects.\nUses pre-trained models to assist annotation?\nSee GitHub repository\nAlso has cloud-based offering and offers annotation services\nSupports:\n\nlabeling images\ndrawing bounding boxes\nmodel assisted labeling using models like YOLO \nmanual semantic segmentation \nautomatic semantic segmentation with SAM \n\n\n🔥 Label Studio\n\nMulti-type data labeling and annotation tool with standardized output format\nWorks on various data types (text, image, audio)\nHas both open source option and paid cloud service\nSee online playground\n\nMicrosoft’s Spatial imagely labeling toolkit\nimglab\n\n\n\n4.3.2.4 Commercial apps\n\nRectLabel\n\nOffline image annotation tool for object detection and segmentation\nHas regular and Pro version\nBuilt for Mac\nSee support page\n\n\n\n\n4.3.2.5 Commercial services\n\nLabelbox\n\nCloud-based commercial platform, albeit with possible free options for academic researchers\n\nRoboflow annotate\n\nOnline platform, with limited free tier\nFree tier does not offer any privacy\n\nSuperAnnotate\n\nHigh level: Full-featured collaborative annotation and modeling platform\nCommercial offering with free tier\n\nMakeSense.ai\n\nIncludes AI models!\nGitHub0\n\nSupervise.ly (commercial with free version)\nLabelerr (commercial with free researcher tier)\nRMSI annotation tools & services\nKili annotation platform (see geoannotation docs)\nSegments.ai labeling platform\nSama\nScaleAI\nDiffgram (see tech docs and GiHub) – commercial but locally installed? Hard to tell!\nDarkLabel\nGroundwork professional labeling services\n\n\n\n4.3.2.6 Fully managed AI & annotation services\n\nAlegion\nManthano\n\n\n\n4.3.2.7 Other platforms and related resources\n\nZooniverse\n\nCrowd-sourcing annotation platform\nSample projects:\n\nArctic Bears\nPenguin Watch\n\n\nDS-Annotate\n\nProvides SAM-assisted and “magic wand” interactivity to accelerate the creation of complex polygons for aerial imagery segmentation\nSee GitHub repo\n\nDeepForest\n\nFrom the Weecology lab\nPython package for training and predicting ecological objects in airborne imagery\nComes with a tree crown object detection model and a bird detection model\nSee GitHub repo\n\n\n\n\n\n4.3.3 Miscellaneous links\n\nSatellite image deep learning (Robin Cole’s site)\nOpen Source Data Annotation & Labeling Tools",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html",
    "href": "sections/hands-on-lab-data-annotation.html",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "",
    "text": "Goal\nThis hands-on lab session is designed to give participants practical experience in data annotation for deep learning. Participants will apply the methods, tools, and best practices discussed in the previous session, working directly with datasets to annotate data effectively.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#key-elements",
    "href": "sections/hands-on-lab-data-annotation.html#key-elements",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "Key Elements",
    "text": "Key Elements\nUse of annotation methods and tools, direct dataset interaction",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#choose-your-own-adventures",
    "href": "sections/hands-on-lab-data-annotation.html#choose-your-own-adventures",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "Choose your own adventure(s)",
    "text": "Choose your own adventure(s)\nIn this section, we’ll provide some links, basic information, and suggested starter activities for a variety of annotation tools available today. Have a look and get your hands dirty!\nNote: You’ll need some images to annotate in each case. Feel free to use any relevant images you might already have, or just do a web search and find something interesting. Of course, when experimenting with the web-based annotation platforms, be sure not to upload anything personal, private, or otherwise sensitive.\nIdeally, you’ll cover:\n\nSimple bounding box annotation\nPolygon, line, and point annotation\nInteractive model-assisted segmentation\nInspecting annotation output files in various formats, including COCO JSON\nOne or more cloud (web-based) tools\n(For the even more adventurous) One or more locally installed tools",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-make-sense",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-make-sense",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.1 Adventure: Make Sense",
    "text": "5.1 Adventure: Make Sense\n\n\n\n\n\n\nWeb-based app, no setup or account required\n\n\n\n\n\n\n.\n\n\n\nMakeSense.ai is a simple, single-user, browser-based image annotation app\nSupports annotation via bounding boxes, polygons, points, and lines\nUpload one or more images, apply/edit annotations, then export annotations\nOffers model-based semi-automated annotation with an accept/reject interface\nIf you prefer, you can also grab the source code and run it locally using npm or Docker\n\nThings to try\n\nUpload one or more images\nPlay around with manually creating various annotations of various classes. What is the experience?\nUse Actions to edit label names, colors, etc\nUse Actions to export annotations. What formats are offered?\nTry exporting polygon annotations in both VGG and COCO formats. How do they compare?\nUse Actions to run the COCO SSD model locally to suggest boxes. How well does it work?\nWhen you’re done: Evaluate this tool with respect to the software considerations in Section 4.3.1",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-roboflow",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-roboflow",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.2 Adventure: Roboflow",
    "text": "5.2 Adventure: Roboflow\n\n\n\n\n\n\nWeb-based app, requires (free) account signup\n\n\n\n\n\n\n.\n\n\n\nRoboflow offers a cloud-hosted, web-based platform for computer vision, including tooling for data annotation along with model training and deployment\nThey offer a limited free tier, which does not offer any privacy (project and images are automatically public)\nNice interface for doing annotations, managing artifacts, and managing the team\n\nThings to try\n\nCreate an account and test project\nUpload one or more images\nGo to the Annotate interface and experiment with different annotation types. How easy is it to create, edit, and delete annotations?\nUse the Smart Polygon tool to create polygons by clicking on an object, then refining by adding more clicks inside and outside the object. What is the experience like? Does this speed up your annotations?\nGo back to the main Annotate menu and note how it is organized to support a coherent, team-based annotation workflow. Check out their collaboration documentation. Imagine how you might use this for a multi-person project.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-cvat",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-cvat",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.3 Adventure: CVAT",
    "text": "5.3 Adventure: CVAT\n\n\n\n\n\n\nWeb-based app, requires (free) account signup\n\n\n\n\n\n\n.\n\n\n\nCVAT can be used as a desktop application that you install & run on your own local computer or server.\nHowever, for today, consider creating your own (free) account for annotating using their hosted platform\nThe V7 CVAT guide might be helpful.\n\nThings to try\n\nCreate a free account\nLog in and create a test Project. At this stage, you’ll need to define at least one relevant label under the Constructor tab (you can edit these later)\nCreate a Task (i.e., a collection of images to annotate) under your Project, and upload one or more images.\nStart an annotation Job within the task. What do you think of the interface? Is the documentation helpful?\nUsing the menu bar on the left, try creating box, polygon, line, and point annotations. Note: Click the Shape button to start each annotation. How is the experience?\nAlso, try creating a 3D cuboid annotation. Figure out how to resize and orient the cube. What do you think?\nLastly, try doing brush-based segmentations.\nAfter doing some annotations, go to Jobs, use the 3-dots selector on your job to open the action menu, and export annotations in a couple of different formats. How do they compare?\nAs another Jobs action, you can click on View analytics and run a performance report.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-zooniverse",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-zooniverse",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.4 Adventure: Zooniverse",
    "text": "5.4 Adventure: Zooniverse\n\n\n\n\n\n\nWeb-based app, no setup or account required\n\n\n\n\n\n\n.\n\n\nZooniverse is a web-based community crowdsourcing platform for data annotation and digitization.\nThings to try\n\nCheck out the Penguin Watch project.\n\nVisit the About, Talk, and Collect pages. Imagine how you might set up your own project to encourage and support a crowdsourced annotation community\nVisit the Classify page, go through the Tutorial, and then see how the Task works.\n\nCheck out the Arctic Bears image classification and interpretation project\nFeel free to search the site for other projects",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-iris-intelligently-reinforced-image-segmentation",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-iris-intelligently-reinforced-image-segmentation",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.5 Adventure: IRIS (Intelligently Reinforced Image Segmentation)",
    "text": "5.5 Adventure: IRIS (Intelligently Reinforced Image Segmentation)\n\n\n\n\n\n\nRequires local installation (Python + JavaScript application).\nSee Installation instructions for setting up IRIS locally with Python/pip.\n\n\n\n\n\n\n.\n\n\nIRIS is a tool for doing semi-automated image segmentation of satellite imagery (or images in general), with a goal of accelerating the creation of ML training datasets for Earth Observation. The user interface provides configurable simultaneous views of the same image for multispectral imagery, along with interactive AI-assisted segmentation.\nUnlike much of the ML we’ll encounter this week, the backend model in this case is a gradient boosted decision tree. The reason this works sufficiently well is that IRIS is geared toward segmenting multispectral imagery into a small number of classes, training from scratch on each image; the model is able to learn the correlation structure between features and labels by leveraging multiple features per pixel after the human-in-the-loop manually segments and labels pixels.\nFor more information, check out the YouTube video with the main creator, Alistair Francis.\nThings to try\n\nRun the IRIS demo that comes with the code\nUsing the onboard help widgets, figure out how to navigate the interface\nTry iteratively labeling some pixels and running the AI. How does it do?\nExperiment with changing the views. How does that improve your ability to manually distinguish and label features like clouds?",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-segment-geospatial-samgeo",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-segment-geospatial-samgeo",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.6 Adventure: Segment-Geospatial (samgeo)",
    "text": "5.6 Adventure: Segment-Geospatial (samgeo)\n\n\n\n\n\n\nRequires local installation (Python library), or can be run in a hosted notebook environment (JupyterLab, Google Collab, etc).\nSee Installation notes.\n\n\n\nThis is an open-source tool that you can either install locally or run in JupyterLab (or Google Colab).\nFirst, check out the online Segment Anything Model (SAM) demo. SAM was developed by Meta AI. It is trained as a generalized segmentation model that is able to segment (but not label) arbitrary objects in an image. It is designed as a promptable tool, which means a user can provide initial point(s) or box(es) that roughly localize an object within an image, and SAM will try to fully segment that object. Alternatively, it can automatically segment an entire image, effectively by self-promtping with a complete grid of points, and then intelligently merging the corresponding segments.\nToday, SAM is used by numerous image annotation tools to provide interactive, AI-assisted segmentation capabilities.\nOne such tool is the segment-geospatial Python package, which provides some base functionality for applying SAM to geospatial data, either programmatically or interactively.\n\n\nNote that in addition to using segment-geospatial directly using Python in a notebook or other environment, you can also play with SAM-assisted segmentation in QGIS and ArcGIS.\nThings to try\n\nRun one or more examples\nAfter running automated segmentation, what do you think about the results?\nWhen doing interactive segmentation, how does it do, and what do you think about the results?",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-label-studio",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-label-studio",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.7 Adventure: Label Studio",
    "text": "5.7 Adventure: Label Studio\n\n\n\n\n\n\nLabel Studio offers a Python application that you can install and run locally, or you can pay to use their cloud-based Enterprise offering.\nSee the Quick Start document with instructions for installing with pip and running locally in a web browser.\n\n\n\n\nMulti-type data labeling and annotation tool with standardized output format\nWorks on various data types (text, image, audio)\nHas both an open-source option and a paid cloud service\nSee the online playground",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#other-things-to-try",
    "href": "sections/hands-on-lab-data-annotation.html#other-things-to-try",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.8 Other things to try",
    "text": "5.8 Other things to try\n\nVGG Image Annotator (VIA)\n\nTry a local installation?",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html",
    "href": "sections/the-building-blocks-of-nn-and-dl.html",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "",
    "text": "Overview\nWelcome to your first step into the exciting world of deep learning! This lesson will guide you through the fundamentals in a simple and friendly way. Think of this as the start of your journey into a fascinating area of artificial intelligence.\nIn this session, we’ll explore:\nBy the end of this lesson, you’ll grasp these key concepts. These building blocks will help you understand how AI works and how you can apply these powerful tools.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#overview",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#overview",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "",
    "text": "Data and Models: Understand why data is crucial and how models act like ‘brains’ to interpret information.  \nLoss Functions and Optimization Algorithms: Discover how computers learn from their mistakes using these tools to improve over time.  \n\n\n\n\n\n\n\n\nGood luck learning!\n\n\n\n\n\n\n    \n        Balance Wheelie Viralhog GIF\n        from Balance Wheelie GIFs",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#introduction",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#introduction",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nThink of deep learning as a powerful tool, much like a car helps you reach a destination. Just as you don’t need to know every mechanical detail to drive, you can start with deep learning by understanding its essential components.  \n\n6.1.1 What does this tool (deep learning) do?\nAt its core, deep learning is about:\n\nFinding a function automatically that maps given inputs to desired outputs 1.\n\nIn simpler terms, deep learning enables computers to learn patterns from data to make predictions or decisions, without needing explicit step-by-step rules. It’s like teaching a child to recognize dogs by showing them many pictures, rather than giving a long list of rules defining a dog.\nHere are a few examples:\n\n\n\nInputs\nOutputs\nFunctions\n\n\n\n\nA sentence or prompt\nText completion\nLLM (e.g., ChatGPT 2)\n\n\nA caption/description\nAn image\nDALL-E 3\n\n\nHistorical weather data\nWeather forecasting\nGraphCast 4\n\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\n\nCan you think of a real-world problem you’d like to solve using deep learning?  \nWhat kind of data would you need (inputs)?  \nWhat results would you want the system to produce (outputs)?\n\n\n\n\n\n6.1.2 Key questions and building blocks of deep learning\nTo find the right function using deep learning, we break the process down into four key questions, which correspond to the fundamental building blocks:\n\nWhat are the inputs and outputs? This relates to the Data we use.  \nWhat kind of functions could we use? This involves choosing Models (the set of possible functions).  \nHow do we know if a function is good or bad? This is where Loss Functions come in to evaluate performance.  \nHow do we find the best function from our chosen set? This is achieved through Optimization Algorithms.\n\nThese four elements form the core of deep learning. Two related concepts, Training (the process of finding the best function) and Inference (using the trained function), tie these blocks together for practical use.  \n\n\n\nDeep learning building blocks\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember, these building blocks are interconnected. Choices made for one block often affect the others, involving trade-offs.  \n\n\nLet’s begin exploring these building blocks.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#data",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#data",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.2 Data",
    "text": "6.2 Data\nData is the starting point of deep learning. It defines the inputs and outputs of the function we aim to learn.\n\n\n\n\n\n\nNote\n\n\n\nFor specific examples related to Arctic research data, please refer to the AI-ready Data section.\n\n\n\n6.2.1 Inputs\nBuilding any deep learning application starts with defining and preparing your input data. Data is the foundation upon which models learn patterns.  \nBefore you start, consider:\n\nData Type: What kind of data is it (images, text, audio, numerical measurements)?  \nData Volume & Representativeness: How much data do you have? Does it accurately reflect the real-world scenarios your model will encounter?  \nTask Requirements: Are there specific needs like granularity (level of detail), temporal consistency (order matters), or spatial coverage?  \nData Quality: Is the data clean? Look out for missing values, outliers (unusual data points), or noise\n\n\n\n\n\n\n\nQuick thought\n\n\n\n\nHow well do you understand the origin and quality of your data?  \nEven well-known datasets like ImageNet can have labeling issues or biases. Always critically examine your data source 5.\n\n\n\nKey steps for preparing input data:\n\n1. Data Collection\n\nFind and gather data from reliable sources. Public datasets or domain-specific repositories can save time 6 7.\nAim for diverse data to cover various scenarios.\nCheck data quality early, looking for inconsistencies or noise.\n\n\n\n2. Data Preparation\n\nHandle missing or outlier data points appropriately (e.g., fill them in using statistical methods or with your domain knowledge about the data) 8 9.\nStandardize or normalize numerical data. This puts features on a similar scale, which prevents some features from dominating the learning process and often helps models train faster 10.\n\n\n\n3. Splitting Data\n\nDivide your data into distinct sets:\n\nTraining Set: Used to teach the model.\nValidation Set: Used to tune model settings (hyperparameters) and check performance during training.\nTesting Set: Used for a final, unbiased evaluation of the model’s performance after training.\n\nEnsure each set represents the overall data distribution. Avoid “data leakage” where information from the validation or test set accidentally influences the training process.  \nConsider the nature of your data when splitting 11:\n\nImbalanced Data: If some categories are rare, use stratified sampling to maintain the correct proportion of categories in each split 12.\nTemporal Data (Time Series): Split based on time (e.g., train on past data, test on future data). Random shuffling usually isn’t appropriate here.\nSpatial Data: Split based on geographic areas to avoid testing on areas very close to training data 13.\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nWhat could go wrong if information from the test set ‘leaks’ into the training process?\nHint: Over-optimistic performance estimate, model won’t generalize well.\n\n\n\n\n4. Data Augmentation (Optional but often helpful)\n\nArtificially increase the size and diversity of your training data by applying relevant transformations.  \nExamples: For images, you might rotate, flip, or change brightness. For text, you might replace words with synonyms 14.  \nUse transformations that make sense for your data type and problem. Don’t create unrealistic data.  \nBe mindful not to introduce transformations that fundamentally change the meaning or label of the data 15.  \n\n\n\n\n\n\n\nNote\n\n\n\nAn interesting paper on data augmentation showing simple copy-paste can be a strong augmentation technique even if the output is not realistic 16.\n\n\n\nSource: Ghiasi et al., 2021\n\n\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nThink about recognizing street signs in images. What’s one type of image augmentation that might be helpful? What’s one that might actually be harmful?\n\nHelpful: slight rotation, brightness change.\nHarmful: flipping horizontally might change meaning, extreme color shifts.\n\n\n\n\n\n\n6.2.2 Outputs\nDefining the desired outputs is as critical as preparing the inputs. The output format dictates what the model predicts and must align with your project goals.  \nConsider:\n\nOutput Type: What kind of prediction do you need?\n\nClassification: Assigning inputs to predefined categories (e.g., “dog” vs. “cat”, “spam” vs. “not spam”).  \nRegression: Predicting a continuous numerical value (e.g., sea ice concentration percentage, temperature).  \nStructured Output: Predicting complex outputs like bounding boxes around objects in an image, or generating a sequence of text.  \n\nOutput Format: How should the prediction be represented? (e.g., a probability distribution over classes, a single number, coordinates).  \nOutput Constraints: Are there specific requirements, like values needing to be within a certain range (e.g., probability between 0 and 1)?\n\nKey steps for preparing output data:\n\n1. Identify Output Type\n\nChoose the type that matches your problem: classification, regression, etc..  \n\n\n\n2. Format Outputs\n\nSelect a format compatible with your chosen model and loss function.\nOne-hot encoding 17 is common for classification tasks with multiple categories. (e.g., [0, 1, 0] for the second category out of three).\nNormalize continuous outputs (regression) if necessary, similar to input normalization.\nFor complex structured outputs, reparameterizing the output format can make the model’s job easier. In other words: “Don’t just predict what you want — predict something easier that leads to what you want.”\n\n\n\n\n\n\n\nAdvanced reading for output reparameterization\n\n\n\n\n\nIn deep learning, how you formulate the output can affect how well the model trains and performs. For example, if you predict the exact coordinates of a bounding box, what happens if we input an image that is much larger than what the model has seen during training?\nEven if two output formulations are mathematically equivalent, one might lead to:\n\nBetter gradient flow\nLower loss variance\nSmoother optimization landscape\nEasier convergence\n\nSome examples:\n\n\n\n\n\n\n\n\n\nTask\nNaive Approach\nSmarter Reparameterization\nWhy it’s better\n\n\n\n\nObject detection\nPredict absolute coordinates of bounding boxes\nPredict offsets from anchor boxes\nEasier to learn, more stable\n\n\nLanguage modeling\nPredict next word directly\nUse token embeddings\nCaptures context better\n\n\nDepth estimation\nPredict raw depth\nPredict inverse depth\nHandles wide range of depths better\n\n\nKeypoint detection\nPredict coordinate directly\nPredict heatmaps\nConverts regression to classification-like problem, easier for CNNs\n\n\n\nRead more about this topic in:\n\nFernández-Delgado et al. (2014) discusses how reformulating outputs using structured prediction can help deep models generalize better.\nGirshick et al. (2014) introduces bounding box regression via anchor offsets.\nHinton et al. (2015) introduces the idea that softening the target distribution helps model learn better.\n\n\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nHere’s an example of label encoding and one-hot encoding for a classification problem with three categories:\n\n\n\nFruit Name\nLabel Encoding\nOne-hot Encoding\n\n\n\n\nApple\n0\n[1, 0, 0]\n\n\nBanana\n1\n[0, 1, 0]\n\n\nOrange\n2\n[0, 0, 1]\n\n\n\n\nLabel encoding: Assigns a single number to each category, like 0, 1, 2.\nOne-hot encoding: Uses a binary vector to represent each category, like [1, 0, 0], [0, 1, 0], [0, 0, 1].\n\nIn the following two cases, which encoding might be more appropriate?\n\nFruit categories: Apple, Banana, Orange.\nSeverity of a storm: Light, Moderate, Severe.\n\nHint: Think about whether the categories have a natural order. For example, does Apple need to come before Banana in the encoding?\n\n\n\n\n3. Data Labeling\n\nAssign the correct “ground truth” output (label) to each input data point in your training and evaluation sets. This provides the reference the model learns from.  \nRefer to the Data Annotation section for detailed labeling techniques.\n\n\n\n\n\n\n\nNote\n\n\n\nBalance the desired detail of your outputs with model complexity, data availability, and computational resources. For instance, predicting exact sea ice percentage (0-100%) is harder than predicting broad categories like “low” (&lt;15%) vs. “high” (&gt;85%).\n\n\n\n\n\n6.2.3 Quantity and quality\n\nQuantity\n\nDoes more data always lead to better results? Not necessarily in isolation.\nInterestingly, research shows that just having more data isn’t the full story. How much data you need is related to the size of your model and the computing power you have. This plot illustrates that for a fixed amount of computing power (FLOPs), there’s an optimal combination of model size and data amount to get the best results. 18.\n\n\n\n\nHoﬀmann et al., (2022)\n\n\n\nFor various model sizes, we choose the number of training tokens such that the final FLOPs is a constant. The cosine cycle length is set to match the target FLOP count. We find a clear valley in loss, meaning that for a given FLOP budget there is an optimal model to train\n\n\n\nQuality\n\nData quality is often more critical than sheer quantity. High-quality, relevant data is essential for reliable models.\nCommon quality issues include:\n\nIncorrect or inconsistent labels.  \nNoise or irrelevant information within the data.  \nPoorly filtered datasets containing duplicates or near-duplicates. \n\nResearch emphasizes that improving dataset quality through careful filtering and cleaning significantly boosts model performance.  \n\nRae et al. (2021) 19.\n\n\nOur data pipeline (Section A.1.1) includes text quality filtering, removal of repetitious text, deduplication of similar documents, and removal of documents with significant test-set overlap. We find that successive stages of this pipeline improve language model downstream performance (Section A.3.2), emphasising the importance of dataset quality.\n\n\nHoffmann et al., (2022) 20.\n\n\nNonetheless, large language models face several challenges, including their overwhelming computational requirements (the cost of training and inference increase with model size) (Rae et al., 2021; Thoppilan et al., 2022) and the need for acquiring more high-quality training data. In fact, in this work we find that larger, high quality datasets will play a key role in any further scaling of language models.\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nIf your model performs poorly, what should you investigate first: trying a bigger model, or carefully check the quality and relevance of your training data?\nHint: Data quality issues are often a primary culprit.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#models",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#models",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.3 Models",
    "text": "6.3 Models\nModels in deep learning represent the set of possible functions that can map inputs to outputs. Think of a model architecture as defining a family of functions; training then selects the specific function (by finding the right parameters) from that family that best fits the data.  \n\n6.3.1 Layers: The Building Blocks of Models\nDeep learning models are constructed by stacking layers. Each layer performs a specific transformation on the data it receives before passing it to the next layer.  \n\n\n\n\n\n\nNote\n\n\n\nDon’t feel overwhelmed by the variety of layers initially. Focus on understanding their general purpose and how they fit into complete models. You can delve deeper into specific layer mechanics later.\n\n\nHere are some common layer types:\n\n1. Fully-Connected (Dense) Layer\n\n\n\nSource: BuiltIn\n\n\n\nWhat it does: Connects every input neuron to every output neuron. Each connection has a learnable weight. It calculates outputs by taking a weighted sum of all inputs 21.\nThink of it as: A system where every input feature can potentially influence every output value.\nUses: Often used in the final stages of classification models to make predictions based on learned features, or to adjust the dimensionality (size) of the data representation.  \nPros: Can learn complex combinations of features. Simple concept.  \nCons: Many parameters (computationally expensive, prone to overfitting). Doesn’t inherently understand spatial or sequential structures in data (e.g., pixel neighborhoods in images).  \n\n\n\n\n\n\n\nInteractive visualization of a fully-connected layer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nFully-connected layers are sometimes criticized for not understanding ‘spatial structure’ in images. Why is that?\nHint: Fully-connected layers treat a 2D image as a flat vector, ignoring pixel neighborhoods.\n\n\n\n\n2. Convolutional Layer (Conv Layer)\n\n\n\nSource: NVIDIA\n\n\n\nWhat it does: Applies learnable filters (kernels) across the input data (often images). Each filter detects specific local patterns (like edges, corners, textures) 22.\nHow it works: Small filters (also called kernels) slide across the input data. At each position, it computes a weighted sum of the input values in the filter’s receptive field. Stride controls the step size of the filter, and padding adds values (usually zeros) around the border to control output size. Output size = (Input size + 2 × Padding - Kernel size) / Stride + 1.\nThink of it as: Using a small magnifying glass that slides over an image, looking for specific visual features.\nUses: Feature extraction in image processing, video analysis, and sometimes other data types where local patterns are important.  \nPros: Parameter sharing (filters are reused, reducing parameters compared to fully-connected layers). Captures spatial hierarchies (early layers find simple features, later layers combine them). Translation invariance (can detect a pattern regardless of its position).  \nCons: Primarily focused on local patterns; may need other mechanisms to capture long-range dependencies. Choosing filter size, stride, and padding requires careful design.  \n\n\n\n\n\n\n\nInteractive visualization of a convolutional layer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nWhat is the main benefit of ‘parameter sharing’ in convolutional layers compared to fully-connected layers when processing images?\nHint: Fewer weights to learn, detects patterns anywhere in the image.\n\n\n\n\n3. Pooling Layer\n\n\n\n\n\n\n\n\n\n\n\nSource: Computersciencewiki\n\n\n\n\nWhat it does: Reduces the spatial dimensions (height/width) of the input, summarizing information within local regions.  \nHow it works: Divides the input into regions. For each region, it outputs a single summary value – typically the maximum (Max Pooling) or the average (Average Pooling) of the values in that region. Global Pooling summarizes across the entire feature map 23.  \nThink of it as: Downsizing an image while trying to keep the most salient information from each small block.\nUses: Reduces computational load and memory usage. Increases robustness to small spatial variations in the input. Helps focus on the most prominent features.  \nPros: Reduces dimensions significantly. Provides some invariance to minor translations. Computationally efficient.  \nCons: Discards information (potentially important details).  \n\n\n\n\n\n\n\nInteractive visualization of a pooling layer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nWhat information is potentially lost when using Max Pooling? Why might we still use it?\nHint: Loses details/exact locations within the region; Gains computational efficiency and some robustness to feature location.\n\n\n\n\n4. Activation Layer (Non-linearity)\n\n\n\nSource: Sefik Ilkin Serengil\n\n\n\nWhat it does: Introduces non-linearity into the model. Without it, stacking linear layers (like Conv or Fully-Connected) would just result in another linear function. Non-linearity allows models to learn complex patterns.  \nHow it works: Applies a fixed mathematical function element-wise to the output of the previous layer. Common activation functions include:\n\nReLU: \\(f(x) = \\max(0, x)\\)\nSigmoid: \\(f(x) = \\frac{1}{1 + e^{-x}}\\)\nTanh: \\(f(x) = \\frac{e^{2x} - 1}{e^{2x} + 1}\\)\n\nThink of it as: A gate or switch that modifies the signal passing through, allowing the network to make more complex “decisions”. ReLU, for example, simply outputs the input if it’s positive, and zero otherwise \\(\\max(0,x)\\).  \nUses: Applied after most convolutional and fully-connected layers to enable learning of complex mappings.  \nPros: Essential for learning non-trivial functions. Different activations have properties suited for different tasks or network parts (e.g., Sigmoid outputs values between 0 and 1, useful for probabilities) 24.\nCons: Poor choices can lead to training difficulties like the “vanishing gradient problem” where gradients become too small for effective learning 25.\n\n\n\n\n\n\n\nInteractive visualization of an activation layer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nIf a network only used linear activation functions (or no activations), what kind of relationship could it learn between inputs and outputs, no matter how many layers it had?\nHint: only linear relationships.\n\n\n\n\n5. Recurrent Layer (e.g., LSTM, GRU)\n\n\n\nSource: geeksforgeeks\n\n\n\nWhat it does: Processes sequential data (like text, time series, speech) by maintaining an internal “memory” or state that captures information from previous steps in the sequence.  \nHow it works: At each step in the sequence, the layer takes the current input and the hidden state from the previous step. It uses these to compute the output for the current step and update its hidden state for the next step. Variants like LSTM (Long Short-Term Memory) 26 and GRU (Gated Recurrent Unit) 27 use gating mechanisms to control what information is remembered, forgotten, or passed on, helping them learn long-range dependencies.  \nThink of it as: Reading a sentence word by word, keeping the context from earlier words in mind to understand the current word.\nUses: Natural Language Processing (NLP), speech recognition, time series forecasting.  \nPros: Explicitly designed to handle sequential dependencies. Can process sequences of varying lengths. LSTMs/GRUs mitigate the vanishing gradient problem common in simpler RNNs.  \nCons: Processing is inherently sequential, making parallelization harder and training potentially slower than feed-forward networks. Can still struggle with extremely long dependencies despite improvements 28.\n\n\n\n\n\n\n\nVisualization of a LSTM cell\n\n\n\n\n\n\n    \n\n\n    Source: Reddit r/TheInsaneApp\n\nThe key components of an LSTM cell are:\n\nThree gates (shown as X symbols in circles) from left to right:\n\nForget gate: This decides what information to throw away or keep from memory.\nInput gate: This decides what new information to add to memory.\nOutput gate: This decides what information to share with the next cell.\n\nInputs:\n\nCurrent input \\(x_t\\)\nPrevious hidden state \\(h_{t-1}\\)\nPrevious cell state \\(C_{t-1}\\)\n\nOutputs:\n\nCurrent hidden state \\(h_t\\)\nCurrent cell state \\(C_t\\)\n\nThe blue box represents the sigmoid function, which outputs a value between 0 and 1. It controls how much information passes through each gate, like a filter that can be partially open or closed.\nThe purple box represents the tanh function, which outputs a value between -1 and 1. It scales the input values.\n\nThe LSTM cell works as follows:\n\n\\(h_{t-1}\\) and \\(x_t\\) are combined together and passed through sigmoid functions as gate control signals.\nThe forget gate determines how much of the previous cell state \\(C_{t-1}\\) (the previous memory) is passed to the next cell. Think of this like deciding which old memories to keep or discard.\nThe input gate determines how much of the current input \\(x_t\\) is added to the cell state. This is like deciding which new information is worth remembering.\nThe output gate determines how much of the current cell state \\(C_t\\) is passed to the next hidden state \\(h_t\\). This is like deciding which parts of your memory to actively think about right now.\n\n\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nWhat is the main benefit of using an LSTM over a basic RNN?\nHint: Better handling of long-term dependencies / avoiding the vanishing gradient problem.\n\n\n\n\n6. Attention Layer\n\n\n\nConvolution and fully connected layers use fixed connection patterns—the same weights apply no matter the input. Attention layers, on the other hand, compute weights based on the input itself, so the connections can change depending on what the model sees. All these weights are learned and updated during training. Source: The AI Summer\n\n\n\nWhat it does: Allows the model to dynamically focus on the most relevant parts of the input sequence when producing an output at a particular step. Instead of relying solely on the final hidden state (like in basic RNNs), it can “look back” at different parts of the input.  \nHow it works: Calculates “attention scores” indicating the relevance of each input element (e.g., each word in a source sentence for translation) to the current output element (e.g., the word being translated). It then computes a weighted sum of the input representations based on these scores, effectively highlighting the important parts 29.\nThink of it as: When translating a sentence, focusing your attention on specific source words relevant to the target word you are about to write.\nUses: Machine translation, text summarization, question answering, image captioning. Core component of Transformer models.  \nPros: Significantly improves performance on tasks requiring alignment between input and output sequences. Can handle long-range dependencies effectively. Provides interpretability by showing where the model “attends”.  \nCons: Can be computationally intensive, especially self-attention (where elements within the same sequence attend to each other), which scales quadratically with sequence length.\n\n\n\n\n\n\n\nVisualization of image attention mechanism (conceptual flow)\n\n\n\n\n\nAttention mechanism is originally proposed for natural language processing tasks. The following visualization shows how it works for images. You can check how it works for text here: How LLMs work and Attention in transformers.\n\n\n\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nIn the context of translating a long sentence, how does attention help the model generate a more accurate translation compared to a model without attention, e.g., LSTM?\nHint: Allows focusing on relevant source words for each target word being generated. This is more accurate than using the last hidden state of the LSTM, especially for long sentences.\n\n\n\n\n\n6.3.2 Common Model Architectures\nLayers are assembled into architectures. Different architectures excel at different types of data and tasks because they embody different assumptions about the data (inductive biases). Often, models serve primarily as powerful feature extractors. Once meaningful features are extracted, they can be fed into simpler “head” layers for specific tasks (like classification or regression).  \nThis modularity allows flexibility:\n\nOne Model, Many Tasks: The same base feature extractor (e.g., a CNN) can be used for image classification, object detection, or segmentation by swapping out the final layers.  \nMany Models, One Task: Different architectures (CNN, Transformer) might be applied to the same task (e.g., image classification), each leveraging different strengths.\n\n\n\n\n\n\n\nNote\n\n\n\nSee Model customization for more details on how to customize models for different data and tasks.\n\n\nWhy different models? Data and tasks vary fundamentally:  \n\nCNNs: Great for grid-like data (images) due to spatial pattern recognition.  \nLSTMs/RNNs: Suited for sequential data, capturing temporal dependencies.  \nTransformers: Excel at modeling relationships between elements in a sequence using attention, regardless of distance.  \nGNNs: Designed for graph-structured data, modeling relationships between connected entities.\n\nChoosing the right architecture matches the structure of your data and task.  \nHere’s a look at some prominent model architectures:\n\n1. Convolutional Neural Networks (CNNs)\n\n\n\nA typical CNN architecture. Source: geeksforgeeks\n\n\n\nBest for: Grid-like data, especially images.\nCore idea: Uses convolutional layers to automatically learn hierarchies of spatial features (edges -&gt; textures -&gt; parts -&gt; objects).\nArchitecture: A typical CNN architecture combines several types of layers:\n\nConvolutional layers: Extract features like edges, textures, and shapes.\nActivation layers: Add non-linearity to learn complex patterns.\nPooling layers: Reduce data dimensions while keeping important information.\nBatch normalization layers 30: Stabilize and accelerate training.\nDropout layers 31: Prevent overfitting by randomly dropping some neurons.\nFully-connected layers: Combine extracted features for predictions.\n\n\n\nStrengths:\n\nParameter efficiency: Instead of learning a separate weight for each pixel (as in a fully-connected layer), CNNs use small filters that slide across the image, sharing weights.\nTranslation invariance: Since the same filter is applied everywhere, CNNs can recognize patterns regardless of their location in the image.\nHierarchical learning 32: CNNs build understanding from simple to complex. Early layers detect edges, corners, and textures. Middle layers combine these into shapes and textures, and deep layers assemble these into recognizable objects or concepts.\n\n\n\n\nHierarchical features in CNNs. Source: CMU 10-708 Probabilistic Graphical Models\n\n\n\n\nExamples:\n\nLeNet 33: Early successful CNN for handwritten digit recognition.\nAlexNet 34: Breakthrough performance on ImageNet (2012).\nVGG 35: Simple and deep architecture.\nResNet 36: Introduced residual connections to enable very deep networks.\nInception/GoogLeNet 37: Used parallel convolutions of different sizes to capture multi-scale features.\nEfficientNet 38: Scaled depth/width/resolution systematically.\n\n\n\nApplications:\n\nImage classification 39: Identify objects and scenes in images (used in search engines).\nMedical image analysis 40: Classify diseases in medical imaging, including radiology, MRI, and CT scans.\nFacial recognition 41: Create facial embeddings for recognition tasks.\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nWhat are the two main advantages of using convolutional layers over fully-connected layers for image classification?\nHint: Parameter sharing/efficiency and translation invariance.\n\n\n\n\n2. Long Short-Term Memory Networks (LSTMs) 42\n\n\n\nSource: Understanding LSTMs by Christopher Olah\n\n\n\nBest for: Sequential data (text, time series, speech).\nCore idea: A type of RNN specifically designed with internal “gates” (forget, input, output) to control information flow, allowing it to remember relevant information over long sequences and forget irrelevant details. Addresses the vanishing gradient problem of simple RNNs.  \nArchitecture: See Visualization of a LSTM cell for the introduction of a LSTM cell and its operation.\nStrengths:\n\nLong-term memory: Can remember information for extended sequences, solving the vanishing gradient problem plaguing simple RNNs 43.\nSelective memory management: Can learn which information to remember and which to forget.\nFlexible sequence handling: Can process inputs of variable length and maintain contextual understanding.\nGradient stability: Special architecture prevents gradients from vanishing or exploding during backpropagation 44.\n\n\n\nVariants:\n\nVanilla LSTM 45: The original architecture introduced by Hochreiter & Schmidhuber.\nGRU (Gated Recurrent Unit) 46: Simplified version with fewer parameters that merges cell and hidden states.\nBidirectional LSTM 47: Processes sequences in both forward and backward directions, i.e., using both past and future context.\nConvLSTM 48: Combines convolutional operations with LSTM for spatiotemporal data, e.g., video classification.\n\n\n\nApplications:\n\nNatural language processing 49: Machine translation, text generation, and sentiment analysis.\nTime series prediction 50: Stock market forecasting, weather prediction, and energy consumption.\nSpeech recognition 51: Converting spoken language to text with contextual understanding.\nMusic generation 52: Creating original musical compositions with temporal coherence.\nAnomaly detection 53: Identifying unusual patterns in sequential data like network traffic.\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nWhat is the high-level purpose of the ‘gates’ in an LSTM cell?\nHint: To control the flow of information - deciding what information to remember, forget, and output.\n\n\n\n\n3. Transformers\n\n\n\nDiagram of the Transformer architecture showing Encoder and Decoder stacks. Source: DataCamp: How Transformers work\n\n\n\nBest for: Sequential data (dominant in modern NLP: text translation, generation, understanding), but increasingly adapted for images (Vision Transformers) and other data types.\nCore idea: Revolutionized sequence modeling by relying almost entirely on self-attention mechanisms instead of recurrence (like LSTMs). This allows the model to weigh the importance of all other elements in the sequence when processing a given element, capturing context effectively regardless of distance. This design is highly parallelizable (faster training!) but requires positional encodings to retain sequence order information since it doesn’t process word-by-word.\nArchitecture: The foundational “Attention Is All You Need” paper 54 introduced an Encoder-Decoder structure:\n\nEncoder: Processes the input sequence (e.g., source language sentence) and builds rich, context-aware representations.\nDecoder: Generates the output sequence (e.g., target language sentence) based on the encoder’s representations and the output generated so far.\n\nBoth Encoder and Decoder are typically stacks of identical layers, each containing key sub-components:\n\nSelf-Attention Layers (often Multi-Head): The core mechanism! Allows each position to ‘attend’ to all other positions (within the encoder, or previous positions in the decoder) to understand context. Multi-head attention allows attending to different types of information simultaneously. (See the Attention Layer section for more details/visualizations).\nPosition-wise Feed-Forward Networks: Process information independently at each position in the sequence.\nResidual Connections & Layer Normalization 55: These are crucial techniques applied around the sub-layers to help gradients flow during training and stabilize the learning process, enabling deeper networks.\nPositional Encodings 56: Added to the input to give the model information about the position of each element in the sequence.\n\n\n\nStrengths:\n\nParallelization: Unlike sequential RNNs, attention calculations can be performed largely in parallel across the sequence, enabling much faster training on modern hardware (GPUs/TPUs).\nLong-Range Dependencies 57: Self-attention directly connects all positions, making it easier to capture relationships between distant elements in a sequence compared to RNNs/LSTMs.\nContext-Aware Representations: Each element’s final representation is influenced by its entire context, leading to deeper understanding.\nScalability 58: Performance generally improves predictably with more data and larger model sizes, following established “scaling laws.”\n\n\n\nPopular Transformer architectures: (Many models build on the original Transformer ideas)\n\nBERT 59: Encoder-only model, highly effective for language understanding tasks (classification, question answering) via pre-training.\nGPT (Generative Pre-trained Transformer) 60 61: Decoder-only model, powerful for text generation tasks, known for scaling to very large sizes.\nViT (Vision Transformer) 62: Adapted the Transformer for computer vision by treating image patches like sequence tokens.\nBART 63: Full Encoder-Decoder model often used for sequence-to-sequence tasks like summarization.\nSwin Transformer 64: A hierarchical Vision Transformer using shifted windows for efficiency.\n\n\n\nApplications: Virtually all modern NLP tasks (translation, summarization, Q&A), increasingly used in computer vision, biology, and other domains.\n\nNatural language processing 65: Machine translation, text summarization, question answering, and text generation, outperforming LSTM-based approaches.\nComputer vision 66: Image classification, object detection, segmentation, and image generation, increasingly surpassing CNN performance.\nSpeech recognition 67: End-to-end speech-to-text.\nMultimodal learning 68: Models combining text, images, and/or audio (e.g., image captioning, text-to-image generation).\nComputational biology 69: Protein structure prediction (AlphaFold2), genomic sequence analysis.\n\n\n\n\n\n\n\n\nNote\n\n\n\nTransformers have largely surpassed LSTMs for state-of-the-art performance in NLP and are becoming competitive alternatives to CNNs in vision, especially when large datasets are available. The original “Attention Is All You Need” paper 70 is a highly recommended read for understanding the foundation. Research on scaling laws 71 suggests performance will likely continue to improve with increased model size and data, though this requires significant computational resources.\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\n\nWhy are ‘Positional Encodings’ necessary in Transformers?\n\nHint: Self-attention itself doesn’t know the order/position of elements; these encodings add that information.\n\nWhat advantage does self-attention provide for understanding long sentences compared to the hidden state passed along in an LSTM?\n\nHint: Allows direct connections/comparisons between any two words, regardless of distance.\n\n\n\n\n4. Autoencoders\n\n\n\nAutoencoder structure: Input -&gt; Hidden Layer (Bottleneck) -&gt; Output (Reconstruction). Source: geeksforgeeks\n\n\n\nBest for: Unsupervised learning tasks like dimensionality reduction, feature learning, data denoising, and generative modeling. Unlike models needing explicit labels, autoencoders learn directly from the input data itself.\nCore idea: An unsupervised network trained to reconstruct its input. It consists of an Encoder that compresses the input into a lower-dimensional latent space (the bottleneck), and a Decoder that tries to reconstruct the original input from this compressed representation. The bottleneck forces the network to learn the most important, efficient features of the data.\nArchitecture: A typical autoencoder consists of three main components:\n\nEncoder: Compresses/encodes the input data into a compact representation.\nBottleneck/Latent space: The compressed, lower-dimensional representation of the input data.\nDecoder: Reconstructs/decodes the original input data from the bottleneck representation.\n\nVariants: (Building on the basic idea)\n\nVanilla autoencoders 72: The basic architecture described above.\nSparse autoencoders 73: Encourage sparsity in the bottleneck for potentially more robust features.\nDenoising autoencoders 74: Trained specifically to reconstruct clean inputs from intentionally corrupted versions, making them good for noise removal and learning robust features.\nVariational autoencoders (VAEs) 75: A generative variant that learns a probability distribution in the latent space, allowing you to generate new data samples similar to the training data.\nConvolutional autoencoders 76: Use convolutional layers in the encoder/decoder, suitable for image data.\nAdversarial autoencoders 77: Combine autoencoders with ideas from Generative Adversarial Networks (GANs).\n\n\n\nApplications:\n\nAnomaly detection 78: Identify unusual data points – they typically have high reconstruction error when passed through an AE trained on normal data.\nImage denoising and restoration 79: Clean up noisy or corrupted images (especially Denoising AEs).\nDimensionality reduction 80: Compress high-dimensional data to a lower dimension while preserving important structure (similar to PCA, but non-linear).\nFeature learning 81: The encoder part can be used to extract meaningful features for downstream supervised tasks.\nRecommender systems 82: Learn latent representations (embeddings) of users and items.\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nHow does the training goal of an autoencoder differ from the supervised classification models (like CNNs) we saw earlier?\nHint: Autoencoders are unsupervised, meaning they don’t need labeled data. They learn to compress data into a lower-dimensional representation, then reconstruct it from that compressed representation.\n\n\n\n\nChoosing the Right Architecture\nAs highlighted, different models can often tackle the same application (e.g., CNNs and Vision Transformers for image classification). The choice depends on factors like:\n\nYour Data: Is it grid-like (images), sequential (text/time), graph-structured, or something else?\nYour Task: Are you classifying, regressing, generating, clustering?\nAvailable Resources: How much data and computing power do you have? Some models are more data-hungry or computationally expensive than others.\nPerformance Needs: Do you need state-of-the-art accuracy, or is speed/efficiency more critical?\n\nUnderstanding the core strengths and inductive biases of each architecture helps in selecting the most appropriate starting point for your problem.\n\n\n\n\n\n\nQuick thought\n\n\n\nBased on the architectures discussed (CNNs, LSTMs, Transformers, Autoencoders):\n\nIf you wanted to classify sentiment (positive/negative) based on customer reviews (text data), which 1-2 architectures seem most appropriate to start with? Why?\nIf you wanted to predict tomorrow’s temperature based on historical weather data (time series), which architecture(s) might you consider?\nIf you wanted to detect fraudulent transactions in a network of users and payments, which architecture fits best?\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough there are many types of deep learning models, some research reveals interesting relationships between them, demonstrating that many models can be viewed as special cases of others. Here are some examples for you to explore:\n\nTransformers are Graph Neural Networks\nInterpret Vision Transformers as ConvNets with Dynamic Convolutions\nEverything is Connected: Graph Neural Networks\nHopfield Networks is All You Need\n\n\n\n\n\n\n6.3.3 Pre-trained models and transfer learning\nIn deep learning, creating models from scratch requires lots of time and computing power, e.g., collecting and labeling a large dataset, designing good network architectures, and training for days, weeks, or months on powerful GPUs. So, how can we, as learners or small teams leverage the power of deep learning without needing Google-level resources?\nPre-trained models and transfer learning offer a simpler approach. Instead of starting from zero, researchers can use models that have already been constructed and learned from massive datasets. This lets them build effective systems faster and with less data, like standing on the shoulders of giants rather than climbing the mountain alone.\n\nPre-trained models\nA pre-trained model is a neural network that has already been trained on a large benchmark dataset to solve a specific task (usually a general one). They have learned valuable features and can be used as-is for their original task or as a starting point for other tasks.\nExample:\n\nImage classification: Pre-trained models like ResNet are trained on the ImageNet dataset, which contains over 14 million images across 1000 categories 83 84.\nText classification: Pre-trained models like BERT are trained on large text corpora (like Wikipedia and BooksCorpus) 85.\n\n\n\nTransfer learning\nTransfer Learning is the technique of taking a pre-trained model (which learned on Task A / Dataset A) and adapting it for your own, usually different but related, Task B / Dataset B. The machine exploits the knowledge gained from the pre-training stage to improve its performance on the new task.\n\n\n\nTransfer learning strategies: Feature Extraction (Strategy 1) and Fine-tuning (Strategy 2). Source: data-science-blog\n\n\nCommon approaches:\n\nFeature Extraction: Use the pre-trained model (mostly frozen, i.e., weights not updated) as a fixed feature extractor. You typically replace or add only the final layer(s) (“head”) to suit your specific task and train only those new layers on your data. Good for smaller datasets or tasks very similar to the original. Faster training.  \nFine-tuning: Start with the pre-trained model’s weights, but allow some or all of them (usually the later layers) to be updated during training on your new dataset, typically using a low learning rate. This adapts the learned features more specifically to your task. Better for larger datasets where you have enough data to tune without overfitting. Can lead to better performance but requires more careful training.  \nPrompt Engineering: (Mainly for large language models) Guide the pre-trained model’s behavior by carefully designing the input text (prompt), often without changing the model’s weights at all.\n\n\n\nBenefits of using pre-trained models and transfer learning\n\nLess data needed: You can often get good results with much smaller datasets for your specific task because the model already has a head start.\nFaster development & training: Training takes less time because you’re training fewer parameters (feature extraction) or only tuning existing ones for a shorter period (fine-tuning).\nBetter performance: Often achieves higher accuracy than training a model from scratch, especially with limited data, because the pre-trained features provide a better starting point.\nAccessibility: Makes powerful deep learning accessible even without massive computational resources or huge datasets. Democratizes AI!\n\n\n\nGeneral guidelines for using pre-trained models and transfer learning\n\nThe pre-trained model’s task should be similar to your task (e.g., ImageNet models for classifying different types of flowers, medical images, or satellite images; BERT for sentiment analysis or question answering; etc.)\nYou have limited data for your task\nYou have limited computational resources\nBe cautious when:\n\nYour task is very different from the pre-trained model’s task (e.g., using an ImageNet model for audio analysis - the learned features might not be relevant)\nYou have a massive, high-quality dataset for your task (training from scratch might eventually yield better results, but transfer learning can still be a good starting point!)\nThe input data format is completely different\n\n\n\n\nHow to implement transfer learning? (Conceptual)\n\nChoose a pre-trained model: Select one appropriate for your data type (image, text) and task complexity (e.g., MobileNet for speed, ResNet for accuracy)\nLoad the model: Use libraries like PyTorch, TensorFlow/Keras, or Hugging Face Transformers. They make it easy!\nAdapt the model: Customize the model to your specific task (e.g., replace the final classification layer). See Model customization for more details.\nChoose strategy: Decide between feature extraction, fine-tuning, or a hybrid approach (freezing some layers and fine-tuning others).\nTrain the model: Train the adaptable parts of the model on your data.\n\n\n\n\n\n\n\nNote\n\n\n\nWe will cover practical implementation of transfer learning later.\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nWhen performing transfer learning, what is the key difference between the feature extraction approach and the fine-tuning approach?\nHint: Whether the weights of the original pre-trained layers are kept frozen or allowed to be updated.\n\n\n\n\n\n6.3.4 Model Customization (for Transfer Learning)\nWhen applying transfer learning, we rarely use the pre-trained model exactly as is. We need to customize it to fit our specific data and task. Conceptually, we can think of the model as having three main parts to consider for customization, as shown in the diagram: Input Adaptation, the Feature Extractor, and Output Adaptation. \n\n\n\n\n\n\nNote\n\n\n\nIn this section, you just need to get a sense of what different parts of the model are for. We will cover the customization in practice in the later sections.\n\n\n\nInput Adaptation (Data Preprocessing)\n\nWhat it is: This involves preparing your input data so that it’s compatible with the feature extractor. Pre-trained models have specific expectations about their input.\nWhy needed?: Your data might have a different size, format, or distribution than the data the model was originally trained on.\nCustomization/Actions: This is typically handled during data preprocessing before feeding data to the model:\n\nImages: Resizing to the expected input dimensions (e.g., 224x224 pixels), ensuring the correct number of color channels (e.g., converting grayscale to RGB if needed), normalizing pixel values using the same mean and standard deviation the original model was trained with.\nText: Using the exact same tokenizer the pre-trained model used, converting text to numerical IDs, padding or truncating sequences to a consistent length expected by the model.\nModel: You may need to adjust the model’s input layer to accept your data type (e.g., adding a new channel dimension for images, or using a different tokenizer for text).\n\n\nEnsuring input compatibility is crucial for the pre-trained features to be meaningful.\n\n\n\nInput adaptation example. (a) Original 6-band input architecture. (b) Adapting 3-band(RGB) data to 6-band input by zero-padding the extra bands. (c) Adapting 3-band(RGB) data to 6-band input by duplicating the RGB bands. (d) Adapting the input layer to accept 3-band(RGB) data. Source: Hsu et al., 2024\n\n\n\n\nFeature Extractor (The Pre-trained Base/Body)\n\nWhat it is: This is the main body of the pre-trained model, containing most of the layers (e.g., convolutional layers in ResNet, transformer layers in BERT).\nIts knowledge: It holds the rich, general-purpose features learned from the original large dataset (like visual patterns or language structures). This is the core knowledge we want to transfer.\nCustomization: The main decision here is whether to keep its weights fixed (frozen) or allow them to be updated (unfrozen) during training on your new task.\n\nFrozen (Feature Extraction strategy): The weights are not changed. The model acts purely as a fixed feature extractor. This is safer with small datasets.\nUnfrozen (Fine-tuning strategy): Some or all layers (usually the later ones) are allowed to update their weights slightly, adapting the learned features more closely to your specific task. Requires more data and careful training (low learning rate!).\n\n\n\n\nOutput Adaptation (The Task-Specific Head)\n\nWhat it is: This involves modifying the final part of the model so it produces the correct output format for your specific task.\nWhy needed?: The original model’s final layer(s) were designed for its task (e.g., classifying 1000 ImageNet categories). Your task (e.g., classifying 2 categories, predicting a single value) requires a different output structure.\nCustomization/Actions:\n\nRemove the old head: Discard the original model’s final classification layer(s).\nAdd a new head: Attach one or more new layers tailored to your task. This new head must be trained on your data.\nExample (Classification): Add a fully connected layer where the number of output units equals the number of your classes, often followed by a suitable activation function (like softmax or sigmoid).\nExample (Regression): Add a fully connected layer with a single output unit (or multiple if predicting multiple values).\nExample (Instance segmentation): Add Mask R-CNN 86 head for predicting masks, bounding boxes, and class labels.\n\n\n\n\n\nOutput adaptation example. The dashed box indicates the output adaptation part (task-specific head). They are heads for classification (left) and instance segmentation (right) respectively.\n\n\n\n\nConsiderations for model customization\n\nTask Similarity: How close is your task to the original pre-training task? Very similar tasks might only need output adaptation (feature extraction). Dissimilar tasks might benefit more from fine-tuning the feature extractor too.\nData Amount: With little data, stick closer to feature extraction (freeze the base) to prevent overfitting. With more data, fine-tuning becomes a viable option to potentially gain more performance.\nResources: Feature extraction is computationally cheaper than fine-tuning.\nExperimentation is key: Often, the best approach involves trying different levels of fine-tuning (e.g., unfreezing just the last block vs. multiple blocks) and seeing what works best for your specific problem.\n\n\n\n\n\n\n\nQuick thought\n\n\n\n\nIf you’re adapting a pre-trained image model for a new classification task with 5 classes, what part of the model absolutely must you change or replace?\n\nHint: The Output Adaptation / final classification layer/head.\n\nWhy is it generally important to use the same normalization (mean/standard deviation) for your input images as was used to train the original pre-trained model?\n\nHint: The learned features expect data in that specific numerical range/distribution.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#loss-functions",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#loss-functions",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.4 Loss Functions",
    "text": "6.4 Loss Functions\nOkay, we have our data (inputs and desired outputs) and a model (a set of possible functions). But how do we know if a specific function chosen by the model is actually any good? How do we measure its mistakes? That’s where the Loss Function comes in.\nA loss function (also called a cost function or objective function) evaluates how well our model’s predictions match the actual target values (the ground truth) from our data. It calculates a single number, the “loss,” which represents the “cost” or “error” of the model’s current performance.\nThink of it like this:\n\nImagine you’re learning to shoot arrows at a target. The loss function is like a scoring system.\nAn arrow hitting the bullseye gets a score of 0 (no error, perfect prediction).\nArrows landing further away get higher scores (higher error, worse prediction).\nYour goal is to adjust your aim (train the model) to get the lowest score possible (minimize the loss).\n\nThe core process:\n\n\n\nLoss function process\n\n\n\nThe model takes an input from your data.\nIt makes a prediction.\nThe loss function compares this prediction to the true target value (the label).\nIt outputs a loss score:\n\nLow Loss: Prediction is close to the target. Good!\nHigh Loss: Prediction is far from the target. Bad!\n\n\nThis loss score is crucial because it provides a signal for how the model needs to adjust itself during training. The overall goal of training is to find the model parameters (weights and biases) that minimize the average loss across the entire training dataset.\n\n6.4.1 Common loss functions\nThe choice of loss function depends heavily on the type of task your model is performing. The two most common types are Regression (predicting numbers) and Classification (predicting categories).\n\n1. Loss Functions for Regression Tasks (Predicting Numbers)\nHere, we’re predicting continuous values (like temperature, house prices, etc.). The loss measures the distance between the predicted and actual values.\n\nMean Squared Error (MSE):\n\nWhat it does: Calculates the average of the squared differences between predicttions (\\(y_{pred}\\)) and true values (\\(y_{true}\\)).\nFormula: \\(L_{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{pred,i} - y_{true,i})^2\\) (\\(n\\) is the number of samples).\nIntuition: Squaring the difference (\\(y_{pred} - y_{true}\\)) has a big impact. Small errors become tiny, but large errors become huge. This means MSE really hates large errors and pushes the model hard to avoid them. It’s quite sensitive to outliers (data points that are wildly different from the rest).\n\nMean Absolute Error (MAE):\n\nWhat it does: Calculates the average of the absolute differences between predicttions and true values.\nFormula: \\(L_{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{pred,i} - y_{true,i}|\\) (\\(n\\) is the number of samples).\nIntuition: Takes the absolute value \\(|y_{pred} - y_{true}|\\). It doesn’t dramatically penalize large errors like MSE does. The makes MAE less sensitive to outlier.\n\n\nVisualization of MSE vs. MAE\nHover over the plot to see the loss values for different error magnitudes.\nSome insights:\n\nMSE grows quadratically (\\(|error|^2\\)) with error magnitude while MAE grows linearly (\\(|error|\\)).\nMSE penalizes larger errors much more severely than MAE.\nMAE is more robust to outliers than MSE.\nFor small error magnitudes, MAE and MSE are almost identical.\nMSE has nicer mathematical properties (differentiable everywhere)\n\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nImagine you’re predicting house prices. Your dataset mostly has houses between $100k and $500k, but there are a couple of mega-mansions listed at $10 million (outliers!). If you want your model to find a good general trend without being overly skewed by those mansions, would MSE or MAE be a safer first choice for your loss function? Why?\n(Hint: Think about how each function treats those potentially huge errors).\n\n\n\n\n2. Loss Functions for Classification Tasks (Predicting Categories)\nHere, we’re assigning inputs to distinct categories (e.g., “dog” vs. “cat”, “spam” vs. “not spam”, “digit 0-9”). Loss functions here often work with probabilities. The model usually outputs a probability for each possible class, and the loss function checks how well these probabilities align with the true class.\nThe Go-To: Cross-Entropy Loss (Log Loss)\nCross-Entropy sounds fancy, but the core idea is simple: it measures how different the model’s predicted probability distribution is from the true probability distribution.\n\n\n\n\n\n\nDefinitions\n\n\n\n\nTrue Probability Distribution: For a given input, the correct class has a probability of 1 (or 100%), and all other classes have a probability of 0.\nModel’s Estimated Probability Distribution: The model outputs its estimated probabilities for each class (e.g., 70% cat, 25% dog, 5% bird).\nCross-Entropy: Calculates a loss based on how much probability the model assigned to the correct class. It heavily penalizes models that are confidently wrong (e.g., predicting 95% “dog” when it’s actually a “cat”).\n\n\n\n\nBinary Cross-Entropy (BCE Loss)\n\nUsed For: Binary classification (only two classes, like 0/1, True/False, Spam/Not Spam).\nWorks with: Models whose final layer outputs a single probability (usually using a Sigmoid activation function). This probability typically represents the likelihood of belonging to the “positive” class (class 1).\nFormula: (Don’t stress over memorizing this now, focus on when to use it!) \\(L_{BCE} = - \\frac{1}{n} \\sum_{i=1}^{n} [y_{true}^{(i)} \\log(p_{pred}^{(i)}) + (1 - y_{true}^{(i)}) \\log(1 - p_{pred}^{(i)})]\\) (where \\(p_{pred}\\) is the predicted probability for the positive class, and \\(y_{true}\\) is 0 or 1).\nIntuition: Compares the model’s predicted probability (e.g., 0.8 probability of being spam) to the true label (1 if it is spam, 0 if it’s not). The closer the prediction is to the true label, the lower the loss. The logarithm part is key to heavily penalizing confident wrong answers.\n\nCategorical Cross-Entropy (CCE Loss)\n\nUsed For: Multi-class classification (more than two classes, e.g., classifying images into “dog”, “cat”, “bird”, “fish”).\nWorks With: Models whose final layer outputs a probability distribution across all classes (usually using a Softmax activation function). These probabilities all sum up to 1.\nFormula Concept: (Again, focus on the application!) \\(L_{CCE} = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{C} y_{true}^{(i,j)} \\log(p_{pred}^{(i,j)})\\) (where C is the number of classes, \\(y_{true}^{(i,j)}\\) is 1 for the true class j and 0 otherwise, and \\(p_{pred}^{(i,j)}\\) is the predicted probability for class j).\nIntuition: Compares the model’s predicted probability list (e.g., [0.1 dog, 0.7 cat, 0.1 bird, 0.1 fish]) to the true list (which would be [0, 1, 0, 0] if it’s actually a cat, often called one-hot encoding). It gives a lower loss if the highest probability is assigned to the correct class.\nExample Walkthrough:\n\nTask: Classify an image into one of 3 classes (cat, dog, bird).\nTrue Label: It’s a ‘cat’. The target output (one-hot encoded) is [1, 0, 0].\nModel Prediction (Probabilities): The model outputs [0.7, 0.2, 0.1] (70% chance it’s a cat, 20% dog, 10% bird).\nCCE Loss Calculation: It primarily focuses on the probability assigned to the correct class. Using the formula, the loss involves \\(−log(0.7)\\), which is about 0.36.\n\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nUsing the Categorical Cross-Entropy example above (True label [1, 0, 0]):\n\nWhat if the model was more confident and correct, predicting [0.95, 0.03, 0.02]? Would the calculated CCE loss be higher or lower than the original loss (which was ~0.36)?\nWhat if the model was confident but wrong, predicting [0.1, 0.8, 0.1] (thinking it’s a dog)? Would the CCE loss be higher or lower than ~0.36?\n\n(Hint: Think about the −log(p) part. What happens to −log(p) when the probability p for the correct class gets closer to 1? What happens when it gets closer to 0?)\n\n\n\n\n\n\n\n\nNote\n\n\n\nKey Takeaway on Cross-Entropy: Don’t let the formulas intimidate you! Just remember: Cross-Entropy is the standard, effective way to measure error for classification tasks. It pushes the model to assign high probability to the correct answer and low probability to the wrong answers.\n\n\n\n\n\n6.4.2 Beyond the Basics: More Tools in the Toolbox\nWhile MSE, MAE, and Cross-Entropy are your workhorses, many other loss functions exist for specialized situations. You don’t need to know them all starting out, but it’s good to be aware they exist!\n\nFor Regression with Outliers:\n\nHuber Loss 87 88: A hybrid of MSE (for small errors) and MAE (for large errors). Tries to get the best of both worlds – less sensitive to outliers than MSE, but still smooth around the minimum.\nLog-Cosh Loss 89: Another smooth function that acts like MSE for small errors but is less steep for large ones, offering some robustness.\n\nFor Classification:\n\nHinge Loss 90 91: Often used with Support Vector Machines (SVMs). Cares about whether the prediction is correctly classified with a certain margin of confidence.\n\nFor Imbalanced Data (where some classes are much rarer than others):\n\nWeighted Cross-Entropy 92: Gives more importance (weight) to errors made on the rare classes.\nFocal Loss 93 94: Modifies cross-entropy to focus training more on hard-to-classify examples, often helping with both imbalance and distinguishing tricky classes.\n\nFor Specific Tasks:\n\nCTC Loss 95 96: Used in sequence tasks like speech recognition where the exact alignment between input audio and output text isn’t known.\nDice Loss 97 / IoU Loss 98: Common in computer vision for image segmentation and object detection, directly measuring the overlap between predicted regions and true regions.\n\nFor Multi-Task Learning: Sometimes a model needs to do several things at once (e.g., classify an object and draw a box around it). You might combine multiple loss functions.\n\n\n\n6.4.3 Choosing Your Loss Function: Practical Tips\n\nStart Standard: Use MSE or MAE for regression, and Binary/Categorical Cross-Entropy for classification. These cover most common scenarios.\nMatch Your Output: What does your model’s final layer produce?\n\nA single number? -&gt; Regression (MSE, MAE).\nA single probability (0 to 1)? -&gt; Binary Classification (BCE Loss).\nProbabilities for multiple classes (summing to 1)? -&gt; Multi-class Classification (CCE Loss).\n\nConsider Your Data/Problem: Do you have significant outliers? (Maybe lean MAE/Huber over MSE). Is your classification data heavily imbalanced? (Maybe look into Weighted CE or Focal Loss later).\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nYour goal is to predict house prices (a single dollar value). Which loss function would Tip #1 and #2 suggest you start with? Now, if you know your dataset includes a few multi-million dollar mansions (outliers) mixed in with mostly average homes, which tip becomes relevant, and what might you consider?”\n\n\n\n\n6.4.4 Watching the Score: Training Loss and Validation Loss\nAs your model trains, you need to track its performance not just on the data it’s learning from (Training Set) but also on data it hasn’t seen before (Validation Set).\n\nTraining Loss: Calculated on the data the model is currently training on. We expect this to go down steadily as the model learns.\nValidation Loss: Calculated on the separate validation dataset (the model doesn’t learn from this data, it’s just used for evaluation). This tells us how well the model is generalizing to new, unseen examples.\n\nPlotting both losses over time gives you crucial insights into how the training is going (click the image to see the full size):\n\n\nThe Dream Scenario (Good Fit): Both training loss and validation loss decrease and flatten out at a low value. Your model is learning useful patterns and generalizing well! 🎉\nThe Memorizer (Overfitting): Training loss keeps going down, but validation loss starts to creep back up (or stays high while training loss plummets). The model has learned the training data too well, including its noise and quirks. It fails to perform well on new data. Uh oh! 😟\n\nCommon Fixes: Get more diverse data (augmentation), simplify the model, add regularization (like Dropout or L1/L2), stop training earlier (early stopping).\n\nThe Slacker (Underfitting): Both training and validation loss remain high or decrease very slowly to a high value. The model isn’t complex enough, hasn’t trained long enough, or isn’t structured well enough to capture the underlying patterns in the data. 😴\n\nCommon Fixes: Train longer, use a more complex model (more layers/neurons), engineer better input features, try a different model architecture, reduce regularization if it’s too strong.\n\n\nMonitoring these curves is like checking your car’s dashboard while driving – it helps you understand what’s happening and make necessary adjustments!\n\n\n6.4.5 The Final Piece: Minimizing Loss is the Name of the Game\nNo matter which loss function you choose, the fundamental goal of the training process remains the same: adjust the model’s internal parameters (weights and biases) to make the calculated loss as low as possible.\nHow does the model actually do that adjustment? That’s the job of the Optimization Algorithm (like Gradient Descent), which uses the output of the loss function to figure out which way to tweak the parameters. We’ll cover optimizers next!",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#optimization-algorithms",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#optimization-algorithms",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.5 Optimization Algorithms",
    "text": "6.5 Optimization Algorithms\nIn the previous section, we’ve figured out how to measure our model’s mistakes using a Loss Function. We know a lower loss score means the model is doing better. But how does the model actually learn from these mistakes and improve? How does it adjust its internal knobs (parameters like weights and biases) to reach that coveted low-loss state?  \nThat’s where Optimization Algorithms step in! They are the engines that drive the learning process.\nAn optimization algorithm uses the information from the loss function to systematically update the model’s parameters in a way that minimizes the loss.\nThink of it like this:\n\n\n\nDescending the loss landscape. Source:Grokking Machine Learning\n\n\n\nImagine you’re on a foggy mountain (the loss landscape), and you want to get to the lowest point in the valley (minimum loss).\nYou can only feel the slope of the ground right under your feet (the gradient calculated from the loss function).\nThe optimization algorithm is your strategy for taking steps downhill based on that slope, hoping to eventually reach the bottom.\n\n\n6.5.1 The Core Idea: Gradient Descent\nThe most fundamental optimization algorithm is Gradient Descent. Its strategy is beautifully simple:\n\nCalculate the Slope: Figure out the slope (the gradient) of the loss landscape at your current position (current model parameters). The gradient tells you the direction of the steepest ascent (uphill).\nTake a Step Downhill: Take a small step in the exact opposite direction of the gradient (downhill). This step adjusts the model’s parameters.\nRepeat: Keep calculating the gradient and taking steps downhill until you can’t go any lower (you’ve hopefully reached a minimum).\n\nThe “size” of the step you take is controlled by a crucial parameter called the Learning Rate.\n\nLearning Rate (α): This small number determines how big of a step you take downhill.\n\nToo Large: You might overshoot the minimum, bouncing around wildly or even going uphill!\nToo Small: You’ll take tiny, tiny steps, and it might take forever to reach the bottom (or get stuck). Finding a good learning rate is often a key part of training a model effectively.\n\n\n\n\n\nDescending the loss landscape with different learning rates. Source: Jeremy Jordan\n\n\n\n\n\n\n\n\nNote\n\n\n\nA good article on how to choose a good learning rate: Jeremy Jordan: How to choose a good learning rate\n\n\n\n\n6.5.2 How Gradients are Calculated: Backpropagation (The Short Story)\nYou might wonder, “How does the algorithm calculate this ‘slope’ or gradient across potentially millions of parameters in a deep network?” The answer is a clever and fundamental process called Backpropagation.\nDon’t worry about the deep math right now! The key is to understand the concept of how the error signal flows backward through the network. Conceptually, backpropagation works like this:\n\nForward Pass: Input data goes forward through the network, layer by layer, using the current parameters (weights and biases) to eventually produce an output prediction.\nCalculate Loss: The loss function compares the model’s prediction to the true target value, calculating the overall error (the loss score).\nBackward Pass: This is where the magic happens! The error signal is propagated backward from the output layer through the network towards the input layer. Using the chain rule from calculus (essentially, a way to calculate derivatives of composite functions), the algorithm efficiently calculates how much each individual parameter (every weight and bias in the network) contributed to the final error. This calculated contribution for each parameter is its gradient.\n\nThis efficient process gives us the gradient for all parameters, telling the optimization algorithm exactly which way is “downhill” (i.e., how to adjust each specific knob) to reduce the loss on the next iteration.\n\n\n\n\n\n\nQuick thought\n\n\n\nWhat problem does backpropagation solve for us during training?\nHint: It efficiently calculates the gradient of the loss with respect to all model parameters.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re interested in a more visual or in-depth understanding of backpropagation without heavy math, resources like 3Blue1Brown’s videos or blog posts on sites like colah’s blog can be very helpful.\n\n\n\n\n6.5.3 Flavors of Gradient Descent: Handling the Data\nCalculating the gradient using every single data point in your training set for every single step can be very computationally expensive, especially with large datasets. This leads to different variations of Gradient Descent:\n\n\n\nGradient Descent Variants. BGD: smooth, direct. SGD: very noisy, zig-zagging. Mini-Batch: moderate, balanced. Source: Medium\n\n\n\nBatch Gradient Descent (BGD) 99:\n\nHow: Calculates the gradient using the entire training dataset for each parameter update.\nPros: Gives a very accurate estimate of the true gradient. Steps are stable and directly towards the minimum.\nCons: Extremely slow and memory-intensive for large datasets. Doesn’t allow for online learning (updating as new data arrives).\n\nStochastic Gradient Descent (SGD) 100 101 102:\n\nHow: Calculates the gradient and updates parameters using only one randomly chosen training example at a time.\nPros: Much faster updates. Requires less memory. Can escape shallow local minima more easily due to the noisy steps. Allows for online learning.\nCons: Updates are very noisy and bounce around a lot. The path towards the minimum is erratic, and it might never settle perfectly at the minimum. Often requires careful tuning of the learning rate.\n\nMini-Batch Gradient Descent 103:\n\nHow: Calculates the gradient and updates parameters using a small, random subset (a “mini-batch”) of the training data (e.g., 32, 64, 128 samples) at each step.\nPros: The Goldilocks solution! Balances the stability of BGD with the speed and efficiency of SGD. Takes advantage of hardware optimizations for matrix calculations. Reduces noise compared to SGD.\nCons: Introduces another hyperparameter (batch size) to tune. Still has some noise compared to BGD.\nStatus: This is the most common approach used in deep learning today. When people just say “SGD”, they often actually mean Mini-Batch SGD.\n\n\n\nBatch Size\nWhen using Mini-Batch Gradient Descent (explained next), this is the number of training samples used in one iteration (one parameter update).\n\nSmaller batch sizes: Lead to faster updates (more updates per epoch) but can be noisier (less stable gradient estimate). Can sometimes help escape local minima. Require less memory per step.\nLarger batch sizes: Provide smoother, more stable updates (more accurate gradient estimate) but require more computational resources (memory) per step and take longer per update. Can sometimes get stuck in sharper minima which might generalize less well.\n\n\n\n\n\n\n\nQuick thought\n\n\n\nImagine training on a dataset with 1,000,000 images.\n\nHow many gradient calculations does Batch GD do per update?\nHow many does Stochastic GD (SGD) do per update?\nIf you use Mini-Batch GD with a batch size of 100, how many gradient calculations (considering the batch average) lead to one parameter update? Which approach seems like the most practical compromise between accuracy and speed?\n\n\n\n\n\n\n6.5.4 Challenges on the Way Down\nThe journey to the minimum loss isn’t always smooth. Optimization algorithms face challenges navigating the complex loss landscape:\n\nLearning Rate Selection: As mentioned, choosing the right learning rate is critical and often requires experimentation. If it’s too large, the algorithm might overshoot the minimum and diverge; if it’s too small, training can be incredibly slow. Techniques like learning rate schedules (gradually decreasing the learning rate during training) are common strategies to help manage this. We will cover this in the next section.\n\n\n\n\nLocal minimum and saddle point. Source: Medium\n\n\n\nLocal Minima: The loss landscape might have many valleys. Simple gradient descent just follows the slope downwards from its starting point, so it might lead you to the bottom of a nearby, smaller valley (a local minimum) instead of finding the deepest valley overall (the global minimum). If the algorithm gets stuck in a local minimum, the model’s performance might be suboptimal.\nSaddle Points: Imagine the shape of a horse saddle – it curves up in one direction and down in another. A saddle point on the loss landscape is similar: the slope (gradient) might be zero or very close to zero, but it’s not actually a minimum. Gradient descent can slow down drastically or get stuck oscillating around these points because there isn’t a clear single direction downhill. (In practice, especially for high-dimensional deep learning problems, saddle points can often be more problematic than local minima).\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization of ravine and gradient descent path. Source: Digital Ocean\n\n\n\n\nRavines/Narrow Valleys: The landscape might feature long, narrow valleys where the slope is very steep across the valley but very gentle along the valley floor. Simple gradient descent can struggle here, oscillating back and forth across the steep walls instead of moving efficiently along the bottom of the valley towards the minimum. This oscillation can make convergence very slow.\n\n\n\n\n\n\n\nQuick thought\n\n\n\n\nWhat’s the potential problem with getting stuck in a ‘local minimum’?\n\nHint: The model’s performance might be okay, but not the best possible, because parameters aren’t optimal.\n\nWhat does a ‘saddle point’ look like on the loss landscape? Why is it problematic for gradient descent?\n\nHint: Flat in some directions, curved in others; gradient is zero or near-zero, so the optimizer doesn’t know where to go or moves very slowly.\n\nIf you see your loss decreasing very slowly, but oscillating significantly between iterations, what landscape feature might be causing this?\n\nHint: A narrow ravine or valley.\n\n\n\n\n6.5.5 Getting Smarter: Learning Rate Scheduling\nManually finding the perfect fixed learning rate can be tough. Often, it’s beneficial to adjust the learning rate during training. This is called Learning Rate Scheduling:\n\n\n\nLearning Rate Scheduling Comparison\n\n\n\nFixed Scheduling: The simplest approach - maintain a constant learning rate throughout training. Easy, but might not be optimal.\nStep Decay: Reduce the learning rate by a certain factor at predefined epochs (e.g., cut it in half every 10 epochs). Allows for larger steps early on and finer adjustments later.\nExponential Decay: Gradually decrease the learning rate exponentially over time. Provides a smoother decrease than step decay.\nCyclical Learning Rates (CLR): Vary the learning rate cyclically between a lower and upper bound. The idea is that periodically increasing the rate can help the model jump out of poor local minima or saddle points.\n\nUsing a schedule often helps the model converge faster and reach a better final solution compared to a fixed learning rate.\n\n\n\n\n\n\nQuick thought\n\n\n\n\nWhy is it often beneficial to decrease the learning rate gradually during training (like in Step or Exponential Decay)?”\n\nHint: Allows larger steps early on when far from the minimum, and smaller, more precise steps later when closer).\n\nWhat is the potential advantage of periodically increasing the learning rate, as seen in Cyclical Learning Rates?\n\nHint: May help the optimizer jump out of poor local minima or saddle points.\n\n\n\n\n6.5.6 Getting Even Smarter: Advanced Optimization Algorithms\nBecause of these challenges, researchers developed more sophisticated optimization algorithms that often work better and require less manual tuning than basic SGD. They adapt the update rule based on the history of the gradients.\n\n\n\n\n\n\nLoss contours for different optimizers.\n\n\n\n\n\n\n\nSaddle point for different optimizers.\n\n\n\n\n\n\nSource: Ruder’s Blog\n\n\n\n\nSGD with Momentum 104:\n\nIdea: Adds inertia to the updates. Imagine a ball rolling downhill – it accumulates momentum and doesn’t just stop instantly if the slope becomes flat or slightly uphill.\nHow: It considers the direction of previous updates. This helps accelerate movement along consistent directions (down the valley floor) and dampens oscillations across narrow ravines.\nBenefit: Often converges faster than basic SGD and navigates tricky landscapes more effectively.\n\nRMSprop (Root Mean Square Propagation) 105:\n\nIdea: Adapts the learning rate for each parameter individually. If a parameter’s gradient has been consistently large, it reduces its effective learning rate; if it’s been small, it increases it.\nHow: Keeps a moving average of the squared gradients for each parameter. Divides the learning rate by the square root of this average.\nBenefit: Helps handle situations where different parameters need different step sizes. Good for non-stationary objectives (where the “shape” of the loss landscape changes).\n\nAdam (Adaptive Moment Estimation) 106:\n\nIdea: The current rockstar! Combines the best of both worlds: Momentum + RMSprop.\nHow: Keeps track of both the momentum (like SGD with Momentum) and the per-parameter scaling of gradients (like RMSprop).\nBenefit: Often works very well across a wide range of problems with relatively little tuning (though the default learning rate might still need adjustment). It’s frequently the default, go-to optimizer for many deep learning tasks.\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere’s a great article on various optimization algorithms: An overview of gradient descent optimization algorithms\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile Adam is often a great starting point, sometimes simpler optimizers like SGD with Momentum can achieve slightly better final performance with careful tuning, especially in computer vision. Don’t be afraid to experiment!\n\n\n\n\n6.5.7 Choosing Your Optimizer: Practical Tips\n\nStart with Adam: For most problems, Adam is a robust and effective choice to begin with. Use common default settings (e.g., learning rate of 0.001).\nConsider SGD with Momentum: If Adam isn’t giving optimal results, or if you’re working in a domain where SGD+Momentum is known to perform well (like some vision tasks), give it a try. It might require more careful learning rate tuning and scheduling.\nExperiment: The best optimizer and its settings (like learning rate, momentum parameters) can depend on the specific model architecture, dataset, and task. Monitoring your training and validation loss curves is key to diagnosing issues and guiding your choices.\n\n\nNow we have all the core building blocks!\n\nData: The inputs and desired outputs.  \nModel: The family of functions that can map inputs to outputs.  \nLoss Function: Measures how bad the model’s predictions are.  \nOptimization Algorithm: Adjusts the model’s parameters to minimize the loss.  \n\nTogether, these elements allow a computer to learn from data through a process called Training. Once trained, the model can be used to make predictions on new, unseen data, which is called Inference.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#training-and-inference",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#training-and-inference",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.6 Training and Inference",
    "text": "6.6 Training and Inference\nWe’ve now assembled all the core building blocks needed for deep learning:  \n\nData: The fuel for learning (inputs and target outputs).\nModel: The potential functions we can learn.\nLoss Function: The scorekeeper measuring model errors.\nOptimization Algorithm: The engine that adjusts the model to reduce errors.\n\nNow, how do these pieces work together in practice? They come together in two main phases: Training and Inference.  \nThink of it like learning to cook:\n\nTraining is like practicing a recipe. You gather ingredients (Data), have a basic recipe structure (Model), taste your dish and see how far it is from the ideal flavour (Loss Function), and then adjust your cooking technique or ingredient amounts (Optimization Algorithm) based on the taste test. You repeat this process until your dish tastes great.\nInference is when you confidently cook the finalized recipe for your guests, serving them the delicious dish you perfected during practice.\n\n\n6.6.1 Training: Teaching the Model\nTraining is the phase where the model learns from the data. It’s an iterative process where we repeatedly show the model examples, measure its mistakes, and adjust its internal parameters (weights and biases) to get better. This iterative process happens in what’s called the Training Loop.\n\nThe Training Loop: A Cycle of Learning\n\n\n\nTraining loop\n\n\nImagine the training loop as one full cycle of studying a chapter in a textbook:\n\nForward Pass: The model takes a batch of input data (e.g., a few images or sentences) and passes it through its layers to make a prediction. (Like reading a section of the chapter).  \nLoss Calculation: The Loss Function compares the model’s predictions for that batch with the actual target values (ground truth). It calculates a loss score representing the error for that batch. (Like answering practice questions and seeing how many you got wrong).  \nBackward Pass (Backpropagation): This is the crucial learning step! The calculated loss is used to compute the gradients – determining how much each parameter in the model contributed to the error. It figures out the “direction of blame” for the mistakes. (Like reviewing your wrong answers to understand why they were wrong).  \nParameter Update: The Optimization Algorithm (like Adam or SGD) takes the gradients and the learning rate, and updates the model’s weights and biases. It nudges the parameters in the direction that should reduce the loss for the next time. (Like correcting your understanding based on the review).  \n\nThis entire cycle (Forward -&gt; Loss -&gt; Backward -&gt; Update) repeats over and over, using different batches of data each time.\n\n\nEpochs, Batches, and Iterations\nYou’ll often hear these terms:\n\nBatch (or Mini-Batch): A small subset of the total training data used in one iteration of the training loop. Using batches makes the training process computationally efficient and often more stable than using single examples. Common batch sizes are 32, 64, 128, etc..  \nIteration: One run through the training loop cycle using a single batch (one forward pass, one backward pass, one parameter update).\nEpoch: One complete pass through the entire training dataset. If your dataset has 1000 examples and your batch size is 100, one epoch consists of 10 iterations (1000 / 100 = 10).\n\nTraining usually involves running for multiple epochs, allowing the model to see the entire dataset several times and gradually refine its parameters.\n\n\nThe Goal of Training\nThe ultimate goal isn’t just to minimize the loss on the training data (which can lead to overfitting, as we saw with the loss curves ). The real goal is to minimize the training loss while ensuring the model performs well on new, unseen data. This ability to perform well on unseen data is called generalization, and we monitor it using the Validation Loss. We aim for that “Good Fit” scenario where both training and validation loss go down and stabilize.\n\n\n\n\n\n\nQuick thought\n\n\n\nWhy is ‘generalization’ considered the true goal of training, rather than just minimizing the loss on the training data?”\nHint: We want the model to perform well on new, unseen data it will encounter in the real world, not just memorize the data it was trained on.\n\n\n\n\n\n6.6.2 Inference: Using the Trained Model\nOnce the training process is complete (e.g., the validation loss stops improving), we have a trained model. Now we can use it for its intended purpose – making predictions on new data it has never seen before. This is the Inference phase.\n\nThe Inference Process\nInference is typically much simpler than training:\n\nInput New Data: Feed new data (e.g., an image you want to classify, a sentence you want to translate) into the trained model.\nForward Pass Only: The data flows through the network’s layers, using the learned parameters (weights and biases) to calculate an output.\nGet Prediction: The model outputs its prediction (e.g., the class label “cat”, the translated sentence, the predicted temperature).\n\nCrucially, during inference:\n\nThere’s no loss calculation (we don’t usually have the “true” answers for new real-world data).\nThere’s no backpropagation.\nThere are no parameter updates. The model’s learned weights are fixed or “frozen”.\n\nInference is just about using the knowledge the model gained during training.\n\n\nEvaluating Performance\nAfter training, and before deploying the model for real-world use, we perform a final evaluation using the Test Set – data that was completely held aside and never used during training or validation. This gives an unbiased estimate of how the model will likely perform in the real world.  \nCommon ways to measure performance (metrics) depend on the task:\n\nClassification: Accuracy (overall percentage correct), Precision (of the positive predictions, how many were actually positive?), Recall (of all the actual positives, how many did we find?), F1-Score (a balance of Precision and Recall) 107 108.\nRegression: Mean Squared Error (MSE) or Mean Absolute Error (MAE) on the test set predictions.\n\n\n\n\n\n\n\nQuick thought\n\n\n\nWhat are the three key steps/processes from the training loop that are absent during inference?\nHint: Loss Calculation, Backward Pass/Backpropagation, Parameter Update\n\n\n\n\n\n6.6.3 Training vs. Inference\nHere’s a quick summary:\n\n\n\n\n\n\n\n\nFeature\nTraining Phase\nInference Phase\n\n\n\n\nGoal\nLearn parameters from data\nMake predictions on new data\n\n\nInput Data\nTraining & Validation Sets\nNew, unseen data (or Test Set)\n\n\nProcess\nForward Pass + Loss + Backward Pass + Update\nForward Pass Only\n\n\nParameters\nContinuously Updated\nFixed / Frozen\n\n\nOutput\nUpdated Model Parameters\nPredictions\n\n\nComputation Cost\nHigh\nLower\n\n\n\n\nPutting it all together\nWith Training and Inference, we can now put all the building blocks together to build a complete deep learning life cycle.\n\n\n\nTraining and Inference\n\n\n\n\n\n\n\n\nQuick thought\n\n\n\nImagine you’ve trained a model to detect spam emails.\n\nWhen you run your massive dataset of labeled emails through the training loop for 10 epochs, is that Training or Inference?\nWhen your email service uses the finished model to decide if a new incoming email is spam or not, is that Training or Inference?\nWhich phase requires more computational power (GPU time, etc.)? Why?",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#conclusion-putting-the-blocks-together",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#conclusion-putting-the-blocks-together",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.7 Conclusion: Putting the Blocks Together",
    "text": "6.7 Conclusion: Putting the Blocks Together\nCongratulations! You’ve journeyed through the essential building blocks of neural networks and deep learning. We’ve seen how these fundamental pieces fit together to enable machines to learn from data.\n\n\n\n\n\n\nProud of you!\n\n\n\n\n\n\n    \n        Kitty Meow GIF\n        from Kitty GIFs\n    \n\n\n\n\n\nLet’s quickly recap what we’ve covered:\n\nData: The crucial starting point. We learned about preparing inputs and outputs, the importance of data quality over sheer quantity, and how data defines the problem we want the model to solve.\nModels: These are the ‘brains’ that learn patterns. We explored fundamental layers (like Dense, Convolutional, Recurrent, Attention) and how they assemble into powerful architectures (CNNs, LSTMs, Transformers, GNNs, Autoencoders), each suited for different types of data and tasks. We also saw the practical power of using pre-trained models and transfer learning to build effective systems efficiently.\nLoss Functions: The ‘scorekeeper’ that tells us how well the model is doing. We discussed how to choose appropriate functions based on the task (MSE/MAE for regression, Cross-Entropy for classification) and the importance of monitoring training and validation loss to diagnose issues like overfitting or underfitting.\nOptimization Algorithms: The ‘engine’ that drives learning. We explored the core idea of Gradient Descent, how Backpropagation calculates the necessary updates, and how advanced optimizers (like Adam) and techniques (like learning rate scheduling) help navigate the complex ‘loss landscape’ more effectively.\n\nThese four components work in concert during the Training phase to adjust the model’s parameters, minimizing the loss and maximizing performance. The ultimate goal is generalization – creating a model that performs well not just on the data it learned from, but on new, unseen data during the Inference phase.\nUnderstanding these building blocks provides you with a solid conceptual foundation. You can now better appreciate how different deep learning systems work under the hood and make informed decisions when approaching new problems.\nNext Steps:\nWe’ve focused on the core concepts so far. In the upcoming sections, we’ll transition from theory to practice. We’ll introduce PyTorch, a powerful deep learning framework, and dive into hands-on labs where you’ll apply these building blocks to build and train your own neural networks. Get ready to bring these ideas to life!",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#footnotes",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#footnotes",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "",
    "text": "https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/ML%20basic%20(v8).pdf↩︎\nhttps://openai.com/chatgpt/overview/↩︎\nhttps://openai.com/index/dall-e/↩︎\nhttps://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/↩︎\nBeyer, L., Hénaff, O. J., Kolesnikov, A., Zhai, X., & Oord, A. V. D. (2020). Are we done with imagenet?. arXiv preprint arXiv:2006.07159.↩︎\nhttps://arcticdata.io/↩︎\nhttps://arctic.noaa.gov/data/↩︎\nhttps://www.geeksforgeeks.org/ml-handling-missing-values/↩︎\nhttps://www.freecodecamp.org/news/how-to-detect-outliers-in-machine-learning/↩︎\nhttps://developers.google.com/machine-learning/crash-course/numerical-data/normalization↩︎\nhttps://datascience.stanford.edu/news/splitting-data-randomly-can-ruin-your-model↩︎\nhttps://en.wikipedia.org/wiki/Stratified_sampling↩︎\nLi, W., Hsu, C. Y., Wang, S., & Kedron, P. (2024). GeoAI Reproducibility and Replicability: a computational and spatial perspective. Annals of the American Association of Geographers, 114(9), 2085-2103.↩︎\nhttps://www.datacamp.com/tutorial/complete-guide-data-augmentation↩︎\nRatner, A. J., Ehrenberg, H., Hussain, Z., Dunnmon, J., & Ré, C. (2017). Learning to compose domain-specific transformations for data augmentation. Advances in neural information processing systems, 30.↩︎\nGhiasi, Golnaz, et al. “Simple copy-paste is a strong data augmentation method for instance segmentation.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.↩︎\nhttps://www.geeksforgeeks.org/ml-one-hot-encoding/↩︎\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., … & Sifre, L. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.↩︎\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., … & Irving, G. (2021). Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.↩︎\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., … & Sifre, L. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.↩︎\nhttps://adamharley.com/nn_vis/mlp/3d.html↩︎\nhttps://cs231n.github.io/convolutional-networks/↩︎\nLin, M., Chen, Q., & Yan, S. (2013). Network in network. arXiv preprint arXiv:1312.4400.↩︎\nhttps://www.v7labs.com/blog/neural-networks-activation-functions↩︎\nhttps://www.geeksforgeeks.org/tanh-vs-sigmoid-vs-relu/↩︎\nUnderstanding LSTM Networks↩︎\nCho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.↩︎\nhttps://www.superdatascience.com/blogs/recurrent-neural-networks-rnn-the-vanishing-gradient-problem↩︎\nhttps://www.youtube.com/watch?v=eMlx5fFNoYc↩︎\nIoffe, Sergey, and Christian Szegedy. “Batch normalization: Accelerating deep network training by reducing internal covariate shift.” International conference on machine learning. PMLR, 2015.↩︎\nSrivastava, Nitish, et al. “Dropout: a simple way to prevent neural networks from overfitting.” The journal of machine learning research 15.1 (2014): 1929-1958.↩︎\nZeiler, Matthew D., and Rob Fergus. “Visualizing and understanding convolutional networks.” Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13. Springer International Publishing, 2014.↩︎\nLeCun, Yann, et al. “Gradient-based learning applied to document recognition.” Proceedings of the IEEE 86.11 (1998): 2278-2324.↩︎\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems 25 (2012).↩︎\nSimonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014).↩︎\nHe, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.↩︎\nSzegedy, Christian, et al. “Going deeper with convolutions.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.↩︎\nTan, Mingxing, and Quoc Le. “Efficientnet: Rethinking model scaling for convolutional neural networks.” International conference on machine learning. PMLR, 2019.↩︎\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems 25 (2012).↩︎\nLitjens, Geert, et al. “A survey on deep learning in medical image analysis.” Medical image analysis 42 (2017): 60-88.↩︎\nSchroff, Florian, Dmitry Kalenichenko, and James Philbin. “Facenet: A unified embedding for face recognition and clustering.” Proceedings of the IEEE conference on computer vision and pattern re(cognition. 2015.↩︎\nHochreiter, Sepp, and Jürgen Schmidhuber. “Long short-term memory.” Neural computation 9.8 (1997): 1735-1780.↩︎\nUnderstanding LSTMs↩︎\nPascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. “On the difficulty of training recurrent neural networks.” International conference on machine learning. Pmlr, 2013.↩︎\nHochreiter, Sepp, and Jürgen Schmidhuber. “Long short-term memory.” Neural computation 9.8 (1997): 1735-1780.↩︎\nCho, Kyunghyun, et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078 (2014).↩︎\nGraves, Alex, and Jürgen Schmidhuber. “Framewise phoneme classification with bidirectional LSTM and other neural network architectures.” Neural networks 18.5-6 (2005): 602-610.↩︎\nShi, Xingjian, et al. “Convolutional LSTM network: A machine learning approach for precipitation nowcasting.” Advances in neural information processing systems 28 (2015).↩︎\nYoung, Tom, et al. “Recent trends in deep learning based natural language processing.” ieee Computational intelligenCe magazine 13.3 (2018): 55-75.↩︎\nSiami-Namini, Sima, Neda Tavakoli, and Akbar Siami Namin. “A comparison of ARIMA and LSTM in forecasting time series.” 2018 17th IEEE international conference on machine learning and applications (ICMLA). Ieee, 2018.↩︎\nGraves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. “Speech recognition with deep recurrent neural networks.” 2013 IEEE international conference on acoustics, speech and signal processing. Ieee, 2013.↩︎\nEck, Douglas, and Juergen Schmidhuber. “Finding temporal structure in music: Blues improvisation with LSTM recurrent networks.” Proceedings of the 12th IEEE workshop on neural networks for signal processing. IEEE, 2002.↩︎\nMalhotra, Pankaj, et al. “Long short term memory networks for anomaly detection in time series.” Proceedings. Vol. 89. No. 9. 2015.↩︎\nVaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems 30 (2017).↩︎\nBa, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. “Layer normalization.” arXiv preprint arXiv:1607.06450 (2016).↩︎\nGehring, Jonas, et al. “Convolutional sequence to sequence learning.” International conference on machine learning. PMLR, 2017.↩︎\nDai, Zihang, et al. “Transformer-xl: Attentive language models beyond a fixed-length context.” arXiv preprint arXiv:1901.02860 (2019).↩︎\nKaplan, Jared, et al. “Scaling laws for neural language models.” arXiv preprint arXiv:2001.08361 (2020).↩︎\nDevlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers for language understanding.” Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers). 2019.↩︎\nBrown, Tom, et al. “Language models are few-shot learners.” Advances in neural information processing systems 33 (2020): 1877-1901.↩︎\nAchiam, Josh, et al. “Gpt-4 technical report.” arXiv preprint arXiv:2303.08774 (2023).↩︎\nDosovitskiy, Alexey, et al. “An image is worth 16x16 words: Transformers for image recognition at scale.” arXiv preprint arXiv:2010.11929 (2020).↩︎\nLewis, Mike, et al. “Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.” arXiv preprint arXiv:1910.13461 (2019).↩︎\nLiu, Ze, et al. “Swin transformer: Hierarchical vision transformer using shifted windows.” Proceedings of the IEEE/CVF international conference on computer vision. 2021.↩︎\nWolf, Thomas, et al. “Transformers: State-of-the-art natural language processing.” Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations. 2020.↩︎\nKhan, Salman, et al. “Transformers in vision: A survey.” ACM computing surveys (CSUR) 54.10s (2022): 1-41.↩︎\nGulati, Anmol, et al. “Conformer: Convolution-augmented transformer for speech recognition.” arXiv preprint arXiv:2005.08100 (2020).↩︎\nRadford, Alec, et al. “Learning transferable visual models from natural language supervision.” International conference on machine learning. PmLR, 2021.↩︎\nJumper, John, et al. “Highly accurate protein structure prediction with AlphaFold.” nature 596.7873 (2021): 583-589.↩︎\nVaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems 30 (2017).↩︎\nKaplan, Jared, et al. “Scaling laws for neural language models.” arXiv preprint arXiv:2001.08361 (2020).↩︎\nHinton, Geoffrey E., and Ruslan R. Salakhutdinov. “Reducing the dimensionality of data with neural networks.” science 313.5786 (2006): 504-507.↩︎\nNg, Andrew. “Sparse autoencoder.” CS294A Lecture notes 72.2011 (2011): 1-19.↩︎\nVincent, Pascal, et al. “Extracting and composing robust features with denoising autoencoders.” Proceedings of the 25th international conference on Machine learning. 2008.↩︎\nKingma, Diederik P., and Max Welling. “Auto-encoding variational bayes.” 20 Dec. 2013.↩︎\nMasci, Jonathan, et al. “Stacked convolutional auto-encoders for hierarchical feature extraction.” Artificial neural networks and machine learning–ICANN 2011: 21st international conference on artificial neural networks, espoo, Finland, June 14-17, 2011, proceedings, part i 21. Springer Berlin Heidelberg, 2011.↩︎\nMakhzani, Alireza, et al. “Adversarial autoencoders.” arXiv preprint arXiv:1511.05644 (2015).↩︎\nSakurada, Mayu, and Takehisa Yairi. “Anomaly detection using autoencoders with nonlinear dimensionality reduction.” Proceedings of the MLSDA 2014 2nd workshop on machine learning for sensory data analysis. 2014.↩︎\nXie, Junyuan, Linli Xu, and Enhong Chen. “Image denoising and inpainting with deep neural networks.” Advances in neural information processing systems 25 (2012).↩︎\nWang, Wei, et al. “Generalized autoencoder: A neural network framework for dimensionality reduction.” Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2014.↩︎\nBengio, Yoshua, et al. “Greedy layer-wise training of deep networks.” Advances in neural information processing systems 19 (2006).↩︎\nSedhain, Suvash, et al. “Autorec: Autoencoders meet collaborative filtering.” Proceedings of the 24th international conference on World Wide Web. 2015.↩︎\nhttps://pytorch.org/vision/main/models.html↩︎\nhttps://github.com/huggingface/pytorch-image-models↩︎\nhttps://huggingface.co/models?pipeline_tag=text-classification&sort=downloads↩︎\nHe, Kaiming, et al. “Mask r-cnn.” Proceedings of the IEEE international conference on computer vision. 2017.↩︎\nHuber Loss↩︎\nHuber Loss in PyTorch↩︎\nSaleh, Resve A., and A. K. Saleh. “Statistical properties of the log-cosh loss function used in machine learning.” arXiv preprint arXiv:2208.04564 (2022).↩︎\nHinge Loss↩︎\nHinge Loss in PyTorch↩︎\nWeighted Cross-Entropy Loss in PyTorch↩︎\nLin, Tsung-Yi, et al. “Focal loss for dense object detection.” Proceedings of the IEEE international conference on computer vision. 2017.↩︎\nFocal Loss in PyTorch↩︎\nGraves, Alex, et al. “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.” Proceedings of the 23rd international conference on Machine learning. 2006.↩︎\nCTC Loss in PyTorch↩︎\nSudre, Carole H., et al. “Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations.” Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Québec City, QC, Canada, September 14, Proceedings 3. Springer International Publishing, 2017.↩︎\nZhou, Dingfu, et al. “Iou loss for 2d/3d object detection.” 2019 international conference on 3D vision (3DV). IEEE, 2019.↩︎\nBatch Gradient Descent↩︎\nStochastic Gradient Descent↩︎\nStochastic Gradient Descent in PyTorch↩︎\nDifference between Batch and Stochastic Gradient Descent↩︎\nMini-Batch Gradient Descent↩︎\nLiu, Yanli, Yuan Gao, and Wotao Yin. “An improved analysis of stochastic gradient descent with momentum.” Advances in Neural Information Processing Systems 33 (2020): 18261-18271.↩︎\nHinton, Geoffrey, Nitish Srivastava, and Kevin Swersky. “Neural networks for machine learning lecture 6a overview of mini-batch gradient descent.” Cited on 14.8 (2012): 2.↩︎\nKingma, Diederik P., and Jimmy Ba. “Adam: A method for stochastic optimization.” arXiv preprint arXiv:1412.6980 (2014).↩︎\nPrecision and Recall↩︎\nF-Score↩︎",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html",
    "href": "sections/intro-to-pytorch.html",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "",
    "text": "Overview\nWelcome to the next exciting step on your deep learning journey! Having explored the fundamental building blocks – Data, Models, Loss Functions, and Optimization Algorithms – you now have a solid conceptual understanding of how deep learning works.\nNow, it’s time to bring those concepts to life! Think of the last lesson as learning the rules of the road and understanding how a car works in principle. This lesson is where we get behind the wheel and learn to drive using a specific, powerful vehicle: PyTorch 1.\nPyTorch is a popular and flexible framework that makes building and training neural networks much more manageable. This session will guide you through its essential components in a simple and friendly way, showing you how the concepts we discussed map directly onto practical code.\nIn this session, we’ll explore:\nBy the end of this lesson, you’ll grasp these key PyTorch components and understand how they implement the deep learning concepts you’ve already learned. This will equip you to start building and experimenting with your own neural networks in the hands-on sections to come!",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#overview",
    "href": "sections/intro-to-pytorch.html#overview",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "",
    "text": "PyTorch Fundamentals: Understand what PyTorch is and why it’s widely used. We’ll start with Tensors, PyTorch’s core data structure, and Autograd, its magic for automatically calculating gradients.\nHandling Data: Learn how PyTorch uses Dataset and DataLoader to efficiently manage and feed data into your models.\nBuilding and Training: Discover how to define Models using nn.Module, select Loss Functions, choose Optimizers, and combine everything into a working Training Loop.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt could be overwhelming to take in all the information at once. Don’t worry if you don’t understand everything right away. The key is to get started and practice. Open a Jupyter Notebook on Google Colab and start playing with the code snippets. As you gain more experience, the concepts will become clearer.\n\n\n\n\n\n\n\n\nLet’s start coding!\n\n\n\n\n\n\n    \n        Cat Computer GIF\n        from Cat GIFs",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#what-is-pytorch",
    "href": "sections/intro-to-pytorch.html#what-is-pytorch",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.1 What is PyTorch?",
    "text": "7.1 What is PyTorch?\nNow that we understand the conceptual building blocks of deep learning, let’s meet the tool we’ll use to put them into practice: PyTorch.\nPyTorch is an open-source deep learning framework developed by Meta AI (formerly Facebook’s AI Research lab) 2. It is designed to provide flexibility and efficiency in building and deploying machine learning models.\nThink back to our analogy: if deep learning concepts are the principles of how a car works, PyTorch is like a specific, well-designed car model – powerful, relatively easy to learn, and equipped with features that make driving (or in our case, building neural networks) smoother.\n\n7.1.1 Why Use a Framework Like PyTorch?\nYou could implement neural network operations using standard numerical libraries like NumPy, but it quickly becomes complex, especially for deep networks and when calculating gradients for training. PyTorch abstracts away much of this complexity.\nConsider implementing a basic convolutional layer:\n\nUsing NumPy (Conceptual Example): Requires manual implementation of sliding windows, dot products, and bias addition. You don’t need to follow every detail, but notice the amount of manual work required compared to the PyTorch equivalent.\n# Conceptual NumPy implementation - verbose and complex\nimport numpy as np\n\n# Input data (Batch, Channels, Height, Width)\nX = np.random.randn(10, 3, 32, 32) \n# Weights (Out_channels, In_channels, Kernel_H, Kernel_W)\nW = np.random.randn(20, 3, 5, 5) \n# Biases (Out_channels)\nb = np.random.randn(20) \n\n# Output placeholder (careful calculation of output size needed)\noutput_h, output_w = 28, 28 # Assuming stride=1, padding=0\nout = np.zeros((10, 20, output_h, output_w)) \n\n# Manual nested loops for convolution\nfor n in range(10): # Batch\n    for c_out in range(20): # Output channels\n        for h in range(output_h): # Output height\n            for w in range(output_w): # Output width\n                # Extract region, perform dot product across input channels, add bias\n                h_start, w_start = h, w\n                h_end, w_end = h_start + 5, w_start + 5\n                region = X[n, :, h_start:h_end, w_start:w_end]\n                convolution_sum = np.sum(region * W[c_out]) + b[c_out]\n                out[n, c_out, h, w] = convolution_sum \n\n# NOTE: This is simplified; correct gradient calculation would add much more complexity!\nUsing PyTorch: Leverages optimized, pre-built layers\nimport torch\nimport torch.nn as nn\n\n# Input data (Batch, Channels, Height, Width)\nX = torch.randn(10, 3, 32, 32) \n\n# Define a convolutional layer (weights/biases handled internally)\nconv_layer = nn.Conv2d(in_channels=3, out_channels=20, kernel_size=5) \n\n# Apply the layer - PyTorch handles the complex operation\nout = conv_layer(X) \n\n# Print output shape (PyTorch calculates it automatically)\nprint(out.shape) # Output: torch.Size([10, 20, 28, 28]) \n\nThis example highlights how PyTorch drastically simplifies deep learning development by providing high-level building blocks. How does PyTorch achieve this? Through several key features:\n\nIt Feels Like Python (Pythonic Integration)\n\nIf you’re comfortable with Python, PyTorch feels remarkably natural. Its API is designed to be intuitive, closely resembling standard Python code.\nIt integrates seamlessly with the Python ecosystem (NumPy, SciPy, etc.). You can use standard Python control flow (if, for) and debugging tools (pdb, print) effectively. This makes learning, prototyping, and debugging faster.\n\nDynamic Computation Graphs (Define-by-Run)\n\nPyTorch builds the graph representing your network’s computations on-the-fly as your Python code runs.\nThink Lego: You add blocks (operations) dynamically, rather than needing a fixed blueprint upfront.\nBenefits: This provides great flexibility for models with variable structures (like RNNs processing different length sentences without requiring complex padding upfront) and makes debugging more straightforward using standard Python tools.\n\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nImagine you want a part of your neural network to behave differently depending on the length of the input sequence. Why might a dynamic graph framework (like PyTorch) make implementing this easier than a framework requiring a fixed graph defined upfront?\nHint: You can use standard Python if statements within your model’s forward pass.\n\n\n\nAutomatic Differentiation (Autograd)\n\nThis is tightly linked to dynamic graphs and is essential for training. PyTorch’s autograd engine automatically calculates the gradients (slopes) of your loss function with respect to all your model’s parameters (weights and biases).\nYou simply define the forward pass (how inputs become outputs), and PyTorch figures out the backward pass (gradient calculation) needed for optimization, saving you from complex manual calculus. We’ll explore this magic in detail soon!\n\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nRemember the “Backward Pass / Backpropagation” step in our conceptual training loop? Which PyTorch feature directly handles the complex calculations needed for this step?\nHint: It automatically figures out the gradients.\n\n\n\nStrong GPU Acceleration\n\nDeep learning requires immense computational power (mostly matrix math). GPUs excel at this due to their parallel processing capabilities.\nPyTorch seamlessly integrates with NVIDIA GPUs (via CUDA). Moving computations from the CPU to the GPU often requires minimal code changes (.to('cuda')) but can result in massive speedups (orders of magnitude) for training and inference.\n\nRich Ecosystem\n\nPyTorch isn’t just the core library. It has a vibrant ecosystem with official libraries tailored for specific domains:\n\nTorchVision 3: For computer vision tasks, offering common datasets, pre-built model architectures, and image tranformation functions.\nTorchText 4: For natural language processing, providing tools for text processing, standard datasets, and common NLP model components.\nTorchAudio 5: For audio processing, including datasets, models, and functions for audio data manipulation.\n\n\n\n\nPre-trained Models and Community 6 7\n\nLeveraging the concept of Transfer Learning is easy in PyTorch. A large community contributes state-of-the-art pre-trained models (especially via TorchVision and platforms like Hugging Face 8).\nYou can easily load these models and adapt them for your own tasks, often achieving great results with less data and training time.\n\n\nIn the next sections, we’ll dive into the specifics, starting with PyTorch’s fundamental data structure: the Tensor.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#pytorch-tensors-the-building-blocks-of-data",
    "href": "sections/intro-to-pytorch.html#pytorch-tensors-the-building-blocks-of-data",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.2 PyTorch Tensors: The Building Blocks of Data",
    "text": "7.2 PyTorch Tensors: The Building Blocks of Data\nIn the previous section, we saw how PyTorch provides high-level tools to simplify deep learning. Now, let’s look under the hood at the most fundamental object you’ll work with: the Tensor.\n\nWhat is a Tensor?\nIf you’ve used NumPy before, you’re already familiar with the concept of a multi-dimensional array (ndarray). A PyTorch Tensor is very similar: it’s a multi-dimensional grid of numerical values. Tensors can represent various forms of data:\n\nA single number (a scalar or 0-dimensional tensor).\nA list of numbers (a vector or 1-dimensional tensor).\nA table of numbers (a matrix or 2-dimensional tensor).\nOr higher-dimensional data, like a color image (which can be represented as a 3D tensor: height x width x color channels) or a batch of images (a 4D tensor: batch size x height x width x channels – although PyTorch often uses batch size x channels x height x width).\n\n\n\nWhy Tensors?\nTensors are the primary way we represent and manipulate data in PyTorch. They are optimized for:\n\nNumerical Computation: Performing mathematical operations efficiently.\nGPU Acceleration: Unlike NumPy arrays, Tensors can be easily moved to and processed on GPUs for massive speedups.\nAutomatic Differentiation: PyTorch’s autograd system (which we’ll cover next) operates directly on Tensors to calculate gradients automatically.\n\n\n\n7.2.1 Creating Tensors\nThere are several ways to create tensors in PyTorch:\n\n\n\n\n\n\nNote\n\n\n\nYou don’t need to memorize all these operations. You can always refer to the PyTorch documentation for a comprehensive list of tensor operations and functions.\n\n\n\nDirectly from data (Python lists or NumPy arrays)\nimport torch\nimport numpy as np\n\n# From a Python list\nlist_data = [[1, 2], [3, 4]]\nt1 = torch.tensor(list_data) \nprint(t1)\n# tensor([[1, 2],\n#         [3, 4]])\n\n# From a NumPy array (shares memory!)\nnumpy_array = np.array([5, 6, 7])\nt2 = torch.from_numpy(numpy_array) \nprint(t2)\n# tensor([5, 6, 7], dtype=torch.int64) # dtype often inferred\nCreating tensors with specific values\n# Tensor of zeros\nshape = (2, 3)\nzeros_tensor = torch.zeros(shape)\nprint(zeros_tensor)\n# tensor([[0., 0., 0.],\n#         [0., 0., 0.]])\n\n# Tensor of ones\nones_tensor = torch.ones(shape)\nprint(ones_tensor)\n# tensor([[1., 1., 1.],\n#         [1., 1., 1.]])\n\n# Tensor with random values (uniform distribution 0 to 1)\nrand_tensor = torch.rand(shape)\nprint(rand_tensor) \n# tensor([[0.1234, 0.5678, 0.9012], # Example random values\n#         [0.3456, 0.7890, 0.2345]])\n\n# Tensor with random values (standard normal distribution)\nrandn_tensor = torch.randn(shape) \nprint(randn_tensor)\n# tensor([[-0.5432,  1.2345, -0.9876], # Example random values\n#         [ 0.6543, -1.5432,  0.1234]])\nCreating tensors similar to other tensors: You can create new tensors that have the same shape and dtype (data type) as an existing tensor\nx_data = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n\n# Create zeros with the same shape and type as x_data\nx_zeros = torch.zeros_like(x_data) \nprint(x_zeros)\n# tensor([[0., 0.],\n#         [0., 0.]])\n\n# Create random numbers with the same shape and type as x_data\nx_rand = torch.rand_like(x_data) \nprint(x_rand)\n# tensor([[0.1111, 0.2222], # Example random values\n#         [0.3333, 0.4444]])\n\n\n\n7.2.2 Tensor Attributes\nEvery tensor has important attributes that describe it:\n\nshape (or .size()): A tuple representing the dimensions of the tensor.\ndtype: The data type of the elements within the tensor (e.g., torch.float32, torch.int64, torch.bool) 9. float32 is the most common for neural network parameters.\ndevice: The device where the tensor’s data is stored (e.g., cpu or cuda:0 for the first GPU).\n\ntensor = torch.randn(3, 4) # Shape (3, 4)\n\nprint(f\"Shape of tensor: {tensor.shape}\") \n# Output: Shape of tensor: torch.Size([3, 4])\n\nprint(f\"Datatype of tensor: {tensor.dtype}\") \n# Output: Datatype of tensor: torch.float32 (default float)\n\nprint(f\"Device tensor is stored on: {tensor.device}\") \n# Output: Device tensor is stored on: cpu (default)\n\n# Creating a tensor with specific dtype and on GPU (if available)\nif torch.cuda.is_available():\n    gpu_tensor = torch.ones(2, 2, dtype=torch.float64, device='cuda')\n    print(f\"\\nGPU Tensor Device: {gpu_tensor.device}\")\n    print(f\"GPU Tensor Dtype: {gpu_tensor.dtype}\")\nelse:\n    print(\"\\nCUDA not available, GPU tensor not created.\")\n\n\n7.2.3 Common Tensor Operations\nPyTorch supports hundreds of operations on tensors. Here are some basics:\n\nElement-wise Operations: Standard math operations apply element by element.\nt1 = torch.tensor([[1., 2.], [3., 4.]])\nt2 = torch.ones(2, 2) * 5 # Creates a tensor [[5., 5.], [5., 5.]]\n\n# Addition\nprint(\"Addition:\\n\", t1 + t2) \n# tensor([[ 6.,  7.],\n#         [ 8.,  9.]])\n\n# Multiplication (element-wise)\nprint(\"Multiplication:\\n\", t1 * t2)\n# tensor([[ 5., 10.],\n#         [15., 20.]])\n\n# In-place operations (modify the tensor directly, often denoted by trailing _)\nt1.add_(t2) # t1 is now modified\nprint(\"t1 after in-place add:\\n\", t1)\n# tensor([[ 6.,  7.],\n#         [ 8.,  9.]])\n\n\n\n\n\n\n\nNote\n\n\n\nOperations often support broadcasting (similar to NumPy) where PyTorch automatically expands tensors of smaller dimensions to match larger ones under certain rules, simplifying code.\n\n\n\nIndexing and Slicing: Works just like NumPy indexing.\ntensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nprint(\"First row:\", tensor[0]) # tensor([1, 2, 3])\nprint(\"Second column:\", tensor[:, 1]) # tensor([2, 5, 8])\nprint(\"Element at row 1, col 2:\", tensor[1, 2]) # tensor(6)\nprint(\"Sub-matrix (rows 0-1, cols 1-2):\\n\", tensor[0:2, 1:3]) \n# tensor([[2, 3],\n#         [5, 6]])\nReshaping Tensors: Changing the shape without changing the data.\ntensor = torch.arange(6) # tensor([0, 1, 2, 3, 4, 5])\n\n# Reshape to 2 rows, 3 columns\nreshaped = tensor.reshape(2, 3) \nprint(\"Reshaped:\\n\", reshaped)\n# tensor([[0, 1, 2],\n#         [3, 4, 5]])\n\n# .view() is similar but requires the new shape to be compatible \n# with the original's memory layout (often faster, shares memory)\nviewed = tensor.view(3, 2) \nprint(\"Viewed:\\n\", viewed)\n# tensor([[0, 1],\n#         [2, 3],\n#         [4, 5]])\n\n# Add a dimension (unsqueeze)\nunsqueezed = tensor.unsqueeze(dim=0) # Add dimension at the beginning\nprint(\"Unsqueezed shape:\", unsqueezed.shape) # torch.Size([1, 6])\n\n# Remove dimensions of size 1 (squeeze)\nsqueezed = unsqueezed.squeeze(dim=0)\nprint(\"Squeezed shape:\", squeezed.shape) # torch.Size([6])\nMatrix Multiplication: Use the @ operator or torch.matmul().\nmat1 = torch.randn(2, 3)\nmat2 = torch.randn(3, 4)\nproduct = mat1 @ mat2 # or torch.matmul(mat1, mat2)\nprint(\"Matrix product shape:\", product.shape) # torch.Size([2, 4])\n\n\n\n7.2.4 The NumPy Bridge\nPyTorch tensors on the CPU can be converted to NumPy arrays and vice-versa very efficiently.\n\nTensor to NumPy: .numpy()\nNumPy to Tensor: torch.from_numpy()\n\n\n\n\n\n\n\nImportant\n\n\n\n\nIf the Tensor is on the CPU, the Tensor and the NumPy array share the same underlying memory location. This means changing one will change the other!\nIf the tensor is on the GPU, you must first move it to the CPU (.cpu()) before converting it to NumPy using .numpy().\n\n\n\n# Tensor to NumPy\ntensor_cpu = torch.ones(5)\nnumpy_arr = tensor_cpu.numpy() \nprint(\"NumPy array:\", numpy_arr) # [1. 1. 1. 1. 1.]\n\ntensor_cpu.add_(1) # Modify the tensor\nprint(\"NumPy array after tensor modified:\", numpy_arr) # [2. 2. 2. 2. 2.] &lt;- It changed!\n\n# NumPy to Tensor\nnumpy_arr = np.zeros(3)\ntensor_from_numpy = torch.from_numpy(numpy_arr)\nprint(\"Tensor:\", tensor_from_numpy) # tensor([0., 0., 0.], ...)\n\nnp.add(numpy_arr, 5, out=numpy_arr) # Modify the NumPy array\nprint(\"Tensor after NumPy array modified:\", tensor_from_numpy) # tensor([5., 5., 5.], ...) &lt;- It changed!\n\n\n\n\n\n\nQuick Thought\n\n\n\nWhy is the shared memory feature of the NumPy bridge both powerful and potentially dangerous if you’re not careful?\nHint: Think about efficiency vs. unintended side effects.\n\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nBased on the PyTorch documentation, can you find the differences between the following functions or attributes?\n\ntorch.view() vs. torch.reshape()\ntorch.cat() vs. torch.stack()\ntorch.unsqueeze() vs. torch.squeeze()\ntorch.cuda.FloatTensor vs. torch.FloatTensor\n\"cpu\" vs. \"cuda\" vs. \"cuda:0\"\n\nHint: Try creating a tensor in Google Colab and playing around with these functions to see what they do.\n\n\nTensors are the starting point for everything else in PyTorch. Understanding how to create and manipulate them is essential before we move on to how PyTorch automatically computes gradients with them using Autograd.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#autograd-automatic-differentiation",
    "href": "sections/intro-to-pytorch.html#autograd-automatic-differentiation",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.3 Autograd: Automatic Differentiation",
    "text": "7.3 Autograd: Automatic Differentiation\nRemember back in our Deep Learning overview, we discussed Optimization Algorithms like Gradient Descent? These algorithms need to know the gradient (the slope or derivative) of the loss function with respect to each model parameter (weights and biases) to update them correctly and minimize the loss. Calculating these gradients manually for complex models would be incredibly tedious and error-prone.\n\n\n\n\n\n\nNote\n\n\n\nCheck out 3Blue1Brown’s video to recap the idea of gradients, backpropagation, and the chain rule.\n\n\nThis is where PyTorch’s magic comes in: torch.autograd, its automatic differentiation engine.\n\nWhat Does Autograd Do?\nAutograd automates the computation of gradients. You define the forward pass of your computation (how inputs produce outputs), and Autograd automatically figures out how to compute the gradients for the backward pass.\n\n\n7.3.1 How Does Autograd Work? (The Concepts)\n\nTracking Operations: PyTorch keeps track of all the operations performed on tensors for which gradient tracking is enabled. It does this by building a dynamic computational graph behind the scenes. This graph represents the relationships between tensors and the operations that created them. Think of it as a recipe recording every step taken.\nThe requires_grad Flag: For Autograd to track operations on a tensor and compute gradients for it later, the tensor’s requires_grad attribute must be set to True.\n\nTensors representing learnable parameters (like the weights and biases in nn.Linear or nn.Conv2d layers) automatically have requires_grad=True.\nInput data tensors typically don’t need gradients, so they usually have requires_grad=False (the default for newly created tensors).\nYou can set it explicitly when creating a tensor: torch.randn(3, 3, requires_grad=True) or change it in-place later: my_tensor.requires_grad_(True).\n\nStarting the Backward Pass: .backward(): Once you have performed your forward pass and computed your final loss value (which must be a scalar – a single number), you call the .backward() method on that scalar loss tensor (e.g., loss.backward()).\nGradient Calculation & Storage: Calling .backward() triggers Autograd to traverse the computational graph backward from the loss scalar. Using the chain rule of calculus, it computes the gradient of the loss with respect to every tensor in the graph that has requires_grad=True.\nThe .grad Attribute: The computed gradients are then accumulated (added) into the .grad attribute of the corresponding leaf tensors (the initial tensors in the graph that had requires_grad=True, typically your model’s parameters).\n\n\nA Simple Example\nLet’s see it in action:\nimport torch\n\n# Create a tensor 'x' that requires gradients\nx = torch.ones(2, 2, requires_grad=True) \nprint(\"x:\\n\", x)\n\n# Perform an operation\ny = x + 2 \nprint(\"y:\\n\", y) \n# y was created by an operation involving x, so it has a 'grad_fn'\n\n# Perform more operations\nz = y * y * 3\nout = z.mean() # Calculate a scalar mean value\nprint(\"out:\", out) # out = tensor(27., grad_fn=&lt;MeanBackward0&gt;)\n\n# Now, compute gradients using backpropagation\nout.backward() \n\n# The gradient dz/dx is computed and stored in x.grad\nprint(\"Gradient of out w.r.t x (x.grad):\\n\", x.grad) \n# tensor([[4.5000, 4.5000],\n#         [4.5000, 4.5000]]) \n# Math check: out = (1/4) * sum(3 * (x+2)^2)\n# d(out)/dx_ij = (1/4) * 3 * 2 * (x_ij+2) = 1.5 * (x_ij+2)\n# Since x_ij = 1, d(out)/dx_ij = 1.5 * (1+2) = 4.5\nIn this example:\n\nWe created x with requires_grad=True.\nWe performed operations (+, *, mean) to get a scalar out. PyTorch built a graph tracking these.\nCalling out.backward() calculated the gradient \\(\\frac{\\partial \\text{out}}{\\partial x}\\) using the chain rule.\nThe result was stored in x.grad.\n\n\n\n\n7.3.2 Important Points about Autograd\n\nGradient Accumulation: As mentioned, gradients computed by .backward() are accumulated into the .grad attribute. They don’t overwrite the previous value; they add to it. This is why, before each training iteration’s backward pass, you must explicitly zero out the gradients from the previous step using optimizer.zero_grad(). Otherwise, gradients from multiple steps would mix, leading to incorrect parameter updates.\nDisabling Gradient Tracking: Sometimes you don’t want PyTorch to track operations (e.g., during model evaluation/inference, or when modifying parameters outside the optimizer). Tracking consumes memory and computation. You can disable it in two main ways:\n\nwith torch.no_grad():: A context manager that disables gradient tracking for any operation within its block. This is the standard way to run inference code.\n.detach(): Creates a new tensor that shares the same data as the original but is detached from the computation history. It won’t require gradients, even if the original did. Useful if you need to use a tensor’s value without affecting gradient calculations later.\n\nx = torch.randn(3, requires_grad=True)\nprint(\"Requires grad:\", x.requires_grad) # True\n\n# Using no_grad context\nwith torch.no_grad():\n    y = x * 2\n    print(\"y requires grad inside no_grad:\", y.requires_grad) # False\n\n# Using detach\nz = x * 3\nz_detached = z.detach()\nprint(\"z requires grad:\", z.requires_grad) # True\nprint(\"z_detached requires grad:\", z_detached.requires_grad) # False\nBackward on Scalars Only: You can only call .backward() implicitly on a tensor containing a single scalar value (like a loss). If you have a non-scalar tensor and need gradients, you typically provide a gradient argument to .backward() specifying how to weight the gradients for each element (this is more advanced).\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nDuring model evaluation (inference), why is it crucial to use with torch.no_grad(): or .detach() before passing data through the model? (Think about efficiency and correctness)\nHint: Do we need gradients when just making predictions? What resources does tracking gradients consume?\n\n\nAutograd is the engine that enables efficient gradient-based optimization in PyTorch. By understanding requires_grad, .backward(), and .grad, along with the concept of gradient accumulation and how to disable tracking, you have the core knowledge needed to understand how models learn during the training loop.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#moving-computations-to-the-gpu",
    "href": "sections/intro-to-pytorch.html#moving-computations-to-the-gpu",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.4 Moving Computations to the GPU",
    "text": "7.4 Moving Computations to the GPU\nWe’ve mentioned that one of PyTorch’s key strengths is its excellent GPU acceleration support. Deep learning often involves vast amounts of computation, especially large matrix multiplications. GPUs are designed for precisely this kind of parallel processing and can dramatically speed up model training and inference compared to using only the CPU.\nPyTorch makes using a GPU remarkably simple using the .to() method (if you have a compatible NVIDIA GPU and have installed the correct PyTorch version with CUDA support).\n\n\n\n\n\n\nNote\n\n\n\nCheck out this video to see the difference between how CPUs and GPUs compute. Deep learning involves tons of matrix multiplications, which are easy to parallelize - that’s why GPUs are so great for deep learning.\n\n\n\nHow to Move Tensors and Models to the GPU\n\nChecking for GPU Availability and Setting the Device\nFirst, you should check if a GPU is available and define a device object that your code can use. This makes your code portable – it will run on the GPU if available, otherwise defaulting to the CPU.\nimport torch\n\n# Check if CUDA (GPU support) is available\nif torch.cuda.is_available():\n    # Set device to the first CUDA device (GPU 0)\n    device = torch.device(\"cuda\") \n    print(f\"CUDA is available. Using device: {device}\")\nelse:\n    # Set device to CPU\n    device = torch.device(\"cpu\")\n    print(f\"CUDA not available. Using device: {device}\") \nNote: If you have multiple GPUs, you can specify a different device like cuda:1 or cuda:0 to use a specific GPU.\nMoving Tensors to the Device\nYou can move a tensor to the selected device using the .to() method:\n# Assuming 'device' is defined as above\n\n# Create a tensor on the CPU (default)\ncpu_tensor = torch.randn(3, 3)\nprint(f\"Original tensor device: {cpu_tensor.device}\")\n\n# Move the tensor to the determined device (GPU or CPU)\ndevice_tensor = cpu_tensor.to(device)\nprint(f\"Moved tensor device: {device_tensor.device}\") \nNote: The .to() method returns a new tensor on the target device (if it’s not already there). It doesn’t modify the original tensor in-place unless you reassign it (cpu_tensor = cpu_tensor.to(device)).\nMoving Models to the Device\nSimilarly, you need to move your neural network model (which is an instance of nn.Module) to the device (We’ll learn more about nn.Module later). This moves all the model’s parameters (which are themselves tensors) to the target device.\nimport torch.nn as nn\n\n# Define a simple model\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(10, 2) # Example layer\n\n    def forward(self, x):\n        return self.linear(x)\n\n# Create an instance of the model (initially on CPU)\nmodel = SimpleModel()\nprint(f\"Model parameter device (before move): {next(model.parameters()).device}\")\n\n# Move the entire model to the determined device\nmodel.to(device)\nprint(f\"Model parameter device (after move): {next(model.parameters()).device}\")\n\n\n\nCrucial Requirement: Same Device!\nFor any operation involving multiple tensors (e.g., passing input data through a model layer), all tensors involved must be on the same device. If you try to perform an operation between a tensor on the CPU and a tensor on the GPU, you will get a runtime error.\nTherefore, a standard pattern in PyTorch training scripts is:\n\nDefine the device.\nMove the model to the device.\nInside the training loop, move each batch of input data and labels to the device before feeding them into the model.\n\n# --- Inside a typical training loop --- \n# Assuming model and device are already defined and model is on device\n\n# Get a batch of data and labels from your DataLoader\n# inputs, labels = data_batch # (DataLoader typically yields CPU tensors)\n\n# Move data to the same device as the model &lt;&lt;&lt; IMPORTANT STEP\n# inputs = inputs.to(device)\n# labels = labels.to(device)\n\n# Now perform the forward pass (model and inputs are on the same device)\n# outputs = model(inputs) \n# ... rest of the loop (loss calculation, etc.) ...\n\n\n\n\n\n\nImportant\n\n\n\nRemember: Always ensure your model and the data being fed into it reside on the same device (cpu or cuda) to avoid runtime errors. Use the .to(device) pattern consistently.\n\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nWhat happens if you consistently move your model and data to and from different devices?\nHint: Think about the performance implications. Moving stuff around costs time.\n\n\nThis simple .to(device) mechanism is fundamental for unlocking the performance potential of PyTorch for deep learning tasks. Now, let’s move on to how PyTorch helps manage the data itself.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#data-handling-dataset-transforms-and-dataloader",
    "href": "sections/intro-to-pytorch.html#data-handling-dataset-transforms-and-dataloader",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.5 Data Handling: Dataset, Transforms, and DataLoader",
    "text": "7.5 Data Handling: Dataset, Transforms, and DataLoader\nIn the previous lecture, we emphasized the critical role of Data. Preparing input data, formatting outputs, splitting into training/validation/testing sets, handling potentially massive datasets, and feeding data efficiently to the model are all essential steps. Doing this manually, especially with operations like shuffling, batching, and data preprocessing/augmentation, can be complex and inefficient.\nPyTorch provides elegant tools within the torch.utils.data module to streamline this process: Dataset, DataLoader, and commonly used transforms (especially from torchvision.transforms).\n\n7.5.1 torch.utils.data.Dataset\nThe Dataset class is an abstraction that represents your dataset. Think of it as a standardized way to access individual data points. PyTorch has two main types, but the most common is the map-style dataset. To create a custom map-style dataset, you typically subclass torch.utils.data.Dataset and override two key methods (we’ll see an example later):\n\n__len__(self): This method should return the total number of samples in your dataset.\n__getitem__(self, idx): This method is responsible for retrieving the single data sample (features and corresponding label/target) at the given index idx. This is often where you’ll implement the logic to load data from disk (e.g., read an image file, load text) and perform initial processing.\n\n\n\n\n\n\n\nNote\n\n\n\nLibraries like torchvision.datasets provide convenient pre-built Dataset classes for many common public datasets (MNIST, CIFAR-10, ImageNet, etc.), handling downloading and setup automatically 10.\n\n\n\n\n7.5.2 Preprocessing and Augmentation with Transforms\nRaw data (like images on disk) is rarely in the exact format a neural network expects (e.g., specific size, numerical range, tensor structure). Furthermore, we often want to apply data augmentation during training to artificially increase the diversity of our dataset and make the model more robust. This is where transforms come in.\nTransforms are functions/classes that perform operations on your data, usually applied within the Dataset’s __getitem__ method. For images, the torchvision.transforms module provides a wide array of useful transforms.\n\nCommon Preprocessing Transforms\n\ntransforms.Resize((height, width)): Resizes the input image to a specific size.\ntransforms.CenterCrop(size): Crops the center of the image.\ntransforms.ToTensor(): Crucial! Converts a PIL Image or NumPy array (H x W x C, range [0, 255]) into a PyTorch FloatTensor (C x H x W, range [0.0, 1.0]). It handles the necessary dimension reordering and scaling.\ntransforms.Normalize(mean, std): Normalizes a tensor image with a specified mean and standard deviation for each channel. This helps stabilize training, as models often perform better with input features centered around zero with unit variance. mean and std are often pre-computed on large datasets like ImageNet as we often use models pre-trained on them for transfer learning.\n\n\n\n\n\n\n\nNote\n\n\n\nToTensor() is a crucial transform that it’s almost always required working with image data from PIL or NumPy, as it performs the necessary conversion and reshaping (HWC -&gt; CHW) that models expect.\n\n\n\n\nCommon Augmentation Transforms (Usually only applied to training data)\n\ntransforms.RandomHorizontalFlip(p=0.5): Randomly flips the image horizontally with a given probability p.\ntransforms.RandomRotation(degrees): Randomly rotates the image by a certain angle range.\ntransforms.ColorJitter(...), transforms.RandomResizedCrop(...), etc.\n\n\n\nChaining Transforms with Compose\nTypically, you want to apply multiple transforms in sequence. transforms.Compose allows you to chain them together neatly:\nimport torchvision.transforms as transforms\n\n# Example transform pipeline for training\ntrain_transform = transforms.Compose([\n    transforms.Resize((256, 256)),      # Resize\n    transforms.RandomCrop(224),         # Randomly crop to 224x224\n    transforms.RandomHorizontalFlip(),  # Augmentation\n    transforms.ToTensor(),              # Convert to tensor (scales to [0, 1])\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], # ImageNet stats\n                         std=[0.229, 0.224, 0.225])  # Normalize\n])\n\n# Example transform pipeline for validation/testing (no augmentation)\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),      # Resize directly to final size\n    transforms.ToTensor(),              # Convert to tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n\nConceptual Example of a Custom Dataset and Transforms\nThe transform pipeline is usually passed to the Dataset during initialization and applied within __getitem__.\nfrom torch.utils.data import Dataset\n# Assume necessary imports like os, pandas, PIL.Image, torch etc.\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None):\n        \"\"\"\n        Args:\n            annotations_file (string): Path to the csv file with annotations.\n            img_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        self.img_labels = self._load_annotations(annotations_file) # e.g., load into pandas DataFrame\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def _load_annotations(self, file_path):\n        # Implement logic to load image names and labels, e.g., from a CSV\n        # Return something like a list of tuples: [('image1.jpg', 0), ('image2.jpg', 1), ...]\n        pass \n\n    def __len__(self):\n        # Returns the total number of samples\n        return len(self.img_labels) \n\n    def __getitem__(self, idx):\n        # 1. Get image path and label based on index\n        img_path = os.path.join(self.img_dir, self.img_labels[idx][0]) \n        label = self.img_labels[idx][1] \n        \n        # 2. Load image (e.g., using PIL)\n        image = Image.open(img_path).convert(\"RGB\") # Example loading\n\n        # 3. Apply transformations HERE before returning (if any) - e.g., resize, normalize, convert to tensor\n        if self.transform:\n            image = self.transform(image)\n            \n        # 4. Return the sample (image tensor, label tensor)\n        return image, torch.tensor(label, dtype=torch.long) \n\n# Usage (conceptual):\n# train_dataset = CustomImageDataset(annotations_file='labels.csv', img_dir='images/', transform=train_transform)\n# val_dataset = CustomImageDataset(annotations_file='labels.csv', img_dir='images/', transform=val_transform)\n\n# inputs, labels = train_dataset[0] # Get the first sample\n\n\n\n\n\n\nQuick Thought\n\n\n\nWhy do we typically apply data augmentation transforms (like RandomHorizontalFlip or RandomRotation) only to the training data and not to the validation or test data?\nHint: What is the goal of augmentation? What do we want to measure during validation/testing?\n\n\n\n\n\n7.5.3 torch.utils.data.DataLoader\nNow that our Dataset (with transforms) can provide processed individual samples, we need an efficient way to iterate over these samples in batches for training. This is the job of the DataLoader.\nDataLoader wraps a Dataset and provides an iterator that yields batches of data automatically. It handles the complexities of:\n\nBatching: Grouping individual samples fetched from the Dataset into mini-batches.\nShuffling: Randomly shuffling the data indices at the beginning of each epoch (crucial for effective training).\nParallel Loading: Using multiple subprocesses (num_workers) to load data in the background, preventing data loading from becoming a bottleneck during training.\n\n\n\n\n\n\n\nNote\n\n\n\nnum_workers &gt; 0 means that the data loading uses subprocesses for loading. It’s best to start from 0 (main process) or a small number (e.g., 2 or 4) and increase it cautiously, as too many workers can sometimes cause issues or increase overhead, depending on your system.\n\n\n\nCreating and Using a DataLoader\nfrom torch.utils.data import DataLoader\n\n# Assume 'train_dataset' and 'val_dataset' are instances of a Dataset class\n# (potentially using train_transform and val_transform respectively)\n\n# Create a DataLoader for the training set\ntrain_loader = DataLoader(\n    dataset=train_dataset, \n    batch_size=64,     # How many samples per batch\n    shuffle=True,      # Shuffle data every epoch (IMPORTANT for training)\n    num_workers=4      # Number of subprocesses for data loading (adjust based on system)\n    # pin_memory=True  # Often used with GPU for faster memory transfers\n)\n\n# Create a DataLoader for the validation set\nval_loader = DataLoader(\n    dataset=val_dataset,\n    batch_size=128,    # Can often use larger batch size for validation\n    shuffle=False,     # No need to shuffle validation data\n    num_workers=4\n    # pin_memory=True\n)\n\n# How to iterate over the DataLoader in a training loop:\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    \n    # Training phase\n    # model.train() \n    for batch_idx, (inputs, labels) in enumerate(train_loader):\n        # 'inputs' is a batch of images, 'labels' is a batch of labels\n        \n        # Move inputs and labels to the correct device (e.g., GPU)\n        # inputs, labels = inputs.to(device), labels.to(device)\n        \n        # --- Your training steps ---\n        # ... (as shown previously) ...\n        # ---------------------------\n\n        if batch_idx % 100 == 0: # Print progress every 100 batches\n            print(f\"  Batch {batch_idx}/{len(train_loader)}\")\n            \n    # Validation phase (using val_loader)\n    # model.eval() \n    # with torch.no_grad():\n    #    for inputs, labels in val_loader:\n            # inputs, labels = inputs.to(device), labels.to(device)\n            # ... evaluation logic ...\n\n\n\n7.5.4 Summary: Dataset, Transforms, and DataLoader\nThese three components form a powerful pipeline for feeding data to your models:\n\nDataset: Defines access to individual raw data samples and applies necessary Transforms.\nTransforms (torchvision.transforms): Preprocess (resize, normalize, ToTensor) and optionally augment individual samples within the Dataset.\nDataLoader: Efficiently wraps the Dataset to provide shuffled batches of processed data, often using parallel workers.\n\nUsing this pipeline makes your data loading code clean, efficient, standardized, and ready for training.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#model-building-nn.module-layers-and-containers",
    "href": "sections/intro-to-pytorch.html#model-building-nn.module-layers-and-containers",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.6 Model Building: nn.Module, Layers, and Containers",
    "text": "7.6 Model Building: nn.Module, Layers, and Containers\nIn our journey through the “Building Blocks of Deep Learning,” we explored the concept of Models – the architectures composed of various layers (like Convolutional, Fully-Connected, Activation layers) that learn to map inputs to outputs. Now, we’ll see how to construct these models using PyTorch’s powerful torch.nn module.\n\n7.6.1 The torch.nn Namespace\ntorch.nn is PyTorch’s dedicated library for building neural networks. It provides implementations of common layers, activation functions, loss functions, and other essential building blocks. The most fundamental component within torch.nn for creating any neural network is the nn.Module base class.\n\n\n7.6.2 nn.Module: The Base for All Models\nEvery neural network model and every custom layer you build in PyTorch should be a class that inherits from nn.Module. This base class provides a lot of essential functionality behind the scenes, such as tracking the model’s parameters (weights and biases) and offering helpful methods (like .to(device) to move the model to a GPU, or .parameters() to get all learnable weights).\nWhen creating your custom model class, you typically need to implement two key methods:\n\n__init__(self) (The Constructor):\n\nThis is where you define and instantiate the layers your network will use. You should assign these layers as attributes of your class (e.g., self.conv1 = nn.Conv2d(...), self.relu1 = nn.ReLU(), self.fc1 = nn.Linear(...)).\nLayers defined here are automatically registered as sub-modules, allowing nn.Module to track their parameters.\n\nforward(self, x) (The Forward Pass):\n\nThis method defines how the input data x flows through the layers you defined in __init__. You call the layers like functions, passing the output of one layer as the input to the next.\nThe forward method specifies the actual computation of your network.\n\n\n\nConceptual Structure\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F # Often used for functional APIs like activation functions\n\nclass MyCustomModel(nn.Module):\n    def __init__(self):\n        super().__init__() # IMPORTANT: Call parent class constructor first!\n        \n        # Define layers here - these become tracked parameters\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n        self.relu1 = nn.ReLU()\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(in_features=..., out_features=10) # '...' depends on conv/pool output size\n        # calculating `...` based on the output dimensions of the preceding layers is a common practical step\n\n    def forward(self, x):\n        # Define the data flow through the layers\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.pool1(x)\n        \n        # Flatten the output for the fully-connected layer \n        # (e.g., x = torch.flatten(x, 1) # Flatten all dimensions except batch)\n        x = x.view(x.size(0), -1) # Alternative flatten using view\n        \n        x = self.fc1(x)\n        # No activation/softmax here - often applied outside or handled by the loss function\n        return x\n\n# Instantiate the model\n# model = MyCustomModel() \n# print(model) # Prints the layers\n# model.to(device) # Move model to GPU/CPU\n\n\n\n\n\n\nNote\n\n\n\nSee we didn’t need to define the backward pass (gradient calculation). It is automatically handled by PyTorch’s Autograd system, as discussed previously. You don’t need to implement it manually when using nn.Module correctly.\n\n\n\n\nNesting Modules\nYou can easily include instances of other nn.Module classes within your model definition, promoting modularity.\n# Define another model that uses CustomModel internally\nclass MyCustomModel2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model1 = MyCustomModel() # Use instance of the previous model\n        self.linear_out = nn.Linear(5, 1) # Takes output of model1 (size 5)\n\n    def forward(self, x):\n        x = self.model1(x) # Pass data through the first model\n        x = self.linear_out(x) # Pass through the final layer\n        return x\n\n# Create an instance\nmodel2 = MyCustomModel2()\nprint(\"\\nNested Model Architecture:\\n\", model2)\n\n# Apply the nested model\noutput2 = model2(input_data)\nprint(f\"\\nOutput shape from MyCustomModel2: {output2.shape}\") # Output: torch.Size([32, 1])\n\n\n\n7.6.3 Common Layers in torch.nn 11\ntorch.nn provides a wide variety of pre-built layers. Here are some you’ll frequently encounter, linking back to concepts from the previous lecture:\n\nLinear Layers\n\nnn.Linear(in_features, out_features)\nApplies a linear transformation (fully-connected layer, dense layer, or dense connection).\n\nConvolutional Layers\n\nnn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\nPerforms 2D convolution, common for image data.\nnn.Conv1d and nn.Conv3d also exist.\n\nPooling Layers\n\nnn.MaxPool2d(kernel_size, stride=None), nn.AvgPool2d(...)\nDownsamples feature maps.\nnn.AdaptiveAvgPool2d is also useful.\n\nActivation Functions\n\nnn.ReLU(), nn.LeakyReLU(), nn.Sigmoid(), nn.Tanh(), nn.Softmax(dim=...)\nIntroduce non-linearity.\nCan be used as modules (e.g., nn.ReLU()) or often via the torch.nn.functional API (e.g., F.relu(...)) within the forward method.\n\n\n\n\n\n\n\n\nNote\n\n\n\nF.relu(...) is a function call, useful for simple stateless operations like activations within forward, while nn.ReLU() is a module, necessary if the operation has internal state or parameters, though less common for base activations.\n\n\n\nRegularization Layers\n\nnn.Dropout(p=0.5): Randomly zeros elements during training.\nnn.BatchNorm1d(num_features), nn.BatchNorm2d(...): Normalizes activations across a batch.\nHelp prevent overfitting and stabilize training.\n\nRecurrent Layers\n\nnn.LSTM(input_size, hidden_size, batch_first=False), nn.GRU(...)\nFor sequential data.\n\nTransformer Layers\n\nnn.Transformer(...), nn.TransformerEncoderLayer(...), nn.TransformerDecoderLayer(...), nn.MultiheadAttention(...)\nBuilding blocks for Transformer models.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a complete list of all available layers, refer to the torch.nn documentation.\n\n\n\n\n7.6.4 Organizing Models: Containers\nFor clarity and structure, especially in complex models, PyTorch provides container modules:\n\n1. nn.Sequential\n\nA container that stacks layers sequentially. Data passed to it flows through each layer in the order they were added (no skipping, branching, or complex connections).\nConvenient for simple, linear architectures.\n\n# Define a model using Sequential\nsequential_model = nn.Sequential(\n    nn.Linear(10, 20),\n    nn.ReLU(),\n    nn.Linear(20, 5) \n)\nprint(\"\\nSequential Model:\\n\", sequential_model)\noutput_seq = sequential_model(input_data)\nprint(f\"Output shape from Sequential: {output_seq.shape}\") # Output: torch.Size([32, 5])\n\n\n2. nn.ModuleList\n\nHolds modules in a Python list-like structure. Useful when you need to iterate over layers or access them by index, perhaps applying them within a loop or complex control flow in your forward method.\nUnlike a standard Python list, modules inside ModuleList are correctly registered (parameters are tracked by PyTorch).\n\n# Define a model using ModuleList (layers applied manually in forward)\nclass ModuleListModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Linear(10, 20), \n            nn.ReLU(), \n            nn.Linear(20, 5)\n        ])\n\n    def forward(self, x):\n        for layer in self.layers: # Manually iterate and apply layers\n            x = layer(x)\n        return x\n\nmodule_list_model = ModuleListModel()\nprint(\"\\nModuleList Model:\\n\", module_list_model)\noutput_ml = module_list_model(input_data)\nprint(f\"Output shape from ModuleListModel: {output_ml.shape}\") # Output: torch.Size([32, 5])\n\n\n3. nn.ModuleDict\n\nHolds modules in a Python dictionary-like structure. Allows you to access layers by name (key).\nUseful for organizing named components or selecting specific layers dynamically in the forward method. Modules are correctly registered.\n\n# Define a model using ModuleDict\nclass ModuleDictModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.ModuleDict({\n            'input_layer': nn.Linear(10, 20),\n            'activation': nn.ReLU(),\n            'output_layer': nn.Linear(20, 5)\n        })\n\n    def forward(self, x):\n        x = self.layers['input_layer'](x)\n        x = self.layers['activation'](x)\n        x = self.layers['output_layer'](x)\n        return x\n\nmodule_dict_model = ModuleDictModel()\nprint(\"\\nModuleDict Model:\\n\", module_dict_model)\noutput_md = module_dict_model(input_data)\nprint(f\"Output shape from ModuleDictModel: {output_md.shape}\") # Output: torch.Size([32, 5])\n\n\n\n\n\n\nQuick Thought\n\n\n\nWhen would you choose to define a model by subclassing nn.Module versus using nn.Sequential?\nHint: Think about the complexity of the data flow through the layers.\n\n\n\n\n\n7.6.5 Accessing Model Parameters\nOnce you’ve defined your model (either via nn.Module or a container), PyTorch makes it easy to access all of its learnable parameters (weights and biases). Some methods to help you do this are:\n\n.parameters(): Returns an iterator over all parameters.\n.named_parameters(): Returns an iterator over all parameters, yielding both the name and the parameter tensor.\n.named_children(): Returns an iterator over immediate children modules, yielding both the name and the module.\n.state_dict(): Returns a dictionary containing all model parameters (learnable and non-learnable). We’ll learn more about this later.\n\n# Example of accessing parameters\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        print(f\"Layer: {name} | Size: {param.size()} | Requires Grad: {param.requires_grad}\")\n\n# Example of accessing named parameters\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        print(f\"Layer: {name} | Size: {param.size()} | Requires Grad: {param.requires_grad}\")\n\n# Example of accessing children modules\nfor name, child in model.named_children():\n    print(f\"Child: {name} | Module: {child}\")\n\n# Example of accessing state_dict\nprint(\"\\nState Dict:\\n\", model.state_dict().keys()) # Returns a dictionary of all parameters\nBy understanding nn.Module, common layers, and containers, you now have the tools to translate the conceptual model architectures discussed earlier into concrete PyTorch code, ready to be trained.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#leveraging-pre-trained-models-transfer-learning",
    "href": "sections/intro-to-pytorch.html#leveraging-pre-trained-models-transfer-learning",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.7 Leveraging Pre-trained Models & Transfer Learning",
    "text": "7.7 Leveraging Pre-trained Models & Transfer Learning\nWe’ve just seen how to build neural network models from scratch using nn.Module and various layers. While essential to understand, training large models (especially deep ones like ResNet or VGG) on large datasets (like ImageNet) requires significant data and computational resources (time, powerful GPUs).\nFortunately, we often don’t need to start from zero! Remember the concepts of Pre-trained Models and Transfer Learning from our “Building Blocks” lecture? The core idea is to take a model already trained on a large general dataset (like ImageNet for images) and adapt it for our specific, often smaller, dataset and task. This usually leads to:\n\nFaster development: Less training time needed.\nLower data requirements: Works well even with smaller datasets.\nBetter performance: Often achieves higher accuracy than training from scratch on limited data.\n\nPyTorch makes using pre-trained models incredibly easy, primarily through the torchvision.models module for computer vision tasks (similar libraries exist for other domains, like Hugging Face’s transformers 12 for NLP).\n\n7.7.1 Loading Pre-trained Models with torchvision.models\nThe torchvision.models submodule contains definitions for many popular model architectures (ResNet, VGG, AlexNet, MobileNet, Vision Transformer, etc.) and provides easy access to weights pre-trained on ImageNet.\nThere are two main ways to load a pre-trained model:\nimport torchvision.models as models\n\n# --- Option 1: Using the newer 'weights' API (Recommended) ---\n# This provides access to different pre-trained weight sets and associated metadata\n# List available weights for resnet18\n# print(models.ResNet18_Weights.DEFAULT) # Often points to IMAGENET1K_V1\n# print(models.list_models(weights=models.ResNet18_Weights)) \n\n# Load resnet18 with the default ImageNet v1 weights\nweights = models.ResNet18_Weights.DEFAULT # Or models.ResNet18_Weights.IMAGENET1K_V1\nmodel_v1 = models.resnet18(weights=weights)\n\n# --- Option 2: Using the older 'pretrained=True' argument ---\n# This typically loads the original ImageNet weights the model was published with\n# model_v2 = models.resnet18(pretrained=True) # Legacy way\n\n# It's generally recommended to use the 'weights' API for clarity and future options.\nmodel = model_v1 \n# Set the model to evaluation mode if just doing inference/inspection\nmodel.eval() \n\nInspect the Model\nOnce loaded, you can print the model to see its architecture, paying close attention to the final layer(s), often called the “classifier” or “fully-connected head”.\n# Print the ResNet18 architecture\nprint(model) \n# Output will show layers like conv1, bn1, layer1, layer2, ..., avgpool, fc\n# Notice the final layer: \n# (fc): Linear(in_features=512, out_features=1000, bias=True) \n# This layer outputs 1000 scores, corresponding to the 1000 ImageNet classes.\n\n\n\n7.7.2 Adapting the Model for Your Task\nThe key step in transfer learning is adapting this pre-trained model for your specific task, which likely has a different number of output classes. We typically modify the final classification layer. There are two main strategies:\n\n1. Feature Extraction\nTreat the pre-trained model (except the final layer) as a fixed feature extractor. We freeze its weights and only train the weights of the new final layer(s) we add. This is suitable when your dataset is small or very similar to the original dataset (e.g., ImageNet).\n# --- Feature Extraction Example ---\n\n# 1. Freeze all parameters in the pre-trained model\nfor param in model.parameters():\n    param.requires_grad = False # Freeze weights\n\n# 2. Replace the final layer (the 'head')\n#    ResNet's final layer is named 'fc'. Others might be 'classifier'.\nnum_features = model.fc.in_features # Get the input feature size of the original fc layer\nnum_my_classes = 10 # Example: Your dataset has 10 classes\n\n# Create a new nn.Linear layer for your task\nmodel.fc = nn.Linear(num_features, num_my_classes) \n# NOTE: Parameters of this new layer automatically have requires_grad=True\n\n# Now, only the parameters of 'model.fc' will be updated during training\n# Optimizer should be created AFTER replacing the head:\n# optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n\n\n\n\n\n\nNote\n\n\n\nThe optimizer should be created after freezing parameters and replacing the head, and should typically only be passed the parameters of the new head. We’ll learn more about optimizers later.\n\nBy creating the optimizer after replacing the head, we ensure the optimizer knows about the final set of parameters in your model.\nBy passing only the parameters of the new head, we make the code intent more clear (we only want to update the new head) and slightly more efficient by telling the optimizer exactly which parameters need updating.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe name of the final layer (fc in the case of ResNet) can vary depending on the model architecture. You can inspect the model by print(model) to see the exact name.\n\n\n\n\n2. Fine-tuning\nStart with the pre-trained weights, but allow some or all of them (usually the later layers) to be updated during training on your new dataset, typically using a low learning rate. This adapts the learned features more closely to your specific task. It generally requires more data than feature extraction.\n# --- Fine-tuning Preparation Example ---\n\n# 1. (Optional) Start by freezing all layers as in feature extraction\n# for param in model.parameters():\n#    param.requires_grad = False \n\n# 2. Replace the head (as before)\nnum_features = model.fc.in_features\nnum_my_classes = 10\nmodel.fc = nn.Linear(num_features, num_my_classes)\n\n# 3. (Later, or from the start) Unfreeze some layers for fine-tuning\n# Example: Unfreeze parameters in the last two layers (layer4 and fc)\n# for name, param in model.named_parameters():\n#     if \"layer4\" in name or \"fc\" in name:\n#          param.requires_grad = True\n\n# 4. Create the optimizer to train ALL parameters where requires_grad=True\n#    Use a much smaller learning rate for the pre-trained parts than for the new head.\n# optimizer = torch.optim.Adam([\n#     {'params': model.conv1.parameters(), 'lr': 1e-5}, # Example: Very low LR for early layers\n#     # ... potentially different LRs for different blocks ...\n#     {'params': model.layer4.parameters(), 'lr': 1e-4}, \n#     {'params': model.fc.parameters(), 'lr': 1e-3} # Higher LR for the new head\n# ], lr=1e-5) # Default LR if not specified in groups\n\n# Fine-tuning requires careful setup of the optimizer and learning rates.\n\n\n\n\n\n\nNote\n\n\n\nFine-tuning usually involves a globally smaller learning rate than training from scratch, even for the unfrozen layers, to avoid destroying the pre-trained features too quickly.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAfter potential freezing/unfreezing steps, you can check which parameters require gradients by:\nfor name, param in model.named_parameters():\n    print(f\"{name}: requires_grad={param.requires_grad}\")\n\n\n\n\nInput Preprocessing\nPre-trained models were trained with specific input preprocessing steps (image size, normalization mean/standard deviation). Normally, you’d need to apply these same transformations to your own data when using these models.\nLuckily, the newer weights API often provides the necessary transforms:\n# Get the appropriate weights object\nweights = models.ResNet18_Weights.DEFAULT \n\n# Get the preprocessing transforms recommended for these weights\npreprocess = weights.transforms() \nprint(\"\\nPreprocessing Transforms required by model:\\n\", preprocess)\n\n# Apply these transforms to your input images in your Dataset's __getitem__\n# Example usage within Dataset:\n# image = Image.open(...)\n# input_tensor = preprocess(image) # Apply the transforms\nUsing these standard transforms ensures your input data matches what the model expects.\n\n\n\n\n\n\nQuick Thought\n\n\n\nYou want to adapt a pre-trained ResNet18 model to classify 5 different types of flowers using a small dataset you collected. Which transfer learning strategy (Feature Extraction or Fine-tuning) would likely be the better starting point, and why? What’s the most critical change you need to make to the loaded model object?\nHint: Consider dataset size and the main goal of adapting the model.\n\n\nTransfer learning with pre-trained models is a cornerstone of modern deep learning practice. PyTorch and torchvision make it accessible, allowing you to leverage powerful models without the need for massive resources, accelerating your path to building effective applications.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#loss-functions-in-pytorch-torch.nn",
    "href": "sections/intro-to-pytorch.html#loss-functions-in-pytorch-torch.nn",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.8 Loss Functions in PyTorch (torch.nn)",
    "text": "7.8 Loss Functions in PyTorch (torch.nn)\nRecall from the “Building Blocks” lecture that the Loss Function is crucial for training. It measures how far the model’s predictions are from the actual target values (the ground truth). This calculated “loss” (a scalar value) tells us how poorly the model is performing on a given sample or batch, and its gradient provides the signal needed by the optimizer to update the model’s parameters.\nPyTorch provides a variety of standard loss functions within the torch.nn module. You typically instantiate a loss function object and then call it like a function, passing the model’s predictions and the true targets.\nimport torch\nimport torch.nn as nn\n\n# General pattern:\n# criterion = nn.SomeLossFunction()\n# ... obtain model predictions and targets ...\n# loss = criterion(predictions, targets)\nBy default, the loss function will compute the mean loss across the samples in a batch (controlled by the reduction='mean' argument). This results in a single scalar loss value ready for .backward().\nThe specific loss function you choose depends heavily on the type of task (regression or classification) and the format of your model’s output.\n\n\n\n\n\n\nNote\n\n\n\nFor a full list of loss functions, see the PyTorch Loss Functions documentation.\n\n\n\n7.8.1 Common Loss Functions\n\n1. For Regression Tasks (Predicting Continuous Values)\n\nnn.MSELoss(): Computes the Mean Squared Error between each element in the prediction and target.\n\nPrediction: Tensor of any shape containing predicted values.\nTarget: Tensor of the same shape containing true values.\n\ncriterion_mse = nn.MSELoss()\npredicted_values = torch.randn(10, 1, requires_grad=True) # e.g., 10 predictions\ntrue_values = torch.randn(10, 1)\nloss_mse = criterion_mse(predicted_values, true_values)\nprint(f\"MSE Loss: {loss_mse.item()}\")\nnn.L1Loss(): Computes the Mean Absolute Error (MAE). Less sensitive to outliers than MSE.\n\nPrediction/Target: Same shape requirements as MSELoss.\n\ncriterion_l1 = nn.L1Loss()\nloss_l1 = criterion_l1(predicted_values, true_values)\nprint(f\"L1 (MAE) Loss: {loss_l1.item()}\")\nnn.SmoothL1Loss(): A combination of L1 and MSE (Huber Loss), often used in object detection bounding box regression. Less sensitive to outliers than MSE but smoother near zero than L1.\n\n\n\n2. For Classification Tasks (Predicting Categories)\n\nnn.CrossEntropyLoss(): The standard choice for multi-class classification. This function is particularly convenient because it combines nn.LogSoftmax and nn.NLLLoss in one step.\n\nPrediction: Expects raw, unnormalized scores (logits) directly from the model’s final linear layer. Shape: (N, C) where N is batch size and C is the number of classes.\nTarget: Expects class indices (long integers) ranging from 0 to C-1. Shape: (N). Do not use one-hot encoded targets with this loss.\n\ncriterion_ce = nn.CrossEntropyLoss()\n# Example: 4 samples, 3 classes\nlogits = torch.randn(4, 3, requires_grad=True) # Raw output from model\n# True class indices (e.g., sample 0 is class 1, sample 1 is class 0, ...)\ntrue_indices = torch.tensor([1, 0, 2, 1], dtype=torch.long)\n\nloss_ce = criterion_ce(logits, true_indices)\nprint(f\"\\nCrossEntropy Loss: {loss_ce.item()}\")\nnn.BCEWithLogitsLoss(): The standard choice for binary classification (two classes) or multi-label classification (where each sample can belong to multiple classes). It combines a Sigmoid layer with the Binary Cross Entropy loss (nn.BCELoss) for better numerical stability.\n\nPrediction: Expects raw logits from the model. Shape typically (N) or (N, 1) for binary, or (N, C) for multi-label.\nTarget: Expects float values representing probabilities or target labels (usually 0.0 or 1.0). Must have the same shape as the input predictions.\n\ncriterion_bce = nn.BCEWithLogitsLoss()\n# Example: Binary classification, 4 samples\nbinary_logits = torch.randn(4, requires_grad=True) # Raw output for positive class\n# True labels (0.0 or 1.0)\nbinary_targets = torch.tensor([1.0, 0.0, 1.0, 0.0])\n\nloss_bce = criterion_bce(binary_logits, binary_targets)\nprint(f\"BCEWithLogits Loss: {loss_bce.item()}\")\n\n# Example: Multi-label classification, 2 samples, 3 classes\nmultilabel_logits = torch.randn(2, 3, requires_grad=True)\n# Targets: sample 0 belongs to class 0 & 2; sample 1 belongs to class 1\nmultilabel_targets = torch.tensor([[1.0, 0.0, 1.0], [0.0, 1.0, 0.0]], dtype=torch.float32) # Explicit dtype\nloss_multilabel = criterion_bce(multilabel_logits, multilabel_targets)\nprint(f\"Multi-label BCEWithLogits Loss: {loss_multilabel.item()}\")\nnn.BCELoss(): Computes Binary Cross Entropy. Requires the input predictions to already be probabilities (i.e., passed through a Sigmoid layer). Less numerically stable than BCEWithLogitsLoss, which is generally preferred.\nnn.NLLLoss(): Negative Log Likelihood loss. Typically used after applying nn.LogSoftmax to the model’s output. nn.CrossEntropyLoss combines these two steps and is usually more convenient for multi-class classification.\n\n\n\n\n\n\n\nNote\n\n\n\nDifferent loss functions have different requirements for their input shapes and targets. Ensure your model’s output and target tensors match the expected shapes and types for the chosen loss function.\n\n\n\n\n\n7.8.2 Using the Loss Function in Training\nThe loss function is used within the training loop after obtaining the model’s predictions:\n# --- Inside a typical training loop ---\n# model = ... (Your nn.Module model)\n# criterion = nn.CrossEntropyLoss() # Choose appropriate loss\n# optimizer = ... (Your optimizer)\n# inputs, targets = ... # Your data batch, on the correct device\n\n# 1. Zero gradients\n# optimizer.zero_grad()\n\n# 2. Forward pass: Get model predictions (logits)\n# outputs = model(inputs)\n\n# 3. Calculate loss\n# loss = criterion(outputs, targets) # &lt;&lt;&lt; Use the loss function\n\n# 4. Backward pass: Compute gradients\n# loss.backward() # &lt;&lt;&lt; Autograd calculates gradients based on the loss\n\n# 5. Update weights\n# optimizer.step()\n# -------------------------------------\n\n\n\n\n\n\nQuick Thought\n\n\n\nYou are building a model to classify images into 10 categories (cat, dog, bird, …, truck). Your model’s final layer is nn.Linear(..., 10).\n\nWhich loss function (nn.CrossEntropyLoss or nn.BCEWithLogitsLoss) is appropriate?\nWhat should the shape of the targets tensor be for a batch size of 32? What should its dtype be?\n\nHint: Think about multi-class vs. binary/multi-label, and what nn.CrossEntropyLoss expects.\n\n\nChoosing the correct loss function based on your task and ensuring your model’s output and target data formats match its requirements are crucial steps in building a successful PyTorch model.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#optimizers-in-pytorch-torch.optim",
    "href": "sections/intro-to-pytorch.html#optimizers-in-pytorch-torch.optim",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.9 Optimizers in PyTorch (torch.optim)",
    "text": "7.9 Optimizers in PyTorch (torch.optim)\nIn the “Building Blocks” lecture, we learned about Optimization Algorithms like Gradient Descent, SGD, Adam, etc. Their purpose is to take the error signal (represented by the loss) and the calculated gradients (telling us the direction of steepest ascent) and use this information to adjust the model’s learnable parameters (weights and biases) in a way that minimizes the loss.\nPyTorch implements various optimization algorithms in the torch.optim package.\n\n7.9.1 Instantiating an Optimizer\nTo use an optimizer, you first need to create an instance of it, telling it which parameters it should manage and what learning rate to use.\nThe general pattern is:\nimport torch.optim as optim\n\n# Assume 'model' is your nn.Module instance\n# optimizer = optim.OptimizerName(params_to_optimize, learning_rate, ...)\n\n# Example using Adam:\nlearning_rate = 0.001 \noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nKey Arguments\n\nparams: An iterable containing the parameters (tensors) the optimizer should update. The most common way to provide this is by passing model.parameters(), which conveniently returns an iterator over all learnable parameters within your nn.Module. For more complex scenarios, one can pass specific lists or dictionaries of parameters. See Fine-tuning in Transfer Learning for an example.\nlr (Learning Rate): Controls the step size for parameter updates. This is arguably the most important hyperparameter to tune. Different optimizers often work best with different learning rate ranges.\n\n\n\n\n7.9.2 Common Optimizers\ntorch.optim provides many choices, mirroring the algorithms discussed conceptually:\n\noptim.SGD(params, lr, momentum=0, weight_decay=0, ...)\n\nImplements Stochastic Gradient Descent.\nOften used with the momentum argument (e.g., momentum=0.9) which implements SGD with Momentum, typically leading to faster convergence than basic SGD.\nCan optionally include weight_decay for L2 regularization.\nUsually requires careful tuning of the learning rate and potentially a learning rate schedule.\n\noptim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, ...)\n\nImplements the Adam algorithm.\nCombines ideas from Momentum and RMSprop, adapting learning rates for each parameter.\nOften works well with default settings (lr=0.001) across a wide range of problems, making it a popular default choice.\n\noptim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, ...)\n\nAdam with decoupled Weight Decay.\nGenerally preferred over standard Adam when using weight decay (L2 regularization), as it implements it in a potentially more effective way.\n\noptim.RMSprop(params, lr=0.01, alpha=0.99, ...)\n\nImplements the RMSprop algorithm.\nAdapts learning rates based on the magnitude of recent gradients.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a full list of optimizers, see the PyTorch Optimizers documentation.\n\n\n\n\n7.9.3 Using the Optimizer in the Training Loop\nThe optimizer performs its main work in two steps within the training loop:\n\noptimizer.zero_grad(): This method must be called at the start of each training iteration (before the backward pass). It resets the .grad attribute of all the parameters the optimizer is managing back to zero. This is crucial because, as we learned in the Autograd section, .backward() accumulates gradients into the .grad attribute. Without zero_grad(), gradients from previous batches would add up, leading to incorrect updates.\noptimizer.step(): This method is called after the gradients have been computed using loss.backward(). It updates the values of the parameters based on the computed gradients stored in their .grad attribute and the specific update rule of the chosen optimization algorithm (e.g., applying momentum, using adaptive learning rates).\n\n\nThe Training Loop Revisited\nLet’s look at the training loop fragment again, highlighting the optimizer’s role:\n# --- Inside a typical training loop ---\n# model = ... \n# criterion = ...\n# optimizer = optim.Adam(model.parameters(), lr=0.001) # Instantiate the optimizer\n\n# inputs, targets = ... # Your data batch\n\n# &gt;&gt;&gt; Step 1: Reset gradients from previous iteration\n# optimizer.zero_grad() \n\n# Forward pass\n# outputs = model(inputs) \n\n# Calculate loss\n# loss = criterion(outputs, targets) \n\n# Backward pass (compute gradients for current batch)\n# loss.backward() \n\n# &gt;&gt;&gt; Step 2: Update model parameters using computed gradients\n# optimizer.step() \n# -------------------------------------\n\n\n\n\n\n\nQuick Thought\n\n\n\nWhat would likely happen during training if you forgot to call optimizer.zero_grad() at the beginning of each iteration?\nHint: Remember that gradients accumulate in the .grad attribute.\n\n\n\n\n\n7.9.4 Learning Rate Scheduling\nAs discussed in the previous lecture, adjusting the learning rate during training can often improve performance and convergence. PyTorch provides tools for this in the torch.optim.lr_scheduler module. You typically create a scheduler after creating your optimizer and call scheduler.step() at the appropriate point in your training loop (often after each epoch, sometimes after each batch depending on the scheduler). Common schedulers include StepLR, MultiStepLR, ReduceLROnPlateau, and CosineAnnealingLR. Exploring schedulers is often a next step after getting a basic training loop working.\n\nExample\n# Create the optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Create the scheduler\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n# In the training loop\nfor epoch in range(num_epochs):\n    for inputs, targets in train_loader:\n        # ... existing training loop code ...\n        # ... forward pass, loss calculation, backward pass ...\n        # ... update weights ...\n        # ... call scheduler.step() ...\n\n\n\n\n\n\nNote\n\n\n\nThe timing of scheduler.step() depends on the scheduler type (e.g., some step per epoch, some per batch, ReduceLROnPlateau steps based on a metric). The example shows it inside the epoch loop, which is common for many schedulers like StepLR.\n\n\nWith the optimizer managing parameter updates based on gradients derived from the loss function, we now have almost all the pieces needed to actually train a PyTorch model. The next step is to put them all together in a complete training loop.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#training-a-model-in-pytorch-the-training-loop",
    "href": "sections/intro-to-pytorch.html#training-a-model-in-pytorch-the-training-loop",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.10 Training a Model in PyTorch (The Training Loop)",
    "text": "7.10 Training a Model in PyTorch (The Training Loop)\nWe’ve reached the heart of the process! In the “Building Blocks” lecture, we discussed the Training phase – an iterative cycle where the model processes data, calculates errors, and adjusts its parameters to improve. Let’s translate that conceptual loop into PyTorch code.\n\nThe Goal Revisited\nRemember, the objective isn’t just to minimize loss on the training data, but to achieve good generalization – performance on new, unseen data. We monitor this using a separate validation dataset.\n\n\nAssembling the Pieces\nWe’ll use the PyTorch components we’ve learned about:\n\nDataLoaders: train_loader and val_loader (providing batches of inputs and targets).\nModel: An nn.Module instance (e.g., model = MyCustomModel()).\nCriterion: A loss function instance (e.g., criterion = nn.CrossEntropyLoss()).\nOptimizer: An optimizer instance linked to the model’s parameters (e.g., optimizer = optim.Adam(model.parameters(), lr=0.001)).\nDevice: The device object (cuda or cpu) for hardware placement.\n\n\n\n7.10.1 The Training Loop Structure\nTraining typically involves two nested loops:\n\nOuter Loop (Epochs): Iterates over the entire dataset multiple times. One pass over the full dataset is called an epoch.\nInner Loop (Batches): Iterates over the mini-batches provided by the DataLoader. Parameter updates happen after processing each batch.\n\n\nThe Training Loop Steps (Inside the Inner Loop)\nFor each batch within an epoch, we perform the following crucial steps:\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n# Assume DataLoader, Model, Criterion, Optimizer, device are defined\n# Also assume train_loader, val_loader are defined\n# e.g.\n# model, criterion, optimizer = ..., ..., ...\n# train_loader, val_loader = ..., ...\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device) # Ensure model is on the correct device!\n# scheduler = ... # Optional: define a learning rate scheduler\n\nnum_epochs = 10 # Example number of epochs\n\nfor epoch in range(num_epochs):\n    # --- Training Phase ---\n    model.train() # 1. Set model to training mode (enables dropout, batchnorm updates)\n    epoch_train_loss = 0.0\n    epoch_train_samples = 0\n\n    print(f\"Epoch {epoch+1}/{num_epochs} - Training...\")\n    # Inner loop: Iterates over batches from the DataLoader\n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n\n        # 2. Move data to the correct device (must match model's device)\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        # 3. Clear previous gradients stored in the optimizer\n        optimizer.zero_grad()\n\n        # 4. Forward pass: Get model outputs (logits)\n        outputs = model(inputs)\n\n        # 5. Calculate loss\n        loss = criterion(outputs, targets)\n\n        # 6. Backward pass: Compute gradients of the loss w.r.t. model parameters\n        loss.backward()\n\n        # 7. Update weights using the optimizer and computed gradients\n        optimizer.step()\n\n        # --- Track statistics for the epoch ---\n        # Accumulate loss (weighted by batch size)\n        # Use loss.item() to get the Python scalar value of the loss tensor\n        epoch_train_loss += loss.item() * inputs.size(0)\n        epoch_train_samples += inputs.size(0)\n\n        # (Optional: Print progress within the epoch)\n        # if batch_idx % 100 == 99: # Print every 100 batches\n        #     print(f'  Batch {batch_idx + 1}/{len(train_loader)} Current Avg Batch Loss: {loss.item():.4f}')\n\n    # Calculate average training loss for the epoch\n    avg_epoch_train_loss = epoch_train_loss / epoch_train_samples\n\n    # --- Validation Phase ---\n    model.eval()  # 1. Set model to evaluation mode (disables dropout, uses running batchnorm stats)\n    epoch_val_loss = 0.0\n    epoch_val_correct = 0\n    epoch_val_samples = 0\n\n    print(f\"Epoch {epoch+1}/{num_epochs} - Validation...\")\n    with torch.no_grad(): # 2. Disable gradient calculations for efficiency\n        for inputs, targets in val_loader:\n            # 3. Move data to device\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n\n            # 4. Forward pass\n            outputs = model(inputs)\n\n            # 5. Calculate loss\n            loss = criterion(outputs, targets)\n            epoch_val_loss += loss.item() * inputs.size(0) # Accumulate validation loss\n\n            # 6. Calculate accuracy (example metric)\n            _, predicted_indices = torch.max(outputs.data, 1) # Get class index with highest score\n            epoch_val_samples += targets.size(0)\n            epoch_val_correct += (predicted_indices == targets).sum().item()\n\n    # Calculate average validation loss and metrics for the epoch\n    avg_epoch_val_loss = epoch_val_loss / epoch_val_samples\n    avg_epoch_val_accuracy = 100.0 * epoch_val_correct / epoch_val_samples\n\n    # (Optional: Step the learning rate scheduler, if defined)\n    # if scheduler:\n    #    scheduler.step() # Or scheduler.step(avg_epoch_val_loss) for ReduceLROnPlateau\n\n    # --- Print Epoch Summary ---\n    print(f\"Epoch {epoch+1} Summary:\")\n    print(f\"  Avg Training Loss: {avg_epoch_train_loss:.4f}\")\n    print(f\"  Avg Validation Loss: {avg_epoch_val_loss:.4f}\")\n    print(f\"  Validation Accuracy: {avg_epoch_val_accuracy:.2f}%\")\n    print(\"-\" * 30)\n\nprint(\"Finished Training\")\n\n\nKey Differences: Training vs. Validation Mode\nNotice the crucial differences when running the validation loop:\n\nmodel.train() vs. model.eval(): These methods switch the behavior of certain layers. model.train() enables dropout and makes BatchNorm use batch statistics. model.eval() disables dropout and makes BatchNorm use its learned running statistics. It’s essential to switch modes correctly.\nGradient Calculation: We wrap the validation loop in with torch.no_grad():. This tells PyTorch not to track operations for gradient calculation, which saves significant memory and computation time, as gradients are not needed for evaluation.\nOptimizer Steps: We do not call optimizer.zero_grad() or optimizer.step() during validation because we are only evaluating the model, not updating its weights.\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nWhy is it important to call model.eval() before running the validation loop? What might happen if you forget and leave the model in train() mode during validation?\nHint: Consider layers like Dropout and Batch Normalization.\n\n\n\n\nMonitoring Training\nThe average training loss, validation loss, and validation accuracy (or other relevant metrics) calculated each epoch are exactly what you would plot to monitor your training progress, just like the conceptual loss curves discussed in the “Building Blocks” lecture. These plots help you diagnose issues like overfitting (validation loss increasing while training loss decreases) or underfitting (both losses high) and decide when to stop training (e.g., using “early stopping” when validation performance plateaus or worsens).\nThis complete training loop structure is the foundation for teaching your PyTorch models. While variations exist, these core steps provide a solid starting point for almost any supervised learning task.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#evaluating-a-model-in-pytorch-metrics-test-loop",
    "href": "sections/intro-to-pytorch.html#evaluating-a-model-in-pytorch-metrics-test-loop",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.11 Evaluating a Model in PyTorch (Metrics & Test Loop)",
    "text": "7.11 Evaluating a Model in PyTorch (Metrics & Test Loop)\nIn the “Building Blocks” lecture, we discussed the Inference phase and the importance of Evaluating Performance using metrics beyond just the loss function. While the validation loss calculated during training gives us a good indicator of generalization, a more formal evaluation using task-specific metrics on unseen data (validation or test sets) is crucial.\n\nWhy Evaluate?\nEvaluation helps us:\n\nAssess Generalization: Understand how well the model performs on data it wasn’t trained on.\nCompare Models: Objectively compare different architectures or hyperparameters.\nMake Decisions: Decide if the model meets the requirements for its intended application or if further training/tuning is needed.\nReport Performance: Provide unbiased performance metrics (especially using the final test set).\n\n\n\nEvaluation Mode: model.eval() and torch.no_grad()\nAs highlighted in the Training Loop section, before performing evaluation or inference, you must remember to:\n\nSet the model to evaluation mode: model.eval()\n\nThis changes the behavior of layers like Dropout (disables it) and Batch Normalization (uses running statistics instead of batch statistics). Failing to do this can lead to inconsistent and worse results.\n\nDisable gradient computation: with torch.no_grad():\n\nThis tells PyTorch not to track gradients, saving memory and computation, as they are not needed for just making predictions.\n\n\n\n\n7.11.1 The Evaluation Loop Structure\nThe loop structure for evaluation (on a validation or test set) is very similar to the validation phase shown in the training loop section.\nimport torch\n# Assume model, criterion, device, and a DataLoader (e.g., val_loader or test_loader) are defined\n# model.to(device) # Ensure model is on the correct device\n\n# --- Evaluation Phase ---\nmodel.eval()  # 1. Set model to evaluation mode!\nall_targets = []\nall_predictions = []\neval_loss = 0.0\n\nprint(\"Evaluating...\")\nwith torch.no_grad(): # 2. Disable gradient calculations!\n    for inputs, targets in val_loader: # Or test_loader\n        \n        # 3. Move data to device\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        \n        # 4. Forward pass\n        outputs = model(inputs) \n        \n        # (Optional) Calculate loss on the batch\n        loss = criterion(outputs, targets)\n        eval_loss += loss.item()\n        \n        # 5. Store predictions and targets for metric calculation\n        #    (Convert to CPU if using external libraries like scikit-learn)\n        #    For classification, store predicted indices or probabilities\n        #    For regression, store predicted values\n        \n        # Example for classification:\n        # _, predicted_indices = torch.max(outputs.data, 1)\n        # all_predictions.append(predicted_indices.cpu()) \n        # all_targets.append(targets.cpu())\n        \n        # Example for regression:\n        # all_predictions.append(outputs.cpu())\n        # all_targets.append(targets.cpu())\n        \n# Concatenate all batches\n# all_predictions = torch.cat(all_predictions)\n# all_targets = torch.cat(all_targets)\n\n# 6. Calculate overall metrics after the loop\navg_eval_loss = eval_loss / len(val_loader)\nprint(f\"Average Evaluation Loss: {avg_eval_loss:.4f}\")\n\n# --- Calculate Task-Specific Metrics (see below) --- \n# accuracy = ...\n# precision = ... \n# recall = ...\n# mae = ...\n\nprint(\"Finished Evaluation\")\n\n\n7.11.2 Calculating Evaluation Metrics\nThe core difference during evaluation is calculating meaningful performance metrics based on the collected outputs and targets. The choice of metrics depends heavily on your task:\n\nCommon Classification Metrics\n\nAccuracy: The most straightforward metric – the proportion of correctly classified samples.\n# --- Inside Evaluation (after loop, assuming classification) ---\n# total_samples = len(all_targets)\n# correct_predictions = (all_predictions == all_targets).sum().item()\n# accuracy = 100.0 * correct_predictions / total_samples\n# print(f\"Accuracy: {accuracy:.2f}%\")\nPrecision, Recall, F1-Score: Crucial for understanding model performance, especially with imbalanced datasets.\n\nPrecision: Of the samples predicted as positive, how many actually were positive? \\(\\frac{TP}{TP + FP}\\)\nRecall (Sensitivity): Of all the actual positive samples, how many did the model find? \\(\\frac{TP}{TP + FN}\\)\nF1-Score: The harmonic mean of Precision and Recall, providing a single balanced score.\n\nConfusion Matrix: A table showing counts of true vs. predicted classes, useful for identifying specific confusion patterns between classes.\nAUC (Area Under the ROC Curve): Measures the ability of the model to distinguish between classes.\n\n\n\nCommon Regression Metrics\n\nMean Squared Error (MSE) / Root Mean Squared Error (RMSE): Average squared difference between predicted and true values. RMSE is the square root of MSE, putting the error back into the original units.\nMean Absolute Error (MAE): Average absolute difference. Less sensitive to outliers than MSE.\nR-squared (R²): Coefficient of determination, indicating the proportion of variance in the target variable predictable from the input features.\n\n\n\nUsing Libraries for Metrics\nCalculating many metrics (especially precision, recall, F1, AUC) correctly can be tricky. It’s highly recommended to use established libraries:\n\ntorchmetrics 13: A PyTorch-native library designed for efficient metric calculation, handling distributed training scenarios as well.\n# Example using torchmetrics (install first: pip install torchmetrics)\n# See docs: https://torchmetrics.readthedocs.io/en/stable/\nimport torchmetrics\n\n# --- Before the evaluation loop ---\n# Define the metric object (e.g., for multi-class accuracy)\n# Move the metric object to the same device as your model and data!\nmetric = torchmetrics.classification.Accuracy(\n            task=\"multiclass\",\n            num_classes=NUM_CLASSES # Replace NUM_CLASSES with your actual number\n        ).to(device)\n\n# --- Inside the evaluation loop (within torch.no_grad()) ---\n# After getting model 'outputs' and 'targets' on the correct device\n# metric.update(outputs, targets) # Update the metric state with batch results\n\n# --- After the evaluation loop ---\n# Compute the final metric over all batches\n# final_accuracy = metric.compute()\n# print(f\"Accuracy (torchmetrics): {final_accuracy:.4f}\")\n# metric.reset() # Reset metric state if you plan to reuse it\nscikit-learn.metrics 14: A widely used library. Requires converting PyTorch tensors to NumPy arrays (.cpu().numpy()) first.\n# Example using scikit-learn (install first: pip install scikit-learn)\n# See docs: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# --- After the evaluation loop ---\n# Ensure predictions and targets are numpy arrays on the CPU\n# all_predictions_np = all_predictions.cpu().numpy()\n# all_targets_np = all_targets.cpu().numpy()\n\n# accuracy = accuracy_score(all_targets_np, all_predictions_np)\n\n# Calculate precision, recall, and F1-score\n# The 'average' parameter determines how scores are calculated for multi-class problems:\n#   - 'weighted': Calculates metrics for each class and averages them,\n#                 weighted by the number of true instances for each class (support).\n#                 Good for imbalanced datasets if you care about overall weighted performance.\n#   - 'macro': Calculates metrics for each class and finds their unweighted mean.\n#              Treats all classes equally, regardless of size.\n#   - 'micro': Calculates metrics globally by counting total true positives,\n#              false negatives, and false positives across all classes.\n#              Often equivalent to accuracy.\n#   - None: Returns the scores for each class individually.\nprecision, recall, f1, _ = precision_recall_fscore_support(\n                            all_targets_np,\n                            all_predictions_np,\n                            average='weighted' # Choose average method\n                        )\n\nprint(f\"Accuracy (sklearn): {accuracy:.4f}\")\nprint(f\"Precision (weighted): {precision:.4f}\")\nprint(f\"Recall (weighted): {recall:.4f}\")\nprint(f\"F1 Score (weighted): {f1:.4f}\")\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nWhy might accuracy alone be a misleading metric for evaluating a classifier trained on a highly imbalanced dataset (e.g., 99% of samples are class A, 1% are class B)? Which other metrics (Precision, Recall, F1) would give a better picture of performance on the rare class B?\nHint: A model predicting class A always would have high accuracy.\n\n\n\n\nThe Final Test Set\nRemember the distinction between validation and test sets. The validation set is used during development to tune hyperparameters (like learning rate, model architecture choices) and for early stopping. The test set should be held aside and used only once at the very end of your project to get an unbiased estimate of your final model’s performance on completely unseen data.\nProper evaluation provides crucial insights into your model’s capabilities and limitations, guiding further development and deployment decisions.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#saving-and-loading-models",
    "href": "sections/intro-to-pytorch.html#saving-and-loading-models",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.12 Saving and Loading Models",
    "text": "7.12 Saving and Loading Models\nTraining a deep learning model can take a significant amount of time and computational resources. Once you have a trained model that performs well, you’ll definitely want to save it!\n\nWhy Save and Load?\n\nResume Training: Save checkpoints during long training runs so you can resume later if interrupted.\nAvoid Retraining: Load a previously trained model for inference or further fine-tuning.\nShare Models: Share your trained model weights with others.\nDeployment: Deploy your model for real-world applications.\n\n\n\nWhat to Save? The state_dict\nPyTorch models have an internal state dictionary (state_dict) that contains all their learnable parameters (weights and biases) and potentially persistent buffers (like the running mean/variance in BatchNorm layers).\nWhile you can save the entire model object using torch.save(model, PATH), this is generally not recommended because it binds the saved file to the specific code structure used when saving. It can easily break if you refactor your code or use it in a different project.\nThe recommended and most common practice is to save only the model’s state_dict. This is more lightweight, portable, and less likely to break.\n\n\n7.12.1 Saving the state_dict\nThis saves only the model’s learned parameters.\nimport torch\nimport torch.nn as nn\n# Assume 'model' is your trained nn.Module instance\n# Assume 'PATH' is the desired file path, e.g., 'my_model_weights.pth' or '.pt'\n\n# Example: Saving the state_dict\nPATH = \"my_trained_model.pth\"\ntorch.save(model.state_dict(), PATH) \n\nprint(f\"Model state_dict saved to {PATH}\")\n\n\n\n\n\n\nNote\n\n\n\nThe common extension for PyTorch models is .pth or .pt. There are some discussions about just using .pt because .pth is a special extension for Python.\n\n\n\n\n7.12.2 Loading the state_dict\nTo load the parameters, you must first create an instance of the same model architecture you used during training. Then, you load the saved state_dict into it.\n# Assume 'YourModelClass' is the class definition for your model\n# Make sure the class definition is available!\n\n# 1. Instantiate the model structure\nmodel_loaded = YourModelClass(*args, **kwargs) # Use same args as original model\n\n# 2. Load the saved state_dict\nPATH = \"my_trained_model.pth\"\nstate_dict = torch.load(PATH) \n\n# 3. Load the state_dict into the model instance\nmodel_loaded.load_state_dict(state_dict)\n# By default, load_state_dict uses strict=True, meaning the keys in the\n# state_dict must exactly match the keys returned by the model's state_dict() method.\n# Setting strict=False can be useful in some transfer learning scenarios\n# if you only want to load partial weights, but requires caution.\n\n# 4. CRUCIAL: Set the model to evaluation mode if using for inference\nmodel_loaded.eval()\n\nprint(\"Model state_dict loaded successfully.\")\n\n# Now you can use model_loaded for inference:\n# with torch.no_grad():\n#    predictions = model_loaded(some_input_data.to(device))\n\n\n\n\n\n\nImportant\n\n\n\nRemember to call model.eval() after loading the weights if you intend to use the model for inference, to ensure layers like Dropout and BatchNorm are in the correct mode.\n\n\n\n\n7.12.3 Saving Checkpoints for Resuming Training\nSometimes, you need to save more than just the model weights to resume training effectively. A common practice is to save a checkpoint dictionary containing:\n\nThe model’s state_dict.\nThe optimizer’s state_dict (to resume optimization state like momentum).\nThe current epoch number.\nThe last recorded loss.\nAny other necessary information (e.g., lr_scheduler.state_dict()).\n\n# --- Example: Saving a Checkpoint ---\n# Assume epoch, loss, optimizer are defined\n\ncheckpoint = {\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n    # Add anything else needed: 'scheduler_state_dict': scheduler.state_dict(), etc.\n}\nCHECKPOINT_PATH = f\"model_epoch_{epoch}.pth\"\ntorch.save(checkpoint, CHECKPOINT_PATH)\nprint(f\"Checkpoint saved to {CHECKPOINT_PATH}\")\n\n\n# --- Example: Loading a Checkpoint to Resume Training ---\n# model = YourModelClass(*args, **kwargs)\n# optimizer = optim.Adam(model.parameters(), lr=...) # Create optimizer *before* loading state\n# CHECKPOINT_PATH = \"model_epoch_X.pth\" # Path to the checkpoint file\n\n# checkpoint = torch.load(CHECKPOINT_PATH)\n\n# model.load_state_dict(checkpoint['model_state_dict'])\n# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n# start_epoch = checkpoint['epoch'] + 1 # Resume from next epoch\n# last_loss = checkpoint['loss']\n# # Load scheduler state if saved: scheduler.load_state_dict(...)\n\n# model.train() # Set model to train mode to resume training\n# # Or model.eval() if loading just for evaluation\n\n# print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n\n\n7.12.4 Handling Devices (CPU/GPU)\nBy default, torch.save saves tensors on the device they currently reside on. To make your saved models more portable (e.g., load a GPU-trained model on a CPU-only machine), it’s good practice to save the state_dict after moving the model to the CPU.\nWhen loading, use the map_location argument in torch.load to specify where you want the tensors to be loaded.\n# --- Saving for Portability (Recommended) ---\n# Move model to CPU before getting state_dict\ntorch.save(model.to('cpu').state_dict(), PATH)\n\n# --- Loading with map_location ---\n# 1. Load onto CPU explicitly\nstate_dict_cpu = torch.load(PATH, map_location=torch.device('cpu'))\n# model.load_state_dict(state_dict_cpu)\n\n# 2. Load onto the current 'device' (GPU if available, else CPU)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# state_dict_mapped = torch.load(PATH, map_location=device)\n# model = YourModelClass(...) # Instantiate model\n# model.load_state_dict(state_dict_mapped) # Load state dict\n# model.to(device) # Ensure model is on the correct device\n\n\n\n\n\n\nNote\n\n\n\nFor more details and advanced scenarios, refer to the official PyTorch documentation on saving and loading models.\n\n\nSaving and loading models, especially using the state_dict, is a fundamental skill for any PyTorch practitioner, enabling persistence, sharing, and deployment.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#common-pitfalls-and-best-practices",
    "href": "sections/intro-to-pytorch.html#common-pitfalls-and-best-practices",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.13 Common Pitfalls and Best Practices",
    "text": "7.13 Common Pitfalls and Best Practices\nAs you start building and training models with PyTorch, you might run into a few common challenges. Here are some of the most common pitfalls and best practices to keep in mind:\n\nPitfall: Tensor Shape Mismatches\n\nProblem: Layers expect inputs of specific dimensions (e.g., nn.Linear expects (BatchSize, InFeatures), nn.Conv2d expects (BatchSize, InChannels, Height, Width)). Feeding a tensor with an incorrect shape will cause runtime errors. This often happens when flattening convolutional outputs before a linear layer or forgetting the batch dimension.\nBest Practice:\n\nPrint Shapes Frequently: Sprinkle print(tensor.shape) throughout your model’s forward method during debugging to track how dimensions change.\nRead Documentation: Carefully check the expected input/output shapes for each PyTorch layer you use.\nUse torch.flatten(x, 1) or x.view(x.size(0), -1): Be mindful when reshaping/flattening. Using view with -1 infers one dimension, which is handy but ensure the other dimensions are correct.\n\n\nPitfall: Device Mismatches (CPU vs. GPU)\n\nProblem: Trying to perform an operation involving tensors located on different devices (e.g., input data on CPU, model on GPU) results in a runtime error.\nBest Practice:\n\nDefine device Early: Use the device = torch.device(...) pattern shown previously.\nMove Model: Move your model to the device once (model.to(device)).\nMove Data in Loop: Consistently move input data and targets to the same device inside your training/evaluation loop (inputs.to(device), targets.to(device)).\nCheck .device: When debugging, check the .device attribute of tensors involved in the failing operation.\n\n\nPitfall: Missmatching Data Types\n\nProblem: Some loss functions expect a different data type than the one provided (e.g., using torch.float32 targets with BCEWithLogitsLoss that expects torch.float64 targets). Operations on tensors of different data types can lead to unexpected results or errors.\nBest Practice: Check the data type of the tensors consistently especially when the operation is your own.\n\nPitfall: Forgetting optimizer.zero_grad()\n\nProblem: PyTorch accumulates gradients by default (adds them to the .grad attribute on each .backward() call). If you forget optimizer.zero_grad() at the start of your training loop iteration, gradients from previous batches will interfere with the current update, leading to incorrect training.\nBest Practice: Make it a habit: Always call optimizer.zero_grad() right at the beginning of your training loop iteration before the forward pass.\n\nPitfall: Forgetting loss.backward() or optimizer.step()\n\nProblem: Forgetting loss.backward() means no gradients are computed. Forgetting optimizer.step() means gradients are computed but the model’s weights are never updated. In either case, the model doesn’t learn.\nBest Practice: Ensure the standard training sequence is followed within the loop: zero_grad() -&gt; forward -&gt; calculate loss -&gt; backward() -&gt; step().\n\nPitfall: Incorrect Evaluation Mode (model.eval(), torch.no_grad())\n\nProblem: Forgetting model.eval() during validation/testing means layers like Dropout and BatchNorm behave as they do in training, leading to inaccurate performance assessment. Forgetting with torch.no_grad(): means unnecessary computation and memory usage for tracking gradients.\nBest Practice: Always call model.eval() before evaluation and wrap the evaluation loop in with torch.no_grad():. Remember to call model.train() when switching back to training.\n\nPitfall: Incorrect Loss Function Inputs/Targets\n\nProblem: Feeding inputs or targets with incorrect shapes, data types, or formats to the loss function (e.g., probabilities instead of logits for BCEWithLogitsLoss, one-hot encoded targets for CrossEntropyLoss, wrong dtype for targets).\nBest Practice: Carefully read the documentation for your chosen loss function. Pay close attention to:\n\nExpected input format (logits vs. probabilities).\nExpected target format (class indices vs. probabilities/labels).\nExpected target dtype (torch.long for indices, torch.float for BCE targets).\nExpected input/target shapes.\n\n\nPitfall: Unintentionally Breaking the Computation Graph\n\nProblem: Performing operations that prevent Autograd from tracking history correctly, often by converting a tensor that requires gradients to NumPy too early, or using non-PyTorch operations mid-graph where gradients are needed.\nBest Practice: Keep computations within PyTorch tensors as long as gradients are required. Use .detach() explicitly when you need a tensor’s value without its history, or use the .item() method to get the Python scalar value from a single-element tensor after the backward pass or within a no_grad() block.\n\nPitfall: Memory Issues (Especially on GPU)\n\nProblem: Running out of GPU memory (CUDA Out of Memory error). Often caused by using excessively large batch sizes, large models, or holding onto unnecessary tensors and their computation history.\nBest Practice:\n\nReduce batch_size.\nUse with torch.no_grad(): during evaluation.\nUse del tensor_variable if large intermediate tensors are no longer needed.\nUse .detach() on tensors where history is no longer required.\nConsider gradient accumulation or model parallelism for very large models (more advanced).\nMonitor memory usage (torch.cuda.memory_allocated(), torch.cuda.memory_summary()).\n\n\nBest Practice: Debugging\n\nDon’t underestimate simple print() statements to check tensor shapes, dtypes, devices, and values at various points.\nUse Python’s standard debugger (pdb or IDE debuggers) – PyTorch’s dynamic nature makes this very effective. Set breakpoints and inspect tensors.\n\nBest Practice: Start Simple and Iterate\n\nWhen building a new model or trying a new technique, start with a very small version of your dataset and a simple model architecture to verify the code runs end-to-end without errors.\nGradually increase complexity, checking results along the way.\n\n\nBeing aware of these common points can help you troubleshoot more effectively and build your PyTorch skills faster. Every developer encounters these issues, so persistence and careful debugging are key!",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#conclusion-bringing-concepts-to-code",
    "href": "sections/intro-to-pytorch.html#conclusion-bringing-concepts-to-code",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.14 Conclusion: Bringing Concepts to Code",
    "text": "7.14 Conclusion: Bringing Concepts to Code\nCongratulations! You’ve successfully navigated the core components of PyTorch, bridging the gap between the fundamental concepts of deep learning and their practical implementation in a powerful framework.\nLet’s quickly recap the key PyTorch tools and techniques we’ve explored, seeing how they map back to the deep learning building blocks:\n\nPyTorch Fundamentals: We learned what PyTorch is and why it’s useful, focusing on Tensors as the core data structure (representing our Data) and Autograd as the engine for automatic gradient calculation (powering Backpropagation for Optimization).\nData Handling Pipeline: We saw how Dataset, Transforms, and DataLoader work together to efficiently load, preprocess, augment, and batch our Data, preparing it for the model.\nModel Definition: We explored how to define Models using nn.Module, common nn.Layers, and containers like nn.Sequential, translating conceptual architectures into code. We also saw how to leverage Pre-trained Models from torchvision.models for Transfer Learning.\nTraining Components: We learned how to instantiate Loss Functions (nn.CrossEntropyLoss, nn.MSELoss, etc.) from torch.nn to measure error, and how to use Optimizers (torch.optim) like Adam or SGD to update model parameters based on gradients.\nThe Workflow: We put everything together in the Training Loop, saw how to Evaluate model performance using metrics, and learned the practical necessity of Saving and Loading models. We also discussed common pitfalls and best practices to help smooth your development process.\n\nUnderstanding these PyTorch components gives you the foundational toolkit needed to implement and experiment with a wide variety of neural networks. You’ve seen how the abstract concepts of data flow, error calculation, and gradient-based learning become concrete operations within this framework.\nNext Steps: Hands-On Labs!\nWe’ve covered a lot of ground conceptually. The best way to solidify this knowledge is through practice! In the upcoming hands-on labs, you’ll apply everything we’ve discussed! Get ready to dive into the code and bring these powerful ideas to life!\n\n    \n        Lets Do This Meme\n        from Lets Do This Memes",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#footnotes",
    "href": "sections/intro-to-pytorch.html#footnotes",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "",
    "text": "https://pytorch.org↩︎\nhttps://pytorch.org↩︎\nhttps://pytorch.org/vision/stable/index.html↩︎\nhttps://pytorch.org/text/stable/index.html↩︎\nhttps://pytorch.org/audio/stable/index.html↩︎\nhttps://pytorch.org/vision/stable/models.html↩︎\nhttps://discuss.pytorch.org↩︎\nhttps://huggingface.co/↩︎\nPyTorch data types↩︎\ntorchvision.datasets↩︎\ntorch.nn↩︎\nHugging Face Transformers↩︎\ntorchmetrics↩︎\nscikit-learn.metrics↩︎",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html",
    "href": "sections/hands-on-lab-pytorch.html",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "",
    "text": "Overview\nWelcome! This hands-on lab session offers practical experience with PyTorch for building, training, and evaluating neural network models.\nFollowing the concepts covered in the lecture (Tensors, Autograd, Data, Models, Loss, Optimizers, Training), you’ll work with a sample dataset, load a pre-trained model, and fine-tune it for a classification task.\nDon’t worry if things seem complex at first – the goal is to get hands-on experience. Feel free to experiment with the code! Let’s get started!",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#overview",
    "href": "sections/hands-on-lab-pytorch.html#overview",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "",
    "text": "Note\n\n\n\nYou can access the Jupyter notebook for this hands-on lab on Google Colab.\nFeel free to experiment with the code, but if you want to save your work, you need to make a copy to your Google Drive (\"File\" -&gt; \"Save a copy in Drive\") in order to save it.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#data-handling",
    "href": "sections/hands-on-lab-pytorch.html#data-handling",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.1 Data Handling",
    "text": "8.1 Data Handling\nAs discussed in the lecture, handling data efficiently is crucial. We’ll use PyTorch’s Dataset and DataLoader classes, along with transforms, to manage our dataset.\n\n8.1.1 Dataset Overview\nIn this lab, we’ll use a sample RTS (Retrogressive Thaw Slumps) dataset from Dr. Yili Yang’s research work.\nWhile originally for semantic segmentation, we’ll adapt it for classification.\nGoal: Classify the number of RTS (1 to 10) present in each image.\nDataset Structure:\ncyber2a/\n│--- rts/\n│    │--- images/  # Folder containing RGB images\n│    │    │--- train_nitze_000.jpg\n│    │    │--- ...\n│--- data_split.json  # Specifies train/valtest splits\n│--- rts_cls.json     # Maps image filenames to RTS counts (labels)\n│--- rts_coco.json    # (Optional) Contains segmentation annotations\n\ndata_split.json: A dictionary with two keys: train and valtest:\n\ntrain: A list of image filenames for training.\nvaltest: A list of image filenames for validation and testing.\n\nrts_cls.json: A dictionary with image filenames as keys and the number of RTS in each image as values.\n\n\n\n8.1.2 Download Dataset\nFirst, let’s download and unzip the dataset.\nThese commands use wget and unzip, common utilities in Colab/Linux environments.\n# Download the dataset (using wget for compatibility)\n\nprint(\"Downloading and extracting dataset...\")\n!wget -O \"cyber2a.zip\" https://www.dropbox.com/scl/fi/1pz52tq3puomi0185ccyq/cyber2a.zip?rlkey=3dgf4gfrj9yk1k4p2znn9grso&st=bapbt1bq&dl=0\nprint(\"Unzipping dataset...\")\n!unzip -o cyber2a.zip -d . &gt; /dev/null # Redirect verbose output\nprint(\"Dataset downloaded and extracted.\")\n\n\n\n\n\n\nOutput\n\n\n\n\n\n\nDownloading and extracting dataset...\n--2025-05-06 14:19:21--  https://www.dropbox.com/scl/fi/1pz52tq3puomi0185ccyq/cyber2a.zip?rlkey=3dgf4gfrj9yk1k4p2znn9grso\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc5729ed030391453abdb4ec7943.dl.dropboxusercontent.com/cd/0/inline/CpIwMysGXSmUYnJ3yA9cnJexOMujzvbpYd4t08AlxfLTAPP7ArE73CDD6mFz63uiTIUx4S0tC9MaQlc738uD9PA_avJytNAk10f6H9YdYkDWLy9IZryD9VnkbdAP1HN2-1_1W2QwBRVa454svSIWDyKD/file# [following]\n--2025-05-06 14:19:21--  https://uc5729ed030391453abdb4ec7943.dl.dropboxusercontent.com/cd/0/inline/CpIwMysGXSmUYnJ3yA9cnJexOMujzvbpYd4t08AlxfLTAPP7ArE73CDD6mFz63uiTIUx4S0tC9MaQlc738uD9PA_avJytNAk10f6H9YdYkDWLy9IZryD9VnkbdAP1HN2-1_1W2QwBRVa454svSIWDyKD/file\nResolving uc5729ed030391453abdb4ec7943.dl.dropboxusercontent.com (uc5729ed030391453abdb4ec7943.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\nConnecting to uc5729ed030391453abdb4ec7943.dl.dropboxusercontent.com (uc5729ed030391453abdb4ec7943.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: /cd/0/inline2/CpJyJsdmpiCC2vMm97SMNW-sDvEYgsFy0MlO1AFcXxeg_mZF8FVcLF4LYs5Z37c1Epljm5eO1bN4dm5Z0KcUZJg3eEdDTIXhRIo1UxgB4tt3oRJP4FkOwjQL4ScBANxBm-jZWwQFgPYFFbLzpFLOuINRt4rxF_BPThVzWHtkZtiVkcNuupCFnS2FN8Lrv0TAlJXYXvxFWH1cwZjsm-jx_30ctPK2uGCvsl2NxVpg2HD7ArJ_jPef73z94JsoSygEkoJxMhH_LpnLiJIxxtvzQYj903HJtChhd8c_4SPwJ2mUDZLINJA-2FyRofzLUB9db6iEadGtbF4ofadhPqYmvuoY2zmtDAG-g9eW9QeoJ2J_BQ56Sa91NOKIuT4J8aUbpkQ/file [following]\n--2025-05-06 14:19:22--  https://uc5729ed030391453abdb4ec7943.dl.dropboxusercontent.com/cd/0/inline2/CpJyJsdmpiCC2vMm97SMNW-sDvEYgsFy0MlO1AFcXxeg_mZF8FVcLF4LYs5Z37c1Epljm5eO1bN4dm5Z0KcUZJg3eEdDTIXhRIo1UxgB4tt3oRJP4FkOwjQL4ScBANxBm-jZWwQFgPYFFbLzpFLOuINRt4rxF_BPThVzWHtkZtiVkcNuupCFnS2FN8Lrv0TAlJXYXvxFWH1cwZjsm-jx_30ctPK2uGCvsl2NxVpg2HD7ArJ_jPef73z94JsoSygEkoJxMhH_LpnLiJIxxtvzQYj903HJtChhd8c_4SPwJ2mUDZLINJA-2FyRofzLUB9db6iEadGtbF4ofadhPqYmvuoY2zmtDAG-g9eW9QeoJ2J_BQ56Sa91NOKIuT4J8aUbpkQ/file\nReusing existing connection to uc5729ed030391453abdb4ec7943.dl.dropboxusercontent.com:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 15860057 (15M) [application/zip]\nSaving to: ‘cyber2a.zip’\n\ncyber2a.zip         100%[===================&gt;]  15.12M  --.-KB/s    in 0.1s    \n\n2025-05-06 14:19:22 (142 MB/s) - ‘cyber2a.zip’ saved [15860057/15860057]\n\nUnzipping dataset...\nDataset downloaded and extracted.\n\n\n\n\n\n\n\n8.1.3 Visualize the Raw Data\nLet’s take a look at a sample image and its label directly from the files before we create our PyTorch Dataset. This helps understand the raw data format.\nWe will display the original image and the image with segmentation overlays (if aavilable) side-by-side for context, although our model will only perform classification.\nimport os\nimport json\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2  # OpenCV for drawing polygons\nimport numpy as np  # NumPy for image array manipulation\n\nprint(\"\\nVisualizing a raw data sample: original and with segmentation overlay...\")\n\n# Define the directory where images are stored and path to COCO annotations\nimg_dir = \"cyber2a/rts/images/\"\ncoco_file_path = \"cyber2a/rts_coco.json\"\n\n# Load the data split file to get lists of training and validation/test images\ntry:\n    with open(\"cyber2a/data_split.json\", 'r') as f:\n        data_split = json.load(f)\nexcept FileNotFoundError:\n    print(\"Error: data_split.json not found. Make sure the dataset extracted correctly.\")\n    data_split = {} # Ensure data_split exists\n\n# Retrieve the list of training images\nimg_list = data_split.get(\"train\", []) # Use .get for safer dictionary access\nif not img_list:\n    print(\"Warning: No training images found in data_split.json.\")\n\n# Load the image labels (RTS counts)\nimg_labels = {} # Initialize\ntry:\n    with open(\"cyber2a/rts_cls.json\", 'r') as f:\n        img_labels = json.load(f)\nexcept FileNotFoundError:\n    print(\"Error: rts_cls.json not found.\")\n\n# --- Load COCO annotations for drawing segmentation ---\ncoco_data = {} # To store loaded coco image_id_map and annotation_map\ntry:\n    with open(coco_file_path, \"r\") as f:\n        rts_coco_json = json.load(f)\n\n    image_id_map = {img_info['file_name']: img_info['id'] for img_info in rts_coco_json.get('images', [])}\n    coco_data['image_id_map'] = image_id_map\n\n    annotation_map = {}\n    for ann in rts_coco_json.get('annotations', []):\n        img_id = ann['image_id']\n        if img_id not in annotation_map:\n            annotation_map[img_id] = []\n        if 'segmentation' in ann and ann['segmentation']:\n            annotation_map[img_id].append(ann['segmentation'])\n    coco_data['annotation_map'] = annotation_map\n    if image_id_map and annotation_map:\n        print(\"COCO segmentation annotations loaded successfully for visualization.\")\n    else:\n        print(\"COCO segmentation annotations loaded, but parts might be empty (e.g. no images or no annotations).\")\n\nexcept FileNotFoundError:\n    print(f\"Warning: Segmentation annotation file '{coco_file_path}' not found. Cannot display segmentation overlays.\")\nexcept json.JSONDecodeError:\n    print(f\"Warning: Error decoding JSON from '{coco_file_path}'. Cannot display segmentation overlays.\")\nexcept Exception as e:\n    print(f\"Warning: An unexpected error occurred while loading COCO annotations from '{coco_file_path}': {e}\")\n# --- End COCO annotation loading ---\n\nif img_list:\n    img_name = img_list[0]\n    img_label_count = img_labels.get(img_name, \"N/A\")\n\n    print(f\"Displaying Image: {img_name}, Original RTS Count: {img_label_count}\")\n    img_path = os.path.join(img_dir, img_name)\n\n    try:\n        pil_image = Image.open(img_path).convert(\"RGB\")\n\n        label_index = \"N/A\"\n        if img_label_count != \"N/A\":\n            try:\n                label_index = int(img_label_count) - 1\n            except ValueError:\n                print(f\"Warning: Could not convert label_count '{img_label_count}' to int for {img_name}\")\n                label_index = \"Error\"\n\n        fig, axs = plt.subplots(1, 2, figsize=(18, 7)) # 1 row, 2 columns\n\n        # --- Display original image (left subplot) ---\n        axs[0].imshow(pil_image)\n        axs[0].axis('off')\n        axs[0].set_title(\"Original Image\")\n\n        # --- Prepare image for annotations (right subplot) ---\n        # Convert PIL image to NumPy array. np.array() typically creates a copy.\n        image_for_overlay = np.array(pil_image)\n\n        annotations_drawn_successfully = False\n        annotation_status_message = \"(Segmentation Annotations Not Loaded)\"\n\n        if coco_data.get('image_id_map') and coco_data.get('annotation_map'):\n            annotation_status_message = \"(No Segmentation Overlay for this Image)\" # Default if COCO loaded but no annot.\n            image_id_map = coco_data['image_id_map']\n            annotation_map = coco_data['annotation_map']\n\n            if img_name in image_id_map:\n                current_image_id = image_id_map[img_name]\n                if current_image_id in annotation_map:\n                    segmentations_for_image = annotation_map[current_image_id]\n                    if segmentations_for_image:\n                        for ann_segmentation_list in segmentations_for_image:\n                            for polygon_coords in ann_segmentation_list:\n                                try:\n                                    polygon = np.array(polygon_coords, dtype=np.int32).reshape((-1, 1, 2))\n                                    # Draw on the NumPy array copy\n                                    cv2.polylines(image_for_overlay, [polygon], isClosed=True, color=(0, 255, 0), thickness=2)\n                                    annotations_drawn_successfully = True\n                                except ValueError as ve:\n                                    print(f\"Warning: Malformed polygon coordinates for {img_name}. Details: {ve}\")\n                                except Exception as e:\n                                    print(f\"Warning: Could not draw a polygon for {img_name}. Details: {e}\")\n                        if annotations_drawn_successfully:\n                             annotation_status_message = \"(Segmentation Overlay Shown)\"\n                        else: # Annotations existed but none could be drawn\n                            annotation_status_message = \"(Error Drawing Segmentation Overlay)\"\n                    else: # No segmentations listed for this image_id\n                        # print(f\"No segmentation data found for image ID {current_image_id} ({img_name}) in annotation_map.\")\n                        annotation_status_message = \"(No Segmentation Data for this Image)\"\n                else: # image_id not in annotation_map\n                    # print(f\"Image ID {current_image_id} ({img_name}) not found in COCO annotation_map.\")\n                    annotation_status_message = \"(Image ID Not in Annotation Map)\"\n            else: # img_name not in image_id_map\n                 # print(f\"Image name '{img_name}' not found in COCO image_id_map.\")\n                 annotation_status_message = \"(Image Not in COCO Map)\"\n        # else: coco_data is empty (file not found or error during load), initial status_message applies.\n\n        axs[1].imshow(image_for_overlay) # Display the image with annotations\n        axs[1].axis('off')\n        axs[1].set_title(f\"Image with Overlay\\n{annotation_status_message}\")\n\n        fig.suptitle(f\"Image: {img_name} | RTS Count (Original Label): {img_label_count} | Model Label (0-Indexed): {label_index}\", fontsize=14)\n        plt.tight_layout(rect=[0, 0.03, 1, 0.92]) # Adjust rect for suptitle\n        plt.show()\n\n    except FileNotFoundError:\n        print(f\"Error: Image file {img_path} not found.\")\n        # If image not found, try to show empty plots or a message\n        fig, axs = plt.subplots(1, 2, figsize=(18, 7))\n        axs[0].text(0.5, 0.5, 'Image not found', horizontalalignment='center', verticalalignment='center', transform=axs[0].transAxes)\n        axs[1].text(0.5, 0.5, 'Image not found', horizontalalignment='center', verticalalignment='center', transform=axs[1].transAxes)\n        axs[0].axis('off')\n        axs[1].axis('off')\n        fig.suptitle(f\"Image: {img_name} - FILE NOT FOUND\", fontsize=14, color='red')\n        plt.show()\n    except Exception as e:\n        print(f\"An error occurred displaying the image {img_name}: {e}\")\nelse:\n    print(\"Cannot display sample: Training image list is empty or could not be loaded.\")\n    # Optionally, display a placeholder if no image can be shown\n    fig, axs = plt.subplots(1, 2, figsize=(18, 7))\n    axs[0].text(0.5, 0.5, 'No image selected', horizontalalignment='center', verticalalignment='center', transform=axs[0].transAxes)\n    axs[1].text(0.5, 0.5, 'No image selected', horizontalalignment='center', verticalalignment='center', transform=axs[1].transAxes)\n    axs[0].axis('off')\n    axs[1].axis('off')\n    fig.suptitle(\"No image available from training list\", fontsize=14)\n    plt.show()\n\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\nVisualizing a raw data sample: original and with segmentation overlay...\nCOCO segmentation annotations loaded successfully for visualization.\nDisplaying Image: train_nitze_183.jpg, Original RTS Count: 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.1.4 Build a Custom PyTorch Dataset\nAs covered in the lecture, torch.utils.data.Dataset is the base class for representing datasets in PyTorch. We need to implement __init__, __len__, and __getitem__.\n\n__init__: Initialize the dataset.\n__len__: Return the total number of data samples in the dataset.\n__getitem__: Load a data sample and its corresponding label based on the index.\n\nimport torch # Import torch here if not already imported\nfrom torch.utils.data import Dataset\n\nclass RTSDataset(Dataset):\n    \"\"\"Custom Dataset for RTS classification.\"\"\"\n    def __init__(self, split, transform=None):\n        \"\"\"\n        Args:\n            split (str): One of 'train' or 'valtest' to specify the dataset split.\n            transform (callable, optional): Optional transform to be applied on a sample.\n                                           As discussed in the lecture, transforms\n                                           preprocess or augment the data.\n        \"\"\"\n        self.img_dir = \"cyber2a/rts/images/\"\n        self.transform = transform\n\n        # Load image filenames based on the split\n        try:\n            with open(\"cyber2a/data_split.json\") as f:\n                data_split = json.load(f)\n            if split == 'train':\n                self.img_list = data_split['train']\n            elif split == 'valtest':\n                self.img_list = data_split['valtest']\n            else:\n                raise ValueError(\"Invalid split: choose either 'train' or 'valtest'\")\n        except FileNotFoundError:\n            print(\"Error: data_split.json not found.\")\n            self.img_list = [] # Initialize as empty list on error\n        except KeyError:\n            print(f\"Error: Split '{split}' not found in data_split.json.\")\n            self.img_list = []\n\n        # Load image labels (RTS counts)\n        try:\n            with open(\"cyber2a/rts_cls.json\") as f:\n                self.img_labels = json.load(f)\n        except FileNotFoundError:\n            print(\"Error: rts_cls.json not found.\")\n            self.img_labels = {} # Initialize as empty dict on error\n\n        print(f\"Initialized RTSDataset for '{split}' split with {len(self.img_list)} images.\")\n\n    def __len__(self):\n        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n        return len(self.img_list)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves the image and its label at the given index `idx`.\n        This is where data loading and transformation happen for a single sample.\n\n        Args:\n            idx (int): Index of the sample to retrieve.\n\n        Returns:\n            tuple: (image, label) where image is the transformed image tensor\n                   and label is the 0-indexed integer label.\n        \"\"\"\n        if idx &gt;= len(self.img_list):\n            raise IndexError(\"Index out of bounds\")\n\n        img_name = self.img_list[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n\n        try:\n            # Load image using PIL\n            image = Image.open(img_path).convert('RGB')\n        except FileNotFoundError:\n            print(f\"Warning: Image file not found at {img_path}. Returning None.\")\n            # Or handle differently, e.g., return a placeholder or skip\n            return None, None\n        except Exception as e:\n            print(f\"Warning: Error loading image {img_path}: {e}. Returning None.\")\n            return None, None\n\n        # Get the label (RTS count) and convert to 0-indexed integer\n        label_count = self.img_labels.get(img_name)\n        if label_count is None:\n            print(f\"Warning: Label not found for image {img_name}. Assigning label -1.\")\n            label = -1 # Or handle differently\n        else:\n            label = int(label_count) - 1 # 0-indexing\n\n        # Apply transformations if they exist\n        if self.transform:\n            image = self.transform(image)\n\n        # Convert label to a tensor (optional but good practice)\n        # Using LongTensor as CrossEntropyLoss expects integer labels\n        label = torch.tensor(label, dtype=torch.long)\n\n        return image, label\n\n\n8.1.5 Test the Custom Dataset\nLet’s create an instance of our RTSDataset and check if __getitem__ works correctly by fetching and displaying a sample.\n# Helper function to display sample images (can handle tensors or PIL Images)\ndef display_sample_images(dataset, num_images=3):\n    \"\"\"Displays sample images from a PyTorch Dataset.\"\"\"\n    fig, axs = plt.subplots(1, num_images, figsize=(15, 5))\n    if num_images == 1:\n        axs = [axs] # Make it iterable if only one image\n\n    for i in range(num_images):\n        if i &gt;= len(dataset):\n            print(f\"Requested image index {i} out of bounds for dataset size {len(dataset)}.\")\n            continue\n\n        img, label = dataset[i]\n\n        if img is None: # Handle cases where __getitem__ returned None\n             print(f\"Skipping display for index {i}, image data is None.\")\n             if num_images == 1: axs[i].set_title(\"Image Load Error\")\n             continue\n\n        # If the dataset applies transforms (including ToTensor),\n        # the image will be a Tensor. We need to convert it back for display.\n        if isinstance(img, torch.Tensor):\n            # Undo normalization and convert C x H x W to H x W x C\n            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n            img = img * std + mean # Unnormalize\n            img = img.permute(1, 2, 0) # C x H x W -&gt; H x W x C\n            img = img.clamp(0, 1) # Ensure values are in [0, 1] range\n            img = img.numpy() # Convert to NumPy array\n\n        axs[i].imshow(img)\n        axs[i].set_title(f\"Sample {i} - Label: {label.item() if isinstance(label, torch.Tensor) else label}\") # Use .item() to get scalar from tensor\n        axs[i].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n# Create the training dataset *without* transforms first to see raw images\ntry:\n    raw_train_dataset = RTSDataset(\"train\", transform=None)\n    if len(raw_train_dataset) &gt; 0:\n         print(\"\\nDisplaying raw samples from dataset:\")\n         display_sample_images(raw_train_dataset, num_images=3)\n    else:\n        print(\"Raw train dataset is empty, cannot display samples.\")\nexcept Exception as e:\n    print(f\"Error creating/displaying raw dataset: {e}\")\n\n\n\n\n\n\nOutput\n\n\n\n\n\n\nInitialized RTSDataset for 'train' split with 756 images.\n\nDisplaying raw samples from dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.1.6 Define Data Transforms and DataLoaders\nNow, let’s define the transformations we want to apply to our images.\nAs discussed in the lecture, these are crucial for preparing data for the model.\nWe’ll then create DataLoaders to handle batching and shuffling.\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader\n\nprint(\"\\nDefining transforms and dataloaders...\")\n\n# Define the transformations:\n# 1. Resize: Ensure all images have the same size, required by many models.\n#            ResNet-18 (and many ImageNet models) often expect 224x224 or 256x256.\n# 2. ToTensor: Converts PIL Image (H x W x C) [0, 255] to PyTorch Tensor (C x H x W) [0.0, 1.0].\n#              This also automatically moves channel dimension first. Crucial step!\n# 3. Normalize: Standardizes pixel values using mean and standard deviation.\n#               Using ImageNet stats is common practice when using pre-trained models,\n#               as it matches the data the model was originally trained on.\n#               Helps with model convergence.\ntransform = T.Compose([\n    T.Resize((256, 256)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# --- Practice Idea ---\n# Try adding data augmentation transforms for the training set!\n# Uncomment and modify the transform_train below. Remember to only use\n# augmentation for the training set, not validation/testing.\n# transform_train = T.Compose([\n#     T.Resize((256, 256)),\n#     T.RandomHorizontalFlip(p=0.5), # Example augmentation\n#     T.RandomRotation(10),         # Example augmentation\n#     T.ToTensor(),\n#     T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n# ])\n# train_dataset = RTSDataset(\"train\", transform=transform_train) # Use augmented transform\n# ---------------------\n\n# Create the training and validation datasets *with* transforms\ntry:\n    train_dataset = RTSDataset(\"train\", transform=transform)\n    val_dataset = RTSDataset(\"valtest\", transform=transform)\n\n    # Display transformed samples to check\n    if len(train_dataset) &gt; 0:\n        print(\"\\nDisplaying transformed samples from training dataset:\")\n        display_sample_images(train_dataset, num_images=3)\n    else:\n        print(\"Train dataset is empty, cannot display transformed samples.\")\n\n    # Create DataLoaders (Lecture Topic: DataLoader)\n    # - `dataset`: The Dataset object to load from.\n    # - `batch_size`: How many samples per batch. Affects memory and training dynamics.\n    # - `shuffle`: Whether to shuffle data every epoch (True for training is crucial!).\n    # - `num_workers`: Number of subprocesses for data loading. Increases speed but uses more memory. Start with 0 or 2.\n    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2) # No shuffle for validation\n\n    print(\"\\nDataLoaders created.\")\n    # Optional: Iterate over one batch to see the output shape\n    try:\n        dataiter = iter(train_loader)\n        images, labels = next(dataiter)\n        print(f\"Sample batch - Images shape: {images.shape}, Labels shape: {labels.shape}\")\n    except StopIteration:\n        print(\"Could not fetch a batch from train_loader (it might be empty).\")\n    except Exception as e:\n         print(f\"Error iterating over DataLoader: {e}\")\n\n\nexcept Exception as e:\n    print(f\"An error occurred during Dataset/DataLoader creation: {e}\")\n\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\nDefining transforms and dataloaders...\nInitialized RTSDataset for 'train' split with 756 images.\nInitialized RTSDataset for 'valtest' split with 138 images.\n\nDisplaying transformed samples from training dataset:\n\n\n\n\n\n\n\n\n\n\nDataLoaders created.\nSample batch - Images shape: torch.Size([4, 3, 256, 256]), Labels shape: torch.Size([4])",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#model-definition",
    "href": "sections/hands-on-lab-pytorch.html#model-definition",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.2 Model Definition",
    "text": "8.2 Model Definition\nAs covered in the lecture, we often don’t need to train models from scratch.\nWe can use Transfer Learning by loading a Pre-trained Model (like ResNet-18 trained on ImageNet) and adapting its final layer for our specific task.\nWe’ll use torchvision.models for this. Remember that the core building block for models in PyTorch is nn.Module.\nfrom torchvision import models\nimport torch.nn as nn # Import the neural network module\n\nprint(\"\\nLoading pre-trained ResNet-18 model...\")\n\n# Load the pre-trained ResNet-18 model.\n# `weights=ResNet18_Weights.DEFAULT` automatically fetches weights pre-trained on ImageNet.\n# This leverages features learned on a large dataset.\nmodel = models.resnet18(weights=models.resnet.ResNet18_Weights.DEFAULT)\n\n# **Adapting the Model Head (Transfer Learning)**\n# The pre-trained ResNet-18 has a final fully connected layer (`fc`) designed\n# for ImageNet's 1000 classes. We need to replace it with a new layer that\n# outputs scores for our 10 classes (RTS counts 1-10, which are labels 0-9).\n\n# 1. Get the number of input features to the original fully connected layer.\nnum_ftrs = model.fc.in_features\nprint(f\"Original ResNet-18 fc layer input features: {num_ftrs}\")\n\n# 2. Create a new fully connected layer (`nn.Linear`) with the correct number\n#    of input features (`num_ftrs`) and the desired number of output classes (10).\n#    The parameters of this new layer will be randomly initialized and trained.\nmodel.fc = nn.Linear(num_ftrs, 10)\nprint(\"Replaced final fc layer for 10 output classes.\")\n\n# Print the model architecture to observe the change in the final `fc` layer.\n# print(\"\\nModified Model Architecture:\")\n# print(model)\n\n# --- Practice Idea ---\n# Try loading a different pre-trained model, like ResNet-34 or MobileNetV2.\n# You'll need to find the name of its final classification layer (it might not be 'fc')\n# and replace it similarly.\n# Example:\n# model_mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n# print(model_mobilenet) # Inspect the layers to find the classifier name\n# num_ftrs_mobilenet = model_mobilenet.classifier[1].in_features # Example for MobileNetV2\n# model_mobilenet.classifier[1] = nn.Linear(num_ftrs_mobilenet, 10)\n# model = model_mobilenet # Use this model instead\n# ---------------------\n\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\nLoading pre-trained ResNet-18 model...\nOriginal ResNet-18 fc layer input features: 512\nReplaced final fc layer for 10 output classes.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#loss-function",
    "href": "sections/hands-on-lab-pytorch.html#loss-function",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.3 Loss Function",
    "text": "8.3 Loss Function\nThe Loss Function measures how far the model’s predictions are from the true labels. For multi-class classification (like our 10 RTS classes), CrossEntropyLoss is the standard choice (as mentioned in the lecture).\nImportant: CrossEntropyLoss expects raw scores (logits) from the model (it applies Softmax internally) and 0-indexed integer labels.\nprint(\"\\nDefining Loss Function...\")\n# Define the loss function\ncriterion = nn.CrossEntropyLoss()\nprint(\"Using CrossEntropyLoss.\")\n\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\nDefining Loss Function...\nUsing CrossEntropyLoss.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#optimization-algorithm",
    "href": "sections/hands-on-lab-pytorch.html#optimization-algorithm",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.4 Optimization Algorithm",
    "text": "8.4 Optimization Algorithm\nThe Optimizer updates the model’s weights based on the gradients calculated during backpropagation to minimize the loss. We’ll use Stochastic Gradient Descent (SGD) with momentum, a common and effective optimizer (see lecture).\nimport torch.optim as optim\n\nprint(\"\\nDefining Optimizer...\")\n# Define the optimizer\n# - `model.parameters()`: Tells the optimizer which parameters to update.\n# - `lr`: Learning Rate - controls the step size of updates. Needs tuning.\n# - `momentum`: Helps accelerate SGD in the relevant direction.\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\nprint(\"Using SGD optimizer with lr=0.001 and momentum=0.9.\")\n\n# --- Practice Idea ---\n# Try using the Adam optimizer instead. It often requires less learning rate tuning.\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n# print(\"Using Adam optimizer with lr=0.001.\")\n# ---------------------\n\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\nDefining Optimizer...\nUsing SGD optimizer with lr=0.001 and momentum=0.9.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#training-and-evaluation-loop",
    "href": "sections/hands-on-lab-pytorch.html#training-and-evaluation-loop",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.5 Training and Evaluation Loop",
    "text": "8.5 Training and Evaluation Loop\nThis is where we put everything together: iterating through the data, feeding it to the model, calculating loss, backpropagating, and updating weights.\nWe’ll also evaluate on the validation set after each epoch.\nimport time\nfrom tqdm import tqdm # tqdm provides progress bars for loops\n\nprint(\"\\nDefining Training and Evaluation Functions...\")\n\ndef train_eval_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=5):\n    \"\"\"\n    Trains and evaluates the model.\n\n    Args:\n        model (nn.Module): The model to train.\n        criterion (nn.Module): The loss function.\n        optimizer (optim.Optimizer): The optimizer.\n        train_loader (DataLoader): DataLoader for the training data.\n        val_loader (DataLoader): DataLoader for the validation data.\n        num_epochs (int): Number of epochs to train.\n\n    Returns:\n        nn.Module: The trained model.\n    \"\"\"\n    # Check for GPU availability and set device (Lecture Topic: GPU Usage)\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    model.to(device) # Move model to the chosen device\n\n    best_val_acc = 0.0 # Keep track of best validation accuracy\n\n    for epoch in range(num_epochs):\n        epoch_start_time = time.time()\n        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n\n        # ---------------------\n        # Training Phase\n        # ---------------------\n        model.train() # **Crucial:** Set model to training mode (enables dropout, batchnorm updates etc.)\n        running_loss = 0.0\n        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")\n\n        for inputs, labels in train_pbar:\n            # Move data to the correct device\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # **Crucial:** Zero the parameter gradients (Lecture Topic: Optimizers)\n            # Otherwise gradients accumulate from previous batches.\n            optimizer.zero_grad()\n\n            # Forward pass: Get model outputs (logits)\n            outputs = model(inputs)\n\n            # Compute the loss\n            loss = criterion(outputs, labels)\n\n            # Backward pass: Compute gradients of the loss w.r.t. parameters\n            loss.backward()\n\n            # Update model parameters based on gradients\n            optimizer.step()\n\n            # Statistics\n            running_loss += loss.item() * inputs.size(0) # Weighted by batch size\n            train_pbar.set_postfix({'loss': loss.item()}) # Show current batch loss in progress bar\n\n        epoch_loss = running_loss / len(train_loader.dataset) # Average loss over dataset\n        print(f\"Training Loss: {epoch_loss:.4f}\")\n\n        # ---------------------\n        # Validation Phase\n        # ---------------------\n        model.eval()  # **Crucial:** Set model to evaluation mode (disables dropout, uses running BN stats)\n        correct = 0\n        total = 0\n        val_loss = 0.0\n        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\")\n\n        # **Crucial:** Disable gradient calculations for efficiency (Lecture Topic: Autograd / Evaluation)\n        with torch.no_grad():\n            for inputs, labels in val_pbar:\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                # Forward pass\n                outputs = model(inputs)\n\n                # Calculate validation loss\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * inputs.size(0)\n\n                # Calculate accuracy\n                _, predicted = torch.max(outputs.data, 1) # Get the index of the max logit\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n                val_pbar.set_postfix({'acc': (100 * correct / total)})\n\n        epoch_val_loss = val_loss / len(val_loader.dataset)\n        epoch_val_acc = 100 * correct / total\n        print(f\"Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_acc:.2f}%\")\n\n        # Simple check for saving the best model (optional)\n        if epoch_val_acc &gt; best_val_acc:\n            best_val_acc = epoch_val_acc\n            # torch.save(model.state_dict(), 'best_model.pth') # Example saving\n            # print(\"Saved new best model.\")\n\n        epoch_duration = time.time() - epoch_start_time\n        print(f\"Epoch Duration: {epoch_duration:.2f} seconds\")\n\n        # --- Practice Idea ---\n        # Add code here to implement early stopping. For example, if the validation\n        # accuracy doesn't improve for, say, 2 epochs, stop the training loop.\n        # You'll need to store the validation accuracy from the previous epoch.\n        # ---------------------\n\n    print(\"\\nFinished Training.\")\n    return model\n\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\nDefining Training and Evaluation Functions...\n\n\n\n\n\n\n8.5.1 Train the Model\nLet’s start the training process for a few epochs.\nprint(\"\\nStarting model training...\")\n# Ensure the model is on the correct device before passing to train function\n# (train_eval_model also moves it, but good practice to do it here too)\n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n\n# Train the model\ntry:\n    model = train_eval_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=5)\nexcept Exception as e:\n     print(f\"An error occurred during training: {e}\")\n\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\nStarting model training...\nUsing device: cuda:0\n\n--- Epoch 1/5 ---\n\n\nTraining Loss: 1.7698\n\n\nValidation Loss: 1.7400, Validation Accuracy: 37.68%\nEpoch Duration: 6.34 seconds\n\n--- Epoch 2/5 ---\n\n\nTraining Loss: 1.4983\n\n\nValidation Loss: 1.7676, Validation Accuracy: 38.41%\nEpoch Duration: 6.26 seconds\n\n--- Epoch 3/5 ---\n\n\nTraining Loss: 1.1677\n\n\nValidation Loss: 2.4771, Validation Accuracy: 39.86%\nEpoch Duration: 5.63 seconds\n\n--- Epoch 4/5 ---\n\n\nTraining Loss: 0.9606\n\n\nValidation Loss: 2.8200, Validation Accuracy: 33.33%\nEpoch Duration: 6.52 seconds\n\n--- Epoch 5/5 ---\n\n\nTraining Loss: 0.8626\n\n\nValidation Loss: 2.3195, Validation Accuracy: 36.23%\nEpoch Duration: 5.75 seconds\n\nFinished Training.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#inference-making-predictions",
    "href": "sections/hands-on-lab-pytorch.html#inference-making-predictions",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.6 Inference (Making Predictions)",
    "text": "8.6 Inference (Making Predictions)\nAfter training, we want to use the model to predict the class (RTS count) for new, unseen images. This is the inference step. Remember to use model.eval() and torch.no_grad().\nprint(\"\\nDefining prediction function...\")\n\ndef predict_image(model, image_path, transform):\n    \"\"\"\n    Predicts the class label for a single image.\n\n    Args:\n        model (nn.Module): The trained model.\n        image_path (str): Path to the image file.\n        transform (callable): The transformations to apply to the image.\n\n    Returns:\n        int: The predicted 0-indexed class label. Returns -1 on error.\n    \"\"\"\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n        # Apply the same transformations used during training/validation\n        image_tensor = transform(image).unsqueeze(0) # Add batch dimension\n\n        # Ensure model and data are on the same device\n        device = next(model.parameters()).device\n        image_tensor = image_tensor.to(device)\n\n        model.eval() # Set model to evaluation mode\n        with torch.no_grad(): # Disable gradients\n            outputs = model(image_tensor)\n            _, predicted = torch.max(outputs, 1)\n            predicted_label = predicted.item() # Get the scalar value\n            return predicted_label\n\n    except FileNotFoundError:\n        print(f\"Error: Image not found at {image_path}\")\n        return -1\n    except Exception as e:\n        print(f\"Error during prediction for {image_path}: {e}\")\n        return -1\n\n# Example: Predict a specific image from the validation set\nimg_name_to_predict = \"valtest_yg_070.jpg\" # Example image\nimg_dir = \"./cyber2a/rts/images\"\nimg_path_to_predict = os.path.join(img_dir, img_name_to_predict)\n\nprint(f\"\\nPredicting class for image: {img_name_to_predict}\")\npredicted_class = predict_image(model, img_path_to_predict, transform)\n\nif predicted_class != -1:\n    # Add 1 back to get the RTS count (since labels are 0-indexed)\n    predicted_rts_count = predicted_class + 1\n    print(f\"Predicted Class Index: {predicted_class}\")\n    print(f\"Predicted RTS Count: {predicted_rts_count}\")\n\n    # --- Practice Idea ---\n    # Choose a *different* image name from the `val_dataset.img_list`\n    # and predict its class using the `predict_image` function.\n    # Example:\n    # if len(val_dataset) &gt; 1:\n    #     img_name_practice = val_dataset.img_list[1] # Get the second image name\n    #     img_path_practice = os.path.join(img_dir, img_name_practice)\n    #     predicted_class_practice = predict_image(model, img_path_practice, transform)\n    #     print(f\"\\nPractice Prediction for {img_name_practice}: {predicted_class_practice}\")\n    # ---------------------\n\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\nDefining prediction function...\n\nPredicting class for image: valtest_yg_070.jpg\nPredicted Class Index: 1\nPredicted RTS Count: 2",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#visualization",
    "href": "sections/hands-on-lab-pytorch.html#visualization",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.7 Visualization",
    "text": "8.7 Visualization\nLet’s visualize the image we just predicted on, showing the predicted RTS count.\nWe’ll also overlay the original segmentation annotations (if available) for context, although our model only performed classification.\nimport matplotlib.pyplot as plt\nimport cv2 # OpenCV for image handling\nimport numpy as np\n\nprint(\"\\nVisualizing prediction...\")\n\ndef display_image_with_annotations(image_name, image_folder, predicted_class):\n    \"\"\"\n    Displays an image with its original annotations (if available) and the\n    predicted class label from our model.\n\n    Args:\n        image_name (str): The name of the image file.\n        image_folder (str): The folder where the image is stored.\n        predicted_class (int): The 0-indexed predicted class label.\n    \"\"\"\n    # Load the COCO annotations (optional, for context only)\n    coco_annotations = {}\n    try:\n        with open(\"cyber2a/rts_coco.json\", \"r\") as f:\n            rts_coco = json.load(f)\n\n        # Create a mapping from image filename to image ID and annotations\n        image_id_map = {img['file_name']: img['id'] for img in rts_coco.get('images', [])}\n        annotation_map = {}\n        for ann in rts_coco.get('annotations', []):\n            img_id = ann['image_id']\n            if img_id not in annotation_map:\n                annotation_map[img_id] = []\n            annotation_map[img_id].append(ann['segmentation'])\n\n        image_id = image_id_map.get(image_name)\n        annotations = annotation_map.get(image_id, []) if image_id else []\n        coco_annotations['annotations'] = annotations # Store for drawing\n\n    except FileNotFoundError:\n        print(\"Warning: rts_coco.json not found. Cannot display annotations.\")\n        coco_annotations['annotations'] = []\n    except Exception as e:\n        print(f\"Warning: Error loading COCO annotations: {e}\")\n        coco_annotations['annotations'] = []\n\n\n    # Read the image using OpenCV\n    img_path = os.path.join(image_folder, image_name)\n    try:\n        cv2_image = cv2.imread(img_path)\n        if cv2_image is None:\n            raise FileNotFoundError\n        # Convert from BGR (OpenCV default) to RGB (matplotlib default)\n        cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n    except FileNotFoundError:\n        print(f\"Error: Image file not found at {img_path}\")\n        return\n    except Exception as e:\n        print(f\"Error reading image {img_path} with OpenCV: {e}\")\n        return\n\n    # Overlay the polygons (optional visualization)\n    for annotation_list in coco_annotations.get('annotations', []):\n        for polygon_coords in annotation_list:\n            try:\n                # Reshape polygon coordinates for cv2.polylines\n                polygon = np.array(polygon_coords, dtype=np.int32).reshape((-1, 1, 2))\n                cv2.polylines(cv2_image, [polygon], isClosed=True, color=(0, 255, 0), thickness=2)\n            except Exception as e:\n                print(f\"Warning: Could not draw polygon {polygon_coords}: {e}\")\n\n\n    # Display the image with the predicted label\n    fig, ax = plt.subplots()\n    ax.imshow(cv2_image)\n    # Add 1 back to predicted_class to show the RTS count\n    ax.set_title(f'Image: {image_name}\\nPredicted RTS Count: {predicted_class + 1}')\n    ax.axis('off')\n    plt.show()\n\n# Visualize the prediction for the example image\nif predicted_class != -1:\n    display_image_with_annotations(img_name_to_predict, img_dir, predicted_class)\nelse:\n    print(\"Cannot visualize prediction due to previous error.\")\n\n\n\n\n\n\nOutput\n\n\n\n\n\n\n\nVisualizing prediction...",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#conclusion-next-steps",
    "href": "sections/hands-on-lab-pytorch.html#conclusion-next-steps",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.8 Conclusion & Next Steps",
    "text": "8.8 Conclusion & Next Steps\nCongratulations! You’ve successfully:\n\nLoaded and prepared data using Dataset, Transforms, and DataLoader.\nLoaded a pre-trained nn.Module (ResNet-18) and adapted it using transfer learning.\nDefined a loss function (CrossEntropyLoss) and optimizer (SGD).\nImplemented and run a basic training and validation loop.\nPerformed inference on a single image.\nVisualized the prediction.\n\nThis covers the fundamental workflow of a PyTorch application!\nWhere to go from here?\n\nExperiment: Try the “Practice Ideas” suggested in the comments above.\nTune Hyperparameters: Adjust the learning rate, batch size, or number of epochs.\nData Augmentation: Implement more complex transforms for the training data.\nDifferent Models: Try other pre-trained architectures from torchvision.models.\nMetrics: Use libraries like torchmetrics or scikit-learn for more detailed evaluation (precision, recall, F1-score).\nLearning Rate Scheduling: Implement a learning rate scheduler (torch.optim.lr_scheduler).\nSaving/Loading: Add code to save your trained model’s state_dict and load it later (as shown in the lecture).\n\nKeep practicing and exploring!",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/permafrost-discovery-gateway.html",
    "href": "sections/permafrost-discovery-gateway.html",
    "title": "9  Permafrost Discovery Gateway",
    "section": "",
    "text": "Overview\nWe have more satellite imagery data than what we know what to do with. There are many people with different knowledges and common passions for permafrost-affected landscapes. At the same time, Alaska and the Arctic at large are starving for basic geospatial information about it’s permafrost-affected landscape and infrastructure and people may have a hard time finding others to help make visions come true. The Permafrost Discovery Gateway (PDG) is an online free tool meant to empower a) researchers to produce and do science with big geospatial data (think sub-meter resolution maps across Alaska and the entire Arctic) and b) agencies and community leaders in land and infrastructure management that involves permafrost. PDG currently offers easy visual exploration via a regular web-browser of datasets that otherwise crush traditional GIS software due to the file size. In the works are, for example, new big datasets of permafrost thaw features and infrastructure, partial dataset download tool, the incorporation of statistical summaries and AI tools that help the user find interesting stories in the big data products, plug-and-play workflows to help you develop your own big dataset, and the monitoring of permafrost thaw near-real time. The PDG can become a gateway where data and people can connect, where technology enables anyone with an internet connection, no matter your technical skills and resources, to connect, explore, and together create.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Permafrost Discovery Gateway</span>"
    ]
  },
  {
    "objectID": "sections/permafrost-discovery-gateway.html#slides",
    "href": "sections/permafrost-discovery-gateway.html#slides",
    "title": "9  Permafrost Discovery Gateway",
    "section": "Slides",
    "text": "Slides\nDownload Slides",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Permafrost Discovery Gateway</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html",
    "href": "sections/ai-ethics.html",
    "title": "10  AI Ethics",
    "section": "",
    "text": "Goal\nReview FAIR and CARE Principles, and their relevance to data ethics. Examine how ethical considerations are shared and considered at the Arctic Data Center. Discuss ethical considerations in AI and machine learning.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#intro-to-data-ethics",
    "href": "sections/ai-ethics.html#intro-to-data-ethics",
    "title": "10  AI Ethics",
    "section": "10.1 Intro to Data Ethics",
    "text": "10.1 Intro to Data Ethics\nThe Arctic Data Center is an openly-accessible data repository. The data published through it is open for anyone to reuse, subject to one of two license: CC-0 Public Domain and CC-By Attribution 4.0. As an open access repository, we prioritize long-term preservation and embrace principles from the data stewardship community, which established a set of best practices for open data management. n adherence, two principles adopted by the Arctic Data Center are FAIR Principles (Findable, Accessible, Interoperable, and Reproducible) and CARE Principles for Indigenous Governance (Collective Benefit, Authority to Control, Responsibility, Ethics). Both of which serve as frameworks in how to consider data ethics.\nThe FAIR Principles\nFAIR speaks to how metadata is managed, stored, and shared.\n\nWhat is the difference between FAIR principles and open science?\nFAIR principles and open science are overlapping concepts, but are distinctive from one another. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data. The FAIR principles place emphasis on machine readability, “distinct from peer initiatives that focus on the human scholar” (Wilkinson et al 2016) and as such, do not fully engage with sensitive data considerations and with Indigenous rights and interests (Research Data Alliance International Indigenous Data Sovereignty Interest Group, 2019). Metadata can be FAIR but not open. For example, sensitive data (data that contains personal information) may not be appropriate to share, however sharing the anonymized metadata that is easily understandable will reduce research redundancy.\n\nResearch has historically perpetuated colonialism and represented extractive practices, meaning that the research results were not mutually beneficial. These issues also related to how data was owned, shared, and used. To address issues like these, the Global Indigenous Data Alliance (GIDA) introduced CARE Principles for Indigenous Data Governance to support Indigenous data sovereignty. CARE Principles speak directly to how the data is stored and shared in the context of Indigenous data sovereignty. CARE Principles (Carroll et al. 2020) stand for:\n\nCollective Benefit - Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data\nAuthority to Control - Indigenous Peoples’ rights and interests in Indigenous data must be recognized and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledges and geographical indicators, are represented and identified within data.\nResponsibility - Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples.\nEthics - Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n\nTo many, the FAIR and CARE principles are viewed by many as complementary: CARE aligns with FAIR by outlining guidelines for publishing data that contributes to open-science and at the same time, accounts for Indigenous’ Peoples rights and interests (Carroll et al. 2020).\nIn Arctic-based research, there is a paradigm shift to include more local Indigenous Peoples, their concerns, and knowledge throughout the research process (Loseto 2020). At the 2019 ArcticNet Annual Scientific Meeting (ASM), a 4-hour workshop was held between Indigenous and non-Indigenous participants to address the challenge of peer-reviewed publications arising when there is a lack of co-production and co-management in the research process between both groups. In the context of peer review, involving Indigenous People and Indigenous Knowledge (IK) not only can increase the validity of research findings, but also ensure the research is meaningful to those most impacted by it. Moreover, it gives power back to the appropriate people to decide who can be knowledge holders of Indigenous knowledge (Loseto et al. 2020). This example underscores the advocacy CARE framework for Indigenous sovereignty, emphasizing the essential integration of people and purpose in the peer review publication stage. Failure to do so perpetuates power imbalances between science institutions and Indigenous communities. Hence, an equitable framework would adhere to the idea ‘Not about us without us’. As an Arctic research community, it is important to reflect on ways we can continue to engage and incorporate Indigenous communities and if there are gaps to address. However, it is important there is no ‘one size fits all’ approach.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#ethics-at-the-arctic-data-center",
    "href": "sections/ai-ethics.html#ethics-at-the-arctic-data-center",
    "title": "10  AI Ethics",
    "section": "10.2 Ethics at the Arctic Data Center",
    "text": "10.2 Ethics at the Arctic Data Center\nTransparency in data ethics is a vital part of open science. Regardless of discipline, various ethical concerns are always present, including professional ethics such as plagiarism, false authorship, or falsification of data, to ethics regarding the handling of animals, to concerns relevant to human subjects research. As the primary repository for the Arctic program of the National Science Foundation, the Arctic Data Center accepts Arctic data from all disciplines. Recently, a new submission feature was released which asks researchers to describe the ethical considerations that are apparent in their research. This question is asked to all researchers, regardless of disciplines.\nSharing ethical practices openly, similar in the way that data is shared, enables deeper discussion about data management practices, data reuse, sensitivity, sovereignty and other considerations. Further, such transparency promotes awareness and adoption of ethical practices.\n \nInspired by CARE Principles for Indigenous Data Governance (Collective Benefit, Authority to Control, Responsibility, Ethics) and FAIR Principles (Findable, Accessible, Interoperable, Reproducible), we include a space in the data submission process for researchers to describe their ethical research practices. These statements are published with each dataset, and the purpose of these statements is to promote greater transparency in data collection and to guide other researchers. For more information about the ethical research practices statement, check out this blog.\nTo help guide researchers as they write their ethical research statements, we have listed the following ethical considerations that are available on our website. The concerns are organized first by concerns that should be addressed by all researchers, and then by discipline.\nConsider the following ethical considerations that are relevant for your field of research.\n\n10.2.1 Ethical Considerations for all Arctic Researchers\n\nResearch Planning\n\nWere any permits required for your research?\nWas there a code of conduct for the research team decided upon prior to beginning data collection?\nWas institutional or local permission required for sampling?\nWhat impact will your research have on local communities or nearby communities (meaning the nearest community within a 100 mile radius)?\n\nData Collection\n\nWere any local community members involved at any point of the research process, including study site identification, sampling, camp setup, consultation or synthesis?\nWere the sample sites near or on Indigenous land or communities?\n\nData Sharing and Publication\n\nHow were the following concerns accounted for: misrepresentation of results, misrepresentation of experience, plagiarism, improper authorship, or the falsification or data?\nIf this data is intended for publication, are authorship expectations clear for everyone involved? Other professional ethics can be found here",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#ai-ethics",
    "href": "sections/ai-ethics.html#ai-ethics",
    "title": "10  AI Ethics",
    "section": "10.3 AI Ethics",
    "text": "10.3 AI Ethics\nArtificial Intelligence (AI) can be thought of as the development of computer systems that can perform tasks we usually think require human intelligence, such as image recognition, language translation, or autonomous movement. The rapid development and adoption of AI tools in the past years, particularly machine learning algorithms, has revolutionized how big datasets are analyzed, transforming decision-making in all sectors of society. However, frameworks to examine the ethical considerations of AI are just emerging, and careful consideration of how to best develop and apply AI systems is essential to the responsible use of these new, rapidly changing tools. In this section, we will give an overview of the FAST Principles put forward by the Alan Turing Institute in their guide for the responsible design and implementation of AI systems [1].",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#the-fast-principles",
    "href": "sections/ai-ethics.html#the-fast-principles",
    "title": "10  AI Ethics",
    "section": "10.4 The FAST Principles",
    "text": "10.4 The FAST Principles\nFAST stands for Fairness, Accountability, Sustainability, and Transparency. The FAST principles aim to guide the ethical development of AI projects from their inception to deployment. The continuous involvement and commitment of software developers, domain experts, technical leads, project managers, rightsholders, and collaborators involved in the AI project is crucial to implement these principles successfully. The following is a brief overview of each of the FAST principles, we greatly encourage you to read through the Alan Turing Institute guide to learn more!\n\n\n\nLeslie, 2019",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#fairness",
    "href": "sections/ai-ethics.html#fairness",
    "title": "10  AI Ethics",
    "section": "10.5 Fairness",
    "text": "10.5 Fairness\nBias can enter at any point of a research project, from data collection and preprocessing, to model design and implementation. This is because AI projects, as any other, are created by human beings who (even with the best of intentions) can introduce error, prejudice, or misjudgement into a system. Fairness refers to the active minimization of bias and commitment to not harm others through the outcomes of an AI system. The FAST principles [1] suggest the following baseline for fairness:\n\nThe designers and users of AI systems ensure that the decisions and behaviours of their models do not generate discriminatory or inequitable impacts on affected individuals and communities. This entails that these designers and users ensure that the AI systems they are developing and deploying:\n\nAre trained and tested on properly representative, relevant, accurate, and generalisable datasets (Data Fairness)\nHave model architectures that do not include target variables, features, processes, or analytical structures (correlations, interactions, and inferences) which are unreasonable, morally objectionable, or unjustifiable (Design Fairness)\nDo not have discriminatory or inequitable impacts on the lives of the people they affect (Outcome Fairness)\nAre deployed by users sufficiently trained to implement them responsibly and without bias (Implementation Fairness)\n\n\n\n\n\n\n\n\nReal-life Example : Insufficient Radar Network\n\n\n\n\n\nThe following figure [2] shows coverage of the national Doppler weather network (green and yellow circles) over a demographic map of the Black population in the southeast US. This would be an example of an issue in data fairness, since radar coverage does not represent the population uniformly, leaving out areas with higher Black population. Problems with outcome fairness could ensue if this non-representative biases an AI model to under-predict weather impacts in such populations.\n\n\n\nMcGovern et al., 2022 by courtesy of Jack Sillin (CC BY 4.0).",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#accountability",
    "href": "sections/ai-ethics.html#accountability",
    "title": "10  AI Ethics",
    "section": "10.6 Accountability",
    "text": "10.6 Accountability\nAccountability in AI projects stems from the shared view that isolated AI models used to automate decisions are not morally responsible in the same way as a decision-making human. As outputs from AI models are increasingly used to make decisions that affect the environment and human lives, there is a critical need for competent human authorities to offer explanations and justifications for the development process, outputs, and ensuing decisions made by AI systems. Such answerability assignments can be challenging, as AI implementations are often the product of big development teams where the responsibility to answer for a project’s outcome may not be delineated, creating an issue known as “the problem of many hands.” The FAST principles encourage the following accountability implementation:\nAccountability by Design: All AI systems must be designed to facilitate end-to-end answerability and auditability. This requires both responsible humans-in-the-loop across the entire design and implementation chain as well as activity monitoring protocols that enable end-to-end oversight and review.\n\n\n\n\n\n\nReal-life Example: AI for natural disasters response\n\n\n\n\n\nAccountability and the ability to audit AI methods can be crucial when model outputs support critical decision-making, such as in natural disasters. In 2021, a New York Times investigation [3]] covered a private company’s premature release of outputs about neighborhoods most affected by potential earthquakes in Seattle. While the initial release erroneously did not show threats for non-residential areas, ensuing updated versions showed non-compatible predictions again. Although the company acknowledged that its AI models would not replace the first responder’s judgment, the lack of audibility and opacity in the model development hindered accountability for any party, ultimately eroding the public confidence in the tools and leading to a loss of public resources.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#sustainability",
    "href": "sections/ai-ethics.html#sustainability",
    "title": "10  AI Ethics",
    "section": "10.7 Sustainability",
    "text": "10.7 Sustainability\nSustainability in the FAST principles includes continuous assessment of the social impacts of an AI system and technical sustainability of the AI model. In the first consideration, the FAST principles advocate for performing a Stakeholder Impact Assessment (SIA) at different stages to help build confidence in the project and uncover unexpected risks or biases, among other benefits. The Alan Turing Institute guide shares a prototype of an SIA [1]. The core of technical sustainability is creating safe, accurate, reliable, secure, and robust AI systems. To achieve these technical goals, teams must implement thorough testing, performance metrics, uncertainty quantification, and be aware of changes to the underlying distribution of data, among other essential practices.\n\n\n\n\n\n\nReal-life Example: SpaceCows\n\n\n\n\n\nThe SpaceCows project [4], [5] in northern Australia is a collaboration between scientists, industry leaders, and local indigenous communities developing AI centered platforms to analyze GPS tracking data collected from feral cows alongside satellite imagery and weather data. Indigenous knowledge and traditional land owners have been at the center of the development, providing guidance and ultimately benefiting from the AI tools to protect their land and cultural sites.\n\n\n\nImportant indigenous cultural sites can be damaged by feral cattle. Image from CSIRO, SpaceCows: Using AI to tackle feral herds in the Top End.\n\n\nVideos with more information on SpaceCows:\nCSIRO rolls out world’s largest remote ‘space cows’ herd management system\nSpaceCows: Using AI to tackle feral herds in the Top End",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#transparency",
    "href": "sections/ai-ethics.html#transparency",
    "title": "10  AI Ethics",
    "section": "10.8 Transparency",
    "text": "10.8 Transparency\nUnder the FAST principles, transparency in AI projects refers to transparency about how an AI project was designed and implemented and the content and justification of the outcome produced by the AI model. To ensure process transparency, the project should show how the design and implementation included ethical, safety, and fairness considerations throughout the project. To clarify the content and explain the outcomes of an AI system, the project should offer plain language, non-technical explanations accessible to non-specialists that convey how and why a model performed the way it did. In this direction, it is essential to avoid a ‘mathematical glass box’ where the code and mathematics behind the algorithm are openly available, but there is a lack of rationale about how or why the model goes from input to output. Finally, the explanations about how the outcomes were produced should become the basis to justify the outcomes in terms of ethical permissibility, fairness, and trustworthiness. A careful consideration of the balance between the sustainability and transparency principles is necessary when dealing with protected or private data.\n\n\n\n\n\n\nReal-life Example: France’s Digital Republic Act\n\n\n\n\n\nThe concern for transparency in using personal data is an active space for debate. In 2018, the French government passed a law to protect citizens’ privacy, establishing the citizen’s “right to an explanation” regarding, among other things, how an algorithm contributed to decisions on their persona and which data was processed [6], [7]. Overall, this legislation aims to create a fairer and more transparent digital environment where everyone can enjoy equal opportunities.\n\n\n\nPhoto by Google DeepMind",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#discussion-activity",
    "href": "sections/ai-ethics.html#discussion-activity",
    "title": "10  AI Ethics",
    "section": "10.9 Discussion Activity",
    "text": "10.9 Discussion Activity\nGiven the discussion of the CARE priciples and the FAST principles, let’s discuss what responsible AI considerations might exist in the context of Arctic research, particularly with respect to Indigneous peoples of the Arctic. Geospatial data spanning the Arctic typically includes the traditional lands and waters of Arctic Indigenous peoples, and often intersects with current local communities distributed throughout the Arctic. Recent work by projects like Abundant Intelligences are starting to explore the intersection of Indigineous Knowledge systems, Artifical Intelligence models, and how to guide the “development of AI [to support] a more humane future”.\nLet’s take, for example, a researcher that wants to run an machine learning model to detect changes in environmental features at a large regional or Arctic scale. We’ve seen several of these so far, including 1) AI predictions of the distribution of permafrost ice wedges and retrogressive thaw slumps across the Arctic; 2) use of AI to detect changes in surface water extent and lake drainage events across the Arctic; 3) use of AI in a mechanistic process models that helps understand the global source/sink tradeoff of permafrost loss and its impact on climate.\n\n\n\n\n\n\nDiscussion questions\n\n\n\nDivide into 5 groups of 4, find a comfortable place to sit, and pick a large-scale AI application that is of interest to the group. Let’s discuss some of the following questions, and more…\n\nThinking of CARE, does that model provide Collective Benefit to Indigenous populations that it might impact?\nThinking of FAST, what would researchers need to do to ensure that their research process could meet the goals of the four categories of Fairness (Outcome Fairness, Data Fairness, Design Fairness, and Implementation Fairness) for Indigenous people in their research area?",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#conclusion",
    "href": "sections/ai-ethics.html#conclusion",
    "title": "10  AI Ethics",
    "section": "10.10 Conclusion",
    "text": "10.10 Conclusion\nAs new AI developments and applications rapidly emerge and transform everyday life, we need to pause and ensure these technologies are fair, sustainable, and transparent. We must acknowledge human responsibility in designing and implementing AI systems to use these novel tools fairly and with accountability. Finally, we acknowledge that the information covered here is a lightning introduction to AI’s ethical considerations and implications. Whether you are a researcher interested in using AI for the first time or a seasoned ML practitioner, we urge you to dive into the necessary and ever-expanding AI ethics work to learn how to best incorporate these concepts into your work.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#further-reading",
    "href": "sections/ai-ethics.html#further-reading",
    "title": "10  AI Ethics",
    "section": "10.11 Further Reading",
    "text": "10.11 Further Reading\nAcademic Data Science Alliance (ADSA) (2024) The Data Sciene Ethos https://ethos.academicdatascience.org [8]\nChen, W., & Quan-Haase, A. (2020) Big Data Ethics and Politics: Towards New Understandings. Social Science Computer Review. [9]\nCrawford, K., & Paglen, T. (2019) Excavating AI: The Politics of Training Sets for Machine Learning. [10]\nGray, J., & Witt, A. (2021) A feminist data ethics of care framework for machine learning: The what, why, who and how. First Monday, 26(12), Article number: 11833 [11]\n\n\n\n\n[1] D. Leslie, “Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector,” Zenodo, Jun. 2019. doi: 10.5281/zenodo.3240529.\n\n\n[2] A. McGovern, I. Ebert-Uphoff, D. J. G. Ii, and A. Bostrom, “Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science,” Environmental Data Science, vol. 1, p. e6, Jan. 2022, doi: 10.1017/eds.2022.5.\n\n\n[3] S. Fink, “This High-Tech Solution to Disaster Response May Be Too Good to Be True,” The New York Times, Aug. 2019, Accessed: Oct. 19, 2024. [Online]. Available: https://www.nytimes.com/2019/08/09/us/emergency-response-disaster-technology.html\n\n\n[4] T. Shepherd, “Indigenous rangers to use SpaceCows program to protect sacred sites and rock art from feral herds,” The Guardian, Sep. 2021, Accessed: Oct. 19, 2024. [Online]. Available: https://www.theguardian.com/australia-news/2021/sep/15/indigenous-rangers-to-use-spacecows-program-to-protect-sacred-sites-and-rock-art-from-feral-herds\n\n\n[5] CSIRO, “SpaceCows: Using AI to tackle feral herds in the Top End.” Accessed: Oct. 19, 2024. [Online]. Available: https://www.csiro.au/en/news/All/News/2021/September/SpaceCows-Using-AI-to-tackle-feral-herds-in-the-Top-End\n\n\n[6] L. Edwards and M. Veale, “Enslaving the Algorithm: From a ‘Right to an Explanation’ to a ‘Right to Better Decisions’?” Social Science Research Network, Rochester, NY, 2018. doi: 10.2139/ssrn.3052831.\n\n\n[7] S. Lo Piano, “Ethical principles in machine learning and artificial intelligence: Cases from the field and possible ways forward,” Humanities and Social Sciences Communications, vol. 7, no. 1, pp. 1–7, Jun. 2020, doi: 10.1057/s41599-020-0501-9.\n\n\n[8] A. D. S. A. (ADSA), “The Data Science Ethos - Operationalizing Ethics in Data Science,” The Data Science Ethos. Accessed: Oct. 19, 2024. [Online]. Available: https://ethos.academicdatascience.org/\n\n\n[9] W. Chen and A. Quan-Haase, “Big Data Ethics and Politics: Toward New Understandings,” Social Science Computer Review, vol. 38, no. 1, pp. 3–9, Feb. 2020, doi: 10.1177/0894439318810734.\n\n\n[10] “Excavating AI.” Accessed: Oct. 19, 2024. [Online]. Available: https://excavating.ai/\n\n\n[11] J. Gray and A. Witt, “A feminist data ethics of care for machine learning: The what, why, who and how,” First Monday, Dec. 2021, doi: 10.5210/fm.v26i12.11833.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html",
    "href": "sections/guest-lecture-yili-arts-dataset.html",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "",
    "text": "Overview\nIn this session, we will introduce and explore the Arctic Retrogressive Thaw Slump (ARTS) dataset. We aim to illuminate the background and motivation behind the ARTS dataset, detail its design elements including functions, metadata, and usage, and underscore its defining features such as scalability, scientific integrity, and the potential for community contribution.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html#outline",
    "href": "sections/guest-lecture-yili-arts-dataset.html#outline",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "Outline",
    "text": "Outline\n\nBackground and motivation of the Arctic\nRetrogressive Thaw Slump (ARTS) data set.\nSource data for the ARTS data set\nDesign of the data set - functions, metadata, usage\nFeatures of the data set - scalable, scientific, contributable\nData Curation Framework - standards, protocols\nThe ARTS repository - user and contributor guideline\nQuestions and discussions",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html#lecturer",
    "href": "sections/guest-lecture-yili-arts-dataset.html#lecturer",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "Lecturer",
    "text": "Lecturer\nYili Yang, Ph.D.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html#slides",
    "href": "sections/guest-lecture-yili-arts-dataset.html#slides",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "Slides",
    "text": "Slides\nDownload Slides",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html#reference",
    "href": "sections/guest-lecture-yili-arts-dataset.html#reference",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "Reference",
    "text": "Reference\n\nhttps://github.com/whrc/ARTS",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html",
    "href": "sections/exploring-advanced-neural-networks.html",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "",
    "text": "Overview\nIn this section, we will explore the journey of the evolution of neural networks, starting with basic classification networks and progressing to more complex tasks of object detection and instance segmentation.\nWe’ll begin with a recap of classification networks, which are designed to identify the presence of objects in images and assign class labels. From there, we’ll move to the realm of object detection, exploring the development of R-CNN 1, Fast R-CNN 2, and Faster R-CNN 3. Each of these models represents a step forward in detecting and localizing objects within images, with Faster R-CNN introducing a more efficient and integrated approach.\nFinally, we’ll delve into Mask R-CNN 4, which extends the capabilities of Faster R-CNN by adding a branch for predicting segmentation masks. This enhancement allows us to not only detect objects but also delineate their exact shapes, enabling precise instance segmentation.\nOur goal is to build on your existing understanding of classification networks, providing you with practical insights into how these networks can be adapted and expanded for more advanced computer vision applications. By the end of this section, you will be equipped with the knowledge to understand and apply these techniques to real-world challenges.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#what-is-instance-segmentation",
    "href": "sections/exploring-advanced-neural-networks.html#what-is-instance-segmentation",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "12.1 What is instance segmentation?",
    "text": "12.1 What is instance segmentation?\n\n\n\nSource: https://manipulation.csail.mit.edu/segmentation.html\n\n\nTo grasp the concept of instance segmentation, let’s compare it with other related tasks:\n\nImage recognition: This is the most basic task, where the model identifies the presence of objects in an image and assigns class probabilities. It’s like saying, “There are sheep and a dog here,” without specifying their locations.\nSemantic segmentation: This advances to classifying each pixel into a category, effectively coloring the entire image based on object classes. However, it doesn’t distinguish between individual instances of the same class. For example, all sheep might be colored the same, treating them as a single entity.\nObject detection: This task detects objects and distinguishes them by drawing bounding boxes around each, providing more detail than image recognition. It identifies individual objects but lacks fine-grained, pixel-level detail.\nInstance segmentation: This is the most detailed task, combining the strengths of object detection and semantic segmentation. It not only distinguishes individual objects but also delineates their exact shapes at the pixel level. It allows for precise analysis and manipulation.\n\n\n\n\n\n\n\nNote\n\n\n\nInstance segmentation is more challenging and computationally intensive than other tasks, and it’s not always necessary for every application.\n\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nConsider which of these tasks would be most useful for your current projects or applications.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#importance-and-applications-of-instance-segmentation",
    "href": "sections/exploring-advanced-neural-networks.html#importance-and-applications-of-instance-segmentation",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "12.2 Importance and applications of instance segmentation",
    "text": "12.2 Importance and applications of instance segmentation\nInstance segmentation plays a crucial role in various fields. Its importance stems from the following factors:\n\nFine-grained precision: Unlike object detection, which only provides bounding boxes, instance segmentation offers pixel-level accuracy. This precision is essential for tasks where understanding the exact shape and position of objects matters.\nObject differentiation: It not only identifies the presence of objects but also distinguishes between individual instances of the same class. This capability is valuable in scenarios where multiple objects of the same type need to be analyzed separately.\nIntegration in complex systems: By providing detailed object masks, instance segmentation enables more sophisticated interactions with objects in the environment, such as robotics and augmented reality applications.\n\nApplications:\n\nMedical imaging: In medical applications, instance segmentation can help identify and analyze specific structures within images, such as tumors or organs 5.\nRobotics: In robotics, instance segmentation enables robots to perceive and interact with their environment more effectively, distinguishing between different objects and obstacles 6.\nEnvironmental monitoring: Instance segmentation can be used in environmental mapping to detect and classify natural features from satellite imagery. This approach enhances the ability to monitor changes in landscapes across large areas with high accuracy 78.\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nConsider how the ability to identify not just the presence of objects, but also their precise contours, might enhance applications you are familiar with.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#transitioning-from-backbone-networks-to-classification-networks",
    "href": "sections/exploring-advanced-neural-networks.html#transitioning-from-backbone-networks-to-classification-networks",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "12.3 Transitioning from backbone networks to classification networks",
    "text": "12.3 Transitioning from backbone networks to classification networks\n In the previous section, we discussed how deep learning models consist of three core components: input adaptation, feature extractor, and output adaptation. Now, let’s explore how these components are configured starting from a backbone network, like ResNet-50, to create a classification network.\nResNet-50 as a backbone network\nUsing a ResNet-50 backbone network as an example, let’s first examine the structure of the model:\n\n\n\n\n\n\nCode snippet to load a pre-trained ResNet-50 model\n\n\n\n\n\nimport torch\nimport torchvision.models as models\n\n# Load the pre-trained ResNet-50 model\nmodel = models.resnet50(pretrained=True)\n\n# Print the model architecture\nprint(model)\n\n\n\nThe output of the code snippet above provides the architecture of the ResNet-50 model, which includes a series of convolutional and batch normalization layers, typically used for feature extraction:\n\n\n\n\n\n\nOutput: ResNet-50 architecture\n\n\n\n\n\nResNet(\n(conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n(bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n(layer1): Sequential(\n(0): BasicBlock(\n(conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n(1): BasicBlock(\n(conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n)\n(layer2): Sequential(\n(0): BasicBlock(\n(conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(downsample): Sequential(\n(0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n(1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n)\n(1): BasicBlock(\n(conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n(layer3): Sequential(\n(0): BasicBlock(\n(conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(downsample): Sequential(\n(0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n(1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n)\n(1): BasicBlock(\n(conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n)\n(layer4): Sequential(\n(0): BasicBlock(\n(conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(downsample): Sequential(\n(0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n(1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n)\n(1): BasicBlock(\n(conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n)\n(avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n(fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n)\n\n\n\nThe output above is a liitle verbose, so let’s summarize the key components of the ResNet-50 model:\n\n\n\n\n\n\nSummary: ResNet-50 architecture\n\n\n\n\n\nResNet(\n(conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n(bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n(relu): ReLU(inplace=True)\n(maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n(layer1): Sequential()\n(layer2): Sequential()\n(layer3): Sequential()\n(layer4): Sequential()\n(avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n(fc): Linear(in_features=512, out_features=1000, bias=True)\n)\nwhere layerx represents the convolutional blocks in the network.\n\n\n\nModifying the ResNet-50 model to perform classification\nTo modify the ResNet-50 model to perform classification, we first need to define the task. For this example, we will classify images into 10 classes, e.g., 10 different types of food. So, for the data, we have\n\ninput: an image\noutput: a 10-dimensional vector of class probabilities\n\n\n\n\n\n\n\nQuick Thought\n\n\n\nHow would you modify the ResNet-50 model to perform classification?\n\n\nLet’s examine the three components of the model and see how we can modify them to perform classification.\n\n\n\n\n\n\nInput Adaptation\n\n\n\n\n\nNo changes needed. The ResNet-50 model is designed to accept images as input, which is suitable for our task.\n\n\n\n\n\n\n\n\n\nFeature Extractor\n\n\n\n\n\nNo changes needed. The pre-trained ResNet-50 model’s layers are retained for feature extraction, leveraging its ability to learn complex patterns from images.\n\n\n\n\n\n\n\n\n\nOutput Adaptation\n\n\n\n\n\nFrom the summary of the ResNet-50 architecture, we can see that the output layer is a linear layer with 1000 output features.\n(fc): Linear(in_features=512, out_features=1000, bias=True)\nIt is designed to output 1000 class probabilities, and we need to modify it to output 10 class probabilities for our task. To do this, we can replace the linear layer with a new linear layer that has 10 output features.\nimport torch.nn as nn\n\n# Replace the last linear layer with a new linear layer that has 10 output features\nmodel.fc = nn.Linear(512, 10)\n\n\n\nDone! We have modified the ResNet-50 model to perform classification. Here is the complete code:\n\n\n\n\n\n\nComplete code\n\n\n\n\n\nimport torch\nimport torchvision.models as models\nimport torch.nn as nn\n\n# Load the pre-trained ResNet-50 model\nmodel = models.resnet50(pretrained=True)\n\n# Replace the last linear layer with a new linear layer that has 10 output features\nmodel.fc = nn.Linear(512, 10)\n\n# Print the model architecture\nprint(model)\n\n\n\n\n\n\n\n\n\nQuick Thought\n\n\n\n\nUsing the ResNet-50 model as a backbone network, is there other way to modify the model to perform classification?\nWith the classification task as an example, try to think of how to modify the model to perform instance segmentation.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#a-step-by-step-guide-to-instance-segmentation",
    "href": "sections/exploring-advanced-neural-networks.html#a-step-by-step-guide-to-instance-segmentation",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "12.4 A step-by-step guide to instance segmentation",
    "text": "12.4 A step-by-step guide to instance segmentation\nTo achieve instance segmentation, let’s begin from object detection and progress to instance segmentation since object detection is like a simplified version of instance segmentation. Below, we list the input and output of object detection and instance segmentation.\n\n\n\n\n\n\n\n\nTask\nInput\nOutput\n\n\n\n\nObject Detection\nImage\nBounding boxes and class labels for each instance\n\n\nInstance Segmentation\nImage\nBounding boxes, masks and class labels for each instance\n\n\n\n\n12.4.1 The First Step: R-CNN\nOur journey begins with the introduction of Region-based Convolutional Neural Networks (R-CNN).\nImagine you are tasked with finding specific items in a crowded room (the image). The first step you need is a strategy to pinpoint potential areas where the items might be located. This is where Selective Search 9 comes into play. It acts like a diligent assistant that systematically scans the image, suggesting potential regions that might contain the items. Once you have these potential regions, the next step is to understand what each region contains. In our example, we use ResNet-50 to extract features from each region. With the features in hand, we then use a linear Support Vector Machine (SVM) to classify each region into one of the 10 classes. But recognizing objects is just part of the story. To ensure precision in locating these objects, we employ a linear regression model to refine the coordinates of the bounding boxes.\nSummary of R-CNN components:\n\nRegion proposal (selective search): Generate potential object regions.\nFeature extraction (ResNet-50): Extract features from each region.\nClassification (linear SVM): Classify each region into one of the 10 classes.\nBounding box regression (linear regression): Refine the coordinates of the bounding boxes.\n\nLimitations of R-CNN:\n\nSlow inference: Processes each region independently, leading to inefficiency.\nComplex training: Requires multiple models and stages.\nInefficient feature extraction: Redundant computations for overlapping regions.\n\n\n\n12.4.2 Evolution to Fast R-CNN\nContinuing our journey towards instance segmentation, we move from the foundational R-CNN to a more advanced and efficient model: Fast R-CNN. This evolution is necessary to overcome the challenges faced by R-CNN.\nIn our quest for faster and more efficient object detection, Fast R-CNN addresses the limitations of its predecessor, R-CNN. Imagine now that instead of examining each potential object region individually, you first take a comprehensive view of the entire scene. This panoramic approach forms the basis of Fast R-CNN.\n\nFeature extraction (single pass): Fast R-CNN starts by processing the entire image in a single forward pass through the network, using a shared convolutional feature map. This method not only accelerates the process but also eliminates the need to repeatedly extract features for overlapping regions, directly addressing the inefficiency of R-CNN.\nRegion proposal (selective search): Although it continues to use Selective Search to suggest possible object regions, the way these proposals are handled is integrated into a more streamlined workflow.\nRoI pooling: To efficiently zoom into specific regions of interest (RoIs), Fast R-CNN introduces RoI Pooling. This component crops and resizes the shared feature map for each proposed region, much like focusing a camera lens on the areas of interest. RoI Pooling ensures that each region is presented in a consistent shape to the classifier, optimizing the use of features extracted in the initial pass.\nClassification and bounding box regression (fully connected layers): Fast R-CNN employs fully connected layers to perform both classification and bounding box regression simultaneously. This integration simplifies the training pipeline and ensures that both tasks benefit from the shared feature representation, in contrast to the separate models used in R-CNN.\n\nSummary of Fast R-CNN Components:\n\nFeature extraction (single pass): Processes the entire image once, reducing redundancy and speeding up inference.\nRegion proposal (selective search): Still used, but proposals are more efficiently integrated into the workflow.\nRoI pooling: Efficiently refines regions of interest for classification and bounding box adjustments.\nClassification and bounding box regression (fully connected layers): Conducts these tasks concurrently, simplifying the model architecture.\n\nSolving R-CNN Limitations:\n\nSlow inference: Addressed by processing the entire image in a single pass, reducing computational load.\nComplex training pipeline: Simplified by integrating classification and regression within a unified model.\nInefficient feature extraction: Eliminated by using shared feature maps, reducing redundant processing.\n\nLimitations of Fast R-CNN:\n\nDependence on slow region proposal methods: The reliance on Selective Search for region proposals limits the speed of the overall system.\nSeparate proposal and detection stages: Although integrated more efficiently than R-CNN, the use of Selective Search still represents a bottleneck.\nPotential for further speed improvements: While faster than R-CNN, Fast R-CNN does not fully utilize potential speed optimizations through end-to-end training of the proposal mechanism.\n\n\n\n12.4.3 Introducing Faster R-CNN\nAs we progress in our journey towards instance segmentation, we arrive at Faster R-CNN, which further refines the processes established by Fast R-CNN. This model addresses the bottlenecks that persisted in previous approaches, particularly regarding region proposal efficiency.\n\n\n12.4.4 The Innovation of Faster R-CNN\nFaster R-CNN represents a significant leap forward in object detection by introducing a component that eliminates the dependence on slow external region proposal methods. This innovation stems from the integration of a Region Proposal Network (RPN), which transforms the object detection process into a unified, end-to-end trainable system.\n\nFeature extraction (single pass): Like Fast R-CNN, Faster R-CNN begins by processing the entire image in a single pass through a convolutional network to produce a shared feature map. This ensures efficient feature extraction and serves as the foundation for both region proposal and object detection tasks.\nRegion proposal network (RPN): The key innovation of Faster R-CNN is the RPN, which is integrated directly into the convolutional feature map. The RPN efficiently generates region proposals by sliding a small network over the shared feature map. This network outputs a set of objectness scores and refined bounding box coordinates for anchors at each spatial location, effectively replacing the slow Selective Search process used in Fast R-CNN.\nRoI pooling: After generating region proposals, Faster R-CNN employs RoI Pooling to extract fixed-size feature maps from the shared feature map for each proposal. This ensures that each region of interest is consistently prepared for the subsequent classification and bounding box refinement stages.\nClassification and bounding box regression (fully connected layers): The proposals are then fed into fully connected layers for simultaneous classification and bounding box regression, refining both the class labels and bounding box coordinates.\n\nSummary of Faster R-CNN Components:\n\nFeature extraction (single pass): Processes the entire image once, providing a shared feature map for all subsequent tasks.\nRegion proposal network (RPN): Generates high-quality region proposals directly from the feature map, replacing slower methods.\nRoI pooling: Standardizes proposal feature maps for further processing.\nClassification and bounding box regression (fully connected layers): Conducts these tasks effectively and efficiently, using the refined proposals.\n\nAddressing Fast R-CNN Limitations:\n\nDependence on slow region proposal methods: Solved by integrating the RPN, which provides fast and efficient region proposals.\nSeparate proposal and detection stages: Unified into a single, end-to-end trainable model.\nPotential for further speed improvements: Realized through the RPN, which allows for significant speed gains and paves the way for real-time object detection capabilities.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#transition-to-instance-segmentation-mask-r-cnn",
    "href": "sections/exploring-advanced-neural-networks.html#transition-to-instance-segmentation-mask-r-cnn",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "12.5 Transition to Instance Segmentation: Mask R-CNN",
    "text": "12.5 Transition to Instance Segmentation: Mask R-CNN\nHaving refined the processes of object detection with Faster R-CNN, we now transition to the realm of instance segmentation with Mask R-CNN. This model not only detects objects but also delineates each instance with precise pixel-level masks, taking the capabilities of Faster R-CNN a step further.\nMask R-CNN builds upon the robust framework of Faster R-CNN, adding a crucial component for instance segmentation. It retains the speed and efficiency of Faster R-CNN while introducing new functionality without compromising performance.\n\nFeature extraction (single pass): Mask R-CNN, like its predecessor, begins by processing the entire image in a single pass through a convolutional network to produce a shared feature map. This feature map serves as the foundation for both region proposal and instance segmentation tasks.\nRegion proposal network (RPN): The RPN continues to play a pivotal role in generating high-quality region proposals directly from the shared feature map. This component ensures that only the most promising regions are considered for further processing.\nRoI Align: To address the spatial misalignment introduced by RoI Pooling in Faster R-CNN, Mask R-CNN introduces RoI Align. This new component improves the precision of feature extraction by avoiding quantization errors, ensuring that the extracted feature maps for each region of interest are perfectly aligned with the input image.\nClassification, bounding box regression, and mask prediction (parallel heads): Mask R-CNN extends the two-head structure of Faster R-CNN by adding a third branch specifically for mask prediction. This additional branch predicts a binary mask for each region of interest, allowing the model to not only classify objects and refine bounding boxes but also generate accurate pixel-level masks.\n\nSummary of Mask R-CNN Components:\n\nFeature extraction (single pass): Processes the entire image once, ensuring efficient use of computational resources.\nRegion proposal network (RPN): Continues to provide fast and efficient region proposals.\nRoI Align: Refines feature extraction for precise spatial alignment.\nClassification, bounding box regression, and mask prediction (parallel heads): Conducts these tasks simultaneously, enabling comprehensive instance segmentation.\n\nEnhancements from Faster R-CNN:\n\nPixel-level precision: Achieved through RoI Align, improving spatial alignment and mask accuracy.\nInstance segmentation capability: Added through the mask prediction branch, enabling the model to generate detailed masks for each object instance.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#conclusion",
    "href": "sections/exploring-advanced-neural-networks.html#conclusion",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "12.6 Conclusion",
    "text": "12.6 Conclusion\nWe’ve explored the evolution from R-CNN to Mask R-CNN, showing how each step builds on the last to improve performance. This process demonstrates how you can apply similar improvements to tackle a wide range of advanced tasks, not just instance segmentation. By understanding these developments, you’re equipped to enhance existing models and create innovative solutions for various challenges in computer vision.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#footnotes",
    "href": "sections/exploring-advanced-neural-networks.html#footnotes",
    "title": "12  Exploring Advanced Neural Networks: Instance Segmentation",
    "section": "",
    "text": "R. Girshick, J. Donahue, T. Darrell, J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” CVPR 2014.↩︎\nR. Girshick, “Fast R-CNN,” ICCV 2015.↩︎\nS. Ren, K. He, R. Girshick, J. Sun, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” NIPS 2015.↩︎\nK. He, G. Gkioxari, P. Dollár, R. Girshick, “Mask R-CNN,” ICCV 2017.↩︎\nhttps://www.linkedin.com/pulse/medical-image-diagnosis-roles-object-detection-segmentation-egvcc↩︎\nhttps://www.youtube.com/watch?v=tNLtXb04i3w↩︎\nZhang, W., Witharana, C., Liljedahl, A. K., & Kanevskiy, M. (2018). Deep convolutional neural networks for automated characterization of arctic ice-wedge polygons in very high spatial resolution aerial imagery. Remote Sensing, 10(9), 1487.↩︎\nLi, W., Hsu, C. Y., Wang, S., Witharana, C., & Liljedahl, A. (2022, November). Real-time GeoAI for high-resolution mapping and segmentation of arctic permafrost features: the case of ice-wedge polygons. In Proceedings of the 5th ACM SIGSPATIAL international workshop on AI for geographic knowledge discovery (pp. 62-65).↩︎\nJ. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders, “Selective search for object recognition,” International Journal of Computer Vision, vol. 104, no. 2, pp. 154-171, 2013.↩︎",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Instance Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "",
    "text": "Goal\nThis session introduces participants to Detectron2, a cutting-edge open-source library developed by Meta AI Research (FAIR) for state-of-the-art object detection and segmentation tasks. Built on the PyTorch deep learning framework, Detectron2 offers a flexible and modular design that makes it easier for researchers and developers to implement and experiment with advanced computer vision models.\nIn this session, participants will be introduced to the fundamentals of Detectron2 and its application in analyzing visual data specific to Arctic research. We will cover the installation process, basic usage, and explore how to train custom models to detect and segment objects relevant to Arctic studies. By the end of this tutorial, participants will have hands-on experience with Detectron2 and understand how to apply it to their own research projects.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#why-use-a-deep-learning-framework",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#why-use-a-deep-learning-framework",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.1 Why use a deep learning framework?",
    "text": "13.1 Why use a deep learning framework?\nDeep learning frameworks like Detectron2 or MMSegmentation provide advantages over core libraries like PyTorch directly. Here are some of the key benefits:\n\nEase of use and rapid prototyping: These frameworks offer pre-built, state-of-the-art model architectures and components that simplify the process of developing complex models. This allows researchers to quickly prototype and experiment with different models without having to worry about the underlying implementation details.\nModular and extensible design: These frameworks are designed to be highly modular, enabling researchers to easily customize and extend various components such as model architectures, training pipelines, evaluation metrics, and more. This flexibility allows for tailored solutions that can be adapted to specific research questions or datasets.\nOptimized performance: These frameworks often include optimized implementations of popular algorithms and techniques, which can lead to faster and more efficient model training and inference. Also, these frameworks are designed to be scalable, allowing researchers to train models on large datasets and scale up to high-performance hardware, e.g., multi-GPU systems.\nCommunity and support: Detectron2 has a large community and extensive documentation, making it easier to find resources and support.\nFocus on research: By abstracting many of the low-level details, these frameworks allow researchers to focus on the core research questions and innovations, enabling faster iteration on new ideas and research directions.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#configuration-system",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#configuration-system",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.2 Configuration system",
    "text": "13.2 Configuration system\nDetectron2’s configuration system provides a flexible and modular way to manage training and evaluation settings. It supports both YAML-based configuration files and Python-based lazy configurations, allowing users to define model architectures, datasets, training hyperparameters, and more.\n\nWhy use the configuration system?\n\nReproducibility: Configs make it easy to share and reproduce experiments.\nModularity: Different components (e.g., dataset, model, solver) are easily interchangeable.\nEase of Use: Instead of defining hyperparameters in scripts, you can manage them in a structured format.\n\n\n\nLazyConfig vs. Default Config\nDetectron2 offers two types of configuration systems:\n\nDefault Config (YAML-based): The traditional configuration system that loads from .yaml files.\nLazyConfig (Python-based): A more flexible system that supports Python expressions and function calls.\n\n\n\nBenefits of LazyConfig\n\nAllows defining configurations programmatically in Python.\nSupports dynamic configurations with function calls.\nReduces redundancy and improves maintainability.\n\n\n\nUsing LazyConfig\n\nLoading a base configuration\n\nLazyConfig organizes configurations as Python modules. You can start by loading a base configuration:\nfrom detectron2.config import LazyConfig\ncfg = LazyConfig.load(\"detectron2/projects/ViTDet/configs/mae_finetune.py\")\n\nModifying configurations\n\nSince LazyConfig is Python-based, modifications can be made directly:\ncfg.train.init_checkpoint = \"path/to/checkpoint.pth\"\ncfg.dataloader.train.dataset.names = \"coco_2017_train\"\ncfg.train.max_iter = 50000\n\nRegistering a new configuration\n\nYou can create your own configuration by defining a Python script:\nfrom detectron2.config import LazyCall\nfrom detectron2.modeling import build_model\n\nconfig = dict(\n    model=LazyCall(build_model)(\n        backbone=dict(type=\"ResNet\", depth=50),\n        num_classes=80,\n    ),\n    solver=dict(\n        base_lr=0.002,\n        max_iter=10000,\n    ),\n)\nSave this file as my_config.py and load it using:\ncfg = LazyConfig.load(\"my_config.py\")\n\nRunning training with LazyConfig\n\nTo train a model using a LazyConfig setup, use:\npython train_net.py --config-file my_config.py\n\n\nBest practices\n\nOrganize configs in a structured manner: Keep different configurations (model, dataset, solver) separate for better maintainability.\nUse function calls: Leverage Python functions to make dynamic changes to configurations.\nExperiment tracking: Store modified configs alongside experiment logs to ensure reproducibility.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#data-system",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#data-system",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.3 Data system",
    "text": "13.3 Data system\n\nDataset registration\nDetectron2 does not assume a fixed dataset format; instead, it requires datasets to be registered before use. Dataset registration involves providing metadata and a function that loads dataset samples.\nRegistering a custom dataset\nDetectron2 uses the DatasetCatalog and MetadataCatalog to manage datasets. To utilize a custom dataset, you need to register it so that Detectron2 knows how to access and interpret your data.\n\nImplement a Function to Load Your Dataset: Create a function that returns your dataset in the format of a list of dictionaries. Each dictionary should contain information about an image and its annotations.\n\ndef my_dataset_function():\n    # Load your dataset and return it as a list of dictionaries\n    return dataset_dicts\n\nRegister the dataset: Use DatasetCatalog.register() to associate your dataset with the function you’ve implemented.\n\nfrom detectron2.data import DatasetCatalog\n\nDatasetCatalog.register(\"my_dataset\", my_dataset_function)\nThis registration allows Detectron2 to access your dataset using the name my_dataset.\nBuilt-in datasets\nDetectron2 includes several built-in datasets such as COCO and LVIS. To use them, install the required dataset and enable it in the config.\nfrom detectron2.data.datasets import register_coco_instances\nregister_coco_instances(\"my_coco\", {}, \"path/to/annotations.json\", \"path/to/images\")\n\n\nData loading\nDetectron2 uses DatasetMapper for loading datasets efficiently. The dataset loader transforms raw dataset samples into a format suitable for training.\nCustom DatasetMapper\nA custom dataset mapper allows applying preprocessing steps before training.\nfrom detectron2.data import DatasetMapper\nfrom detectron2.data import detection_utils as utils\nimport torch\n\nclass MyDatasetMapper(DatasetMapper):\n    def __call__(self, dataset_dict):\n        dataset_dict = dataset_dict.copy()\n        image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n        dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1))\n        return dataset_dict\n\n\nConfiguring DataLoader\nDetectron2 provides a flexible data loader that can be modified based on batch size, augmentations, and transformations.\n\nBuild a data loader: Use build_detection_train_loader() for training and build_detection_test_loader() for evaluation.\n\nfrom detectron2.data import build_detection_train_loader\n\ndata_loader = build_detection_train_loader(cfg)\nfor batch in data_loader:\n    print(batch)  # Process batch\nThese functions handle batching, shuffling, and other data loading operations.\n\n\nData Augmentation\nData augmentation is a technique to improve model generalization by applying random transformations to the input data during training. Detectron2 integrates detectron2.data.transforms for efficient data augmentation.\n\nDefine augmentations: Create a list of augmentation operations.\n\nfrom detectron2.data import transforms as T\n\naugmentations = [\n    T.RandomBrightness(0.9, 1.1),\n    T.RandomFlip(prob=0.5),\n    T.RandomCrop(\"absolute\", (640, 640))\n]\n\nApply augmentations: Integrate these augmentations into your data loading pipeline.\n\nfrom detectron2.data import DatasetMapper\n\nmapper = DatasetMapper(cfg, is_train=True, augmentations=augmentations)\ndata_loader = build_detection_train_loader(cfg, mapper=mapper)",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#model-system",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#model-system",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.4 Model system",
    "text": "13.4 Model system\n\nBuilding Models from Configuration\nDetectron2 employs configuration files to define model architectures and parameters. To construct a model from a configuration, you can use the build_model function:\nfrom detectron2.modeling import build_model\nmodel = build_model(cfg)  # cfg is a configuration object\nThis function initializes the model structure with random parameters. To load pre-trained weights or previously saved parameters, utilize the DetectionCheckpointer:\nfrom detectron2.checkpoint import DetectionCheckpointer\ncheckpointer = DetectionCheckpointer(model)\ncheckpointer.load(file_path_or_url)  # Load weights from a file or URL\n\n\nModel Input and Output Formats\nDetectron2 models accept inputs as a list of dictionaries, each corresponding to an image. The required keys in these dictionaries depend on the model type and its mode (training or evaluation). Typically, for inference, the dictionary includes:\n\n“image”`: A tensor representing the image in (C, H, W) format.\n“height”and“width”` (optional): Desired output dimensions.\n\nDuring training, additional keys like \"instances\" (which contains ground truth annotations) are necessary. The model outputs are also structured as dictionaries, with formats varying based on the specific task (e.g., bounding box detection, segmentation).\n\n\nCustomizing and Extending Models\nDetectron2’s modular design allows for extensive customization. You can modify existing components or add new ones to tailor the models to your requirements. A common approach is to register new components, such as a custom backbone:\nfrom detectron2.modeling import BACKBONE_REGISTRY, Backbone, ShapeSpec\nimport torch.nn as nn\n\n@BACKBONE_REGISTRY.register()\nclass CustomBackbone(Backbone):\n    def __init__(self, cfg, input_shape):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=16, padding=3)\n\n    def forward(self, image):\n        return {\"conv1\": self.conv1(image)}\n\n    def output_shape(self):\n        return {\"conv1\": ShapeSpec(channels=64, stride=16)}\nAfter defining and registering your custom component, you can specify it in your configuration:\ncfg.MODEL.BACKBONE.NAME = \"CustomBackbone\"\nmodel = build_model(cfg)\nThis method allows you to integrate new architectures or functionalities seamlessly. For more detailed instructions on writing and registering new model components, see the official tutorial on writing models.\n\n\nConstructing Models with Explicit Arguments\nWhile configuration files offer a convenient way to build models, there are scenarios where you might need more control. In such cases, you can construct model components with explicit arguments in your code. For example, to use a custom ROI head:\nfrom detectron2.modeling.roi_heads import StandardROIHeads\n\nclass CustomROIHeads(StandardROIHeads):\n    def __init__(self, cfg, input_shape):\n        super().__init__(cfg, input_shape)\n        # Customize as needed\nThen, integrate it into your model:\nfrom detectron2.modeling import build_model\n\ncfg = ...  # your configuration\nmodel = build_model(cfg, roi_heads=CustomROIHeads(cfg, input_shape))",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#training-system",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#training-system",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.5 Training system",
    "text": "13.5 Training system\n\nHow Training Works in Detectron2\nAt its core, training a model in Detectron2 involves:\n\nLoading a dataset: Preparing images and annotations.\n\nConfiguring the model: Defining architecture, hyperparameters, and other settings.\n\nTraining with a Trainer: Using built-in tools or writing a custom loop.\n\nEvaluating performance: Running inference and analyzing metrics.\n\nDetectron2 provides two main ways to handle training:\n\nUsing a pre-built Trainer (Recommended for most users)\n\nWriting a custom training loop (For advanced customization)\n\n\n\nUsing the Default Trainer (Easy & Standard Approach)\nIf you want to train a model with minimal setup, Detectron2 offers DefaultTrainer, which automates most of the process.\nSteps to Train a Model with DefaultTrainer\nStep 1: Prepare Your Configuration\nModify a base config file and set paths, batch sizes, and hyperparameters.\nfrom detectron2.config import get_cfg\n\ncfg = get_cfg()\ncfg.merge_from_file(\"path/to/config.py\")\ncfg.DATASETS.TRAIN = (\"my_dataset_train\",)\ncfg.DATASETS.TEST = (\"my_dataset_val\",)\ncfg.OUTPUT_DIR = \"./output\"\ncfg.SOLVER.MAX_ITER = 5000  # Number of training iterations\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # Adjust for your dataset\nStep 2: Train Using DefaultTrainer\nfrom detectron2.engine import DefaultTrainer\n\ntrainer = DefaultTrainer(cfg)\ntrainer.resume_or_load(resume=False)\ntrainer.train()\nThis will handle data loading, logging, checkpointing, and evaluation automatically.\nStep 3: Evaluate the Model\nTo test your model on a validation dataset:\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\n\nevaluator = COCOEvaluator(\"my_dataset_val\", cfg, False, output_dir=\"./output/\")\nval_loader = build_detection_test_loader(cfg, \"my_dataset_val\")\nprint(inference_on_dataset(trainer.model, val_loader, evaluator))\n\n\nCustomizing Training (For Advanced Users)\nIf DefaultTrainer doesn’t fit your needs, you can modify it or write a fully custom training loop.\nOption 1: Overriding DefaultTrainer Methods\nFor example, if you need a custom evaluation function:\nclass MyTrainer(DefaultTrainer):\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n\ntrainer = MyTrainer(cfg)\ntrainer.train()\nOption 2: Using Hooks for Extra Functionality\nHooks allow you to add logic at specific points during training. For example, printing a message every 100 iterations:\nfrom detectron2.engine import HookBase\n\nclass PrintIterationHook(HookBase):\n    def after_step(self):\n        if self.trainer.iter % 100 == 0:\n            print(f\"Iteration {self.trainer.iter}\")\n\ntrainer = DefaultTrainer(cfg)\ntrainer.register_hooks([PrintIterationHook()])\ntrainer.train()\nOption 3: Writing a Fully Custom Training Loop\nFor full control, you can write your own loop instead of using DefaultTrainer:\nfrom detectron2.engine import SimpleTrainer\nfrom detectron2.solver import build_optimizer\n\nmodel = build_model(cfg)\noptimizer = build_optimizer(cfg, model)\ndata_loader = build_detection_train_loader(cfg)\n\ntrainer = SimpleTrainer(model, data_loader, optimizer)\ntrainer.train()\nThis is useful when experimenting with novel training strategies.\n\n\nLogging and Monitoring Training Progress\nDetectron2 provides event storage to track training metrics like loss and accuracy. You can log custom metrics inside your model:\nfrom detectron2.utils.events import get_event_storage\n\nstorage = get_event_storage()\nstorage.put_scalar(\"my_custom_metric\", value)\nFor visualization, you can use TensorBoard:\ntensorboard --logdir ./output\nThis helps you track training progress interactively.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#evaluation-system",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#evaluation-system",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.6 Evaluation system",
    "text": "13.6 Evaluation system\nIn Detectron2, evaluation is managed through the DatasetEvaluator interface. This interface processes pairs of inputs and outputs, aggregating results to compute performance metrics. Detectron2 offers several built-in evaluators tailored to standard datasets like COCO and LVIS. For instance, the COCOEvaluator computes metrics such as Average Precision (AP) for object detection, instance segmentation, and keypoint detection. Similarly, the SemSegEvaluator is designed for semantic segmentation tasks.\n\nUtilizing Built-in Evaluators\nTo evaluate a model using Detectron2’s built-in evaluators, you can employ the inference_on_dataset function. This function runs the model on all inputs from a specified data loader and processes the outputs using the chosen evaluators.\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\n\n# Initialize the evaluator\nevaluator = COCOEvaluator(\"your_dataset_name\", cfg, False, output_dir=\"./output/\")\n\n# Create a data loader for the test dataset\nval_loader = build_detection_test_loader(cfg, \"your_dataset_name\")\n\n# Perform inference and evaluation\neval_results = inference_on_dataset(model, val_loader, evaluator)\nIn this script, replace \"your_dataset_name\" with the name of your dataset as registered in Detectron2. The COCOEvaluator is initialized with the dataset name and configuration (cfg). The build_detection_test_loader function creates a data loader for the test dataset. Finally, inference_on_dataset runs the model on the test data and evaluates the results using the evaluator.\n\n\nCreating Custom Evaluators\nWhile Detectron2’s built-in evaluators cover many standard scenarios, you might encounter situations where custom evaluation logic is necessary. In such cases, you can implement your own evaluator by extending the DatasetEvaluator class. This involves defining methods to reset the evaluator, process each batch of inputs and outputs, and compute the final evaluation metrics. For example, to create an evaluator that counts the total number of detected instances across the validation set:\nfrom detectron2.evaluation import DatasetEvaluator\n\nclass InstanceCounter(DatasetEvaluator):\n    def reset(self):\n        self.count = 0\n\n    def process(self, inputs, outputs):\n        for output in outputs:\n            self.count += len(output[\"instances\"])\n\n    def evaluate(self):\n        return {\"total_instances\": self.count}\nIn this custom evaluator, the reset method initializes the count, the process method updates the count based on the number of instances in each output, and the evaluate method returns the total count. You can integrate this custom evaluator into your evaluation pipeline alongside built-in evaluators:\nfrom detectron2.evaluation import DatasetEvaluators\n\n# Combine multiple evaluators\nevaluator = DatasetEvaluators([COCOEvaluator(\"your_dataset_name\", cfg, False), InstanceCounter()])\n\n# Perform inference and evaluation\neval_results = inference_on_dataset(model, val_loader, evaluator)\nBy combining evaluators, you can perform comprehensive evaluations in a single pass over the dataset, which is efficient and convenient.\nEvaluating on Custom Datasets\nWhen working with custom datasets, it’s essential to ensure they adhere to Detectron2’s standard dataset format. This compatibility allows you to leverage existing evaluators like COCOEvaluator for your custom data. If your dataset follows the COCO format, you can use the COCOEvaluator directly. Otherwise, you might need to implement a custom evaluator tailored to your dataset’s structure and evaluation criteria.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html",
    "href": "sections/ai-workflows-and-mlops.html",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "",
    "text": "Instructors\nBen Galewsky, Sr. Research Software Engineer National Center for Supercomputing Applications (NCSA) University of Illinois Urbana-Champaign",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#overview",
    "href": "sections/ai-workflows-and-mlops.html#overview",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "Overview",
    "text": "Overview\nMachine learning models have become a vital tool for most branches of science. The process and tools for training these models on the lab’s desktop is often fragile, slow, and not reproducible. In this workshop, we will introduce the concept of MLOps, which is a set of practices that aims to streamline the process of developing, training, and deploying machine learning models. We will use the popular open source MLOps tool, MLflow, to demonstrate how to track experiments, package code, and deploy models. We will also introduce Garden, a tool that allows researchers to publish ML Models as citable objects.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#outline",
    "href": "sections/ai-workflows-and-mlops.html#outline",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to MLOps\nIntroduction to MLflow\nTracking experiments with MLflow\nPackaging code with MLflow\nDeploying models with MLflow\nPublishing models with Garden",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#three-challenges-for-ml-in-research",
    "href": "sections/ai-workflows-and-mlops.html#three-challenges-for-ml-in-research",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.1 Three Challenges for ML in Research",
    "text": "14.1 Three Challenges for ML in Research\n\nTraining productivity\nTraining reproducibility\nModel citability",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#introcution-to-mlops",
    "href": "sections/ai-workflows-and-mlops.html#introcution-to-mlops",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.2 Introcution to MLOps",
    "text": "14.2 Introcution to MLOps\nMachine Learning Operations (MLOps) is a set of practices that combines Machine Learning, DevOps, and Data Engineering to reliably and efficiently deploy and maintain ML models in production. Just as DevOps revolutionized software development by streamlining the bridge between development and operations, MLOps brings similar principles to machine learning systems.\nThink of MLOps as the infrastructure and practices that transform ML projects from experimental notebooks into robust, production-ready systems that deliver real reproducible scientific value.\n\nWhy Researchers Need MLOps\nAs a researcher, you’ve likely experienced the following challenges:\n\nInability to harness computing resources to robustly search hyperparameter space\nDifficulty reproducing results from six months ago\nRetraining is too painful to consider creating better models after new data becomes available\nTracking experiments becomes unwieldy as projects grow\nCollaboration among researchers in your group is difficult\nAdvisors have little visibility into students’ work\n\nMLOps addresses these pain points by providing:\n\n1. Reproducibility\n\nVersion control for data, code, and models\nAutomated documentation of experiments\nContainerization for consistent environments\n\n\n\n2. Automation\n\nAutomated training pipelines\nContinuous integration and deployment (CI/CD)\nAutomated testing and validation\n\n\n\n3. Production Monitoring\n\nReal-time performance monitoring\nData drift detection\nAutomated model retraining triggers\n\n\n\n4. Governance\n\nModel lineage tracking\n\n\nMLOps isn’t just another buzzword—it’s a crucial evolution in how we develop and deploy ML systems. As models become more complex and requirements for reliability increase, MLOps practices become essential for successful ML implementations.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#mlflow-a-comprehensive-platform-for-the-ml-lifecycle",
    "href": "sections/ai-workflows-and-mlops.html#mlflow-a-comprehensive-platform-for-the-ml-lifecycle",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.3 MLflow: A Comprehensive Platform for the ML Lifecycle",
    "text": "14.3 MLflow: A Comprehensive Platform for the ML Lifecycle\n\nWhat is MLflow?\nMLflow is an open-source platform designed to manage the end-to-end machine learning lifecycle. Created by Databricks, it provides a unified set of tools that address the core challenges in developing, training, and deploying machine learning models. MLflow is language-agnostic and can work with any ML library, algorithm, or deployment tool.\n\n\nThe MLFlow Tracking Server\nThere are two components to MLFlow, the Python, R, and Java libraries that allow you to instrument your training code and a tracking server that records training runs and hosts the model repository. For serious production use, you will need a hosted tracking server. Fortunately, for tutorials and personal use, you can run a local tracking server.\n\n\nKey Components of MLflow\nMLFlow consists of three main components:\n\n\n\n\n\n\nTracking\n\n\n\n\n\nThe tracking server of MLFlow allows researchers to log and compare model parameters, metrics, and artifacts across multiple runs. With MLFlow’s tracking features, users can record hyperparameters, evaluation metrics, model versions, and even source code, making it easier to reproduce results and collaborate with team members. The platform provides a user-friendly interface to visualize and compare different experiments, helping practitioners identify the most promising models and configurations. Additionally, MLFlow’s tracking capabilities integrate seamlessly with popular ML frameworks, enabling users to incorporate experiment logging into their existing workflows with minimal code changes. This comprehensive approach to tracking enhances model development efficiency and facilitates better decision-making throughout the machine learning process.\n\nKey Concepts in Tracking:\n\nParameters: key-value inputs to your code\nMetrics: numeric values (can update over time)\nArtifacts: arbitrary files, including data, models and plots\nSource: training code that ran\nVersion: version of the training code\nTags and Notes: any additional information\n\nimport mlflow\n\nwith mlflow.start_run():\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_metric(\"accuracy\", 0.85)\n    mlflow.log_artifact(\"model.pkl\")\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\n\n\n\nMLFlow Projects provide a standardized format for packaging and organizing machine learning code to make it reproducible and reusable across different environments. A Project in MLFlow is essentially a directory containing code, dependencies, and an MLProject file that specifies the project’s entry points and environment requirements. This structure enables data scientists to share their work with teammates who can reliably execute the same code, regardless of their local setup. The MLProject file can define multiple entry points, each specifying its parameters and command to run, making it flexible for different use cases within the same project. MLFlow supports various environments for project execution, including conda environments, Docker containers, and system environments, ensuring consistency across different platforms. This standardization not only improves collaboration but also simplifies the deployment process, as projects can be easily versioned and moved between development and production environments.\n\nKey Concepts in Projects\n\nPackaging format for reproducible ML runs\n\nAny code folder or GitHub repository\nOptional MLproject file with project configuration\n\nDefines dependencies for reproducibility\n\nConda (+ R, Docker, …) dependencies can be specified in MLproject\nReproducible in (almost) any environment\n\nExecution API for running projects\n\nCLI / Python / R / Java\nSupports local and remote execution\n\n\nname: myproject\n\npython_env: python_env.yaml\n\nentry_points:\n  main:\n    parameters:\n      learning_rate: {type: float, default: 0.01}\n    command: \"python train.py --lr {learning_rate}\"\n\n\n\n\n\n\n\n\n\n\nModels\n\n\n\n\n\nThe Models component of MLFlow provides a standardized way to package and deploy machine learning models across different platforms and serving environments. MLFlow’s model format includes all the code and dependencies required to run the model, making it highly portable and easy to share. The platform supports a variety of popular ML frameworks like scikit-learn, TensorFlow, and PyTorch, allowing models to be saved in a framework-agnostic format using the MLFlow Model Registry. This registry acts as a centralized repository where teams can version their models, transition them through different stages (like staging and production), and maintain a clear lineage of model iterations. MLFlow also provides built-in deployment capabilities to various serving platforms such as Kubernetes, Amazon SageMaker, and Azure ML, streamlining the process of moving models from experimentation to production. Additionally, MLFlow’s model serving feature allows for quick local deployment of models as REST endpoints, enabling easy testing and integration with other applications.\n\nKey Concepts in Models\n\nPackaging format for ML Models\n\nAny directory with MLmodel file\n\nDefines dependencies for reproducibility\n\nConda environment can be specified in MLmodel configuration\n\nModel creation utilities\n\nSave models from any framework in MLflow format\n\nDeployment APIs\n\nCLI / Python / R / Java\n\nModel Versioning and Lifecycle -Model repository",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#scaling-up-training-with-mlflow-slurm",
    "href": "sections/ai-workflows-and-mlops.html#scaling-up-training-with-mlflow-slurm",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.4 Scaling Up Training With MLFlow-Slurm",
    "text": "14.4 Scaling Up Training With MLFlow-Slurm\nWith your project defined in MLProject file, it’s easy to scale up your workflows by launching them onto a cluster.\nThere is a plugin for MLFlow developed by NCSA called mlflow-slurm\nTo use, you have to create a json file that tells the plugin how to configure slurm jobs.\n{\n  \"partition\": \"cpu\",\n  \"account\": \"bbmi-delta-cpu\",\n  \"mem\": \"16g\",\n  \"modules\": [\"anaconda3_cpu\"]\n}\nWith this in place you can launch a training run on your cluster with the command\nmlflow run --backend slurm \\\n          --backend-config slurm_config.json \\\n          examples/sklearn_elasticnet_wine",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#how-mlflow-solves-common-mlops-challenges",
    "href": "sections/ai-workflows-and-mlops.html#how-mlflow-solves-common-mlops-challenges",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.5 How MLflow Solves Common MLOps Challenges",
    "text": "14.5 How MLflow Solves Common MLOps Challenges\n\nTraining productivity\n\nTrack impact of hyperparameter and code changes on model quality\nRun hyperparameter sweeps and find best run\nSwitch between desktop to supercomputer\n\n\n\nTraining reproducibility\n\nEnforced use of reproducible runtime environments\nTrace models back to specific runs\n\n\n\nModel citeability\n\nPublish models to repository\nVersioning and lifecycle events",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#hands-on-tutorial",
    "href": "sections/ai-workflows-and-mlops.html#hands-on-tutorial",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.6 Hands-On Tutorial",
    "text": "14.6 Hands-On Tutorial\nFor this tutorial you will need a local JupyterLab running from a conda environment as outlined in Section 0.\nYou will also need a local MLFlow tracking server.\nTo install and run an MLFlow tracking server locally, you first need to install MLFlow via pip with:\npip install mlflow\nOnce installed, you can start a basic tracking server by running the command\nmlflow server --backend-store-uri sqlite:///mlflow.db\nThis sets up a server using SQLite for storage, creates a local directory for artifacts, binds to all network interfaces, and runs on port 5000. You can then access the MLFlow UI by navigating to http://localhost:5000 in your web browser, where you can view, compare, and manage your machine learning experiments.\nWe will be using a GPU powered JuypterHub provided by NCSA. Connection instructions will be provided in the classroom.\nThe example code is in the Cyber2A-RTS-ToyModel repo. It has a notebook along with some libraries to keep the notebook focused on the MLOps aspects of the code.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#sharing-models",
    "href": "sections/ai-workflows-and-mlops.html#sharing-models",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.7 Sharing Models",
    "text": "14.7 Sharing Models\nNow that we have trained and validated a model we will want to first of all, share it with other members of our research group.\n\nMLFlow Model Repository\nExpert users within our research group will have access to the MLFlow tracking server and model repository. You can test the model as an artifact from an existing run. Once you are satisfied with its performance, you can publish it to the MLFlow model repository with the Register Model button on the tracking server.\nPublished models are given sequential version numbers so colleauges can rely on a stable model for their research. Models in the repository can also follow a lifecycle with MLFlow model aliases. Members of the research group who are not activly involved in model development may just want to use the current best model. The researcher who is training the model can decide which version others should use. MLFlow allows you to pull down the model symbolically.\nmlflow.pyfunc.load_model(\"models:/rts@prod\")\n\n\nModels as Citable Objects\nPublishing your MLProject and training code to a Git repo and making the data publicly readable through a data repository is a way for others to reproduce your models. However, to make your work truly reusable it is better to publish the weights of your trained model in a way that is findable, citable, and usable.\nAt a minimum, you should publish your model on Hugging Face. You can include a README and an notebook demonstrating how to use the model. HuggingFace allows you to mint a DOI that you can cite in your publications.\nHere’s an example with the RTS model:\n\nhttps://huggingface.co/bengal1/RTS/tree/main\n\nA new facility called Garden takes this a step further. Your model is hosted as an endpoint which is a hosted function-as-a-service which allows anyone to perform inference with your model without needing to install anything.\nOur example model is hosted at 10.26311/x49j-2v19\nYou can run a remote inference:\nfrom garden_ai import GardenClient\ngarden_client = GardenClient()\n\nrts_garden = garden_client.get_garden('10.26311/5fb6-f950')\nimage_url=\"https://github.com/cyber2a/Cyber2A-RTS-ToyModel/blob/main/data/images/valtest_yg_055.jpg?raw=true\"\npred = rts_garden.identify_rts(image_url)",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#reference",
    "href": "sections/ai-workflows-and-mlops.html#reference",
    "title": "14  AI Workflows and MLOps: From Development to Deployment",
    "section": "14.8 Reference",
    "text": "14.8 Reference\n\nMLflow\nGarden",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html",
    "href": "sections/hands-on-lab-ai-workflows.html",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "",
    "text": "Goal\nThis lab section will give students experience publishing MLModels as citeable objects that can be easily found and reused by other researchers. Students will learn how to package a model up as an executable function and publish it to garden.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#prerequisites",
    "href": "sections/hands-on-lab-ai-workflows.html#prerequisites",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Prerequisites",
    "text": "Prerequisites\nWe will be using the Modal service to create and host executable python functions. In order to use this you will need on your laptop:\n\nPython 3.9 or higher\nA free Globus account (https://www.globus.org/)\nA free modal account (https://www.modal.com/)\n\nYou will need to let the instructor know the email address associated with your globus account to be added to the garden publishers group.\nOnce you are added to the garden publishers group, you should be able to see the Garden Model Upload Form",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#assumptions",
    "href": "sections/hands-on-lab-ai-workflows.html#assumptions",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Assumptions",
    "text": "Assumptions\nDuring this lab we will use some pre-trained models to demonstrate the process of packaging and publishing a model. You are also welcome to use your own models if you have them.\nIn order for your model to be published to garden, it must be:\n\nOpen and already trained\n\nThe code is open source and in a package or a public repository\nYour model weights are stored somewhere public (or you can put them somewhere public)\n\nYour model isn’t massive\n\nYour model can fit in one GPU’s memory\nYour model weights are on the order of 10 GB or less",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#step-1-hello-garden",
    "href": "sections/hands-on-lab-ai-workflows.html#step-1-hello-garden",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Step 1: Hello, Garden!",
    "text": "Step 1: Hello, Garden!\nAll of the examples in this lab are available in the workshop repository. Clone the repository to your local machine:\ngit clone https://github.com/cyber2a/ml-workflows.git\nIn this step we will upload an existing trivial model to garden and execute it from a Google Colab notebook.\nUse the hello_garden.py from the example repo and upload it from the garden form.\nKeep it a “test garden” and don’t worry too hard about the metadata\nOnce you’ve created it, go make sure you can invoke it remotely. Try out this google colab notebook to see it in action.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#step-2-learn-how-to-iterate-on-modal-apps",
    "href": "sections/hands-on-lab-ai-workflows.html#step-2-learn-how-to-iterate-on-modal-apps",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Step 2: Learn how to iterate on Modal apps",
    "text": "Step 2: Learn how to iterate on Modal apps\nGarden uses the modal service to host models and make it easy to perform inference from a noteook. This is a commercial service, but the free tier is entirely sufficient for most purposes and won’t require a credit card.\nIn this step we will learn more about creating and debugging a modal app.\nInstall the modal package:\npip install modal\nDownload needs_a_tweak.py and try running it with\nmodal run needs_a_tweak.py\nEdit and re-run til it works the way you want it to.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#step-3-stage-and-execute-a-model",
    "href": "sections/hands-on-lab-ai-workflows.html#step-3-stage-and-execute-a-model",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Step 3: Stage and execute a model",
    "text": "Step 3: Stage and execute a model\nNow we will create a modal function that has your model’s weights baked into the container so that our functions don’t need to download the model every time they run.\nYour model weights will need to live somewhere publicly accessible. Garden can’t pull them from your machine.\n\nSee git_model_staging.py for an example of how to use weights hosted in GitHub or Hugging Face.\nSee figshare_model_staging.py for an example of how to pull weights from a FigShare download link.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#step-4-run-a-real-model",
    "href": "sections/hands-on-lab-ai-workflows.html#step-4-run-a-real-model",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Step 4: Run a “Real Model”",
    "text": "Step 4: Run a “Real Model”\nTake a look at the Retrogressive Thaw Slump model. You can execute the model from the google colab notebook to see it in action.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-ai-workflows.html#next-steps",
    "href": "sections/hands-on-lab-ai-workflows.html#next-steps",
    "title": "15  Hands-On Lab: AI Workflows",
    "section": "Next Steps",
    "text": "Next Steps\nNow that you have successfully published a model to garden, you can share the link with your colleagues and collaborators. You can also use the model in your own research projects by invoking it from a Google Colab notebook or other python environment.\nThe Garden project provides $5/month in free compute credits to all users. This can go a surprisingly long way, however if you are planning to do high throughput infrence, then you may need to BYOM (Bring Your Own Modal). You can take the python function you uploaded to garden and directly uplaod to Modal and run it from your own account.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hands-On Lab: AI Workflows</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html",
    "href": "sections/foundation-models.html",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "",
    "text": "16.1 Overview\nFoundation models (FM) are deep learning models trained on massive raw unlabelled datasets usually through self-supervised learning. FMs enable today’s data scientists to use them as the base and fine-tune using domain specific data to obtain models that can handle a wide range of tasks (language, vision, reasoning etc.). In this chapter, we provide an introduction to FMs, its history, evolution, and go through its key features and categories, and a few examples. We also briefly discuss how foundation models work. This chapter will be a precursor to the hands-on session that follows on the same topic.\nIn this session, we take a closer look at what constitutes a foundation model, a few examples, and some basic principles around how it works.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#overview",
    "href": "sections/foundation-models.html#overview",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "",
    "text": "Fig : Image source- 2021 paper on foundation models by Stanford researchers [1]",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#outline",
    "href": "sections/foundation-models.html#outline",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "16.2 Outline",
    "text": "16.2 Outline\n\nOverview of foundation models\nTypes of foundation models\nArchitecture\nSegment Anything Model (SAM 2)\nRetrieval Augmented Generation",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#introduction",
    "href": "sections/foundation-models.html#introduction",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "16.3 Introduction",
    "text": "16.3 Introduction\n\n16.3.1 Traditional ML vs Deep Learning vs Foundation Models\nTraditional machine learning involves algorithms that learn patterns from structured data. Techniques like decision trees, support vector machines, and linear regression fall under this category. These methods often require feature engineering, where domain knowledge is used to select and transform input features to improve model performance. Traditional machine learning excels in scenarios with limited data and interpretable results.\nDeep learning is a subset of machine learning that employs neural networks with multiple layers (hence “deep”). These models automatically learn features from raw data, making them particularly powerful for complex tasks like image and speech recognition. Deep learning excels with large datasets and can capture intricate patterns but often requires significant computational resources and can be harder to interpret compared to traditional methods.\nFoundation models, such as GPT and BERT, represent a new paradigm in AI. These large-scale models are pre-trained on vast amounts of data and can be fine-tuned for specific tasks with minimal additional training. Earlier neural networks were narrowly tuned for specific tasks. With a little fine-tuning, foundation models can handle jobs from translating text to analyzing medical images. Foundation models generally learn from unlabeled datasets, saving the time and expense of manually describing each item in massive collections. Foundation models leverage transfer learning, allowing them to generalize across different tasks more effectively than traditional machine learning and deep learning models.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#foundation-models",
    "href": "sections/foundation-models.html#foundation-models",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "16.4 Foundation Models",
    "text": "16.4 Foundation Models\nFoundation models, introduced in 2021 by Standford Researchers [1], are characterized by their enormous neural networks trained on vast datasets through self-supervised learning. These models serves as a “foundation” on which many task-specific models can be built by adaptation. Their capabilities improves with more data, requiring substantial computational power for training. These models can be adapted to various downstream tasks and are designed for reuse, leveraging transfer learning to enhance performance across different applications.\n\n\n\n\n\nFig : 2021 paper on foundation models by Stanford researchers [1]\n\n\nWith the start of availability of big data for training, evidence showed that performance improves with size. The field came to the conclusion that scale matters, and with the right model architecture, intelligence comes with large-scale data.\nHere’s a few examples of foundation models and their parameter count:\n\nCLIP [2] - 63 million parameters\nBERT [3] - 345 million parameters\nGPT-3 [3] - 175 billion parameters\n\nWikipedia consists of only 3% of its training data\n\nGPT-4 [4] - 1.8 trillion parameters\n\n\n\n\nFig : Growth in compute power. (Source: GPT-3 paper [3])",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#types-of-foundation-models",
    "href": "sections/foundation-models.html#types-of-foundation-models",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "16.5 Types of foundation models",
    "text": "16.5 Types of foundation models\nFoundation models can be classified on the basis of its modality and its underlying architecture.\n\n\n\nCriteria: Modality\nCriteria: Architecture\n\n\n\n\nLanguage Models\nTransformer Models\n\n\nVision Models\nGenerative Models\n\n\nMultimodal Models\nDiffusion Models\n\n\n\n\n16.5.1 Types of foundation models (Modality)\n\n16.5.1.1 Language models\nLanguage models are trained for natural language processing tasks. The primary training objective for LLMs is often next-token prediction, where the model learns to predict the next word in a sequence given the preceding context. This is achieved through a vast amount of text data, enabling the model to learn grammar, facts, and even some reasoning patterns. LLMs tend to be good at various NLP related tasks, like translation, conversational AI, sentiment analysis, content summarization etc., to name a few.\nHere’s some examples of language models:\n\nGPT-3\nGPT-4\nLlama 3.2 [5]\n\n\n\n16.5.1.2 Vision models\nVision models are trained for computer vision tasks. The primary training objective of vision models is to effectively learn representations that enable accurate predictions or useful transformations based on visual data. Vision models tend to be good at tasks like object detection, segmentation, facial recognition, etc.\nHere’s some examples of vision models:\n\nGPT-4-turbo\nSAM [6]\nCLIP [5]\nSwin-transformer [7]\n\n\n\n16.5.1.3 Multimodal models\nMultimodal models are designed to process and understand multiple types of data modalities, such as text, images, audio, and more. These models can handle various data types simultaneously, allowing them to learn relationships and correlations between different forms of information (e.g., associating text descriptions with images). By training on datasets that include multiple modalities, multimodal foundation models learn to create a unified representation space where different types of data can be compared and processed together. This often involves shared architectures for encoding different modalities. These models can often perform well on tasks they haven’t been specifically trained on, thanks to their ability to leverage learned relationships across modalities. This makes them versatile for applications in various domains. Many multimodal models, like CLIP and DALL-E, use contrastive learning to improve their understanding of how different modalities relate. They aim to maximize similarity between paired data (e.g., an image and its caption) while minimizing similarity between unrelated pairs. These models can often perform well on tasks they haven’t been specifically trained on, thanks to their ability to leverage learned relationships across modalities. This makes them versatile for applications in various domains. Multimodal foundation models are used in diverse areas such as image and video captioning, visual question answering, cross-modal retrieval, and interactive AI systems that require understanding and generating multiple types of content.\nHere’s some examples of multimodal foundation models:\n\nGPT-4o\nDALL-E [8]\nCLIP [5]\nSora [9]\nGemini [10]\n\n\n\n\n16.5.2 Types of foundation models (Architecture)\n\n16.5.2.1 Transformer models\nIntroduced in 2017 by the paper “Attention is all you need” [11], the transformer architecture revolutionized NLP by enabling models to efficiently capture complex relationships in data without the limitations of recurrence. This architecture is known for its ability to handle sequential data efficiently. Its parallel processing capabilities and scalability have made it a foundational model for many state-of-the-art systems in various domains, including image processing and speech recognition. Checkout “The Illustrated Transformer” (blog post)[https://jalammar.github.io/illustrated-transformer/] for a detailed overview of the transformer architecture.\n\n\n\nFig : Transformer architecture\n\n\n\n16.5.2.1.1 Attention Mechanism\nAttention is, to some extent, motivated by how we pay visual attention to different regions of an image or correlate words in one sentence [12]. We can explain the relationship between words in one sentence or close context. When we see “eating”, we expect to encounter a food word very soon. The color term describes the food, but probably not so much with “eating” directly.\n\n\n\nFig : One word attends to other words in the same sentence differently\n\n\nCheck out Lilian Weng’s blog post [12] and MIT class on deep learning for detailed overview of attention mechanism.\n\n\n16.5.2.1.2 Key components of transformer architecture:\n\nSelf-Attention Mechanism:\n\n\nPurpose: Allows the model to weigh the importance of different words in a sequence relative to each other, capturing dependencies regardless of their distance.\nFunction: For each input token, self-attention computes a set of attention scores that determine how much focus to place on other tokens. This is done using three vectors: Query (Q), Key (K), and Value (V).\nCalculation: The attention score is computed as a dot product of Q and K, followed by a softmax operation to normalize it. The output is a weighted sum of the V vectors based on these scores.\n\nIn the example below, the self-attention mechanism enables us to learn the correlation between the current words and the previous part of the sentence.\n\n\n\nFig : The current word is in red and the size of the blue shade indicates the activation level [13]\n\n\n\nPositional Encoding:\n\n\nPurpose: Since transformers do not have a built-in notion of sequential order, positional encodings are added to the input embeddings to provide information about the position of tokens in the sequence.\nImplementation: Positional encodings use sine and cosine functions of different frequencies to generate unique values for each position.\n\n\nMulti-Head Attention:\n\n\nFunction: Instead of having a single set of attention weights, the transformer employs multiple attention heads, each learning different aspects of the relationships between tokens.\nProcess: The input is split into multiple sub-vectors, and self-attention is applied in parallel. The outputs of each head are concatenated and linearly transformed.\n\n\nFeed-Forward Networks:\n\n\nPurpose: After the multi-head attention step, each token’s representation is passed through a feed-forward neural network, which applies transformations independently to each position.\nStructure: Typically consists of two linear transformations with a ReLU activation in between.\n\n\nLayer Normalization and Residual Connections:\n\n\nLayer Normalization: Applied to stabilize and speed up training by normalizing the outputs of each layer.\nResidual Connections: Shortcuts are added around sub-layers (e.g., attention and feed-forward) to facilitate the flow of gradients during training, helping to mitigate the vanishing gradient problem.\n\n\nStacking Layers:\n\n\nTransformers consist of multiple layers of multi-head attention and feed-forward networks, allowing for deep representations of the input data.\n\n\nOutput Layer:\n\n\nFor tasks like language modeling or translation, the final layer typically uses a linear transformation followed by a softmax activation to predict the next token or class.\n\nThere are more than 50 major transformer models [14]. The transformer architecture is versatile and can be configured in different ways. The transformer architecture can support both auto-regressive and non-auto-regressive configurations depending on how the self-attention mechanism is applied and how the model is trained.\n\nAuto-Regressive Models: In an auto-regressive setup, like the original GPT (Generative Pre-trained Transformer), the model generates text one token at a time. During training, it predicts the next token in a sequence based on the previously generated tokens, conditioning on all prior context. This means that at each step, the model only attends to the tokens that come before the current position, ensuring that future tokens do not influence the prediction.\nNon-Auto-Regressive Models: Other models, like BERT (Bidirectional Encoder Representations from Transformers) [3], are designed to be non-auto-regressive. BERT processes the entire input sequence simultaneously and is trained using masked language modeling, where some tokens in the input are masked, and the model learns to predict them based on the surrounding context.\n\nGPT-3 and CLIP models utilize transformers as the underlying architecture.\n\n\n\n16.5.2.2 Generative-Adversarial models\nIntroduced in 2014, Generative Adversarial Networks (GANs) [15] involves two neural networks (generator-discriminator network pair) contest with each other in the form of a zero-sum game, where one agent’s gain is another agent’s loss. Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics.\n\n\n\nFig : GAN basic architecture\n\n\nIn a GAN,\n\nthe generator learns to generate plausible data. The generated instances become negative training examples for the discriminator.\nThe discriminator learns to distinguish the generator’s fake data from real data. The discriminator penalizes the generator for producing implausible results.\n\nWhen training begins, the generator produces obviously fake data, and the discriminator quickly learns to tell that it’s fake:\n\n\n\nFig : GAN training - early phase. Image source: Google developers blog\n\n\nAs training progresses, the generator gets closer to producing output that can fool the discriminator:\n\n\n\nFig : GAN training - mid phase\n\n\nFinally, if generator training goes well, the discriminator gets worse at telling the difference between real and fake. It starts to classify fake data as real, and its accuracy decreases. The training procedure for generator is to maximise the probability of discriminator making a mistake.\n\n\n\nFig : GAN training complete\n\n\nHere’s a picture of the whole system:\n\n\n\nFig : GAN architecture\n\n\nA disadvantage of GAN is potentially unstable training and less diversity in generation due to their adversarial training nature. StyleGAN [16] and BigGAN [17] are example of models that utilize GAN as the underlying architecture.\n\n\n16.5.2.3 Diffusion models\nDiffusion models, introduced in 2020 [18], are inspired by non-equilibrium thermodynamics. They define a Markov chain of diffusion steps to slowly add random noise to data and then learn to reverse the diffusion process to construct desired data samples from the noise [19].\n\n16.5.2.3.1 Key components of diffusion models\n\nForward Diffusion Process:\n\n\nThe forward process gradually adds Gaussian noise to the training data over a series of time steps. This process effectively transforms the original data into pure noise.\n\n\nReverse Diffusion Process:\n\n\nThe reverse process aims to denoise the noisy data back into a sample from the data distribution. This process is learned through a neural network.\nAt each time step, the model predicts the mean and variance of the distribution of the previous step conditioned on the current noisy data. The network outputs parameters that help in gradually removing the noise.\n\n\nTraining Objective:\n\n\nThe model is trained to minimize the difference between the predicted clean data and the actual data at each step of the diffusion process. This is often done using a mean squared error (MSE) loss between the predicted noise and the actual noise added during the forward process.\n\n\nSampling:\n\n\nTo generate new samples, the process starts with pure noise and applies the learned reverse diffusion process iteratively. Over multiple time steps, the model denoises the input until it resembles a sample from the training distribution.\n\n\n\n\nFig : Training a diffusion model. Image source : Lilweng’s blog\n\n\nDiffusion models can generate high-resolution and diverse images, often outperforming GANs in certain tasks. They are generally more stable to train compared to GANs, as they do not rely on adversarial training dynamics.\nStable-diffusion [20], DALL-E [8], Sora are some of the most common models utilizing diffusion architecture.\n\n\n\n\n16.5.3 Foundation Models - Applications\nHaving explored the foundational principles and capabilities of foundation models, we can now delve into specific applications that leverage their power. Two prominent techniques that build upon the capabilities of these models are Segment Anything Model (SAM) and Retrieval-Augmented Generation (RAG).",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#segment-anything-model",
    "href": "sections/foundation-models.html#segment-anything-model",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "16.6 Segment Anything Model",
    "text": "16.6 Segment Anything Model\nSegment Anything Model (SAM) is a foundation model for the Promptable Visual Segmentation (PVS) task. PVS inspired from prompt engineering in NLP that user prompts can be a powerful tool for pre-training foundation models and downstream tasks. It is developed by the Fundamental AI Research (FAIR) team at Meta [6]. SAM is a simple and flexible framework that can segment any object in an image by providing a combination of one or more prompts - namely points, bounding boxes, or other segmentation masks. SAM is based on the transformer architecture and is trained on a large-scale dataset of images and their corresponding segmentation masks.\nThe latest version of SAM, SAM 2, can segment images and videos and uses a unified architecture for both tasks [21]. It is designed to handle complex scenes with multiple objects and can generate high-quality segmentations with minimal user input. The model can be used for various applications, including image editing, object detection, and video analysis.\nSince large-scale datasets for segmentation are unavailable, the research team created a data engine to generate segmentation masks, which were then manually annotated when developing SAM. The model was trained on diverse images to improve its generalization capabilities. This model-in-the-loop self-supervised training created two datasets: SA-1B containing 1B segmentation masks from about 11M privacy preserving images and SA-V dataset containing 642.6K masklets (spatio-temporal segmentation masks) from 50.9K videos.\n\n\n\n\n\n\nFigure 16.1: SAM 2 Architecture. Image source: [21]",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#retrieval-augmented-generation-rag",
    "href": "sections/foundation-models.html#retrieval-augmented-generation-rag",
    "title": "16  Foundation Models: The Cornerstones of Modern AI",
    "section": "16.7 Retrieval-Augmented Generation (RAG)",
    "text": "16.7 Retrieval-Augmented Generation (RAG)\nLarge pre-trained Language Models (LLMs) have revolutionized natural language processing, but they come with inherent limitations that necessitate the development of techniques like Retrieval-Augmented Generation (RAG). This chapter explores the motivations behind RAG by examining the constraints of traditional LLMs.\n\n\n\n\n\nFig : A typical user interaction with LLM\n\n\n\n16.7.1 Limitations of Large Language Models\n\nLack of Specific Knowledge Access\n\nLLMs, despite their vast training data, cannot access specific knowledge bases or datasets that weren’t part of their original training. This limitation becomes apparent when dealing with specialized domains or recent information.\n\nAbsence of User-Specific Data\n\nLLM has not seen “your” data - the unique, often proprietary information that organizations and individuals possess. This gap can lead to generic responses that lack the nuance and specificity required in many real-world applications.\n\nDomain-Specific Knowledge Challenges\n\nWhen specific domain knowledge is required, the traditional approach has been to fine-tune the LLM. However, this process can be resource-intensive and may not always yield optimal results, especially for niche or rapidly evolving fields. For example, an LLM fine-tuned on chemistry domain might not be suitable for a researcher in a physics lab. Hence a particle-physics scientist will have to fine-tune a model on the lab-specific data, which might not be useful for a quantum physics lab.\n\n\n\n\n\nFig : Fine-tuning LLMs. Image source : datacamp blong\n\n\n\nLack of Source Attribution\n\nLLMs generate responses based on patterns learned during training, but they don’t provide sources for their information. This lack of attribution can be problematic in contexts where verifiability and credibility are crucial.\n\nHallucinations\n\nOne of the most significant issues with LLMs is their tendency to produce “hallucinations” - plausible-sounding but factually incorrect or nonsensical information. This phenomenon can undermine the reliability of the model’s outputs. See Lilweng’s blog post [22] on hallucinations for detailed information.\n\n\n\n\n\n\n\nFig : LLM Hallucination examples\n\n\n\n\nOutdated Information\n\nThe knowledge of an LLM is static, frozen at the time of its training. This leads to the problem of outdated information, where the model cannot account for recent events, discoveries, or changes in the world.\nRetrieval-Augmented Generation emerges as a solution to these limitations. By combining the generative capabilities of LLMs with the ability to retrieve and incorporate external, up-to-date information, RAG offers a path to more accurate, current, and verifiable AI-generated content. In the following sections, we will explore how RAG works, its advantages, and its potential applications in various domains.\n\n\n16.7.2 Introduction to RAG\nIntroduced in 2020 [23], RAG framework can be thought of as combining two techniques -\n\nGeneration\n\nDone by LLMs.\nLLM models used are typically tuned for question-answering\nLLM responds to a user query.\n\nRetrieval-Augmented\n\nUse an external database to store specific knowledge\nRetrieve the required information from the provided knowledge base\nProvide this retrieved information to the LLMs as context to answer user question.\n\n\nLet’s now compare the traditional LLM and RAG approaches\n\n16.7.2.1 Traditional LLM approach\n\n\n\nFig : Traditional LLM approach\n\n\n\nUser Input: The process begins with the user submitting a question.\nPrompt Engineering: The user’s question is combined with a pre-defined prompt.\nLLM Processing: The combined prompt and question are fed into the LLM.\nResponse Generation: The LLM generates and returns a response based on its training.\n\n\n\n16.7.2.2 RAG approach\n\n\n\nFig : RAG approach\n\n\n\nUser Input: As before, the user submits a question.\nKnowledge Base Query: The question is used to query a knowledge base.\nDocument Retrieval: Relevant documents are retrieved from the knowledge base.\nPrompt Construction: A prompt is constructed using:\n\nThe original question\nRetrieved relevant documents\nAny additional context or instructions\n\nLLM Processing: The comprehensive prompt is fed into the LLM.\nResponse Generation: The LLM generates a response based on both its pre-trained knowledge and the provided context.\n\n\n\n\n\n\n\n\nWithout RAG\nWith RAG\n\n\n\n\nNo ability to access a specific knowledge/domain\nPoint to a knowledge base\n\n\nNo sources\nSources cited in LLM response\n\n\nHallucinations\nLLM response is grounded by relevant information from knowledge base\n\n\nOut-of-date information\nUpdate the knowledge base with new information\n\n\n\nRAG has multiple use-cases. One of the most common usecase is a chatbot that can answer specific questions. For example, lets say we have a chatbot that has the knowledge base of documentation on supercomputers. This chatbot could help users write scripts/jobs(SLURM scripts) to be submitted to supercomputers, can help guide users to specific section in the documentation if they are stuck, and might even help in debugging errors once their script runs on a supercomputer. If a new SLURM package is introduced, the supercomputer maintainers just need to update the SLURM documentation, which gets pulled into the knowledge base. Hence the chatbot will always have access to the latest information.\nMore detailed information about RAG and its implementation will be discussed in detail in the hands-on part.\n\n\n\n\n[1] R. Bommasani et al., “On the opportunities and risks of foundation models,” ArXiv, 2021, Available: https://crfm.stanford.edu/assets/report.pdf\n\n\n[2] A. Radford et al., “Learning transferable visual models from natural language supervision,” CoRR, vol. abs/2103.00020, 2021, Available: https://arxiv.org/abs/2103.00020\n\n\n[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” CoRR, vol. abs/1810.04805, 2018, Available: http://arxiv.org/abs/1810.04805\n\n\n[4] OpenAI et al., “GPT-4 technical report.” 2024. Available: https://arxiv.org/abs/2303.08774\n\n\n[5] A. Dubey et al., “The llama 3 herd of models.” 2024. Available: https://arxiv.org/abs/2407.21783\n\n\n[6] A. Kirillov et al., “Segment anything.” 2023. Available: https://arxiv.org/abs/2304.02643\n\n\n[7] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using shifted windows.” 2021. Available: https://arxiv.org/abs/2103.14030\n\n\n[8] A. Ramesh et al., “Zero-shot text-to-image generation,” CoRR, vol. abs/2102.12092, 2021, Available: https://arxiv.org/abs/2102.12092\n\n\n[9] Y. Liu et al., “Sora: A review on background, technology, limitations, and opportunities of large vision models.” 2024. Available: https://arxiv.org/abs/2402.17177\n\n\n[10] G. Team et al., “Gemini: A family of highly capable multimodal models.” 2024. Available: https://arxiv.org/abs/2312.11805\n\n\n[11] A. Vaswani et al., “Attention is all you need,” CoRR, vol. abs/1706.03762, 2017, Available: http://arxiv.org/abs/1706.03762\n\n\n[12] L. Weng, “Attention? attention!” lilianweng.github.io, 2018, Available: https://lilianweng.github.io/posts/2018-06-24-attention/\n\n\n[13] J. Cheng, L. Dong, and M. Lapata, “Long short-term memory-networks for machine reading,” CoRR, vol. abs/1601.06733, 2016, Available: http://arxiv.org/abs/1601.06733\n\n\n[14] X. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and M. Kazi, “Transformer models: An introduction and catalog.” 2024. Available: https://arxiv.org/abs/2302.07730\n\n\n[15] I. J. Goodfellow et al., “Generative adversarial networks.” 2014. Available: https://arxiv.org/abs/1406.2661\n\n\n[16] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for generative adversarial networks,” CoRR, vol. abs/1812.04948, 2018, Available: http://arxiv.org/abs/1812.04948\n\n\n[17] A. Brock, J. Donahue, and K. Simonyan, “Large scale GAN training for high fidelity natural image synthesis,” CoRR, vol. abs/1809.11096, 2018, Available: http://arxiv.org/abs/1809.11096\n\n\n[18] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” CoRR, vol. abs/2006.11239, 2020, Available: https://arxiv.org/abs/2006.11239\n\n\n[19] L. Weng, “What are diffusion models?” lilianweng.github.io, Jul. 2021, Available: https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\n\n\n[20] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” CoRR, vol. abs/2112.10752, 2021, Available: https://arxiv.org/abs/2112.10752\n\n\n[21] N. Ravi et al., “SAM 2: Segment anything in images and videos,” arXiv preprint arXiv:2408.00714, 2024, Available: https://arxiv.org/abs/2408.00714\n\n\n[22] L. Weng, “Extrinsic hallucinations in LLMs.” lilianweng.github.io, Jul. 2024, Available: https://lilianweng.github.io/posts/2024-07-07-hallucination/\n\n\n[23] P. S. H. Lewis et al., “Retrieval-augmented generation for knowledge-intensive NLP tasks,” CoRR, vol. abs/2005.11401, 2020, Available: https://arxiv.org/abs/2005.11401",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html",
    "href": "sections/hands-on-lab-foundation-models.html",
    "title": "17  Hands-On Lab: Foundation Models",
    "section": "",
    "text": "17.1 Overview\nThe hands-on lab on foundation models will focus on building and applying foundation models for some example use cases. The main goal of this session is to get more familiarized with foundation models and in interacting with them.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#source-code",
    "href": "sections/hands-on-lab-foundation-models.html#source-code",
    "title": "17  Hands-On Lab: Foundation Models",
    "section": "17.2 Source Code",
    "text": "17.2 Source Code\nVisit https://github.com/ncsa/cyber2a-workshop and follow the instructions in the README file to set up and run the Jupyter Notebooks used in this hands-on lab.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#image-segmentation-using-segment-anything-model-2-sam-2",
    "href": "sections/hands-on-lab-foundation-models.html#image-segmentation-using-segment-anything-model-2-sam-2",
    "title": "17  Hands-On Lab: Foundation Models",
    "section": "17.3 Image Segmentation using Segment Anything Model 2 (SAM 2)",
    "text": "17.3 Image Segmentation using Segment Anything Model 2 (SAM 2)\n\n17.3.1 Background\nImage segmentation is a fundamental computer vision technique of dividing an image into one or more regions or objects [1]. Promptable Visual Segmentation (PVS) is a new type of segmentation that combines the flexibility of prompts with the power of computer vision models to enable users to segment images interactively based on prompts.\nFor this section of the foundational model hands-on session, we will use the [2]. SAM 2 is a foundation model for the PVS task trained on large-scale generic data that can predict object segmentation masks based on input prompts. These prompts include points, bounding boxes (e.g., rectangles), masks, or combinations. The model converts the image into an image embedding (a dense vector representation of the image), which it then uses to predict segmentation masks based on a user prompt.\nOne prominent class in the SAM 2 source code is SAM2ImagePredictor, which provides an easy interface to the model. Users can attach an input image to the model using its set_image method, which calculates the image embeddings. Then, the users can use the predict method to share prompts (user inputs) that help with the segmentation mask prediction.\n\n\n\n\n\n\nNote\n\n\n\nThe Jupyter Notebook for this hands-on session is available within the https://github.com/ncsa/cyber2a-workshop repository here. You can clone or download this repository directly from GitHub.\nThis notebook reuses some code segments (e.g., helper methods, imports, loading the model, etc.) from the image predictor example initially published in the SAM 2 source code repository. SAM 2 source code is released under the Apache License, Version 2.0, and Meta Platforms, Inc. and affiliates hold the copyright for the image_predictor_example.ipynb notebook. We have adapted and modified the image predictor example notebook to use data files from Arctic datasets and included specific activities for the Cyber2A Workshop.\n\n\n\n\n17.3.2 Data\nImages used in this hands-on lab section are from the LeConte Glacier Unmanned Aerial Vehicle (UAV) imagery dataset [3]. Specifically, we use a low-resolution version (640 x 427) of images with IDs 20180917-112527 and 20180917-115018 from the zip file located at this URL.\nBefore continuing with the rest of the sections, open the segmentation.ipynb notebook from the running Jupyter Notebook server.\n\n\n17.3.3 Environment Setup\nFirst, we import the necessary packages, download SAM 2 model checkpoints, and define methods for visualizing the results. Here, model checkpoints are files containing model weights and architecture saved after a certain number of iterations of model training. These checkpoints are used to load the model and make predictions.\n\n\nShow code\n\nimport os\n# if using Apple MPS, fall back to CPU for unsupported ops\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n\n\nShow code\n\nos.chdir(\"SAM_checkpoints\")\n!sh download_checkpoints.sh\nos.chdir(\"..\")\n\nNow, we use the code to select the device for computation. Depending on the machine running the segmentation notebook, you could choose between a CPU, GPU, or Metal Performance Shaders (MPS) for computation. The code snippet below shows how to select the device for computation.\n\n\nShow code\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# Select the device for computation. We will be using CUDA to run this notebook. Other options are provided for running this notebook in different environments.\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\nprint(f\"using device: {device}\")\n\nif device.type == \"cuda\":\n    # use bfloat16 for the entire notebook\n    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n    if torch.cuda.get_device_properties(0).major &gt;= 8:\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\nelif device.type == \"mps\":\n    print(\n        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n    )\n\nNext, we use the code to define methods for visualizing the results. The show_mask method displays a single segmentation mask, the show_points method displays the points, the show_box method displays the bounding box, and the show_masks method displays the image with the segmentation masks, points, and bounding boxes.\n\n\nShow code\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\nnp.random.seed(3)\n\ndef show_mask(mask, ax, random_color=False, borders = True):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask = mask.astype(np.uint8)\n    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    if borders:\n        import cv2\n        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n        # Try to smooth contours\n        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n    ax.imshow(mask_image)\n\ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n\ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n\ndef show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n    for i, (mask, score) in enumerate(zip(masks, scores)):\n        plt.figure(figsize=(10, 10))\n        plt.imshow(image)\n        show_mask(mask, plt.gca(), borders=borders)\n        if point_coords is not None:\n            assert input_labels is not None\n            show_points(point_coords, input_labels, plt.gca())\n        if box_coords is not None:\n            # boxes\n            show_box(box_coords, plt.gca())\n        if len(scores) &gt; 1:\n            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n        plt.axis('off')\n        plt.show()\n\n\n\n17.3.4 Example Image 1\nNow, we read the first example image (data/images/20180917-112527-reduced.jpg), create an object, and display it with a grid for estimating point and box coordinates.\n\n\nShow code\n\nimage = Image.open('data/images/20180917-112527-reduced.jpg')\nimage = np.array(image.convert(\"RGB\"))\n\nplt.figure(figsize=(10, 10))\nplt.imshow(image)\nplt.grid(visible=True)\nplt.axis('on')\nplt.show()\nOutput: \n\n\n\n17.3.5 Loading the SAM 2 model and configuration\nNow, let’s load the SAM 2 model and configuration file. We load the model from the SAM_checkpoints directory and the configuration file from the configs directory. We use the sam2.1_hiera_large model checkpoint and the sam2.1_hiera_l.yaml configuration file. Other model versions and their corresponding configuration files can also be used, but the accuracy of the segmentation outputs may vary.\n\n\nShow code\n\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\nsam2_checkpoint = \"SAM_checkpoints/sam2.1_hiera_large.pt\"\nmodel_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n\nsam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\npredictor = SAM2ImagePredictor(sam2_model)\n\nNow, we process the image to produce an image embedding by calling the SAM2ImagePredictor.set_image method. The SAM2ImagePredictor object stores this embedding and stores the subsequent mask prediction.\n\n\nShow code\n\npredictor.set_image(image)\n\n\n\n17.3.6 Specifying an object or region using a single point\nIn this example image, to prompt for the glacier region, let’s choose a point on it.\nPoints are a type of input to the model. They’re represented in (x,y) format and have corresponding labels 1 or 0, which represent the foreground and background, respectively. As we will see later, we can input multiple points, but here, we use only one. The show_points method displays the selected point using a star icon.\n\n\nShow code\n\ninput_point = np.array([[600, 400]])\ninput_label = np.array([1])\n\nplt.figure(figsize=(10, 10))\nplt.imshow(image)\nshow_points(input_point, input_label, plt.gca())\nplt.axis('on')\nplt.show()\n\nOutput: \n\n\n17.3.7 Predicting the segmentation mask\nNow, we predict the segmentation mask using the selected point as input. The predict method of the SAM2ImagePredictor object predicts the segmentation mask based on the input point. The show_masks utility method displays the segmentation mask on the image.\n\n\nShow code\n\nmasks, scores, logits = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=True,\n)\nsorted_ind = np.argsort(scores)[::-1] # Sorting the scores in decreasing order\nmasks = masks[sorted_ind]\nscores = scores[sorted_ind]\nlogits = logits[sorted_ind]\n\nshow_masks(image, masks, scores, point_coords=input_point, input_labels=input_label, borders=True)\n\nOutput:   \n\n\n17.3.8 Activity 1: Specifying an object or region using multiple points\nWe can see that the single input point can be ambiguous, and the model has returned multiple sub-regions within the glacier image. We can alleviate this by providing multiple points as input. We can also provide a mask from a previous model iteration to help improve the prediction. When specifying a single object with multiple prompts, we can ask the model to generate a single mask by setting multimask_output=False.\n\n\nShow code\n\n# E.g., input format for specifying two points\n# input_point = np.array([[x1, y1], [x2, y2]])\n# input_label = np.array([1, 1])\n\n# TODO: In the below piece of code, replace \"None\" with your two input points. You can specify more points if needed, but please make sure to increase the labels as well.\n\ninput_point = np.array([[600, 400], [500, 200]])\ninput_label = np.array([1, 1])\n\nFor example, we can use [600, 400] and [500, 200] as the two points to specify the glacier region.\n\n\nShow code\n\ninput_point = np.array([[600, 400], [500, 200]])\ninput_label = np.array([1, 1])\n\nmask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask from previous iteration\n\nmasks, scores, _ = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    mask_input=mask_input[None, :, :],\n    multimask_output=False,\n)\n\nshow_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)\n\nOutput: \nWe can see that providing multiple points as input has helped the model to predict a more accurate segmentation mask for the glacier region.\n\n\n17.3.9 Example Image 2\nNow, let’s read the second example image, create an object, and display it with grid for estimating point and box coordinates.\n\n\nShow code\n\nimage = Image.open('data/images/20180917-115018-reduced.jpg')\nimage = np.array(image.convert(\"RGB\"))\n\nplt.figure(figsize=(10, 10))\nplt.imshow(image)\nplt.grid(visible=True)\nplt.axis('on')\nplt.show()\n\n# Replace image in the predictor with the new image\npredictor.set_image(image)\n\nOutput: \n\n\n17.3.10 Activity 2: Specifying an object or region using multiple points (foreground and background)\nA background point (with label 0) can be supplied to exclude the glacier and water surrounding it and just include the glacial discharge.\n\n\nShow code\n\n# E.g., input format for specifying three points\n# input_point = np.array([[x1, y1], [x2, y2], [x3, y3]])\n# input_label = np.array([1, 1, 0])\n\n# TODO: In the below piece of code, use two or three input points, of which at least one needs to be a background point (water). You can specify more points if needed, but please make sure to increase the labels as well.\ninput_point = None\ninput_label = None\n\nFor example, we can use [600, 200] and [600, 300] as the foreground points and [600, 250] as the background point to specify the glacial discharge region.\n\n\nShow code\n\ninput_point = np.array([[600, 200], [600, 300], [600, 250]])\ninput_label = np.array([1, 1, 0])\n\nmask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask from previous iteration\n\nmasks, scores, _ = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=False,\n)\n\nshow_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)\nOutput: \nWe can see that with three points, including one background point, the model has predicted a large section of the glacial discharge region. With additional points (foreground and/or background), users can further guide the model to predict more accurate segmentation masks.\n\n\n17.3.11 Activity 3: Specifying a specific object with a box\nWe will specify the glacial discharge region using a bounding box in this activity. The bounding box is represented as [x1, y1, x2, y2], where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner of the box.\n\n\nShow code\n\n# E.g., input format for specifying a box\n# input_box = np.array([x1, y1, x2, y2])\n\n# TODO: In the below piece of code, replace \"None\" with a box coordinate.\ninput_box = None\nFor example, we can use [400, 150, 640, 400] as the bounding box to specify the glacial discharge region.\n\n\nShow code\n\ninput_box = np.array([400, 150, 640, 400])\n\nmasks, scores, _ = predictor.predict(\n    box_coords=input_box,\n    multimask_output=False,\n)\n\nshow_masks(image, masks, scores, box_coords=input_box)\nOutput: \nThe model has predicted the glacial discharge region based on the bounding box input, but the results are not perfect. Additional prompts, along with the bounding box, can improve this.\nWe have provided three optional activities for you to try out in the segmentation notebook, including one that does automatic segment generation without prompts. Feel free to experiment with different prompts and see how the model responds.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#retrieval-augmented-generation-rag-hands-on",
    "href": "sections/hands-on-lab-foundation-models.html#retrieval-augmented-generation-rag-hands-on",
    "title": "17  Hands-On Lab: Foundation Models",
    "section": "17.4 Retrieval Augmented Generation (RAG) Hands-On",
    "text": "17.4 Retrieval Augmented Generation (RAG) Hands-On\nWe will use Langchain framework for this section of the hands-on session. Langchain is a framework that provides tools and libraries for building and deploying AI models. It is built on top of PyTorch and HuggingFace transformers.\nSuggested code references: - Langchain RAG from scratch github - Langchain RAG tutorial\nSession hands-on code in github.com/ncsa/cyber2a-workshop\nSession technical details in course book : cyber2a.github.io/cyber2a-course/sections/foundation-models.html\nIn this section, we will build a chatbot using the RAG system, i.e., a chatbot that has access to your specific knowledge base and answers questions related to that knowledge base.\n\n17.4.1 RAG Recap\n\n\n\n\n\n\n\nWithout RAG\nWith RAG\n\n\n\n\nNo ability to access a specific knowledge/domain\nPoint to a knowledge base\n\n\nNo sources\nSources cited in LLM response\n\n\nHallucinations\nLLM response is grounded by relevant information from knowledge base\n\n\nOut-of-date information\nUpdate the knowledge base with new information\n\n\n\n\n\n\nRAG approach\n\n\n\n\n17.4.2 RAG\nWe can think of the RAG system as combining two techniques: 1. Retrieval 2. Generation , where the Generation step is augmented or improved by the Retrieval step.\nA specialized database typically retrieves the data; an LLM normally does the generation part.\n\nRetrieval\n\n\nSetup a knowledge base\nRetrieve documents relevant to the user query\n\n\nGeneration\n\n\nUsing LLMs\nUse the retrieved documents as context\n\n\n\n17.4.3 Hands-on Environment Setup\n\nLLM For the hands-on session on RAG, we will use OpenAI GPT-4o-mini and a Llama3.2:8b model hosted on an Ollama instance. You will need an OpenAI API key to access the OpenAI models. To access the Llama model, you must set up an Ollama instance and have an API key associated with that instance. Details on setting up an Ollama instance are available here. Users can also download a Llama model on their local machine and run this hands-on code without an Ollama instance. However, we recommend using an Ollama instance as this would enable api calls (curl requests) to access the LLM.\nCompute requirements There are no GPU requirements for this hands-on. However, we recommend some memory on your local system, as we will be using your device’s local memory for a small database. We recommend testing out the code for this hands-on session in a Jupyter notebook. For instructions on launching a Jupyter notebook, see here.\nCode The code is available at github.com/ncsa/cyber2a-workshop. Feel free to clone or download this repo directly from GitHub. Steps to clone the repo and access the rag.ipynb file:\n\nOpen a terminal\nRun command: git clone https://github.com/ncsa/cyber2a-workshop\nNavigate to the foundation_models/hands_on directory in the cloned repo.\n\nData requirements If you clone or download the repo, a data directory will be created. Feel free to add your favorite documents, such as TXT, PDF, CSV, or docx files, to the data/docs directory. We will insert these documents into our specialized database.\nEnvironment variables You will find an env.example file in the GitHub repo. This file will provide you with the environment variables required for this course. Add your OpenAI API key, Ollama API key, your data folder (currently data/docs), and a data collection name (e.g., data-collection). The OpenAI API key will be used to access the OpenAI models. Ollama API key will be used to access your Ollama instance. The collection name is a way to recognize a collection/table in the specialized database, that we will be setting up soon. Edit the env.example file to contain your values and save the file. Rename env.example to env.txt.\nPackage requirements This tutorial will require the following list of packages.\n\njupyter\npandas\npython-dotenv==1.0\nqdrant-client==1.12\n# langchain\nlangchain==0.3\nlangchain-community==0.3\nlangchain-core==0.3\nlangchain-openai==0.2\nlangchainhub==0.1\n# openai\nopenai==1.54\n# rst file loaders\n# pandoc\npypandoc==1.14\nunstructured==0.16\nInstructions for installing the required packages:\n\nCreate a virtual environment with python version not less than 3.10.\n\nUse conda/miniconda to set up a virtual environment. If you have a virtual environment with python&gt;=3.10 with a jupyter kernel, use that env/kernel and skip the below steps.\n\n\nDownload the latest Miniconda3 installer from the Miniconda web page.\n\n\nFrom the Terminal (Mac/Linux) or Command Prompt (Windows) add conda-forge package repository/channel to your environment:\n\n\nconda config --add channels conda-forge\n\n\nCreate the python environment (for this example we choose name rag-python3.9):\n\n\nconda create -n rag-python3.10 python=3.10\n\n\nActivate the environment: python conda activate rag-python3.10\n\n\n\nCreate a requirements.txt file and copy-paste the above packages as-is. You could also use the requirements.txt file from the cloned Github repository cyber2a-workshop\nType pip install -r requirements.txt\n\nLet’s start by importing some basic python packages.\n\n\nShow code\n\n# basic imports\nimport os\nimport json\nimport logging\nimport sys\nimport pandas as pd\n\nfrom dotenv import load_dotenv\nload_dotenv('env.txt', override=True)\n\n# create and configure logger\nlogging.basicConfig(level=logging.INFO, datefmt='%Y-%m-%dT%H:%M:%S',\n                    format='%(asctime)-15s.%(msecs)03dZ %(levelname)-7s : %(name)s - %(message)s',\n                    handlers=[logging.StreamHandler(sys.stdout)]\n                    )\n# create log object with current module name\nlog = logging.getLogger(__name__)\n\n\n\n17.4.4 RAG - Retrieval-Augmented Generation\nWe will focus on the “retrieval” part of RAG for this section.\nRAG - Retrieval Steps\n\nPrepare data\nCreate a database and insert data\nSearch the database and retrieve relevant documents according to the search query.\n\nAs mentioned earlier, the RAG system gives the LLM access to our knowledge base, which has specific information for our use case.\n\n17.4.4.1 Data Preparation\nLet’s consider that our knowledge base contain only textual data. The data present in the GitHub repo contains proceedings from the Arctic Data Symposium 2023\n\n17.4.4.1.1 Load data\nSince we have different file types, we will need different types of data loaders to read these different data formats. - Langchain provides different data loaders for different file types - Eg: Langchain CSVLoader is essentially a wrapper for Python csv.DictReader class - Data is loaded into Langchain Document object format - The Document class has page_content and metadata attributes. - The page_content is the textual content parsed from the document files. - The metadata can be user-defined or default (class defined) key-value pairs. These key-value pairs can be used for filtering the documents retrieved from the database. - By default, each file type has its own metadata content. Eg: PDF file has source and page. - Filtering methods are not shown in this course. These methods will be well-documented in the database tool that you choose (explained later in the vectorDB section).\n\n\n\nLangchain document class\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nFor details on langchain packages, please refer to their documentation and source-code.\nIf using an IDE (PyCharm, VSCode, etc), Ctrl+click, or Command+click on the package and it should open-up its source code.\n\n\n\n\nNow, let’s load some data. This code loads all files in a directory. For now, we have only PDF files.\n\n\nShow code\n\n# data loaders\nfrom langchain_community.document_loaders import CSVLoader, DataFrameLoader, PyPDFLoader, Docx2txtLoader, UnstructuredRSTLoader, DirectoryLoader\n\n# Defining a class for data loaders. All data loaders are defined in this class\nclass DataLoaders:\n    \"\"\"\n    various data loaders\n    \"\"\"\n    def __init__(self, data_dir_path):\n        self.data_dir_path = data_dir_path\n    \n    def csv_loader(self):\n        csv_loader_kwargs = {\n                            \"csv_args\":{\n                                \"delimiter\": \",\",\n                                \"quotechar\": '\"',\n                                },\n                            }\n        dir_csv_loader = DirectoryLoader(self.data_dir_path, glob=\"**/*.csv\", use_multithreading=True,\n                                    loader_cls=CSVLoader, \n                                    loader_kwargs=csv_loader_kwargs,\n                                    )\n        return dir_csv_loader\n    \n    def pdf_loader(self):\n        dir_pdf_loader = DirectoryLoader(self.data_dir_path, glob=\"**/*.pdf\",\n                                    loader_cls=PyPDFLoader,\n                                    )\n        return dir_pdf_loader\n    \n    def word_loader(self):\n        dir_word_loader = DirectoryLoader(self.data_dir_path, glob=\"**/*.docx\",\n                                    loader_cls=Docx2txtLoader,\n                                    )\n        return dir_word_loader\n    \n    def rst_loader(self):\n        rst_loader_kwargs = {\n                        \"mode\":\"single\"\n                        }\n        dir_rst_loader = DirectoryLoader(self.data_dir_path, glob=\"**/*.rst\",\n                                    loader_cls=UnstructuredRSTLoader, \n                                    loader_kwargs=rst_loader_kwargs\n                                    )\n        return dir_rst_loader\n\nLoad the data\n\n\nShow code\n\n# load data\ndata_dir_path = os.getenv('DATA_DIR_PATH', \"data/docs\")\ndata_loader = DataLoaders(data_dir_path=data_dir_path)\nlog.info(\"Loading files from directory %s\", data_dir_path)\n# instantiate loaders\ndir_csv_loader = data_loader.csv_loader()\ndir_word_loader = data_loader.word_loader()\ndir_pdf_loader = data_loader.pdf_loader()\ndir_rst_loader = data_loader.rst_loader()\n# call load method\ncsv_data = dir_csv_loader.load()\nword_data = dir_word_loader.load()\npdf_data = dir_pdf_loader.load()\nrst_data = dir_rst_loader.load()\n\nSince our test data only has pdf documents, only the pdf_data will have values. Let’s see how the first document looks like :\n# only printing the first document in pdf_data\nfor doc in pdf_data:\n    print(doc)\n    break\nThe above code will display the first document. The page_content will be the text content from the document and metadata gives you the page number and source file. The metadata field currently has default values, set by the class the document is loaded from (in this case PDFLoader class). For other classes, metadata would differ. For example, if loading a CSV file using CSVLoader, the metadata will have row number instead of page number. Users have the option to customize metadata as required by simple code changes.\nAs seen from the page_content and metadata value, the first document only has the text data from the first page. Langchain PDFLoader loads pdf documents in pages. Each document will be one pdf page.\n\n\n17.4.4.1.2 Format the data\nAs the previous code block shows, each document is in a Document class with attributes page_content and metadata. The LLM can only access the textual content (page_content), so let’s reformat the documents accordingly. However, we still need metadata, which is helpful for filtering purposes.\nUsers could also customize metadata to have similar key-value pairs across different documents. This would be helpful if several types of documents are inserted into one database and the metadata is used to filter across them.\nSteps implemented in the below code block: - Convert data to a list of texts and metadata - Custom metadata is set so that metadata is same for all different data sources.\n\n\nShow code\n\n# get text and metadata from the data\ndef get_text_metadatas(csv_data=None, pdf_data=None, word_data=None, rst_data=None):\n    \"\"\"\n    Each document class has page_content and metadata properties\n    Separate text and metadata content from Document class\n    Have custom metadata if needed\n    \"\"\"\n    csv_texts = [doc.page_content for doc in csv_data]\n    # custom metadata\n    csv_metadatas = [{'source': doc.metadata['source'], 'row_page': doc.metadata['row']} for doc in csv_data]   # default metadata={'source': 'filename.csv', 'row': 0}\n    pdf_texts = [doc.page_content for doc in pdf_data]\n    pdf_metadatas = [{'source': doc.metadata['source'], 'row_page': doc.metadata['page']} for doc in pdf_data]  # default metadata={'source': 'data/filename.pdf', 'page': 8}\n    word_texts = [doc.page_content for doc in word_data]\n    word_metadatas = [{'source': doc.metadata['source'], 'row_page': ''} for doc in word_data] \n    rst_texts = [doc.page_content for doc in rst_data]\n    rst_metadatas = [{'source': doc.metadata['source'], 'row_page': ''} for doc in rst_data]         # default metadata={'source': 'docs/images/architecture/index.rst'}\n\n    texts = csv_texts + pdf_texts + word_texts + rst_texts\n    metadatas = csv_metadatas + pdf_metadatas + word_metadatas + rst_metadatas\n    return texts, metadatas\n\ntexts , metadatas = get_text_metadatas(csv_data, pdf_data, word_data, rst_data)\n\nLet’s print the number of texts and metadata\nprint(\"Number of PDF texts: \", len(texts))\nprint(\"Number of PDF metadata: \", len(metadatas))\n\n\n17.4.4.1.3 Chunking\nChunking involves breaking large amounts of data into smaller, more manageable pieces. LLMs have a limited context window and cannot take in the entire dataset at once. For example, GPT-4 has a token limit of 128k. The Langchain document class also chunks PDF documents by page. However, we will need to make sure that all the content fits within the LLM token limit of 128k, so we will try splitting the documents meaningfully. We first split by pages, then by sections, then by paragraphs, then by new lines, then by sentences, by words, and lastly by characters (e.g., what if there’s a word with more than 128k characters!). This is just to ensure that no content is lost.\n\n\n\nChunking\n\n\nSteps implemented in the below code block: - Split texts into chunks - Return a list of document chunks (list of langchain document class) - Here, we select a chunk size of 1000 and an overlap of 200 tokens. There is no set rule for this choice. However, this is a recommended pattern. - Chunk sizes determine the granularity of information being searched for. - The chunk size should be smaller if very granular information is required. If the chunk size is larger, the overall content of the documents is returned.\n\n\nShow code\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\nfrom typing import List\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=1000,\n        chunk_overlap=200,\n        separators=[\n            \"\\n\\n\", \"\\n\", \". \", \" \", \"\"\n        ]  # try to split on paragraphs... fallback to sentences, then chars, ensure we always fit in context window\n    )\n\ndocs: List[Document] = text_splitter.create_documents(texts=texts, metadatas=metadatas)\n\nNow, let’s see if the first document changed and how many documents are available after chunking.\nprint(docs[0])\nprint(\"Number of documents: \", len(docs))\n\n\n17.4.4.1.4 Vector embeddings\nNeural networks do not understand characters or texts. However, they understand numbers and are really good at numerical computation. Hence, textual data is converted to vectors of real-valued numbers.\n\n\n Image source: MIT Deep Learning course slides\nVector embeddings are mathematical representations of data points in a high-dimensional space. In the context of natural language processing:\n\nWord Embeddings: Individual words are represented as real-valued vectors in a multi-dimensional space.\nSemantic Capture: These embeddings capture the semantic meaning and relationships of the text.\nSimilarity Principle: Words with similar meanings tend to have similar vector representations.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nThe words ‘word embeddings’, ‘vector embeddings’, and ‘embeddings’ will be used interchangeably throughout this course.\n\n\n\n\n\n\nVectors\n\n\nIn the above vector example, “King” and “Queen” have the same relationship as “man” and “woman.” The “King” is at a similar distance from “man” and “queen” from “woman.”\nThese word embeddings are learned by feeding a model vast amounts of text. Models specialized in generating these text embeddings are called embedding models. Word2Vec is one of the first (very basic) embedding models. The GloVE model was one of the more popular models that learned word embeddings, i.e., one of the first models that was trained as a word embedding model. There are many open-source embedding models, e.g., Text embedding models from HuggingFace. Check out the HuggingFace Embedding models leaderboard to compare different embedding models. We will be using the OpenAI text embedding model, which, according to OpenAI documentation, has a maximum token limit of 8191.\n# embeddings \nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\nAnd now, we have completed step 1 of RAG RAG - Retrieval Steps\n1. Prepare data\n\nCreate a knowledge base and insert data\nSearch the database and retrieve relevant documents according to the search query.\n\n\n\n\n17.4.4.2 Knowledge Database\nIn the age of burgeoning data complexity and high-dimensional information, traditional databases often fail to efficiently handle and extract meaning from intricate datasets. Enter vector databases, a technological innovation that has emerged as a solution to the challenges posed by the ever-expanding data landscape. (Source: beginner’s blog post on vector DB)\n\n17.4.4.2.1 Vector database\nVector databases have gained significant importance in various fields due to their unique ability to efficiently store, index, and search high-dimensional data points, often referred to as vectors. These databases are designed to handle data where each entry is represented as a vector in a multidimensional space. Vectors can represent a wide range of information, such as numerical features, embeddings from text or images, and even complex data like molecular structures.\nVector databases store data as vector embeddings and are optimized for fast retrieval and similarity search. Vector database records are vectors, and the distance between them corresponds to whether the vectors are similar or not. Vectors that are closer are more similar than vectors that are farther apart. \n\n17.4.4.2.1.1 How vector databases work\nLet’s start with a simple example of dealing with an LLM such as ChatGPT. The model has large volumes of data with a lot of content, and they provide us with the ChatGPT application.\n\n\n\nVectorDB within RAG. Source: KDnuggets blog post\n\n\nSo let’s go through the steps of retrieval using vectorDB.\n\nWe first partition the data (to be used in the knowledge base) into chunks\nUse embedding model to create vector embeddings for the data (create indexes)\nInsert data vector embeddings into the database, with some reference to the original content (metadata).\nUser query is converted to vector embeddings using the same embedding model used for data.\nVectorDB searches the knowledgebase for vector embeddings similar to the user query.\nVectorDB returns similar document chunks and sends it back to the user.\n\nNow, let’s see how it works in the vector database.\n\n\n\nVectorDB pipeline. Source: pinecone blog post\n\n\nThe three main stages that a vector database query goes through are:\n\nIndexing\n\nAs explained in the example above, once the data vector embedding moves into the vector database, it then uses a variety of algorithms to map the vector embedding to data structures for faster searching.\n\nQuerying\n\nOnce it has gone through its search, the vector database compares the queried vector (user query) to indexed vectors, applying a similarity metric to find the nearest neighbor.\n\nPost Processing\n\nDepending on the vector database you use, the vector database will post-process the final nearest neighbor to produce a final output to the query. We could also possibly re-rank the nearest neighbors for future reference.\n\n\n17.4.4.2.1.2 Inserting documents into VectorDB\n\n\n\nInserting into VectorDB. Source : Blog.demir\n\n\n\n\n\n17.4.4.2.2 Vector Store\n\nWe will use Qdrant vector store for this example\nFor this tutorial, we will utilize local memory for storage\nQdrant has a docker image that can be used to create a vector store and hosted remotely\nOne can configure a Qdrant docker image to run locally and have a Qdrant client that makes API requests.\nQdrant creates a collection from the inserted documents (similar to a table in SQL databases)\nBlog post on vector stores link\n\nLet’s create a Qdrant vector store in local memory\n\n\nShow code\n\n# creating a Qdrant vector store in local memory\n\nfrom langchain_community.vectorstores import Qdrant\n\n# qdrant collection name\ncollection_name = os.getenv('QDRANT_COLLECTION_NAME', \"data-collection\")\n\n# create vector store in local memory\nvectorstore = Qdrant.from_documents(\n    documents=docs, # pass in the chunked docs\n    embedding=embeddings,  # use this embedding model\n    location=\":memory:\",  # Local mode with in-memory storage only\n    collection_name=collection_name,  # give a collection name\n    )\n\nAnd now, we have completed step 2 of RAG retrieval RAG - Retrieval Steps\n1. Prepare data\n2. Create a knowledge base and insert data\n\nSearch the database and retrieve relevant documents according to the search query.\n\n\n\n\n17.4.4.3 Retrieve relevant documents\nCreate a retriever from the vector store. This retriever performs similarity search and retrieves similar document chunks from the Qdrant vector store.\n# Retriever to retrieve relevant chunks\nretriever = vectorstore.as_retriever()\nAnd now, we have completed all 3 steps of RAG retrieval\n1. Prepare data\n2. Create a vector store and insert data\n3. Search the vector store and retrieve relevant documents\n\n\n\n17.4.5 RAG - Retrieval-Augmented Generation\nWe will now move on to the “Generation” part of RAG. Here, the LLMs do most of the heavy lifting.\n\n17.4.5.1 LLM\n\nLLMs are pre-trained large language models\nTrained to predict the next word (token), given some input text.\nOpen-source models - HuggingFace leaderboard\nFor this HandsOn, we will use OpenAI GPT-4o-mini and the Ollama Llama3.2:3.2B model hosted by NCSA.\n\nLet’s see how best to communicate/prompt these LLM models for RAG.\n\n\n17.4.5.2 Prompting\nPrompting is a crucial technique in effectively communicating with Large Language Models (LLMs) to achieve desired outcomes without modifying the underlying model. As LLMs become more sophisticated, the art of crafting effective prompts has emerged as a key skill in natural language processing and AI applications. Check out LilianWeng’s blog post [4], medium blog post on prompt engineering.\nPrompting is often an iterative process. It typically requires multiple trial-and-error attempts to achieve the desired effect. Each iteration can provide insights into how the model interprets and responds to different input structures.\n\n17.4.5.2.1 Key Elements of Effective Prompting\n\nDefining a Persona\n\nAssigning the LLM a specific role or behavior can significantly influence its responses. By giving it a defined persona, the model will attempt to respond in a manner that aligns with that role. This can improve the quality and relevance of its answers.\nExample: “You are a helpful research assistant.”\nThis prompt frames the model’s responses to be in line with the behavior expected of a research assistant, such as providing accurate information and being resourceful.\n\nSetting Guardrails\n\nGuardrails provide boundaries or conditions within which the model should operate. This is particularly useful to avoid misleading or incorrect information. You can ask the model to refrain from answering if it’s unsure of the response.\nExample: “If you don’t know the final answer, just say ‘I don’t know’.”\nThis instructs the LLM to admit uncertainty instead of generating a potentially incorrect answer, thereby increasing reliability.\n\nProviding Clear Instructions\n\nGiving the LLM specific actions to perform before generating responses ensures that it processes the necessary information correctly. This is important when dealing with tasks like reviewing files or using external data.\nExample: “Read the data file before answering any questions.”\nThis directs the LLM to review relevant materials, improving the quality of the subsequent answers.\n\nSpecifying Response Formats\n\nYou can enhance the usefulness of responses by specifying the desired output format. By doing this, you ensure the model delivers information in a form that aligns with your needs.\nExample: “Respond using markdowns.”\nThis ensures the LLM outputs text in Markdown format, which can be helpful for structured documents or technical writing.\n\n\n17.4.5.2.2 Prompt template\n\nUse Langchain hub to pull prompts\n\neasy to share and reuse prompts\ncan see what are the popular prompts for specific use cases\nEg: rlm/rag-prompt \n\nUse a prompt template Langchain PromptTemplate to generate custom prompts\n\nincludes input parameters that can be dynamically changed\ncan include instructions and other prompting patterns\n\n\nqa_prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n    1. If the question has some initial findings, use that as context.\n    2. If you don't know the answer, don't try to make up an answer. Just say **I can't find the final answer but you may want to check the following sources** and add the source documents as a list.\n    3. If you find the answer, write the answer in a concise way and add the list of sources that are **directly** used to derive the answer. Exclude the sources that are irrelevant to the final answer.\n\n    {context}\n\n    Question: {question}\n    Helpful Answer:\"\"\"\n\nrag_chain_prompt = PromptTemplate.from_template(qa_prompt_template) \nLet’s use the rlm/rag-prompt from Langchain hub.\n# prompting\n\nfrom langchain import hub\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n\n\n17.4.5.3 Call LLM\n\nWe will use\n\nOpenAI GPT-4o-mini and\nOllama llama3.2 model (hosted by NCSA)\n\nEach model has its own formats and parameters\nFormat the documents as string to pass on to the LLM\n\n# formatting the documents as a string before calling the LLM\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n17.4.5.3.1 Call LLM - without RAG\n\n\nShow code\n\n# call open ai GPT-4o-mini\nfrom langchain_openai import ChatOpenAI\n\n# create a chat openai model\nllm: ChatOpenAI = ChatOpenAI(\n            temperature=0,\n            model=\"gpt-4o-mini\",\n            max_retries=500,\n        )\n\n# call GPT4o-mini. \n# No RAG. Not giving any instructions/context to the LLM.\n\nllm.invoke(\"What is the capital of the world?\")\n\n# Notice the OpenAI LLM response format: content , metadata\n\n\nShow code\n\n# call ollama llama3:latest\n\nfrom langchain_community.llms import Ollama\n\nollama_api_key = os.getenv('OLLAMA_API_KEY')\nollama_jwt_token = os.getenv('OLLAMA_JWT_TOKEN')\nollama_headers = {\"Authorization\": f\"Bearer {ollama_api_key}\"}\n\n# create a ollama model\nollamallm: Ollama = Ollama(\n    base_url=\"https://ollama.software.ncsa.illinois.edu/ollama\",\n    model=\"llama3.2:latest\",\n    headers=ollama_headers\n    )\n\n# call llama3 model\n# No RAG. Not giving any prompt/specific instructions to the LLM\nollamallm.invoke(\"What is the capital of the world?\")\n\n# Notice the Llama LLM response format: plain text\n\n\n\n\n17.4.6 RAG System\nLet’s bring it all together\n\n\n\nFig : RAG system. Image source : blog.demir\n\n\n\nUser Submits Query: The user inputs a query into the system. This is the initial step where the user’s request is captured.\nRAG System Query Relevant Documents: The RAG system processes the user’s query and searches for relevant documents.\nDocument Database Returns Documents: The document database receives the request for relevant documents and returns the documents it finds to the RAG system.\nCombine The Query & The Documents: The RAG system takes the documents provided by the document database and combines them with the original query.\nLLM Returns Answer: The combined query and documents are sent to a Large Language Model (LLM), which generates an answer based on the information provided.\nRAG System Return Answer to User: Finally, the answer generated by the LLM is sent back through the RAG system.\n\n\n17.4.6.1 RAG chain with OpenAI\nLet’s code the RAG chain with OpenAI LLM\n\n\nShow code\n\n# rag chain\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\nopenai_rag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt  # change to custom prompt here\n    | llm  # change openAI llm model here\n    | StrOutputParser()\n)\n\nThe above code implements the RAG system - Context is the retrieved docs from the retriever/vector db - RunnablePassthrough() is used to pass the user query as is to the chain - format_docs is used to format the documents as a string - prompt is the prompt used to call LLM with - llm is used to call the LLM - StrOutputParser() is used to parse the output from the LLM\nLet’s call the OpenAI RAG chain\n# call openai rag chain\nopenai_rag_chain.invoke(\"What were the goals of the symposium?\")  # change the user query in the text here\n# call openai rag chain\n# This should ideally give \"I dont know\" - different from the llm.invoke() method where we do not give a custom prompt\nopenai_rag_chain.invoke(\"What is the capital of the world?\")  # change the user query in the text here\nFeel free to try out other queries and test out other prompts.\n\n\n17.4.6.2 RAG chain with Llama model\nLet’s code the RAG chain with Llama\n\n\nShow code\n\n# ollama rag chain\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\nollama_rag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | ollamallm\n    | StrOutputParser()\n)\n\n# call ollama rag chain\nollama_rag_chain.invoke(\"Who is the president of USA?\")\n# This should ideally give \"I dont know\" since the question asked is outside of the context in the vector store. \n# None of the document chunks in the vector store will have good similarity with the user query\n# Notice that Llama model does not give \"I dont know\" answer. However, it does say that the information is outside of the context provided.\n# Removes hallucinations and grounds the answer. \n\n\n\n\n\n\nTip\n\n\n\n\n\nGPT models are generally good at following instructions. OpenAI had an “InstructGPT” model which was specifically trained to follow instructions\n\n\n\n\n\n17.4.6.3 Adding sources to RAG\nNow that we have used RAG to control hallucinations and ground the LLM responses, let’s add source citations to the LLM generated response.\n\n\nShow code\n\n## adding sources to openai rag chain\n\nfrom langchain_core.runnables import RunnableParallel\n\nopenai_rag_chain_from_docs = (\n    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nopenai_rag_chain_with_source = RunnableParallel(\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n).assign(answer=openai_rag_chain_from_docs)\n\n# call openai rag chain with source\n# this will return the answer and the sources (context)\nopenai_rag_chain_with_source.invoke(\"What were the goals of the symposium?\")\nopenai_rag_chain_with_source.invoke(\"Why is tundra restoration and rehabilitation important\")\nopenai_rag_chain_with_source.invoke(\"Who is Bernadette Adams?\")\n\n\n17.4.6.4 RAG Steps\nThat concludes the RAG implementation. We have completed all the steps for Retrieval-Augmented Generation (RAG)\n\nPrepare data\nCreate a vector store and insert into db\nSearch the vector store and retrieve relevant documents\nCall LLM with the user query and the retrieved documents\nReturn the LLM response to the user",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#conclusion",
    "href": "sections/hands-on-lab-foundation-models.html#conclusion",
    "title": "17  Hands-On Lab: Foundation Models",
    "section": "17.5 Conclusion",
    "text": "17.5 Conclusion\nIn this hands-on lab on Foundation Models, we learned how to set up and use the Segment Anything Model 2 (SAM 2) for image segmentation tasks and implement a chatbot using the RAG system. We delved into many details related to both use cases and learned how to interact with the foundation models used in them. We hope this hands-on lab has given you a practical understanding of foundation models and how to interact with them.\n\n\n\n\n[1] IBM, “What Is Image Segmentation?  IBM.” Sep. 2023. Accessed: Dec. 21, 2024. [Online]. Available: https://www.ibm.com/think/topics/image-segmentation\n\n\n[2] N. Ravi et al., “SAM 2: Segment Anything in Images and Videos.” arXiv, Oct. 2024. doi: 10.48550/arXiv.2408.00714.\n\n\n[3] J. Amundson, “LeConte Glacier Unmanned Aerial Vehicle (UAV) imagery, LeConte Glacier, Alaska, 2018,” 2019, doi: 10.18739/A2445HC19.\n\n\n[4] L. Weng, “Prompt engineering,” lilianweng.github.io, Mar. 2023, Available: https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html",
    "href": "sections/reproducibility.html",
    "title": "18  Reproducibility",
    "section": "",
    "text": "18.1 Goal\nDownload Reproducibility slides\nThis session aims to highlight the importance of reproducibility in AI-driven Arctic research. Participants will learn about the challenges and best practices for ensuring that AI models and their results can be reproduced by other researchers, a cornerstone for building trust and advancing the field. The discussion will cover strategies for documenting experiments, sharing data and code, and using version control systems.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#introduction",
    "href": "sections/reproducibility.html#introduction",
    "title": "18  Reproducibility",
    "section": "18.2 Introduction",
    "text": "18.2 Introduction\nReproducibility is not a new topic when it comes to artificial intelligence and machine learning in science, but is more important than ever as AI research is often criticized for not being reproducible. This becomes particularly problematic when validation of a model requires reproducing it.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#why-is-reproducibility-important",
    "href": "sections/reproducibility.html#why-is-reproducibility-important",
    "title": "18  Reproducibility",
    "section": "18.3 Why is Reproducibility Important?",
    "text": "18.3 Why is Reproducibility Important?\n\n18.3.1 Reproducible vs. Replicable\nReproducibility is important in science because it allows other researchers to validate the results of a study and/or use the same analysis for processing their data, promoting open science and collaboration.\n\nReproducible means that other researchers can take the same data, run the same analysis, and get the same result.\nReplicable means other researchers can take different data, run the same analysis, and get their own result without errors.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#the-reproducibility-checklist",
    "href": "sections/reproducibility.html#the-reproducibility-checklist",
    "title": "18  Reproducibility",
    "section": "18.4 The Reproducibility Checklist",
    "text": "18.4 The Reproducibility Checklist\nThe Reproducibility Checklist was created by Canadian computer scientist, Joelle Pineau, with the goal of facilitating reproducible machine learning algorithms that can be tested and replicated. When publishing your model, it is beneficial to work through this checklist and ensure that you’re including the items on this checklist. The checklist is as follows:\nFor all models and algorithms, check that you include:\n\nA clear description of the mathematical setting, algorithm, and/or model\nAn analysis of the complexity (time, space, sample size) of any algorithm\nA link to a downloadable source code*, with specification of all dependencies, including external libraries\n\nFor any theoretical claim, check that you include:\n\nA statement of the result\nA clear explanation of each assumption\nA complete proof of the claim\n\nFor all figures and tables that include empirical results, check that you include:\n\nA complete description of the data collection process, including sample size\nA link to a downloadable version of the dataset or simulation environment\nAn explanation of any data that was excluded and a description of any preprocessing step\nAn explanation of how samples were allocated for training, validation, and testing\nThe range of hyperparameters considered, method to select the best hyperparameter configuration, and specification of each hyperparameter used to generate results\nThe exact number of evaluation runs\nA description of how experiments were run\nA clear definition of the specific measure of statistics used to report results\nClearly defined error bars\nA description of results with central tendency (e.g., mean) and variation (e.g., standard deviation)\nA description of the computing infrastructure used\n\n*With sensitive data or proprietary code, scientists may not wish to release all of their code and data. In this case, data can be anonymized and/or partial code can be released that won’t run but can be read and reproduced.\nConsider the sensitivity of your data/code when publishing.\n\nSensitive data should be anonymized before publishing\nResearchers or organizations may only release partial code if their code is proprietary\nBe sure that the partial code released can still be read and reproduced",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#sharing-code",
    "href": "sections/reproducibility.html#sharing-code",
    "title": "18  Reproducibility",
    "section": "18.5 Sharing Code",
    "text": "18.5 Sharing Code\nThe first step to solving the problem of reproducibility is sharing the code that was used to generate the model. This allows other researchers to:\n\nValidate the model\nTrack code construction and see any author annotations\nExpand on published work\n\nDespite this, sharing code does not always mean that models are fully reproducible. Many machine learning models are trained on restricted datasets or require extensive computing power for training the model. Because of this, there are a few additional criteria that improve reproducibility including:\n\nData and metadata availability (must be included without question)\nTransparency of the code you’re using and dependencies needed to run the code\nEasily installable computational analysis tools and pipelines\nInstalled software should behave the same on every machine and should have the same runtime\n\n\n18.5.1 Trips and Tricks to Sharing Code\n\nAvoid using absolute file paths when reading in data (and in general the use of slashes, as these differ between operating systems)\n\n https://keytodatascience.com/python-read-csv-txt-file/\n\nClean your data within your code\nAvoid copy/pasting in a spreadsheet\nAlways keep an unedited version of your raw data\n\nA general guide to publishing reproducible work:  https://medium.com/data-science/scientific-data-analysis-pipelines-and-reproducibility-75ff9df5b4c5",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#model-repositories",
    "href": "sections/reproducibility.html#model-repositories",
    "title": "18  Reproducibility",
    "section": "18.6 Model Repositories",
    "text": "18.6 Model Repositories\nPyTorch Hub is a pre-trained model repository designed to facilitate reproducibility and enable new research. It is easily usable with Colab and Papers with Code, but models must be trained on openly accessible data.\n\nPapers with Code is an open source hub for publications that include direct links to GitHub code, no account needed to access datasets.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#version-control",
    "href": "sections/reproducibility.html#version-control",
    "title": "18  Reproducibility",
    "section": "18.7 Version Control",
    "text": "18.7 Version Control\n\nVersion control is the process of keeping track of every individual change by each contributor that’s saved in a version control framework, or a special database. Keeping a history of these changes to track model performance relative to model parameters saves the time you’d spend retraining the model.\nThe three components of version control in machine learning are:\n\nCode: We recommend writing and storing your model code in the same language as your implementation code to make it easier to maintain all code and dependencies\nData: Versioning should link the data to the appropriate metadata and note any changes in either\nModel: The model connects your code and data with your model parameters and analysis\n\nUsing a version control system ensures easier:\n\nCollaboration\n\nCollaborators can easily pull changes from a shared repository, push their own changes, annotate their code, and revert back to previous versions of their model\n\nVersioning\n\nIf your model breaks, you’ll have a log of any changes that were made, allowing you or others to revert back to a stable version\n\nDependency tracking\n\nYou can test more than one model on different branches or repositories, tune the model parameters and hyperparameters, and monitor the accuracy of each implemented change\n\nModel updates\n\nVersion control allows for incrementally released versions while continuing the development of the next release",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#summary",
    "href": "sections/reproducibility.html#summary",
    "title": "18  Reproducibility",
    "section": "18.8 Summary",
    "text": "18.8 Summary\nConsider the following to ensure your model is reproducible:\n\nUse the reproducibility checklist for algorithms, theoretical claims, and figures/tables.\nAnonymize any sensitive data and remove proprietary code before publishing\n\nBUT still provide training data and enough code for others to replicate your model\n\nShare data and metadata, be transparent in any dependencies needed to run your model, use easily installable computational analysis tools and pipelines, and ensure installed software behaves the same on every machine (i.e. runtime)\nUse a pre-trained model repository (ex. PyTorch Hub) and publish to open-source journals/websites (ex. Papers with Code)\nPractice efficient version control (recommend GitHub if working with collaborators)",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#hands-on-activity",
    "href": "sections/reproducibility.html#hands-on-activity",
    "title": "18  Reproducibility",
    "section": "18.9 Hands-On Activity",
    "text": "18.9 Hands-On Activity\nLEGO Metadata: In groups of 3-5 people, take ~15 minutes to create a structure out of LEGO bricks and write instructions for a group who will recreate your structure based on these instructions.\nGroups will then be rotated and given instructions and LEGO pieces from another group where they will have ~15 minutes to attempt to recreate that group’s structure.\nWe will have a closing group discussion about this activity. Some questions include:\n\nWhat were some assumptions you made while writing your instructions?\nWere there any unexpected hurdles you encountered when writing your instructions or trying to replicate another group’s structure?\nWhat did you find most difficult about this activity?\nNow that you see how successful or unsuccessful the other group was in recreating your structure, is there anything you would do differently?\n\nThis activity was adapted from the Lego Metadata for Reproducibility Game Pack (doi: 10.36399/gla.pubs.196477) developed by Mary Donaldson and Matt Mahon at the University of Glasgow.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#references-resources",
    "href": "sections/reproducibility.html#references-resources",
    "title": "18  Reproducibility",
    "section": "18.10 References & Resources",
    "text": "18.10 References & Resources\n\nGundersen, Odd Erik, and Sigbjørn Kjensmo. 2018. “State of the Art: Reproducibility in Artificial Intelligence”. Proceedings of the AAAI Conference on Artificial Intelligence 32 (1).\nGundersen, Odd Erik, Yolanda Gil, and David W. Aha. “On Reproducible AI: Towards Reproducible Research, Open Science, and Digital Scholarship in AI Publications.” AI Magazine 39, no. 3 (September 28, 2018): 56–68.\n“How the AI Community Can Get Serious about Reproducibility.” Accessed September 18, 2024.\nAbid, Areeba. “Addressing ML’s Reproducibility Crisis.” Medium, January 7, 2021.\nPyTorch. “Towards Reproducible Research with PyTorch Hub.” Accessed September 18, 2024.\nStojnic, Robert. “ML Code Completeness Checklist.” PapersWithCode (blog), April 8, 2020.\nAkalin, Altuna. “Scientific Data Analysis Pipelines and Reproducibility.” Medium, July 5, 2021.\nHashesh, Ahmed. “Version Control for ML Models: What It Is and How To Implement It.” neptune.ai, July 22, 2022.\nNCEAS Learning Hub\nDonaldson, M. and Mahon, M. 2019. LEGO® Metadata for Reproducibility game pack. Documentation. University of Glasgow.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/the-fun-and-frontiers-of-ai.html",
    "href": "sections/the-fun-and-frontiers-of-ai.html",
    "title": "19  The Fun and Frontiers of AI: Innovation, Imagination, Interaction",
    "section": "",
    "text": "Overview\nThis presentation, “The Fun and Frontiers of AI in Arctic Science,” explores how artificial intelligence (AI) is transforming Arctic research. It introduces key AI subfields, including computer vision and natural language processing, and highlights their applications in environmental science, such as mapping permafrost thaw, forecasting weather patterns, and analyzing vast datasets. The slides showcase real-world projects like PolarHub and PolarGlobe, which leverage AI for data discovery and climate visualization. Additionally, the presentation discusses AI-driven search and recommendation systems, deep learning for sea ice monitoring, and the challenges of using large models in scientific research. Through these examples, it illustrates how AI can enhance scientific discovery while emphasizing the need for responsible and informed AI applications in Arctic studies.",
    "crumbs": [
      "<b>Day 5: AI Frontiers</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>The Fun and Frontiers of AI: Innovation, Imagination, Interaction</span>"
    ]
  },
  {
    "objectID": "sections/the-fun-and-frontiers-of-ai.html#slides",
    "href": "sections/the-fun-and-frontiers-of-ai.html#slides",
    "title": "19  The Fun and Frontiers of AI: Innovation, Imagination, Interaction",
    "section": "Slides",
    "text": "Slides\nDownload Slides",
    "crumbs": [
      "<b>Day 5: AI Frontiers</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>The Fun and Frontiers of AI: Innovation, Imagination, Interaction</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] A.\nK. Liljedahl et al., “Pan-Arctic ice-wedge\ndegradation in warming permafrost and its influence on tundra\nhydrology,” Nature Geoscience, vol. 9, no. 4, pp.\n312–318, Apr. 2016, doi: 10.1038/ngeo2674.\n\n\n[2] A.\nA. Vasiliev, D. S. Drozdov, A. G. Gravis, G. V. Malkova, K. E. Nyland,\nand D. A. Streletskiy, “Permafrost degradation in the\nWestern Russian Arctic,”\nEnvironmental Research Letters, vol. 15, no. 4, p. 045001, Apr.\n2020, doi: 10.1088/1748-9326/ab6f12.\n\n\n[3] S.\nL. Smith, H. B. O’Neill, K. Isaksen, J. Noetzli, and V. E. Romanovsky,\n“The changing thermal state of permafrost,” Nature\nReviews Earth & Environment, vol. 3, no. 1, pp. 10–23, Jan.\n2022, doi: 10.1038/s43017-021-00240-1.\n\n\n[4] T.\nA. Douglas, M. R. Turetsky, and C. D. Koven, “Increased rainfall\nstimulates permafrost thaw across a variety of Interior\nAlaskan boreal ecosystems,” npj Climate and\nAtmospheric Science, vol. 3, no. 1, pp. 1–7, Jul. 2020, doi: 10.1038/s41612-020-0130-4.\n\n\n[5] R.\nÍ. Magnússon et al., “Extremely wet summer events enhance\npermafrost thaw for multiple years in Siberian\ntundra,” Nature Communications, vol. 13, no. 1, p. 1556,\nMar. 2022, doi: 10.1038/s41467-022-29248-x.\n\n\n[6] L.\nM. Farquharson, V. E. Romanovsky, W. L. Cable, D. A. Walker, S. V.\nKokelj, and D. Nicolsky, “Climate Change\nDrives Widespread and Rapid\nThermokarst Development in Very\nCold Permafrost in the Canadian\nHigh Arctic,” Geophysical Research\nLetters, vol. 46, no. 12, pp. 6681–6689, 2019, doi: 10.1029/2019GL082187.\n\n\n[7] D.\nNotz and J. Stroeve, “Observed Arctic sea-ice loss\ndirectly follows anthropogenic CO2 emission,”\nScience, vol. 354, no. 6313, pp. 747–750, Nov. 2016, doi: 10.1126/science.aag2345.\n\n\n[8] D.\nM. Nielsen, M. Dobrynin, J. Baehr, S. Razumov, and M. Grigoriev,\n“Coastal Erosion Variability at the\nSouthern Laptev Sea\nLinked to Winter Sea\nIce and the Arctic\nOscillation,” Geophysical Research Letters,\nvol. 47, no. 5, p. e2019GL086876, 2020, doi: 10.1029/2019GL086876.\n\n\n[9] L.\nBruhwiler, F.-J. W. Parmentier, P. Crill, M. Leonard, and P. I. Palmer,\n“The Arctic Carbon Cycle\nand Its Response to Changing\nClimate,” Current Climate Change Reports,\nvol. 7, no. 1, pp. 14–34, Mar. 2021, doi: 10.1007/s40641-020-00169-5.\n\n\n[10] T.\nK. F. Campbell, T. C. Lantz, R. H. Fraser, and D. Hogan, “High\nArctic Vegetation Change\nMediated by Hydrological\nConditions,” Ecosystems, vol. 24, no. 1,\npp. 106–121, Jan. 2021, doi: 10.1007/s10021-020-00506-7.\n\n\n[11] S.\nC. Davidson et al., “Ecological insights from three\ndecades of animal movement tracking across a changing\nArctic,” Science, vol. 370, no. 6517, pp.\n712–715, Nov. 2020, doi: 10.1126/science.abb7080.\n\n\n[12] L.\nSuter, D. Streletskiy, and N. Shiklomanov, “Assessment of the cost\nof climate change impacts on critical infrastructure in the circumpolar\nArctic,” Polar Geography, vol. 42, no. 4,\npp. 267–286, Oct. 2019, doi: 10.1080/1088937X.2019.1686082.\n\n\n[13] M.\nL. Druckenmiller et al., “The\nArctic,” Bulletin of the American Meteorological\nSociety, vol. 102, no. 8, pp. S263–S316, Aug. 2021, doi: 10.1175/BAMS-D-21-0086.1.\n\n\n[14] M.\nPhilipp, A. Dietz, S. Buchelt, and C. Kuenzer, “Trends in\nSatellite Earth Observation for\nPermafrost Related\nAnalyses—A Review,”\nRemote Sensing, vol. 13, no. 6, p. 1217, Jan. 2021, doi: 10.3390/rs13061217.\n\n\n[15] “Changing state of Arctic\nsea ice across all seasons - IOPscience.” Accessed:\nOct. 18, 2024. [Online]. Available: https://iopscience.iop.org/article/10.1088/1748-9326/aade56\n\n\n[16] “AI in\nAnalytics: Top Use\nCases and Tools.” Accessed: Oct. 18,\n2024. [Online]. Available: https://www.marketingaiinstitute.com/blog/how-to-use-artificial-intelligence-for-analytics\n\n\n[17] M.\nI. Jordan and T. M. Mitchell, “Machine learning:\nTrends, perspectives, and prospects,”\nScience, vol. 349, no. 6245, pp. 255–260, Jul. 2015, doi: 10.1126/science.aaa8415.\n\n\n[18] A.\nVaswani et al., “Attention is all you need,” in\nAdvances in neural information processing systems, I. Guyon, U.\nV. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R.\nGarnett, Eds., Curran Associates, Inc., 2017. Available: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n\n\n[19] C.\nWitharana et al., “An\nObject-Based Approach for\nMapping Tundra\nIce-Wedge Polygon\nTroughs from Very High\nSpatial Resolution Optical\nSatellite Imagery,” Remote\nSensing, vol. 13, no. 4, p. 558, Jan. 2021, doi: 10.3390/rs13040558.\n\n\n[20] C.\nWitharana et al., “Ice-wedge polygon detection in\nsatellite imagery from pan-Arctic regions,\nPermafrost Discovery Gateway,\n2001-2021,” 2023, doi: 10.18739/A2KW57K57.\n\n\n[21] L.\nEdwards and M. Veale, “Enslaving the Algorithm:\nFrom a ‘Right to an\nExplanation’ to a ‘Right to\nBetter Decisions’?” Social\nScience Research Network, Rochester, NY, 2018. doi: 10.2139/ssrn.3052831.\n\n\n[22] S.\nFink, “This High-Tech\nSolution to Disaster Response\nMay Be Too Good to\nBe True,” The New York Times,\nAug. 2019, Accessed: Oct. 19, 2024. [Online]. Available: https://www.nytimes.com/2019/08/09/us/emergency-response-disaster-technology.html\n\n\n[23] D.\nLeslie, “Understanding artificial intelligence ethics and safety:\nA guide for the responsible design and implementation of\nAI systems in the public sector,” Zenodo, Jun. 2019.\ndoi: 10.5281/zenodo.3240529.\n\n\n[24] S.\nLo Piano, “Ethical principles in machine learning and artificial\nintelligence: Cases from the field and possible ways forward,”\nHumanities and Social Sciences Communications, vol. 7, no. 1,\npp. 1–7, Jun. 2020, doi: 10.1057/s41599-020-0501-9.\n\n\n[25] A.\nMcGovern, I. Ebert-Uphoff, D. J. G. Ii, and A. Bostrom, “Why we\nneed to focus on developing ethical, responsible, and trustworthy\nartificial intelligence approaches for environmental science,”\nEnvironmental Data Science, vol. 1, p. e6, Jan. 2022, doi: 10.1017/eds.2022.5.\n\n\n[26] T.\nShepherd, “Indigenous rangers to use SpaceCows\nprogram to protect sacred sites and rock art from feral herds,”\nThe Guardian, Sep. 2021, Accessed: Oct. 19, 2024. [Online].\nAvailable: https://www.theguardian.com/australia-news/2021/sep/15/indigenous-rangers-to-use-spacecows-program-to-protect-sacred-sites-and-rock-art-from-feral-herds\n\n\n[27] CSIRO, “SpaceCows:\nUsing AI to tackle feral herds in the\nTop End.” Accessed: Oct. 19, 2024.\n[Online]. Available: https://www.csiro.au/en/news/All/News/2021/September/SpaceCows-Using-AI-to-tackle-feral-herds-in-the-Top-End\n\n\n[28] A.\nD. S. A. (ADSA), “The Data Science\nEthos - Operationalizing Ethics\nin Data Science,” The Data Science\nEthos. Accessed: Oct. 19, 2024. [Online]. Available: https://ethos.academicdatascience.org/\n\n\n[29] W.\nChen and A. Quan-Haase, “Big Data Ethics\nand Politics: Toward New\nUnderstandings,” Social Science Computer\nReview, vol. 38, no. 1, pp. 3–9, Feb. 2020, doi: 10.1177/0894439318810734.\n\n\n[30] “Excavating AI.”\nAccessed: Oct. 19, 2024. [Online]. Available: https://excavating.ai/\n\n\n[31] J.\nGray and A. Witt, “A feminist data ethics of care for machine\nlearning: The what, why, who and how,” First\nMonday, Dec. 2021, doi: 10.5210/fm.v26i12.11833.\n\n\n[32] “Checklist to Examine\nAI-readiness for Open\nEnvironmental Datasets,”\nfigshare. Jun. 2022. doi: 10.6084/m9.figshare.19983722.v1.\n\n\n[33] S.\nLong and T. Romanoff, “AI-Ready\nOpen Data.”\nAI-Ready Open Data\n Bipartisan Policy\nCenter, 2023. Accessed: Oct. 19, 2024. [Online]. Available:\nhttps://bipartisanpolicy.org/explainer/ai-ready-open-data/\n\n\n[34] O.\nBenjelloun et al., “Croissant Format\nSpecification,” Croissant site. 2024.\nAccessed: Oct. 20, 2024. [Online]. Available: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n\n\n[35] M.\nD. Mahecha et al., “Earth system data cubes unravel\nglobal multivariate dynamics,” Earth System Dynamics,\nvol. 11, no. 1, pp. 201–234, Feb. 2020, doi: 10.5194/esd-11-201-2020.\n\n\n[36] “ERA5 hourly data on single\nlevels from 1940 to present.” doi: https://doi.org/10.24381/cds.adbb2d47.\n\n\n[37] O.\nJ. Reichman, M. B. Jones, and M. P. Schildhauer, “Challenges and\nopportunities of open data in ecology.” Science (New York,\nN.Y.), vol. 331, no. 6018, pp. 703–5, Feb. 2011, doi: 10.1126/science.1197962.\n\n\n[38] I.\nNitze et al., “DARTS:\nMulti-year database of AI detected\nretrogressive thaw slumps (RTS) and active layer detachment\nslides (ALD) in hotspots of the circum-arctic permafrost\nregion - v1,” 2024, doi: 10.18739/A2RR1PP44.\n\n\n[39] A.\nD. of F. Game, D. of C. and Fisheries, and A.-Y.-K. Region,\n“Salmon age, sex, and length data from\nArctic-Yukon-Kuskokwim\nRegion of Alaska, 1960-2017,” 2018,\ndoi: 10.5063/SN07CZ.\n\n\n[40] M.\nD. Wilkinson et al., “The FAIR\nGuiding Principles for scientific data\nmanagement and stewardship,” Scientific Data, vol. 3, p.\n160018, Mar. 2016, doi: 10.1038/sdata.2016.18.\n\n\n[41] M.\nD. Wilkinson, S.-A. Sansone, E. Schultes, P. Doorn, L. O. Bonino da\nSilva Santos, and M. Dumontier, “A design framework and exemplar\nmetrics for FAIRness,” Scientific Data,\nvol. 5, p. 180118, Jun. 2018, doi: 10.1038/sdata.2018.118.\n\n\n[42] G.\nPeng et al., “Harmonizing quality measures of\nFAIRness assessment towards machine-actionable quality\ninformation,” International Journal of Digital Earth,\nvol. 17, no. 1, p. 2390431, Dec. 2024, doi: 10.1080/17538947.2024.2390431.\n\n\n[43] M.\nJones et al., “MetaDIG:\nEngaging Scientists in the\nImprovement of Metadata and\nData,” Figshare, 2016, doi: 10.6084/m9.figshare.4055808.v1.\n\n\n[44] M.\nJones, P. Slaughter, and T. Habermann, “Quantifying\nFAIR: Metadata improvement and guidance in the\nDataONE repository network.” 2019. doi: https://doi.org/10.5063/f1kp80gx.\n\n\n[45] S.\nS. Chong, M. Schildhauer, M. O’Brien, B. Mecum, and M. B. Jones,\n“Enhancing the FAIRness of Arctic\nResearch Data Through\nSemantic Annotation,” Data Science\nJournal, vol. 23, no. 1, Jan. 2024, doi: 10.5334/dsj-2024-002.\n\n\n[46] R.\nBommasani et al., “On the opportunities and risks of\nfoundation models,” ArXiv, 2021, Available: https://crfm.stanford.edu/assets/report.pdf\n\n\n[47] A.\nRadford et al., “Learning transferable visual models from\nnatural language supervision,” CoRR, vol.\nabs/2103.00020, 2021, Available: https://arxiv.org/abs/2103.00020\n\n\n[48] J.\nDevlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of deep bidirectional transformers for language\nunderstanding,” CoRR, vol. abs/1810.04805, 2018,\nAvailable: http://arxiv.org/abs/1810.04805\n\n\n[49] OpenAI et al., “GPT-4 technical\nreport.” 2024. Available: https://arxiv.org/abs/2303.08774\n\n\n[50] A.\nDubey et al., “The llama 3 herd of models.” 2024.\nAvailable: https://arxiv.org/abs/2407.21783\n\n\n[51] A.\nKirillov et al., “Segment anything.” 2023.\nAvailable: https://arxiv.org/abs/2304.02643\n\n\n[52] Z.\nLiu et al., “Swin transformer: Hierarchical vision\ntransformer using shifted windows.” 2021. Available: https://arxiv.org/abs/2103.14030\n\n\n[53] A.\nRamesh et al., “Zero-shot text-to-image\ngeneration,” CoRR, vol. abs/2102.12092, 2021, Available:\nhttps://arxiv.org/abs/2102.12092\n\n\n[54] Y.\nLiu et al., “Sora: A review on background, technology,\nlimitations, and opportunities of large vision models.” 2024.\nAvailable: https://arxiv.org/abs/2402.17177\n\n\n[55] G.\nTeam et al., “Gemini: A family of highly capable\nmultimodal models.” 2024. Available: https://arxiv.org/abs/2312.11805\n\n\n[56] A.\nVaswani et al., “Attention is all you need,”\nCoRR, vol. abs/1706.03762, 2017, Available: http://arxiv.org/abs/1706.03762\n\n\n[57] L.\nWeng, “Attention? attention!”\nlilianweng.github.io, 2018, Available: https://lilianweng.github.io/posts/2018-06-24-attention/\n\n\n[58] J.\nCheng, L. Dong, and M. Lapata, “Long short-term memory-networks\nfor machine reading,” CoRR, vol. abs/1601.06733, 2016,\nAvailable: http://arxiv.org/abs/1601.06733\n\n\n[59] X.\nAmatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and M.\nKazi, “Transformer models: An introduction and catalog.”\n2024. Available: https://arxiv.org/abs/2302.07730\n\n\n[60] I.\nJ. Goodfellow et al., “Generative adversarial\nnetworks.” 2014. Available: https://arxiv.org/abs/1406.2661\n\n\n[61] T.\nKarras, S. Laine, and T. Aila, “A style-based generator\narchitecture for generative adversarial networks,” CoRR,\nvol. abs/1812.04948, 2018, Available: http://arxiv.org/abs/1812.04948\n\n\n[62] A.\nBrock, J. Donahue, and K. Simonyan, “Large scale GAN\ntraining for high fidelity natural image synthesis,”\nCoRR, vol. abs/1809.11096, 2018, Available: http://arxiv.org/abs/1809.11096\n\n\n[63] L.\nWeng, “What are diffusion models?”\nlilianweng.github.io, Jul. 2021, Available: https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\n\n\n[64] J.\nHo, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic\nmodels,” CoRR, vol. abs/2006.11239, 2020, Available: https://arxiv.org/abs/2006.11239\n\n\n[65] R.\nRombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n“High-resolution image synthesis with latent diffusion\nmodels,” CoRR, vol. abs/2112.10752, 2021, Available: https://arxiv.org/abs/2112.10752\n\n\n[66] N.\nRavi et al., “SAM 2: Segment anything in images and\nvideos,” arXiv preprint arXiv:2408.00714, 2024,\nAvailable: https://arxiv.org/abs/2408.00714\n\n\n[67] P.\nS. H. Lewis et al., “Retrieval-augmented generation for\nknowledge-intensive NLP tasks,” CoRR, vol.\nabs/2005.11401, 2020, Available: https://arxiv.org/abs/2005.11401\n\n\n[68] L.\nWeng, “Extrinsic hallucinations in LLMs.”\nlilianweng.github.io, Jul. 2024, Available: https://lilianweng.github.io/posts/2024-07-07-hallucination/\n\n\n[69] L.\nWeng, “Prompt engineering,” lilianweng.github.io,\nMar. 2023, Available: https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n\n\n[70] P.\nNorvig and S. J. Russell, Artificial intelligence: A modern\napproach, 3rd ed. Pearson, 2004. Available: https://books.google.com/books/about/Artificial_Intelligence.html?id=8jZBksh-bUMC\n\n\n[71] D.\nO. Hebb, The organization of behavior: A neuropsychological\ntheory. New York: Wiley, 1949. Available: https://en.wikipedia.org/wiki/The_Organization_of_Behavior\n\n\n[72] F.\nRosenblatt, “The perceptron: A probabilistic model for information\nstorage and organization in the brain,” Psychological\nReview, vol. 65, no. 6, pp. 386–408, 1958, doi: 10.1037/H0042519.\n\n\n[73] M.\nBennett, A brief history of intelligence: Evolution, AI, and the\nfive breakthroughs that made our brains, Hardcover. Harper,\n2023.\n\n\n[74] Inc. PitchBook Data, “Artificial\nintelligence & machine learning report, Q2 2024,” PitchBook,\n2024. Available: https://pitchbook.com/news/reports/q2-2024-artificial-intelligence-machine-learning-report\n\n\n[75] D.\nKawahara, S. Ozeki, and I. Mizuuchi, “A curiosity algorithm for\nrobots based on the free energy principle,” pp. 53–59, 2022, doi:\n10.1109/SII52469.2022.9708819.\n\n\n[76] T.\nWang, F. Wang, Z. Xie, and F. Qin, “Curiosity model policy\noptimization for robotic manipulator tracking control with input\nsaturation in uncertain environment,” Frontiers in\nNeurorobotics, vol. 18, 2024, doi: 10.3389/fnbot.2024.1376215.\n\n\n[77] IBM, “What Is\nImage Segmentation? \nIBM.” Sep. 2023. Accessed: Dec. 21, 2024. [Online].\nAvailable: https://www.ibm.com/think/topics/image-segmentation\n\n\n[78] N.\nRavi et al., “SAM 2: Segment\nAnything in Images and\nVideos.” arXiv, Oct. 2024. doi: 10.48550/arXiv.2408.00714.\n\n\n[79] J.\nAmundson, “LeConte Glacier\nUnmanned Aerial Vehicle\n(UAV) imagery, LeConte Glacier,\nAlaska, 2018,” 2019, doi: 10.18739/A2445HC19.\n\n\n[80] T.\nJackson, Artificial intelligence. New Burlington, 2025, p.\n176.",
    "crumbs": [
      "References"
    ]
  }
]