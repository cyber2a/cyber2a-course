[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "",
    "text": "Course Overview\nAI for Arctic Research represents an introduction to Artificial Intelligence (AI) techniques produced by the Cyber2A project, an innovative training program to empower the Arctic science community with advanced AI-driven data analytics and cyberinfrastructure (CI) skills to tackle the pressing challenges facing the Arctic and thus our planet. Today, Artificial Intelligence has become one of the most powerful tools to analyze Arctic big data and enable new ways of data-driven discovery. However, training on these emerging topics is often missing in current undergraduate and graduate curricula, particularly for active Arctic researchers. This project aims to fill this skills gap in order to foster the growth of an Arctic science workforce with strong data science skills through a series of complementary and mutually reinforcing training activities.\nThe week-long course is designed with a modular curriculum, where each module can be incorporated into learning activities across Universities and other organizations. The curriculum is free to be re-used, licensed under a CC-BY Attribution license and covers 5 main topical areas:\nThe sections presented here fit into a one-week workshop as follows, but the modules can be also used individually:",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nPlease note that by participating in this activity you agree to abide by the NCEAS Code of Conduct.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#session-structure-and-content",
    "href": "index.html#session-structure-and-content",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "Session Structure and Content",
    "text": "Session Structure and Content\nThe course has been designed with several guiding principals in mind.\n\nBalance Theory and Hands-On Work: Spend about one-third of the time on theory and the other two-thirds on hands-on activities.\nBuild Gradually: Start with the basics and build up gradually. Expand both the theory and hands-on tasks as you go along.\nChoose Your Tools: You can use either Jupyter Notebook or VSCode for the hands-on parts of your session. Choose whichever one you’re more comfortable with.\nOpen Data Licensing: Use open data for examples that can be ethically shared and re-used, both within the workshop and when the course materials are used and incorporated into other courses.\nOffer Support: Make sure participants know how to ask for help if they get stuck. Regularly ask participants if they’re keeping up and adjust the pace if needed.\nExtra Resources: Provide additional materials like readings or videos for participants who want to learn more.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "About this book",
    "text": "About this book\nCitation:\n\nWenwen Li, Anna Liljedahl, Matthew B. Jones, Chia-Yu Hsu, Alyona Kosobokova, Jim Regetz, Chandi Witharana, Yili Yang, Ben Galewsky, Minu Mathew, Sandeep Puthanveetil Satheesan, Nicole Greco, Kenton McHenry, Carmen Galaz García, Kate Holman Billmeier. 2024. AI for Arctic Research. Arctic Data Center. doi:10.18739/A2222R77V\n\nThe materials in this book are licensed for reuse, and are available from the cyber2a-course github repository. The book is written in Quarto, a cross platform markdown-based platform for writing books and technical materials that works with Python, R, and other languages.\n\nGetting Started: You can find a guide to getting started with Quarto, including editing and previewing content locally with various tools, here.\nQuarto Guide: You can find a comprehensive guide to Quarto here.\nFormat: You can choose to write your content in either Jupyter Notebook (.ipynb) or Markdown files (.qmd). Quarto can render both formats. And both formats can be easily included in other teaching materials.\nEmbed Notebooks: If you choose to write your content in Markdown but have a separate Jupyter Notebook for the hands-on part, you can embed the notebook. Follow the guide here to learn how to do this.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese course materials were developed with funding for Cyber2A from the National Science Foundation under award # 2230034 to W. Li and M. Jones and award # 2230035 to A. Liljedahl and K. McHenry",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Cyber2A: AI for Arctic Research",
    "section": "License",
    "text": "License\n\nCyber2A: AI for Arctic Research is licensed under CC BY 4.0",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "",
    "text": "1.1 The changing Arctic\nThe Arctic is one of the Earth’s remaining frontiers that is critical to the Earth’s climate system. Climate warming and change have pushed the Arctic ecosystem to a tipping point: the frozen is becoming unfrozen with subsequent dramatic impact to its terrestrial and coastal landscapes. Permafrost warming and degradation are documented across the Arctic[1], [2], [3], and are coupled with long-term global warming and extremes in air temperature and precipitation [4], [5], [6]. Further, Arctic sea ice is decreasing rapidly [7], which increases coastal erosion rates across the globe [8]. The Arctic region is remote and is experiencing dramatic changes with local and global implications due to the shift from ice to water: altered soil carbon fluxes [9], changes in vegetation cover [10], shifts in animal behavior [11], and challenges to infrastructure [12]. Accordingly, the transformation of ice to water through degrading permafrost and melting sea and lake ice reverberates through the entire Arctic ecosystem and, therefore, enlists the interest of a broad range of earth, engineering, and social science disciplines [13]. Remote sensing of satellite imagery is an important approach in developing Arctic baseline information, monitoring change, and exploring physical processes [14], [15]. Today, there exist important climatic, geological, biological and sociological data that are yet to be exploited by the Arctic science community. To make the best possible use of these data to address the pressing challenges facing the Arctic environment and Arctic people, the more advanced methods and tools that are available need to be applied. AI-driven analytics, especially those incorporating deep machine learning, can process Arctic big data, automatically detect hidden patterns, and derive new knowledge to enable a new wave of data-driven discovery [16].",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#the-changing-arctic",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#the-changing-arctic",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "",
    "text": "Arctic mountains",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#ai-for-arctic-challenges",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#ai-for-arctic-challenges",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.2 AI for Arctic Challenges",
    "text": "1.2 AI for Arctic Challenges\n\n\n\n\n\n\n“AI will be the most transformative technology since electricity.” – Eric Schmidt\n\n\n\n\n\n\n\n\n\n“AI is just another model.” – Unknown\n\n\n\n\n\n\n\n\n\nArtificial Intelligence\n\n\n\n\n\nArtificial Intelligence (AI) in its broadest sense describes the focus on computing systems that exhibit forms of intelligence. Multiple approaches towards AI have been identified, including:\n\nKnowledge Representation and Reasoning to gain a semantic, logical model of a system\nMachine Learning in which statistical models are used for pattern recognition and prediction\nNatural Language Processing for communication in human languages such as English\nExpert Systems using rule-based logical systems for decision-making\nLarge Language Models for filtering and generating language\n…\n\n\n\n\nThe pursuit of AI as a field has been around since the 1956 with the Dartmouth Workshop, but really took a leap forward in the 2010’s with rising performance of computing hardware and new techniques in machine learning, particularly in the field of deep learning. More recently, AI has entered the public consciousness with the promotion of large language models (LLMs) such as the GPT-3 transformer model and related generative AI systems that are based on foundation models and can quickly generate new outputs [17].\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\nMachine learning (ML) is the subfield of AI concerned with pattern detection using statistical models, which then can be applied to unseen data for prediction and extrapolation without explicit instructions [18]. This mechanistic view of ‘learning’ supports robust evaluation of error and has applications in computer vision, image recognition, speech recognition, text processing and filtering, and many more areas.\n\n\n\nTechniques for machine learning are often divided into three types (supervised, unsupervised, and reinforcement learning). These techiques differ based on the feedback provided to the learning system:\n\nSupervised learning: Training input data are labeled (often manually) by a human, and the algorithm learns by generalizing from these inputs to predict correct outputs\nUnsupervised learning: Without labels, the ML algorithm is designed to detect patterns and structure in the input, often using techniques like gradient descent, clustering, and classification algorithms.\nReinforcement learning: A ML algorithm learns dynamically from interactive input to solve a problem or learn a goal, where correct responses are rewarded (weighted) higher than less correct responses. Learning then becomes an optimization/hill-climbing problem.\n\nThese general approaches all have strengths and weaknesses, and are often used in combination to tackle different aspects of a learning problem.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#geospatial-ai",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#geospatial-ai",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.3 Geospatial AI",
    "text": "1.3 Geospatial AI\nIn this course, we will more narrowly focus on geospatial applications of AI, and particularly on the use of deep learning techniques that employ, for example, convolutional neural networks for feature recognition tasks across massive image datasets such as satellite imagery. As we’ll see during the course, advances in computing hardware, and particularly in available Graphical Processing Unit (GPU) performance have enabled massive growth in the scale of models that can be generated. Today, we can train deep learning models on high-resolution, sub-meter scale satellite imagery (e.g., pan-Arctic, 50cm Maxar imagery), and apply the generated models across the Arctic to better understand change at Arctic scales.\nFor one example, Witharana et al. [19] trained a convolutional neural network model on Maxar imagery, and used the trained model to detect permafrost ice-wedges across the entire Arctic at sub-meter scale [20], producing a map of over a billion vector features, and the first-ever permafrost map at this scale.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#welcome-and-introductions",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#welcome-and-introductions",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.4 Welcome and Introductions",
    "text": "1.4 Welcome and Introductions\nLet’s kick the week off with a warm welcome and round of introductions. We’ll start with our Cyber2A project instructors and speakers, and then introduce each of our participants. Everyone is here due to a deep interest in finding solutions to challenges in Arctic science, and everyone is on their own personal journey through data and science. To learn a little about one another, let’s share:\n\nName and affiliation\nYour data science background (be brief!)\nOne! thing you’d like to get out of the course\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/breaking-the-ice-with-ai-in-arctic-science.html#cyber2a-project",
    "href": "sections/breaking-the-ice-with-ai-in-arctic-science.html#cyber2a-project",
    "title": "1  Breaking the Ice with AI in Arctic Science",
    "section": "1.5 Cyber2A project",
    "text": "1.5 Cyber2A project\nDespite the power of these machine learning techniques for Arctic research, the Arctic community has been somewhat delayed compared to other geoscience disciplines in adopting these techniques.\n\nThe Cyber2A project aims to build an Arctic learning community to stimulate the use of GeoAI through data science education. This short-course represents a first pass at a survey of relevant AI techniques that would be useful across Arctic regions and disciplines. The goal is to produce an online curriculum and materials that can be used for self-paced learning by Arctic researchers, and can be included in University graduate and undergraduate courses. While there are many online tutorials on machine learning and AI, these materials will specifically target the types of data and challenges typically found in Arctic research, and focus in on the techniques that will make data science learning more approachable.\nThis course is also a starting point, and not an endpoint. We welcome feedback, suggestions, revisions, and edits to the materials. We want people to adopt, adapt, and revise the materials, and, importantly, contribute those changes back so that others can benefit from these curricular advances. Look for more from Cyber2A as we continue to engage in promoting the use of GeoAI across the Arctic.\n\n\n\n\n\n[1] A. K. Liljedahl et al., “Pan-Arctic ice-wedge degradation in warming permafrost and its influence on tundra hydrology,” Nature Geoscience, vol. 9, no. 4, pp. 312–318, Apr. 2016, doi: 10.1038/ngeo2674.\n\n\n[2] A. A. Vasiliev, D. S. Drozdov, A. G. Gravis, G. V. Malkova, K. E. Nyland, and D. A. Streletskiy, “Permafrost degradation in the Western Russian Arctic,” Environmental Research Letters, vol. 15, no. 4, p. 045001, Apr. 2020, doi: 10.1088/1748-9326/ab6f12.\n\n\n[3] S. L. Smith, H. B. O’Neill, K. Isaksen, J. Noetzli, and V. E. Romanovsky, “The changing thermal state of permafrost,” Nature Reviews Earth & Environment, vol. 3, no. 1, pp. 10–23, Jan. 2022, doi: 10.1038/s43017-021-00240-1.\n\n\n[4] T. A. Douglas, M. R. Turetsky, and C. D. Koven, “Increased rainfall stimulates permafrost thaw across a variety of Interior Alaskan boreal ecosystems,” npj Climate and Atmospheric Science, vol. 3, no. 1, pp. 1–7, Jul. 2020, doi: 10.1038/s41612-020-0130-4.\n\n\n[5] R. Í. Magnússon et al., “Extremely wet summer events enhance permafrost thaw for multiple years in Siberian tundra,” Nature Communications, vol. 13, no. 1, p. 1556, Mar. 2022, doi: 10.1038/s41467-022-29248-x.\n\n\n[6] L. M. Farquharson, V. E. Romanovsky, W. L. Cable, D. A. Walker, S. V. Kokelj, and D. Nicolsky, “Climate Change Drives Widespread and Rapid Thermokarst Development in Very Cold Permafrost in the Canadian High Arctic,” Geophysical Research Letters, vol. 46, no. 12, pp. 6681–6689, 2019, doi: 10.1029/2019GL082187.\n\n\n[7] D. Notz and J. Stroeve, “Observed Arctic sea-ice loss directly follows anthropogenic CO2 emission,” Science, vol. 354, no. 6313, pp. 747–750, Nov. 2016, doi: 10.1126/science.aag2345.\n\n\n[8] D. M. Nielsen, M. Dobrynin, J. Baehr, S. Razumov, and M. Grigoriev, “Coastal Erosion Variability at the Southern Laptev Sea Linked to Winter Sea Ice and the Arctic Oscillation,” Geophysical Research Letters, vol. 47, no. 5, p. e2019GL086876, 2020, doi: 10.1029/2019GL086876.\n\n\n[9] L. Bruhwiler, F.-J. W. Parmentier, P. Crill, M. Leonard, and P. I. Palmer, “The Arctic Carbon Cycle and Its Response to Changing Climate,” Current Climate Change Reports, vol. 7, no. 1, pp. 14–34, Mar. 2021, doi: 10.1007/s40641-020-00169-5.\n\n\n[10] T. K. F. Campbell, T. C. Lantz, R. H. Fraser, and D. Hogan, “High Arctic Vegetation Change Mediated by Hydrological Conditions,” Ecosystems, vol. 24, no. 1, pp. 106–121, Jan. 2021, doi: 10.1007/s10021-020-00506-7.\n\n\n[11] S. C. Davidson et al., “Ecological insights from three decades of animal movement tracking across a changing Arctic,” Science, vol. 370, no. 6517, pp. 712–715, Nov. 2020, doi: 10.1126/science.abb7080.\n\n\n[12] L. Suter, D. Streletskiy, and N. Shiklomanov, “Assessment of the cost of climate change impacts on critical infrastructure in the circumpolar Arctic,” Polar Geography, vol. 42, no. 4, pp. 267–286, Oct. 2019, doi: 10.1080/1088937X.2019.1686082.\n\n\n[13] M. L. Druckenmiller et al., “The Arctic,” Bulletin of the American Meteorological Society, vol. 102, no. 8, pp. S263–S316, Aug. 2021, doi: 10.1175/BAMS-D-21-0086.1.\n\n\n[14] M. Philipp, A. Dietz, S. Buchelt, and C. Kuenzer, “Trends in Satellite Earth Observation for Permafrost Related Analyses—A Review,” Remote Sensing, vol. 13, no. 6, p. 1217, Jan. 2021, doi: 10.3390/rs13061217.\n\n\n[15] “Changing state of Arctic sea ice across all seasons - IOPscience.” Accessed: Oct. 18, 2024. [Online]. Available: https://iopscience.iop.org/article/10.1088/1748-9326/aade56\n\n\n[16] “AI in Analytics: Top Use Cases and Tools.” Accessed: Oct. 18, 2024. [Online]. Available: https://www.marketingaiinstitute.com/blog/how-to-use-artificial-intelligence-for-analytics\n\n\n[17] A. Vaswani et al., “Attention is all you need,” in Advances in neural information processing systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., Curran Associates, Inc., 2017. Available: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n\n\n[18] M. I. Jordan and T. M. Mitchell, “Machine learning: Trends, perspectives, and prospects,” Science, vol. 349, no. 6245, pp. 255–260, Jul. 2015, doi: 10.1126/science.aaa8415.\n\n\n[19] C. Witharana et al., “An Object-Based Approach for Mapping Tundra Ice-Wedge Polygon Troughs from Very High Spatial Resolution Optical Satellite Imagery,” Remote Sensing, vol. 13, no. 4, p. 558, Jan. 2021, doi: 10.3390/rs13040558.\n\n\n[20] C. Witharana et al., “Ice-wedge polygon detection in satellite imagery from pan-Arctic regions, Permafrost Discovery Gateway, 2001-2021,” 2023, doi: 10.18739/A2KW57K57.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Breaking the Ice with AI in Arctic Science</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html",
    "href": "sections/ai-for-everyone.html",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "",
    "text": "Goals\nThis session aims to introduce AI to a non-specialist audience, ensuring that participants from any background can understand these essential concepts. The focus will be on explaining key terminology and the basic principles of machine learning and deep learning. By the end of this session, participants will have a solid foundational knowledge of key AI concepts, enabling them to better appreciate and engage with more advanced topics in the following sessions.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#the-foundations-of-ai",
    "href": "sections/ai-for-everyone.html#the-foundations-of-ai",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.1 The Foundations of AI",
    "text": "2.1 The Foundations of AI\nBefore reading the definition, take a moment: What do you think AI is? How would you define it?\n\n\n\n\n\n\nWhat is AI?\n\n\n\nArtificial Intelligence (AI) refers to the development of computer systems capable of performing tasks that typically require cognitive functions associated with human intelligence, such as recognizing patterns, learning from data, and making predictions.\n\n\nBut… there is a minor issue with this definition. What exactly is human intelligence?\nRecognizing patterns, learning, and making predictions are all functions of intelligence, but what lies at the core of a “conscious human”? Why is self-awareness important in cognition, and what evolutionary function does subjective, conscious experience serve?\nIn the philosophy of mind, this phenomenon is referred to as qualia, and there is still no definitive scientific answer to why qualia exist—at least, not yet (see theories of consciousness for more information).\nBut today, let’s focus on a simpler question. How do humans think?\nHistorically, before the 1950s–1960s, scientists believed humans think through a series of if/else statements (e.g., “If I drink more coffee, I’ll be jittery,” or “If a seagull spots my pizza, it’ll try to snatch a bite”). Geoffrey Hinton, a cognitive psychologist and computer scientist, was one of the advocates for an opposing idea: that humans think more experientially or probabilistically. For instance, based on the cloud cover today and similar past experiences, there’s a high probability of rain, so I’ll grab an umbrella. This idea laid the foundation for probabilistic algorithms and, ultimately, the field of Machine Learning.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#machine-learning",
    "href": "sections/ai-for-everyone.html#machine-learning",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.2 Machine Learning",
    "text": "2.2 Machine Learning\nTo quickly recap, AI is a broad term encompassing efforts to replicate aspects of human cognition. Machine Learning is a subset of AI that focuses on algorithms enabling computers to learn from data and build probabilistic models.\n\n\n\n\n\n\nWhat is ML?\n\n\n\nMachine Learning (ML) is a subset of AI that specifically focuses on algorithms that allow computers to learn from data and create probabilistic models.\n\n\n\n\n\nSource: Original comic by sandserif\n\n\nMachine Learning includes various types and techniques, but in this workshop we’ll primarily focus on Neural Networks (NNs).",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#neural-networks",
    "href": "sections/ai-for-everyone.html#neural-networks",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.3 Neural Networks",
    "text": "2.3 Neural Networks\nNNs are loosely inspired by the structure of the human brain and consist of interconnected nodes, or neurons, that process information. The principle “neurons that fire together, wire together” [1] captures the idea that the strength of their connections, known as weights, adjusts based on experience.\n\n\n\n\n\nFigure 2.1: Source [2] The parts of neuron: a cell body with a nucleus, branching dendrites, and a long axon connecting with thousands of other neurons at synapses.\n\n\n\n\n\n\nFigure 2.2: Structure of a neural network: Ramón y Cajal’s drawing of the cells of the chick cerebellum, from Estructura de los centros nerviosos de las aves, Madrid, 1905\n\n\n\n\n\n\n\n\n\n\nWhat is NN?\n\n\n\nNeural Network (NN) is a foundational technique within the field of machine learning. NNs are designed to simulate the way the human brain processes information by using a series of connected layers, or neurons, that transform and interpret input data.\n\n\nThe Perceptron [3], one of the earliest neural network models, was invented in 1957 by psychologist Frank Rosenblatt, who unfortunately did not live long enough to witness the far-reaching impact of his work. Rosenblatt’s Perceptron was a physical machine with retina-like sensors as inputs, wires acting as the hidden layers, and a binary output system. This invention marked the early stages of artificial intelligence, laying the groundwork for the powerful neural networks we use today.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#exercise-nn-playground",
    "href": "sections/ai-for-everyone.html#exercise-nn-playground",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.4 Exercise: NN Playground",
    "text": "2.4 Exercise: NN Playground\n\n\n\n\n\n\nWeb-based app, no setup or account required: playground.tensorflow.org\n\n\n\n\n\n\n.\n\n\nLevel 1: Browse around\n\nSwitch between different choices of datasets (on the left). See if anything changes.\nAdjust the ratio of training to test data. Does the quality of the output vary?\nExperiment with noise and batch size parameters. How does the output change?\n\nOrange indicates negative values, while blue represents positive values. Typically, an 80/20 split for training and testing data is used. Smaller datasets may need a 90% training portion for more examples, while larger datasets can reduce training data to increase test samples. Background colors illustrate the network’s predictions, with more intense colors representing higher confidence in its prediction. Adding noise during training helps the model generalize by recognizing true patterns, enhancing robustness and stability with real-world noisy data.\nLevel 2: Things to try\n\nAdd or remove hidden layers. Notice how it affects the neural network’s performance?\n\n\n\n\n\n\n\nHidden Layers\n\n\n\nHidden Layers are the layers that are neither input nor output. You can think of the values computed at each layer of the network as a different representation for the input X. Each layer transforms the representation produced by the preceding layer to produce a new representation.\n\n\n\nChange the number of neurons in the hidden layers. Can you see any impact on model predictions?\n\nStart with one hidden layer and one or two neurons, observing predictions (orange vs. blue background) against actual data points (orange vs. blue dots). With minimal layers and neurons, predictions are often inaccurate. Increasing hidden layers and neurons improves alignment with the actual data, illustrating how added complexity helps the model learn and approximate complex patterns more accurately.\nLevel 3: More things to try!\n\nExperiment with different features to see their impact on predictions.\nManually adjust the weight. You can see that the thickness of the line changed, which represents the strength of the connection.\n\n\n\n\n\n\n\nWeights\n\n\n\nWeights are parameters within the neural network that transform input data as it passes through layers. They determine the strength of connections between neurons, with each weight adjusting how much influence one neuron has on another. During training, the network adjusts these weights to reduce errors in predictions.\n\n\n\nChange the learning rate to observe its effect on training speed and accuracy.\nTry various activation functions to see how they influence model performance.\nExperiment with different problem types (e.g., classification vs. regression) and analyze the outcomes.\n\nAs you press the play button, you can see the number of epochs increase. In an Artificial Neural Network, an epoch represents one complete pass through the training dataset.\n\n\n\n\n\n\nLearning rate\n\n\n\nThe learning rate is a key setting or hyperparameter that controls how much a model adjusts its weights during training. A higher rate speeds up learning but risks overshooting the optimal solution, while a lower rate makes learning more precise but slower. It’s one of the most crucial settings when building a neural network.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#backpropagation",
    "href": "sections/ai-for-everyone.html#backpropagation",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.5 Backpropagation",
    "text": "2.5 Backpropagation\nInitially, neural networks were quite shallow feed-forward networks. Adding more hidden layers made training them difficult. However, in the 1980s—often referred to as the rebirth of AI—the invention of the backpropagation algorithm revolutionized the field. It allowed for efficient error correction and gradient calculation across layers, making it possible to train much deeper networks than before.\n\n\n\n\n\n\nWhat is backpropagation?\n\n\n\nBackpropagation is an algorithm that calculates the error at the output layer of a neural network and then “back propagates” this error through the network, layer by layer. It updates the connections (weights) between neurons to reduce the error, allowing the model to improve its accuracy during training.\n\n\n\n\n\n\n\nFigure 2.3: Backpropagation\n\n\n\n\n\n\nFigure 2.4: Source (3Blue1Brown)\n\n\n\n\nThus, the backpropagation algorithm enabled the training of neural networks with multiple layers, laying the foundation for the field of deep learning.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#deep-learning",
    "href": "sections/ai-for-everyone.html#deep-learning",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.6 Deep Learning",
    "text": "2.6 Deep Learning\n\n\n\n\n\n\nDeep Learning\n\n\n\nDeep Learning (DL) is a subset of ML, that uses multilayered neural networks, called deep neural networks.\n\n\n\n\n\nFigure 2.5: Source: Artificial Intelligence - A modern approach. [2]\n\n\n\nFigure 2.7: (a) A shallow model, such as linear regression, has short computation paths between inputs and output. (b) A decision list network has some long paths for some possible input values, but most paths are short. (c) A deep learning network has longer computation paths, allowing each variable to interact with all the others.\n\nDeep learning (DL) techniques are typically classified into three categories: supervised, semi-supervised, and unsupervised. Additionally, reinforcement learning (RL) is often considered a partially supervised technique, sometimes overlapping with unsupervised methods.\nSupervised Learning involves learning from labeled data, where models directly learn from input-output pairs. Common examples include Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Transformers. These models are generally simpler in terms of training and achieve high performance.\nSemi-Supervised Learning combines a small amount of labeled data with a large amount of unlabeled data, often using auto-labeling techniques. Examples include Self-training models, where a model iteratively labels data to improve, and Graph Neural Networks (GNNs), which are useful for understanding relationships between data points.\nUnsupervised Learning relies on unlabeled data, focusing on identifying patterns or structures. Popular models include Autoencoders, Generative Adversarial Networks (GANs), and Restricted Boltzmann Machines (RBMs).\n\n\n\nFigure 2.6: Organogram of AI algorithms.\n\n\nDespite advances in backpropagation, deep learning, computing power, and optimization, neural networks still face the problem known as catastrophic forgetting — losing old knowledge when trained on new tasks. Current AI models are often “frozen” and specialized, needing complete retraining for updates, unlike even simple animals that can continuously learn without forgetting [4]. This limitation is one of the reason that led to the development of specialized deep learning models, each with unique architectures tailored to specific tasks. Let’s explore how each of these models can be applied in scientific research!\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\n\n\n\nConvolutional Neural Networks (CNN) are artificial neural networks designed to process structured data like images. Originally inspired by the mammalian visual cortex, CNNs attempt to mimic how our brains process visual information in layers. First, brains interpret basic shapes like lines and edges and then move to more complex structures, like recognizing a dog ear or the whole animal. This feature, known as “invariance” allows us to recognize objects even if they’re rotated or appear in a different part of our vision.\nCNNs simulate this by using small, repeated filters, or kernels, that scan parts of an image to find basic shapes, edges, and textures, regardless of their location. This scanning process, called convolution, enables early CNN layers to detect simple patterns (like lines) and deeper layers to identify more complex shapes or objects.\nApplication: CNNs are highly effective for image-related tasks, making them ideal for analyzing satellite or drone imagery in ecology, identifying structures in biomedical imaging, and classifying galaxies in astrophysics.\n\n\n\nFigure 2.7: Source (Bennett, 2023) [4] Convolutional Neural Networks\n\n\nLimitations of CNNs\nIronically, though CNNs were inspired by the mammalian visual system, they struggle with tasks that even simpler animals like fish handle easily. CNNs have trouble with rotated or differently angled objects. Current work around it is to have variations of object images in training data with all kinds of different angles [4].\nWhile CNNs follow a layered structure, recent research reveals that the brain’s visual processing is more flexible and not as hierarchical as once believed. Our visual system can “skip” layers or process information in parallel, allowing simultaneous handling of different types of visual input across various brain regions.\n\n\n\n\n\n\n\n\n\nRecurrent Neural Networks\n\n\n\n\n\nRecurrent Neural Networks (RNN) are artificial neural networks designed to process sequential data. By incorporating cycles in their computation graph, RNNs can “remember” previous inputs, making them especially useful for tasks where context is important.\nApplication: These models are commonly used for time series data, such as weather forecasting, monitoring ecological changes over time, and analyzing temporal patterns in genomic data.\n\n\n\nFigure 2.8: Recurrent Neural Network. Source: dataaspirant.com\n\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning\n\n\n\n\n\nReinforcement Learning (RL) is a learning technique in which an agent interacts with the environment and periodically receives rewards (reinforcements) or penalties to achieve a goal.\nWith supervised learning, an agent learns by passively observing example input/output pairs provided by a “teacher.” Reinforcement Learning is one of the attempts to solve the catastrophic forgetting problem and introduce AI agents that can actively learn from their own experience in a given environment.[2]\n\n\n\nFigure 2.8: Reinforcement Learning\n\n\nApplications: RL is applied in robotics and can also assist with experiment simulation in science, environmental monitoring, autonomous driving, and creating AI opponents in gaming.\nOne example of RL is the Actor-Critic model, which divides the learning process into two roles: the Actor, who explores the environment and makes decisions, and the Critic, who evaluates these actions. The Critic provides feedback on the quality of each action, helping the Actor balance exploration (trying new actions) with exploitation (choosing actions with known rewards). Recent research has explored various algorithms to model curiosity in artificial agents [5] [6].\n\n\n\n\n\n\n\n\n\nLLMs: Transformers\n\n\n\n\n\nLarge Language Models (LLM) are a type of neural network that has revolutionized natural language processing (NLP). Trained on massive datasets, these models can generate human-like text, translate languages, create various forms of content, and answer questions informatively (e.g., GPT-3, Gemini, Llama).\n\n\n\nFigure 2.9: Large Language Models. Source: Artificial Intelligence - A modern approach. [2]\n\n\nTransformers, introduced in 2017, revolutionized NLP by introducing a mechanism called self-attention. Unlike previous models like Recurrent Neural Networks (RNNs) that processed language sequentially, Transformers use self-attention to assess relationships between all words in a sentence simultaneously. This allows them to dynamically focus on different parts of the input text and weigh the importance of each word in relation to others. This allows them to understand context and meaning with much better accuracy.\n\n\n\nFigure 2.10: Evolution of Natural Languge Processing. Source[2]\n\n\nThe success of LLMs has driven AI’s recent surge in popularity and research. Between 2010 and 2022, the volume of AI-related publications nearly tripled, climbing from about 88,000 in 2010 to over 240,000 by 2022. Likewise, AI patent filings have skyrocketed, increasing from roughly 3,500 in 2010 to over 190,000 in 2022. In the first half of 2024 alone, AI and machine learning companies in the United States attracted $38.6 billion in investment out of a total of $93.4 billion. [7]",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#ai-beyond-machine-learning",
    "href": "sections/ai-for-everyone.html#ai-beyond-machine-learning",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.7 AI Beyond Machine Learning",
    "text": "2.7 AI Beyond Machine Learning\nWithin the field of AI, there are many techniques that don’t rely on ML principles.\n\n\n\n\n\n\n\nTechnique\nDescription\n\n\n\n\nIf/Else or Rule-Based Systems\nCollections of predefined rules or conditions (if statements) to make decisions.\n\n\nSymbolic AI (Logic-Based AI)\nLogical rules and symbols to represent knowledge, focusing on reasoning through deductive logic.\n\n\nGenetic Algorithms (Evolutionary Algorithms)\nOptimization algorithms inspired by natural selection.\n\n\nFuzzy Logic\nA form of logic that works with “degrees of truth”, making it useful for uncertain or ambiguous scenarios.\n\n\nKnowledge Representation and Reasoning (KR&R)\nTechniques for structuring and processing information, often using ontologies and semantic networks.\n\n\nBayesian Networks\nProbabilistic graphical models that represent relationships between variables.\n\n\n\nRecent research increasingly combines various AI paradigms, such as symbolic AI and Knowledge Representation and Reasoning (KR&R), with Machine Learning (ML) to achieve a higher level of effectiveness tailored to specific tasks.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#the-future-of-ai-in-science",
    "href": "sections/ai-for-everyone.html#the-future-of-ai-in-science",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "2.8 The Future of AI in Science",
    "text": "2.8 The Future of AI in Science\nAI is transforming the scientific method by supporting each step of scientific discovery. Let’s consider how various AI techniques can be applied at each stage of the scientific process:\n\nObservation: Using computer vision for data collection.\nHypothesis: Clustering data with unsupervised learning.\nExperiment: Simulating environments through reinforcement learning.\nData Analysis: Simplifying data with PCA and classifying insights using neural networks or SVMs.\nConclusion: Combining LLMs with KR&R to generate complex findings and insights.\n\n\nIn conclusion, regardless of model type, high-quality data is essential for accurate AI predictions and insights. In the next sessions, we’ll explore practical tips for working with well-prepared, high-quality data.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-for-everyone.html#do-you-have-any-questions",
    "href": "sections/ai-for-everyone.html#do-you-have-any-questions",
    "title": "2  AI for Everyone: An Introductory Overview",
    "section": "Do You Have Any Questions?",
    "text": "Do You Have Any Questions?\nFeel free to reach out!\nEmail: alyonak@nceas.ucsb.edu\nWebsite: alonakosobokova.com\nYouTube: Dork Matter Girl\n\n\n\n\n[1] D. O. Hebb, The organization of behavior: A neuropsychological theory. New York: Wiley, 1949. Available: https://en.wikipedia.org/wiki/The_Organization_of_Behavior\n\n\n[2] P. Norvig and S. J. Russell, Artificial intelligence: A modern approach, 3rd ed. Pearson, 2004. Available: https://books.google.com/books/about/Artificial_Intelligence.html?id=8jZBksh-bUMC\n\n\n[3] F. Rosenblatt, “The perceptron: A probabilistic model for information storage and organization in the brain,” Psychological Review, vol. 65, no. 6, pp. 386–408, 1958, doi: 10.1037/H0042519.\n\n\n[4] M. Bennett, A brief history of intelligence: Evolution, AI, and the five breakthroughs that made our brains, Hardcover. Harper, 2023.\n\n\n[5] D. Kawahara, S. Ozeki, and I. Mizuuchi, “A curiosity algorithm for robots based on the free energy principle,” pp. 53–59, 2022, doi: 10.1109/SII52469.2022.9708819.\n\n\n[6] T. Wang, F. Wang, Z. Xie, and F. Qin, “Curiosity model policy optimization for robotic manipulator tracking control with input saturation in uncertain environment,” Frontiers in Neurorobotics, vol. 18, 2024, doi: 10.3389/fnbot.2024.1376215.\n\n\n[7] Inc. PitchBook Data, “Artificial intelligence & machine learning report, Q2 2024,” PitchBook, 2024. Available: https://pitchbook.com/news/reports/q2-2024-artificial-intelligence-machine-learning-report",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>AI for Everyone</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html",
    "href": "sections/ai-ready-data-in-arctic-research.html",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "",
    "text": "Goal\nThis session dives into the concept of ‘AI-ready data’ in Arctic science and geoscience, highlighting the importance of suitable data for AI applications. Participants will learn about creating and managing metadata and organizing data repositories. We’ll discuss best practices for data preparation and structuring for AI processing. By the end, participants will clearly understand AI-ready data characteristics and the steps to transform raw data for AI applications.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#are-we-ready-for-ai",
    "href": "sections/ai-ready-data-in-arctic-research.html#are-we-ready-for-ai",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.1 Are we ready for AI?",
    "text": "3.1 Are we ready for AI?\n\n\n\n\n\n\nWhat is AI-Ready Data?\n\n\n\nData that are accessible, preferably open, and well-documented, making them easily interpretable and machine-readable to simplify reuse.\n\n\nThis is really a variant on Analysis Ready Data (ARD), or, more recently, “Analysis Ready, Cloud Optimized (ARCO)” data.\n\n\n\nMahecha et al. 2020. [1] Visualization of the implemented Earth system data cube. The figure shows from the top left to bottom right the variables sensible heat (H), latent heat (LE), gross primary production (GPP), surface moisture (SM), land surface temperature (LST), air temperature (Tair), cloudiness (C), precipitation (P), and water vapour (V). The resolution in space is 0.25° and 8 d in time, and we are inspecting the time from May 2008 to May 2010; the spatial range is from 15° S to 60° N, and 10° E to 65° W.\n\n\nWorking with xarray and zarr, one can access many multi-petabyte earth systems datasets like CMIP6 (Coupled Model Intercomparison Project Phase 6) and ERA5 (Earth Re). For an overview of Zarr, see the Arctic Data Center Scalable Computing course chapter on Zarr.\nTake, for example, the ERA5 reanalysis dataset [2], which is normally downloadable in bulk from the Copernicus Data Service. ARCO-ERA5 is an Analysis Ready, Cloud Optimized variant of ERA5 which has been reprocessed into a consistent 0.25° global grid, and chunked and saved in Zarr format with extensive metadata such that spatial and temporal subsets are easily extracted. Hosted on the Google Cloud Storage service in a public bucket (gcp-public-data-arco-era5), anyone can easily access slices of this massive multi-petabyte dataset from anywhere on the Internet, and can be doing analysis in seconds. Let’s take a quick peek at this massive dataset:\nimport xarray\n\nds = xarray.open_zarr(\n    'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3',\n    chunks=None,\n    storage_options=dict(token='anon')\n)\nds\n\nWith one line of code, we accessed 273 climate variables (e.g., 2m_temperature, evaporation, forecast_albedo) spanning 8 decades at hourly time scales. And while this dataset is massive, we can explore it from the comfort of our laptop (not all at once, for which we would need a bigger machine!).\nSo, there’s nothing really special about AI-Ready data, in that a lot of the core requirements for Analysis Ready Data are exactly what are needed for AI modeling as well. Labeling is probably the main difference. Neverthless, many groups have gotten motivated by the promise of AI, and particularly machine learning, across disciplines. For example, the federal government has been ramping up readiness for AI across many agencies. In 2019, the White House Office of Science Technology and Policy (OSTP) started an AI-Readiness matrix, which was followed shortly by the National AI Initiative Act in 2020 [3].\n\nFor example, management agencies have started entire new programs to prepare data and staff for the introduction of AI and machine learning into their processes. One such program with a focus on AI-Ready data is NOAA’s Center for Artificial Intelligence (NCAI).\n\n\n\nNOAA NCAI\n\n\nIn beginning to define AI-Ready data for NOAA, Christensen et al. 2020 defined several axes for evaluation, including data quality, data acess, and data documentation. We’ll be dinving into many of these today and over the course of the week.\n\n\n\n\n\n\n\n\n\n\n\n\nData Quality\n\n\n\n\nCompleteness\nConsistency\nLack of bias\nTimeliness\nProvenance and Integrity\n\n\n\n\n\n\n\n\n\n\n\nData Access\n\n\n\n\nFormats\nDelivery options\nUsage rights\nSecurity / privacy\n\n\n\n\n\n\n\n\n\n\n\nData Documentation\n\n\n\n\nDataset Metadata\nData dictionary\nIdentifier",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#open-data-foundations",
    "href": "sections/ai-ready-data-in-arctic-research.html#open-data-foundations",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.2 Open Data Foundations",
    "text": "3.2 Open Data Foundations\nPreservation and open data access are the foundation of Analysis-Ready and AI-ready data. While all modeling and analysis requires access to data, the ability for AI to encompass massive swaths of information and combine disparate data streams makes open data incredibly valuable. And while the open data movement has seen massive growth and adoption, it’s an unfortunate fact that most research data collected today are still not published and accessible, and challenges to the realization of open data outlined by Reichman et al. (2011) are still prominent today [4].",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#arctic-data-center",
    "href": "sections/ai-ready-data-in-arctic-research.html#arctic-data-center",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.3 Arctic Data Center",
    "text": "3.3 Arctic Data Center\nNevertheless, progress has been made. The National Science Foundation Office of Polar Programs Data, Code, and Sample Management Policy (DCL 22-106) embraces the need to preserve, document, and share the data and results from NSF-funded research, and since 2016 has funded the Arctic Data Center to provide services supporting reseach community data needs. The center provides data submission guidelines and data curation support to create well-documented, understandable, and reusable data from the myriad projects funded by NSF and globally each year. In short, the Arctic Data Center provides a long-term home for over 7000 open, Arctic datasets that are AI-Ready. Researchers increasingly deposit large datasets from remote sensing campaigns using unmanned aerial vehicles (UAV), field expeditions, and observing networks, all of which are prime content for AI.\n\n\n\n\n\n\n\nArctic Data Center Catalog\n\n\n\nIn addition to raw observational data and remote sensing imagery, the ADC also stores and distributes model output, labeled training data, and other derived data products. A recent example comes from the Permafrost Discovery Gateway project, in which Neitze et al. used machine learning on multispectral PlanetScope imagery to extract high-resolution geospatial footprints for retrogressive thaw slumps (RTS) and active layer detachment (ALD) slides across the circum-Arctic permafrost region [5]. In addition, the dataset includes human-generated training labels, processing code, and model checkpoints – just what is needed for further advances in this critical field of climate research.\n\n\n\n\n\n\nDARTS retrogressive thaw slump dataset doi:10.18739/A2RR1PP44\n\n\nWhile this and other valuable data for cross-cutting analysis are available from the Arctic Data Center, there are many other repositories that hold relevant data as well. Regardless of which repository a researher has chosen to share their data, the important thing to remember is to do so – data on your laptop or a University web server are rarely accessible and ready for reuse.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#dataone",
    "href": "sections/ai-ready-data-in-arctic-research.html#dataone",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.4 DataONE",
    "text": "3.4 DataONE\nDataONE is a network designed to connect over 60 global data repositories (and growing) to improve the discoverability and accessiblilty of data from across the world. DataONE provides global data search and discovery by harmonizing myriad metadata standards used across the world, and providing an interoperability API across repositories to make datasets findable and programatically accessible regardless of where they live.\n\nFor example, a query across DataONE in 2024 revealed over 4500 datasets held by 16 different repositories, most of which are not specifically tied to Greenland research, per se.\n\nLooking across the whole of the Arctic, we found over 98,000 datasets from 39 data repositories. It is notable that only 6 of those repositories are focused on Arctic research (like the Arctic Data Center), while the rest are either general repositories or discipline specific repositories. For example, Pangaea as a generalist repository has the most datasets with over 10,000, but there are also significant and important data sets on archeology (TDAR), hydrology (HydroShare) and geochemistry (EarthChem).\n\n\n\nGraph of Arctic Data across DataONE",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#metadata-harmonization",
    "href": "sections/ai-ready-data-in-arctic-research.html#metadata-harmonization",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.5 Metadata harmonization",
    "text": "3.5 Metadata harmonization\nOne of the main roles of DataONE is to promote interoperability and improve the quality and discoverability of global data holdings – all of direct benefit to AI Ready data. DataONE promotes the use of detailed, discipline-specific metadata standards that enable researchers to comprehensively document the structure, contents, context, and protocols used when collecting data. For example, a good metadata record records not only the bibliographic information about the Dataset creators, but also documents the spatial and temporal extent of the data, the methods used to collect it, the types of measured properties that were observed or modeled, and other details that are fundamental to the proper interpretation and reuse of the data. Different disciplines focus on different standards: in ecology and environmental science, where biological metadata on taxonomy are important, the Ecological Metadata Language (EML) is used extensively, whereas in geospatial science where time and space are critical, the emphasis is on the ISO 19115 family of metadata standards. Overall, DataONE supports more than a dozen metadata variants, and can be extended to support more. Across the Arctic, we find datasets that use many different metadata approaches.\n\nDataONE harmonizes these standards by cross-walking them conceptually and making the data available for search through an integrated discovery portal and API. And DataONE promotes semantic labeling of the data as well, particularly for measurement types (e.g., fork length for fish length meeasurements) and dataset classification [6]. These annotations are indexed against controlled, ontologically-precise term labels that are stored in queryable systems. For example, the Ecosystem Ontology (ECSO, the Environment Ontology (ENVO), and many others contain precisely defined terms that are useful for precise dataset labeling to differentiate subtly different terms and concepts.\n\n\n\nA sub-Arctic salmon-related dataset [7], showing annotations for each of the measured variables in the dataset. Each annotation is to a precisely defined concept or term from a controlled vocabulary, allowing subtle differences in methodology to be distinguished, which helps with both data discovery and proper reuse. The underlying metadata model is machine-readable, allowing search systems, and amchine learning harvesters to make use of this structured label data.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#croissant-metadata-for-machine-learning",
    "href": "sections/ai-ready-data-in-arctic-research.html#croissant-metadata-for-machine-learning",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.6 Croissant metadata for machine learning",
    "text": "3.6 Croissant metadata for machine learning\nWhile domain-specific metadata dialects continue to proliferate, an increasing number of data repositories support schema.org as a lingua franca to describe datasets on the web for discoverability. The Science on Schema.org (SOSO) project provides interoperability guidelines for using schema.org metadata in dataset landing pages, and DataONE supports search across repositories that produce schema.org. That said, the dialect is fairly lightweight, somewhat lossely defined, and therefore permits some ambiguity in usage. But it has the major advantage that, as a graph-based metadata dialect, it can be easily extended to support new terms and use cases.\nThe Croissant specification [8] extends schema.org with more precise and structured metadata to enable machine-interpretation and use of datasets across multiple tools. While the vocabulary is not as rich as, for example, the ISO 19115 metadata for geospatial metadata, it does provide a more strict structural definition of data types and contents that plain schema.org. A quote from the specification illustrates its intended scope [8]:\n\nThe Croissant metadata format simplifies how data is used by ML models. It provides a vocabulary for dataset attributes, streamlining how data is loaded across ML frameworks such as PyTorch, TensorFlow or JAX. In doing so, Croissant enables the interchange of datasets between ML frameworks and beyond, tackling a variety of discoverability, portability, reproducibility, and responsible AI (RAI) challenges.\n\nCroissant has also explicitly defined metadata to meet the needs of machine-learning tools and algorithms. For example, Crosissant supports the definition of categorical values, data splits for training, testing, and prediction, labels/annotations, specification of bounding boxes and segmentation masks.\nLabel Data. As an example, Crioissant has specific metadata fields designed to capture which fields with the data contain critical label data, which are used by supervised learning workflows. The Croissant metadata class cr:Label can be used in a RecordSet to indicate that a specific field contians labels that apply to the that record.\n{\n  \"@type\": \"cr:RecordSet\",\n  \"@id\": \"images\",\n  \"field\": [\n    {\n      \"@type\": \"cr:Field\",\n      \"@id\": \"images/image\"\n    },\n    {\n      \"@type\": \"cr:Field\",\n      \"@id\": \"images/label\",\n      \"dataType\": [\"sc:Text\", \"cr:Label\"]\n    }\n  ]\n}\nThe intention is that multiple tools, all supporting the Crosissant metadata model, will be able to exchange ML-related data seamlessly. A number of ML tools support Croissant out-of-the-box, but only a tiny fraction of the datasets available today use this nascent vocabulary. In addition, it lacks the sophistication of Analysis Ready, Cloud Optimized (ARCO) data standards like Zarr that permit seamless access to massive data with minimal overhead. But it has a lot of promise for streamlining AI-Ready data, and can be used on top of exiting standards like Zarr.\n\n\n\nCommon ML data providers like Hugging Face and Kaggle could use Croissant to produce ML-optimized datasets, which in turn can be seamlessly loaded and used with compatible ML libraries such as TensorFlow and PyTorch. Image credit: [8]",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#data-quality-1",
    "href": "sections/ai-ready-data-in-arctic-research.html#data-quality-1",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.7 Data Quality",
    "text": "3.7 Data Quality\nOne of the primary determinants of AI-Ready data is whether the data are of sufficient quality for the intended purpose. While the quality of a dataset may have been high for the iniital hypothesis for which it was generated, it might be quite low (e.g., due to biased or selective sampling) at the scales at which machine learning might operate. Consequently, it is fundamentally important to assess the quality of datasets during the planning and execution of an AI project. Questions such as the following would be of prime interest:\n\nDoes the dataset represent the complete population of interest?\nDoes training data reflect an unbiased sample of that population?\nAre the data well-documented, enabling methodological interpretation?\nDid data collection procedures follow standards for responsible AI and ethical reseatch practices?\nAre the data Tidy (normalized) and structured (see Tidy Data lesson)?\nAre geospatial data accessibly structured, with sufficient metadata (e.g., about the Coordinate Reference System)?\n…\n\n\nMany of these issues are encompassed by the FAIR Principles, which are intended to ensure that published data are Finadable, Accessible, Interoperable, and Reusable [9], [10]. While there are a huge variety of methods to assess data quality in use across disciplines, some groups have started to harmonize rubrics for data quality and how to represent data quality results in metadata records (see [11]).\nDataONE is one such group that has operationalized FAIR Assessment [11]. Within the DataONE network, all compatible datasets are evaluated using an automated FAIR rubric which rates the dataset on 52 FAIR checks [12], [13]. Each of these checks is atomic, and looks at a small facet of the dataset quality, but combined they give a powerful assessment of dataset readiness for various analytical purposes. And these suites of checks are extensible, so different groups can create suites of automated quality assessment checks that match their needs and recommendations as a community.\n\n\n\n\nDataONE FAIR assessment\n\n\n\nWe see a marked improvement in dataset quality across the FAIR axes as datasets go through our curation process at the Arctic Data Center.\nThe Arctic Data Center team is currently working to extend this quality assessment suite to deeper data quality checks. Most of the current checks are based on metadata, mainly because these are accessible through the centralized DataONE network, whereas data are distributed throughout the network, and are much larger. The ADC data quality suite will assess generic quality checks and domain-specific quality checks to produce both dataset-level and project-level quality reports. Some of the key types of checks that are posisble with the system include:\n\n\n\nGeneric Checks\nDomain/Discipline Checks\n\n\n\n\nDomain and range values\nTree diameters don’t get smaller\n\n\nData type conformance\nUnit and measurement semantics\n\n\nData model normal\nSpecies taxonomy consistent\n\n\nChecksum matches metadata\nOutlier flagging\n\n\nMalware scans\nAllometric relations among variables\n\n\nCharacters match encoding\nCalibration and Validation conformance\n\n\nPrimary keys unique\nTemporal autocorrelation\n\n\nForeign keys valid\nData / model agreement",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/ai-ready-data-in-arctic-research.html#esip-ai-readiness-checklist",
    "href": "sections/ai-ready-data-in-arctic-research.html#esip-ai-readiness-checklist",
    "title": "3  AI-Ready Data in Arctic Research",
    "section": "3.8 ESIP AI-Readiness Checklist",
    "text": "3.8 ESIP AI-Readiness Checklist\nTo round out our survey of AI-Readiness in data, let’s look at the AI-Readiness Checklist that has been developed by an inter-agency collaboration at the Earth Science Information Partners (ESIP) [14]. This checklist was designed as a way to evaluate the readiness of a data product for use in specific AI workflows, but it is general enough to apply to a wide variety of datasets. In general, the checklist asks questions about four major areas of readiness:\n\nData Preparation\nData Quality\nData Documentation\nData Access\n\nThe challenge with this checklist and others is that readiness is truly in the eye of the beholder. Each project has unique data needs, and what is a fine dataset for one analytical purpose (e.g., a regional model) may be entirely inadequate for another (e.g., a pan-Arctic model).\n\n\n\n\n\n\nExercise: Assess Dataset Readiness\n\n\n\nThere are a huge variety of datasets available from DataONE and the Arctic Data Center, and many other repositories. In this exercise we will do a quick assessment of AI Readiness for the ice wedge polygon permafrost dataset from the Permafrost Discovery Gateway project [[5]], using the ESIP AI-Readiness Checklist.\nLink to dataset: - doi:10.18739/A2KW57K57 - Visualize Permafrost Ice Wedge Polygon data on PDG:\nWe’re going to break into groups, and each group will work on a portion of the evaluation for the dataset. The groups are:\n\nGroup A (Preparation)\nGroup B (Data Quality)\nGroup C (Data Documentation)\nGroup D (Data Access)\n\nInstructions: - Make a copy of the AI-Readiness Checklist spreadsheet - Split into 4 groups and try to quickly answer the questions (we won’t really have time, so don’t get too bogged down)\nQuestions to consider as you are doing the assessment: - How much time would it take for you to do a true assessment? - How useful would this assessment be to you if it were available for most datasets? - Is there a correct answer to the checklist questions?\n\n\n\n\n\n\n\n[1] M. D. Mahecha et al., “Earth system data cubes unravel global multivariate dynamics,” Earth System Dynamics, vol. 11, no. 1, pp. 201–234, Feb. 2020, doi: 10.5194/esd-11-201-2020.\n\n\n[2] “ERA5 hourly data on single levels from 1940 to present.” doi: https://doi.org/10.24381/cds.adbb2d47.\n\n\n[3] S. Long and T. Romanoff, “AI-Ready Open Data.” AI-Ready Open Data  Bipartisan Policy Center, 2023. Accessed: Oct. 19, 2024. [Online]. Available: https://bipartisanpolicy.org/explainer/ai-ready-open-data/\n\n\n[4] O. J. Reichman, M. B. Jones, and M. P. Schildhauer, “Challenges and opportunities of open data in ecology.” Science (New York, N.Y.), vol. 331, no. 6018, pp. 703–5, Feb. 2011, doi: 10.1126/science.1197962.\n\n\n[5] I. Nitze et al., “DARTS: Multi-year database of AI detected retrogressive thaw slumps (RTS) and active layer detachment slides (ALD) in hotspots of the circum-arctic permafrost region - v1,” 2024, doi: 10.18739/A2RR1PP44.\n\n\n[6] S. S. Chong, M. Schildhauer, M. O’Brien, B. Mecum, and M. B. Jones, “Enhancing the FAIRness of Arctic Research Data Through Semantic Annotation,” Data Science Journal, vol. 23, no. 1, Jan. 2024, doi: 10.5334/dsj-2024-002.\n\n\n[7] A. D. of F. Game, D. of C. and Fisheries, and A.-Y.-K. Region, “Salmon age, sex, and length data from Arctic-Yukon-Kuskokwim Region of Alaska, 1960-2017,” 2018, doi: 10.5063/SN07CZ.\n\n\n[8] O. Benjelloun et al., “Croissant Format Specification,” Croissant site. 2024. Accessed: Oct. 20, 2024. [Online]. Available: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n\n\n[9] M. D. Wilkinson et al., “The FAIR Guiding Principles for scientific data management and stewardship,” Scientific Data, vol. 3, p. 160018, Mar. 2016, doi: 10.1038/sdata.2016.18.\n\n\n[10] M. D. Wilkinson, S.-A. Sansone, E. Schultes, P. Doorn, L. O. Bonino da Silva Santos, and M. Dumontier, “A design framework and exemplar metrics for FAIRness,” Scientific Data, vol. 5, p. 180118, Jun. 2018, doi: 10.1038/sdata.2018.118.\n\n\n[11] G. Peng et al., “Harmonizing quality measures of FAIRness assessment towards machine-actionable quality information,” International Journal of Digital Earth, vol. 17, no. 1, p. 2390431, Dec. 2024, doi: 10.1080/17538947.2024.2390431.\n\n\n[12] M. Jones, P. Slaughter, and T. Habermann, “Quantifying FAIR: Metadata improvement and guidance in the DataONE repository network.” 2019. doi: https://doi.org/10.5063/f1kp80gx.\n\n\n[13] M. Jones et al., “MetaDIG: Engaging Scientists in the Improvement of Metadata and Data,” Figshare, 2016, doi: 10.6084/m9.figshare.4055808.v1.\n\n\n[14] “Checklist to Examine AI-readiness for Open Environmental Datasets,” figshare. Jun. 2022. doi: 10.6084/m9.figshare.19983722.v1.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI-Ready Data in Arctic Research</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html",
    "href": "sections/data-annotation.html",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "",
    "text": "Goals\nThis session explores the critical role of training data in deep learning, focusing on data annotation methods, tools, and strategies for acquiring high-quality data. Participants will learn how well-annotated data supports effective deep learning models, understanding the challenges and best practices in data annotation. By the end, participants will be equipped to prepare their datasets for deep learning.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#key-elements",
    "href": "sections/data-annotation.html#key-elements",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "Key Elements",
    "text": "Key Elements\nTraining data’s role, annotation methods/tools, annotated data’s importance, annotation challenges, annotation best practices, dataset preparation",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#annotation-fundamentals",
    "href": "sections/data-annotation.html#annotation-fundamentals",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "4.1 Annotation Fundamentals",
    "text": "4.1 Annotation Fundamentals\n\n\n\n\n\n\nHighlights\n\n\n\n\nReiterate ideas related to supervised learning, and the core idea of learning from examples\nDiscuss key role of labeling/annotation in general for generating examples to learn from\nTake a quick tour of label/annotation examples across various ML applications (structured data, text, audio, image, video, etc)\nTalk about some general challenges of procuring/producing labeled data for Machine Learning\n\n\n\n\n4.1.1 Fueling intelligence: It’s All About the Data!\nThe modern AI renaissance is driven by the synergistic combination of Computing advances, more & better data for training, and algorithmic innovations.\n\n\n\nSource: OECD.ai\n\n\nEach of these is critical, but you really can’t overstate the importance of massively upscaling training and validation data. Indeed, to a large extent, the most important recent advances in algorithms and compute have been those that allow us to efficiently use huge amounts of data. The more data available, the better the model can learn.\nRemember that in Machine Learning:\n\nYou are building a model to produce some desired output for a given input. Imagine handing a model an aerial photo that contains a water body, or a camera trap video that contains a bear, or an audio recording that captures a the song of a particular bird species. In each case, you want the model to correctly detect, recognize, and report the relevant feature.\nTo achieve this, you do not build this model by instructing the computer how to detect the water body or the bear or the bird species. Instead, you assemble many (often many, many!) good examples of the phenomena of interest, and feed them to an algorithm that allows the model to adaptively learn from these examples. Now, in practice there may be rule-based guardrails, but we can talk about that separately later in the course.\n\n\n\n\n.\n\n\nMuch of this course is about understanding what kinds of model structures and learning algorithms allow this seemingly magical learning to happen inside the computer, and what the end-to-end process looks like. But for now, we are going to focus on the input data. And as we embark, it is essential is that this core concept makes sense to you:\nFor any project involving development of an AI model, you will quite likely be starting with a generic algorithm that has limited or even zero specific knowledge of your particular application area. Unlike with “classical” modeling, the way you will adapt it to apply to your project is not by hand-tweaking parameters or choosing functional forms describing your phenomenon of interest, but rather by exposing this generalized algorithm to many relevant examples to learn from.\nBottom line, much like vehicles without fuel, even the best training algorithms in the world will just sit and gather dust if they don’t have sufficient data to learn from!\n\n\n\nSource: Walking Dead Fandom\n\n\nUltimately, although you will need to have an understanding of algorithmic and models, and learn how to operationalize them on compute platforms, your success in applying AI (especially if you are training and/or fine-tuning models, not simply applying pre-trained models) will depend on your ability to implement a robust and effective data pipeline, from data collection methods to data annotation to data curation.\n\n\n\n\n\nSource: DZone\n\n\nIn this module, we focus on data annotation.\n\n\n4.1.2 What is annotation?\nData annotation is the process of labeling or marking up data with information that is not already explicit in the data itself.\nIn general, we do this to provide important and relevant context or meaning to the data. As humans, especially in knowledge work, we do this all the time for the purpose of sharing information with others.\n\n\n\nSource: PowerPoint Tricks\n\n\nIn the context of Machine Learning and AI, our objective is to teach a model how to create accurate and useful annotations itself when it encounters new, unannotated data in the future. In order to do this, we need to provide the model with annotated examples that it can train on.\nTo put it a different way, annotation is the process of taking some data just like the kind of data you will eventually feed into the model, and attaching to it the correct answer to whatever question you will be asking the model about that data.\nSimply put, annotation refers to labeling data with information that a model needs to learn, and is not already inherently present in the data.\n\n\n\n\n\n\nThe term “annotation” is synonymous with “labeling”\n\n\n\n\n4.1.2.1 Examples\n\n\n\n\n\n\n\nTabular Data Annotation\n\n\n\n\n\n\n\nLabel (aka Target) column: Species\n\nWhen working with tabular data, we don’t usually talk about “annotating” the data. Nevertheless, the concept of labeling for supervised learning tasks (such as classification and regression) still applies, and indeed it’s common practice to refer to the data used for classification and regression model training as “labeled data”. Labeled tabular data contains a column designated as the target for learning, i.e. the column containing the value that a model learns to predict. Depending on the context (and background of the writer/speaker), you might also hear this referred to as the label, outcome variable, dependent variable, or even just y variable. If this is not already inherently present in the dataset, it must be added by an annotator before proceeding with modeling.\n\n\n\n\n\n\n\n\n\nText Annotation\n\n\n\n\n\n\n\nSentiment: Positive\nParts of speech: most::adv, beautiful::adj\nNamed entity: Alaska\n\n\n\n\n\n\n\n\n\n\nAudio Annotation\n\n\n\n\n\n\n\nVoice recognition\nSpeech to text\n\n\n\n\n\n\n\n\n\n\nImage Annotation\n\n\n\n\n\n\n… our focus today and this week! See details below.\n\n\n\n\n\n\n\n\n\n\nVideo Annotation\n\n\n\n\n\n\nLike image annotation, but with many frames! The focus is often on tracking movement of objects, detecting change, and recognizing activities.\n\n\n\n\n\n\n4.1.3 Why is annotation so important?\nWe’ve already talked about the critical role of data overall in enabling supervised learning, and the role of annotation in explicitly adding or revealing the information in the data.\n\nMore specifically, the annotated data will be used at training time, when a specific learning algorithm will use the information in your annotated data to update internal parameters to yield a specific parameterized (aka “trained”) version of the model that can do a sufficiently good job at getting the right answer when exposed to new data that it hasn’t seen before, and doesn’t have labels.\nThe overall volume and quality of the annotations will have a huge impact on the following characteristics of a model trained on those data:\n\nAccuracy\nPrecision\nGeneralizability\n\nObviously there is a bit of tension here! The point of training the model is do something for you. But in order for the AI to be able to do this, you have to first teach it how, which means doing the very thing that you want it to do.\nThink of it like hiring a large team of interns. Yes, it takes extra time up front to get them trained up. But once you do that, you’re able to scale up operations far beyond what you could do on your own.\nThis raises a few questions that we’ll touch on as we proceed through the course:\n\nIs there a model out there that already knows at least something about what I’m trying to do, so I’m not training it from scratch? Maybe yes! This is a benefit that foundation models (and more generally, transfer learning) offer. To build on the human intern analogy, if you can hire undergrad researchers studying in a field relevant to the task, you’re likely to move much faster than if you hired a 1st grader!\nHow much annotated data do I need? Unfortunately, there is no simple answer. It depends on complexity of task, the clarity of the information, etc. So as we’ll discuss, best practice is to proceed iteratively.\n\n\n\n4.1.4 Annotation challenges\nBy now it should be clear that your goal in the data annotation phase is to quickly and correctly annotate a large enough corpus of inputs that collectively provide an adequate representation of information you want the model to learn.\nHere are some of the key challenges to this activity:\n\n\n\n\n\n\nScalability\n\n\n\n\n\nSimply put, annotating large datasets can be time-consuming!\nThis is especially the case for more complex annotation tasks. Identifying a penguin standing on a rock is one thing, but comprehensively identifying and label all land cover types present in a satellite image is much more time-consuming. Multiply this task by hundreds or thousands, and you’ve quite a scaling challenge!\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nCost\n\n\n\n\n\nCosts become important in conjunction with the scalability challenge.\nYou may find you need to pay for:\n\nAnnotators’ time, whether they are directly employed or used via a service\nAnnotation software or services, if you go with a commercial tool vendor\nData storage, if you are leveraging your own hardware and/or cloud providers like AWS to store large amounts of data\nCPU/GPU cycles, if you are leveraging your own hardware or cloud services to run annotation software, especially if you are using AI-assisted annotation capabilities\n\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nQuality control\n\n\n\n\n\nAnnotation is not always straightforward and easy, but as we’ve discussed, effective model training depends on producing sufficiently high quality annotations of sufficiently high quality training data.\nSome factors to consider:\n\nSource data quality. Is the information signal clear in the data? And does the input dataset include a sufficiently diverse set of examples that are representive of what the model will encountered when deployed?\nAnnotation consistency. Do the annotations capture information in the same way across images? This becomes an even bigger factor when multiple annotators are involved. Clear annotation guidelines and tracking various consistency metrics can help here.\nAnnotation quality. Are the annotations accurate, precise, and complete? Have annotators introduced bias?\n\nIn the end, you will likely need to strike balance between speed and quality. Determining the right goalposts for “good enough” will require experimentation and iterative model training/testing.\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nSubjectivity\n\n\n\n\n\nIn some applications, there is no clear correct answer! In that case, especially without clear guidelines and training, different annotators can interpret data differently. This can leading to inconsistent labels, which in turn will negatively impact model training and lead to degraded model performance.\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nData and annotation management\n\n\n\n\n\nOn a practical front, effectively managing a large-scale annotation activity also requires managing and organizing all associated annotation artifacts, including both the input data and the generated annotations.\nIf you are performing annotation across a team of people, you also need to likely need to keep track of multiple annotations per data object (performed across multiple annotators), metadata associated with those annotations (e.g., how long each annotator took to complete the task), and various metrics for monitoring annotation and annotator performance over time.\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nData privacy & security\n\n\n\n\n\nThis is especially important if you use a cloud-based tool for annotation.\nAsk yourself: What is their data privacy and security policy, and is it sufficient to meet your needs?\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nBias & Ethics\n\n\n\n\n\nManaging bias and ethics is not an annotation-specific problem, and we’ll discuss this later in the cousre. However, bear in mind that annotation can be a major factor, because it is a step in the modeling process when some specific human knowledge (i.e., what the annotators know) is attached to the input data, and will very directly exposed to the model during training. This creates an opportunity for injecting bias, exposing sensitive or private information, among other things.\n\n\n\n\n\n\n\n\nNoun Project (CC BY 3.0)\n\n\n\n\n\n\n\n\nCallout: Annotating satellite imagery\n\n\n\n\nLabeling of satellite imagery brings its own specific challenges. Consider:\n\nScenes are often highly complex and rich in detail\nGeographic distortion: Angle of sensor\nAtmospheric distortion: Haze, fog, clouds\nVariability over time:\n\nWhat time of day? Angle of the sun affects visible characteristics\nWhat time of year? Many features change seasonally (e.g. deciduous forest, grasslands in seasonally arid environments, snow cover, etc)\nFeatures change! Forests are cut, etc. Be mindful of the difference between labeling an image and labeling a patch of the earth’s surface.\n\nIt’s often desirable to maintain the correspondence between pixels and their geospatial location, for cross-reference with maps and/or other imagery\n\n\n\n\n\n4.1.5 Annotation best practices\nThis list could certainly be longer, but if you remember and apply these practices, you’ll start off on a good foot.\n\n\n\n\n\n\nDevelop a thorough annotation protocol\n\n\n\n\n\nCreate and maintain clear labeling instructions.\n\n\n\n\n\n\n\n\n\nProvide annotator training\n\n\n\n\n\n\nWork with annotators to make sure they understand the domain, use cases, and overall purpose of the project.\nProvide specific guidance about what to do in ambiguous or difficult cases, in order to help standardize annotations.\nConsider having new annotators apply annotations on a set of sample inputs, assess those annotations, and provide clear feedback with reference to what they could or should do better.\n\n\n\n\n\n\n\n\n\n\nHave a quality control process\n\n\n\n\n\nTo ensure sufficient quality, plan on doing regular checks, running cross-validations, and having feedback loops.\nFirst, periodically conduct manual annotation reviews to ensure compliance with instructions. This might include having a recognized expert on the team randomly selecting a subset of annotated images to assess.\nSecond, identify and calculate quality metrics on an ongoing basis, targeting each of the following:\nConsensus. To measure the degree to which different annotators on the team are providing similar annotations, have multiple annotors annotate some of the same images, and calculate a consensus measure like Inter-annotator agreement (IAA). Several flavors of this metric exist, such as Cohen’s kappa (to compare 2 labelers) and Fleiss’ kappa (to compare &gt;2 labelers).\nAccuracy. In cases where there’s a known “correct” answer, either for all images or some subset thereof, calculate annotation performance metrics. Here are a couple of examples: - For bounding boxes, calculate a metric like Intersection over union (IoU): Take the area of overlap between the ground truth box and the annotated box, and divide by total area of the (unioned) boxes. - For detected objects overall, calculate standard metrics like precision (proportion of labeled objects that are correctly labeled) and recall (proportion of all objects that were correctly labeled)\nCompleteness. Keep track of annotation completeness overall. For example, when doing bounding box annotation for an object detection task, ensure that all drawn boxes are associated with a valid label.\n\n\n\n\n\n\n\n\n\nProceed iteratively!\n\n\n\n\n\nIn a nutshell:\n\nStart small\nRefine and improve as you go\nScale gradually",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#image-annotation-methodology",
    "href": "sections/data-annotation.html#image-annotation-methodology",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "4.2 Image Annotation Methodology",
    "text": "4.2 Image Annotation Methodology\n\n\n\n\n\n\nHighlights\n\n\n\n\nDiscuss the primary types of image annotations\nDiscuss the common image-related AI/ML tasks requiring annotation\nDiscuss different methods for annotating images\nDescribe a high level annotation workflow\n\n\n\nIt’s important to understand and recognize the difference between image annotation types, tasks, and methods. Note that this isn’t universal or standardized terminology, but it’s pretty widespread.\nIn this context:\n\nAn annotation type describes the specific format or structure of the annotation used to convey information about the data critical for supporting the task.\nAn annotation task is the specific objective that the annotations are meant to support, i.e. the job you want your AI application to do. In the computer vision context, this typically means identifying or understanding something about an image, and conveying that information in some specific form.\nAn annotation method refers to the process or approach used to create the annotations.\n\n\n4.2.1 Image Annotation Types\nThe type of annotation you apply will depend partly on the task (see next section), as different annotation types are naturally suited for different tasks. However, the decision will also be driven in part by time, cost, and accuracy considerations.\n\n\n\n\n\n\nImage tags\n\n\n\n\n\nTags are categorical labels, words, or phrases associated with the image as a whole, without explicit linkage to any localized portion of the image. \n\nLabel: beach\nCaption: “Embracing the serenity of the shore, where the sky meets the ocean #outdoor #beachlife #nature”\n\n\n\n\n\n\n\n\n\n\nBounding boxes\n\n\n\n\n\nBounding boxes are rectangles drawn around objects to localize them within an image. \nTypically they are axis-aligned, meaning two sides are parallel with the image top/bottom, and two sides are parallel with the image sides, but sometimes rotation is supported.\n\n\n\n\n\n\n\n\n\nPolygons\n\n\n\n\n\nGeneralizing the bounding box concept, polygons are a series of 3 or more connected line segments (each with definable end coordinates) that form a closed shape (i.e. the end of the last segment is the beginning of the first segment), used to more precisely localize objects or areas by outlining their shape. \n\n\n\n\n\n\n\n\n\nSegmentations\n\n\n\n\n\nSegmentations involve assigning a class label to individual pixels (or collectively, to regions of individual pixels) in an image. Segmentation may be done either fully for all pixels, or partially only for pixels associated with phenomena of interest.\nIn practice, segmentations are produced either by drawing a polygon to circumscribe relevant pixels, or using a brush tool to select them in entire swaths at a time\n\n\n\n\n\n\n\n\n\n\nKeypoints\n\n\n\n\n\nKeypoints are simply points, used for denoting specific landmarks or features (e.g., skeletal points in human pose estimation). \n\n\n\n\n\n\n\n\n\nPolylines\n\n\n\n\n\nPolylines are conceptually similar to polygons, but they do not form a closed shape. Instead, the lines are used to mark linear features such as roads, rivers, powerlines, or boundaries. \n\n\n\n\n\n\n\n\n\n3D Cuboids\n\n\n\n\n\n3D cuboids are bounding boxes extended to three dimensions. These are often used in LiDAR data which is represented as a 3-dimensional point cloud, but can also be used to indicate depth of field in a 2D image when the modeling task involves understanding position in three dimensions. \n\n\n\n\n\n4.2.2 Image Annotation Tasks\nThe task you choose will depend on the type of information you want the model to extract from the images. Here are the key types of annotation tasks in computer vision:\n\n\n\n\n\n\nImage Classification\n\n\n\n\n\nImage classification is the task of assigning an entire image to a category.\nThe classification typically refers to some singular dominant object or feature (e.g., “Polar bear”) within the image, or some defining characteristic of the image (e.g., “Grassland”), but the details depend on the specific use case motivating the modeling exercise.\n\n\n\n\n\n\n\n\n\nImage Captioning\n\n\n\n\n\nImage captioning is the task of generating textual descriptions of the image. It is conceptually similar to image classification, but involves producing freeform text for each image rather than assigning the image to one of a set of pre-defined categorical classifications.\n\n\n\n\n\n\n\n\n\nObject Detection\n\n\n\n\n\nObject detection is the task of identifying one or more objects or discrete entities within an image.\nNote that object detection involves two distinct sub-tasks:\n\nLocalization: Where is the object within the image?\nClassification: What is the localized object?\n\n\n\n\n\n\n\n\n\n\nImage Segmentation\n\n\n\n\n\nSegmentation is the task of associating individual pixels with labels for purpose of enabling detailed image analysis (e.g., land-use segmentation). In some sense, you can think of it as object detection reported at the pixel level.\nThere are three distinct kinds of segmentation, illustrated below for the following image:\n\nSemantic Segmentation assigns a class label to each pixel in the image, without differentiating individual instances of that class. It is best for amorphous and uncountable “stuff”. In the image below, notice the segmentation and separation of the foreground grass from the background trees from the water in the middle. Also notice that the bears are all lumped together in one segment.\n\nInstance Segmentation separately detects and segments each object instance. It’s therefore similar to semantic segmentation, but identifies the existence, location, shape, and count of objects. It is best for distinct and countable “things”. Notice the separately identified four bears in the image below:\n\nPanoptic Segmentation) combines semantic segmentation + instance segmentation by labeling all pixels, including differentiation of discrete and separately objects within categories. Notice the complete segmentation in the image below, including both the various background types as well as the four distinct bears.\n\nFor more on Panoptic Segmentation, check out the research publication.\n\n\n\n\n\n\n\n\n\nTemporal Annotation\n\n\n\n\n\nTemporal annotation is the task of labeling satellite images over time to track changes in environmental features.\n\n\n\n\n\n4.2.3 Image Annotation Methods\nThe annotation method largely boils down to whether annotations are done manually versus with some level of supporting automation. Ultimately, the choice involves project-specific determination of the cost, speed, and quality of human annotation relative to what can be achieved with available AI assistance.\n\n\n\n\n\n\nManual Annotation\n\n\n\n\n\nWith purely manual annotation, all labeling is done by human annotators.\nNote that good tooling may help make this process easier and more efficient, but ultimately it is up to the human annotator to fully apply annotations to unlabeled inputs.\n\n\n\n\n\n\n\n\n\nSemi-Automated Annotation\n\n\n\n\n\nWith semi-automated annotation, machines assist humans in generating annotations, but humans are still heavily involved in real time with labeling decisions, ranging from actually applying the annotations to refining AI-generated annotations.\nThis can take a few different forms. For example:\n\nModel-based filtering: A model is trained to recognize images with any candidate objects (as compared to empty scenes), and is used to reduce the number of images passed to the human annotator.\nModel-assisted labeling: A pre-trained model generates a candidate annotation, which the human can accept, reject, or modify in some way (e.g., size, position, category).\nActive Learning: A model is learning how to annotate the images alongside the human, and actively decides which images the human should label to accelerate model training the fastest.\n\n\n\n\n\n\n\n\n\n\nAutomated Annotation with Human Validation\n\n\n\n\n\nAt the level of automated annotation with human validation, AI models generate most annotations autonomously. Humans only review the results after the fact, typically checking accuracy metrics at a high level and perhaps inspecting a random sample of annotations, rather than reviewing every annotation.\nExample: A pre-trained model processes satellite images and automatically labels roads, rivers, and forests across thousands of images. A human reviewer then inspects a small percentage of these results to confirm the annotations are accurate, fixing any errors and perhaps fine-tuning the model before the dataset is finalized.\nAt first glance, it might seem illogical that this scenario could exist! If you already have a model that can do the annotation, then don’t you already have a model to do the actual task you want to do?\nIn practice, however, there are some cases where this might be applicable:\n\nOne scenario involves model distillation. Imagine there exists a big, expensive, and/or proprietary (i.e., hidden behind an API) model that does the task you want, and perhaps a lot more. You can use this model to annotate a dataset that you use to train a more compact or economical model that you own and control. In the end, you have effectively distilled the source model’s capability into your own model, through the annotated training data set.\nA second scenario is when you do indeed already have a trained model to perform annotations, whether your own or someone else’s, and are now using it to automatically annotate vasts amounts of data that will serve as inputs to some other machine learning or analysis pipeline. Indeed, in research settings, this is usually the end objective! When you reach this point in the process, you will effectively be doing automated annotation with human validation to ensure that the results are reasonable in aggregate.\n\n\n\n\n\n\n\n\n\n\nFully Automated Annotation\n\n\n\n\n\nRare in practice! Under fully automated annotation, trained models generate annotations with no human involvement, and the quality is deemed sufficient without review.\nThis is typically only relevant in very specific settings, namely in environments where the image data is very highly controlled. For example, consider images that were produced in a lab setting where the composition of the images is highly controlled, or images that were generated synthetically by some known computational agent (e.g., in video games). A related approach with synthetic data involves using trained AI models to generate both the images and their corresponding annotations, in which case the annotation ground truth for each image.\n\n\n\n\n\n4.2.4 Data Annotation Workflow\n\n\n\n\n\n\n1 - Data collection\n\n\n\n\n\nFirst step: Get a sufficiently large and diverse set of data to annotate and subsequently train on.\nYou may already have a set of images from your own research, e.g. from a set of camera traps or aerial flights. Or perhaps you already have a clear use case around detecting features in a particular satellite dataset, and have already procured the imagery. If so, great.\nIf you don’t have your own imagery – and maybe even if you do – you may want to consider augmenting it with additional images if you don’t have enough diversity or content in your own imagery. Depending on your use cases, you may want to poke around public mage datasets like ImageNet.\n\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n2 - Tool Selection\n\n\n\n\n\nTime to choose your annotation tool/platform!\nThere are many options, and lots of factors to consider. See the next section for plenty more detail.\n\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n3 - Data preprocessing\n\n\n\n\n\nBefore proceeding, it’s almost always useful (some sometimes essential) to apply various preprocessing tasks to your data to make it easeir to annotatate and/or eventually train on.\n\n\n\nSource: Medium\n\n\nHere are some categories of common preprocessing tasks:\nReformatting. If relevant, you may need to convert your source images into a better file format for your task. Beyond this, it may be useful to rotate, crop, rescale, and/or reproject your images to get them into a consistent structural format.\nBasic data cleaning. - For example, with satellite or aerial imagery, you may find it useful to apply pre-processing stesp such as filtering to removing noise, correcting for atmospheric conditions, correcting other distortion, adjusting brightness/contrast/color.\nFeature enhancement. Other context-specific transformations may be useful for “bringing out” information for the model (and human annotators) to use, leading to faster and/or better model outcomes. For an example, list to this story about how careful transformations of Sentinel 2 imagery provided a huge boost in the detection of field boundaries as part of the UKFields project.\n\n\n\n\n\n\n\n\n\n4 - Guideline Development\n\n\n\n\n\nAs we discussed earlier, before you begin in earnest, it’s critical that you develop specific guidelines for annotators to follow when doing the annotation using the selected tool.\nNote: These should be written down! Some annotation platforms provide a way to document instructions within the tool, but if yours doesn’t (and probably even if it does), you should create and maintain your own written documentation\n\n\n\nSource: Acquiro\n\n\nOften this will be based on a combination of prior knowledge and task familiarity. To the extent that nobody on the project has extensive experience with the task at hand, it’s often helpful to do some prototyping to inform development of the guidelines.\n\n\n\n\n\n\n\n\n\n5 - Annotation\n\n\n\n\n\nIt’s time to annotate!\n\n\n\nSource: shaip\n\n\nKeep in mind the following image annotation best practices. They may not always hold, but in general:\n\nKeeping bounding boxes and polygons “tight” to the object:\nFor occluded objects, annotate as if the entire object were in view\nIn general, label partial objects cut off at the edge\nLabel all relevant objects in the image. Otherwise, “negative” labels will hamper model learning.\n\nAbove all else, remember, consistency is critical!\n\n\n\n\n\n\n\n\n\n6 - Quality Assurance\n\n\n\n\n\nReview the annotations for quality, and if needed, refine by returning to an earlier step in the workflow.\n\nNote that although QA is identified here as a discrete stage in the workflow, in practice quality is achieved through deliberate attention at multiple stages in the process, including:\n\nInitial annotator workforce training before any annotation is done\nContinuous monitoring during the annotation process\nFinal post-annotation review\n\n\n\n\n\n\n\n\n\n\n7 - Data Export\n\n\n\n\n\nFinalize and output the annotated data for model training.\n\nTypically you will need to get the data into some particular format before proceeding with model training. If your annotation tool can export in this format, you’re all set. If not, you’ll need to export in some other format and then use a conversion tool that you either find or create yourself.\n\n\n\nFrom here, presumably you’ll move on to model training!\nRemember this key best practice: Iterate! You will almost certain not proceed through the annotation workflow in one straight shot. Plan to do some annotations, train, test, fix annotations, figure out whether/how to do more and/or better annotations, refine your annotation approaches, etc.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/data-annotation.html#annotation-tools-platforms",
    "href": "sections/data-annotation.html#annotation-tools-platforms",
    "title": "4  Data Annotation: The Foundation of Deep Learning Models",
    "section": "4.3 Annotation Tools & Platforms",
    "text": "4.3 Annotation Tools & Platforms\n\n\n\n\n\n\nHighlights\n\n\n\n\nGet a sense of what kind of tools are out there today!\nDiscuss high level considerations for choosing a tool\nReview some specific tools out there today\nHighlight how fast things are changing!\n\n\n\n\n4.3.1 High level considerations\nHere are some questions you should be asking…\n\n\n\n\n\n\nWhat annotation types are supported?\n\n\n\n\n\nDoes the tooling allow you do create the kinds of annotations necessary for your task? This probably the first and most fundamental question you should be asking!\n\n\n\n\n\n\n\n\n\nWhat import image formats are supported?\n\n\n\n\n\nCan the tool read images in the right format?\nFortunately, most tools can automatically take a wide range standard image formats including JPG, PNG, BMP, and TIF, and more.\nHowever, if you are working with spatial imagery, including GeoTIFFs, most tools will not natively read in your data. You will need to convert between formats, or choose a tool that is explicitly designed to handle that kind of data.\n\n\n\n\n\n\n\n\n\nWhat output annotation formats are supported?\n\n\n\n\n\nWhile image formats are reasonably standardized, image annotation formats are more diverse. In general, the format you need will be dictated by the constraints of whatever modeling tasks and tooling you will be using to train and validate a model with your annotated data.\nSome annotation software, especially the major players and cloud-based offerings, support a diverse set of output formats, whereas others output only a limited number of formats – or even just their own idiosyncratic format! In that case, you may need to do a conversion to get your annotations in the right format. Fortunately, there’s a good chance that somebody else has already been down this path, and if you search around, you may find a script or package that can do it for you.\nExample formats (not exhaustive!):\n\nVarious JSON formats\n\nCOCO JSON\nVGG Image Annotator JSON\nLabelMe JSON\n\nYOLO TXT\nPascal VOC XML\nTensorFlow TFRecord\n… and lots more …\n\nSee this great page for exploring many different formats.\n\n\n\n\n\n\n\n\n\nWho will be doing the annotation?\n\n\n\n\n\n\nIn-house: You and your team.\nCrowdsource: The broader community.\nOutsource: External people with whom you contract, either directly or through a 3rd party annotation services company. Yes, these do exist!\n\n\n\n\n\n\n\n\n\n\nHow can I assess annotation quality?\n\n\n\n\n\nWe’ve discussed the importance of having high quality annotations, and briefly covered various types of quality assessment. Some tools leave it entirely up to you to handle this, but others have features that help in this area. This can include:\n\nAutomatic calculation of various quality metrics\nConfigurable mechanisms for distributing images among annotators, and choosing how many annotators will see each image\nVarious other forms of annotation process metadata and analytics\n\n\n\n\n\n\n\n\n\n\nIs the tool easy to use?\n\n\n\n\n\nAs with any category of software, some options will be easier to use than others. For image annotation, where you are likely going to want to scale up to a large number of images, small speed-ups in the annotation process will really start to add up over time.\nConsider:\n\nIs the software easy to navigate in general?\nDoes the annotation interface have responsive, reliable, and easy-to-use UI elements for creating, modifying, and deleting image annotations?\nAre there effective keyboard shortcuts to help speed up manual annotations?\nDoes the tool offer effective model-assisted or other “smart” annotation capabilities?\nAre there well-designed features for managing your images, annotations, and ovearll workflow?\nIs there any useful API support to enable programmatic upload, download, or other automation?\n\n\n\n\n\n\n\n\n\n\nHow much am I willing to pay for tooling?\n\n\n\n\n\nIn short, some software options are free, wherease others are commercial offerings with varying costs and prices tiers. As you compare features, consider what you’re willing (and able) to pay for.\n\n\n\n\n\n\n\n\n\nHow is the software licensed?\n\n\n\n\n\nSome annotation software apps and libraries are open source, whereas others are proprietary. You may want to lean toward the open source options if you want to be able to review the source code and understand how it works, and/or (perhaps more importantly) have the option of modifying it to better meet your needs. Of course, general speaking, the open source options will typically also be free, whereas proprietary software is more likely to come with costs.\n\n\n\n\n\n\n\n\n\nWhere does the software run?\n\n\n\n\n\nDo you care if the software runs on your local computer? Do you want it to be something that you deploy and run on your own managed server, either locally or on a VM hosted in a public cloud? Or would you prefer to use a pure cloud-based annotation platform (i.e., a SaaS offering) that somebody else maintains and you access via a browser and/or API?\nAs with any software decision, there are pros and cons to each option.\nBear in mind that with image annotation, any cloud-based offering raises security and privacy considerations, as your images and annotations will reside on somebody else’s server. Consider whether this is a concern for you.\n\n\n\n\n\n\n\n\n\nWhat collaboration features are there?\n\n\n\n\n\n\nWhat collaborative features are offered?\n\n\n\n\n\n\n4.3.2 Tools & services galore\nNote that for geospatial image data annotation in particular, historically there’s been a divide between these two approaches:\n\nMature GIS platforms (QGIS, ArcGIS, etc) -\n\nFirst-class geospatial data and imagery support\nNative capabilities for drawing and editing features like points, lines, and polygons\nBut all of menus and heavyweight UI around robust spatial feature management can impede fast & efficient annotation\nLimited or no support for the broader annotation workflow and lifecycle\n\nImage annotation software and platforms (LabelBox, RoboFlow)\n\nReally nice and constantly improving\nMostly generic with respect to supporting annotation for Computer Vision tasks, not full-featured around environmental research applications, especially with respect to Remote Sensing imagery with spatial component, multispectral bands, etc\n\n\nIn between, you’ll find a few dedicated software packages for environmental and/or spatial image annotation. However, because this is a small niche, you’ll find that they’re often rough around the edges, and likely have a very focused (i.e., limited) set of features addressing only the specific use cases of relevance to the development team. On the plus side, usually they are developed as open source projects, so if you’re up for the investment, you may want to consider contributing or extending these tools to meet your needs.\n\n4.3.2.1 Open-Source Tools for Image Annotation\n\nLabelImg\n\nHigh level: An open-source tool for creating bounding boxes.\nUsed for object detection mainly, maybe??\nOnly supports bounding boxes for annotation\n“Graphical image annotation tool and label object bounding boxes in images”\nIt is written in Python and uses Qt for its graphical interface.\nAnnotations are saved as XML files in PASCAL VOC format, the format used by ImageNet. Besides, it also supports YOLO and CreateML formats\nSee this third-party video tutorial\n\nVGG Image Annotator (VIA)\n\nHigh level: A flexible (but manual) tool for image, video, and audio annotation.\nServerless web application, runs locally and self-contained in a browser, with no network connection required\nReleased in 2016, still maintained, based out of Oxford\nSee demo\n\nLabelMe\n\nOriginally built as an online annotation tool, now distributed\nNow distributed as a deployable web application that you can ran on a local web server\nNot to be confused with this independent Python/QT port of labelme\nWait and what about this labelme GitHub repo??\n\nIRIS (Intelligently Reinforced Image Segmentation)\n\nProvides semi-automated annotation for image segmentation, geared toward multi-band satellite imagery\n\n\n\n\n4.3.2.2 GIS platforms with annotation plugins\n\nQGIS\nArcGIS\n\n\n\n4.3.2.3 Hybrid solutions with both desktop and hosted options\n\nCVAT (Computer Vision Annotation Tool):\n\nOpen-source tool for video and image annotation, widely used in computer vision projects.\nUses pre-trained models to assist annotation?\nSee GitHub repository\nAlso has cloud-based offering and offers annotation services\nSupports:\n\nlabeling images\ndrawing bounding boxes\nmodel assisted labeling using models like YOLO \nmanual semantic segmentation \nautomatic semantic segmentation with SAM \n\n\n🔥 Label Studio\n\nMulti-type data labeling and annotation tool with standardized output format\nWorks on various data types (text, image, audio)\nHas both open source option and paid cloud service\nSee online playground\n\nMicrosoft’s Spatial imagely labeling toolkit\nimglab\n\n\n\n4.3.2.4 Commercial apps\n\nRectLabel\n\nOffline image annotation tool for object detection and segmentation\nHas regular and Pro version\nBuilt for Mac\nSee support page\n\n\n\n\n4.3.2.5 Commercial services\n\nLabelbox\n\nCloud-based commercial platform, albeit with possible free options for academic researchers\n\nRoboflow annotate\n\nOnline platform, with limited free tier\nFree tier does not offer any privacy\n\nSuperAnnotate\n\nHigh level: Full-featured collaborative annotation and modeling platform\nCommercial offering with free tier\n\nMakeSense.ai\n\nIncludes AI models!\nGitHub0\n\nSupervise.ly (commercial with free version)\nLabelerr (commercial with free researcher tier)\nRMSI annotation tools & services\nKili annotation platform (see geoannotation docs)\nSegments.ai labeling platform\nSama\nScaleAI\nDiffgram (see tech docs and GiHub) – commercial but locally installed? Hard to tell!\nDarkLabel\nGroundwork professional labeling services\n\n\n\n4.3.2.6 Fully managed AI & annotation services\n\nAlegion\nManthano\n\n\n\n4.3.2.7 Other platforms\n\nZooniverse? Crowd-sourcing annotation platform\n\nE.g. The Arctic Bears Project\n\nDeepForest\n\nFrom the Weecology lab\nPython package for training and predicting ecological objects in airborne imagery\nComes with a tree crown object detection model and a bird detection model\nSee GitHub repo\n\n\n\n\n\n4.3.3 Miscellaneous links\n\nSatellite image deep learning (Robin Cole’s site)\nOpen Source Data Annotation & Labeling Tools",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Annotation: The Foundation of Deep Learning Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html",
    "href": "sections/hands-on-lab-data-annotation.html",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "",
    "text": "Goal\nThis hands-on lab session is designed to give participants practical experience in data annotation for deep learning. Participants will apply the methods, tools, and best practices discussed in the previous session, working directly with datasets to annotate data effectively.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#key-elements",
    "href": "sections/hands-on-lab-data-annotation.html#key-elements",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "Key Elements",
    "text": "Key Elements\nUse of annotation methods and tools, direct dataset interaction",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#choose-your-own-adventures",
    "href": "sections/hands-on-lab-data-annotation.html#choose-your-own-adventures",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "Choose your own adventure(s)",
    "text": "Choose your own adventure(s)\nIn this section, we’ll provide some links, basic information, and suggested starter activities for variety of annotation tools available today. Have a look and get your hands dirty!\nNote: You’ll need some images to annotate in each case. Feel free to use any relevant images you might already have, or just do a web search and find something interesting. Of course, when experimeting with the web-based annotation platforms, be sure not to use upload anything personal, private, or otherwise sensitive.\nIdeally you’ll cover:\n\nSimple bounding box annotation\nPolygon, line, and point annotation\nInteractive model-assisted segmentation\nInspecting annotation output files in various formats, including COCO JSON\nOne or more cloud (web-based) tools\n(For the even more adventurous) One or more locally installed tools",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-make-sense",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-make-sense",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.1 Adventure: Make Sense",
    "text": "5.1 Adventure: Make Sense\n\n\n\n\n\n\nWeb-based app, no setup or account required\n\n\n\n\n\n\n.\n\n\n\nMakeSense.ai is a simple, single-user, browser-based image annotation app\nSupports annotation via bounding boxes, poylgons, points, and lines\nUpload one or more images, apply/edit annotations, then export annotations\nOffers model-based semi-automated annotation with an accept/reject interface\nIf you prefer, you can also grab the source code and run it locally using npm or Docker\n\nThings to try\n\nUpload one or more images\nPlay around with manually creating various annotations of various classes. What is the experience?\nUse Actions to edit label names, colors, etc\nUse Actions to export annotations. What formats are offered?\nTry exporting polygon annotations in both VGG and COCO format. How do they compare?\nUse Actions to run the COCO SSD model locally to suggest boxes. How well does it work?\nWhen you’re done: Evaluate this tool with respect to the software considerations in Section 4.3.1",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-roboflow",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-roboflow",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.2 Adventure: Roboflow",
    "text": "5.2 Adventure: Roboflow\n\n\n\n\n\n\nWeb-based app, requires (free) account signup\n\n\n\n\n\n\n.\n\n\n\nRoboflow offers a cloud-hosted, web-based platform for computer vision, including tooling for data annotation along with model training and deployment\nThey offer a limited free tier, which does not offer any privacy (project and images are automatically public)\nNice interface for doing annotations, managing artifacts, managing team\n\nThings to try\n\nCreate an account and test project\nUpload one or more images\nGo to the Annotate interface and experiment with different annotation types. How easy is it to create, edit, and delete annotations?\nUse the Smart Polygon tool to create polygons by clicking on an object, then refining by adding more clicks inside and outside the object. What is the experience like? Does this speed up your annotations?\nGo back to the main Annotate menu and note how it is organized to support a coherent, team-based annotation workflow. Check out their collaboration documentation. Imagine how you might use this for a multi-person project.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-cvat",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-cvat",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.3 Adventure: CVAT",
    "text": "5.3 Adventure: CVAT\n\n\n\n\n\n\nWeb-based app, requires (free) account signup\n\n\n\n\n\n\n.\n\n\n\nCVAT can be used as a desktop application that you install & run on your own local computer or server.\nHowever, for today, consider creating your own (free) account for annotating using their hosted platform\nThe V7 cvat guide might be helpful\n\nThings to try\n\nCreate a free account\nLog in and create a test Project. At this stage, you’ll need to define at least one relevant label under the Constructor tab (you can edit these later)\nCreate a Task (i.e., a collection of images to annotate) under your Project, and upload one or more images.\nStart an annotation Job within the task. What do think of the interface? Is the documentation helpful?\nUsing the menu bar on the left, try creating box, polygon, line, and point annotations. Note: Click the Shape button to start each annotation. How is the experience?\nAlso try creating a 3D cuboid annotation. Figure out how to resize and orient the cube. What do you think?\nLastly, try doing brush-based segmentations.\nAfter doing some annotations, go to Jobs, use the 3-dots selector on your job to open the action menu, and export annotations in a couple different formats. How do they compare?\nAs a another Jobs action, you can do click on View analytics and run a performance report.",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-zooniverse",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-zooniverse",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.4 Adventure: Zooniverse",
    "text": "5.4 Adventure: Zooniverse\n\n\n\n\n\n\nWeb-based app, no setup or account required\n\n\n\n\n\n\n.\n\n\nZooniverse is a cool community crowdsourcing platform on the web, for data annotation and digitization.\nThings to try\n\nCheck out the Penguin Watch project.\n\nVisit the About, Talk, and Collect pages. Imagine how you might set up your own project to encourage and support a crowdsourced annotation community\nVisit the Classify page, go through the Tutorial, and then see how the Task works.\n\nAlso check out the Arctic Bears image classification and interpretation project\nFeel free to search the site for other projects",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-iris-intelligently-reinforced-image-segmentation",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-iris-intelligently-reinforced-image-segmentation",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.5 Adventure: IRIS (Intelligently Reinforced Image Segmentation)",
    "text": "5.5 Adventure: IRIS (Intelligently Reinforced Image Segmentation)\n\n\n\n\n\n\nRequires local installation (Python + JavaScript application).\nSee Installation instructions with for setting IRIS up locally with Python/pip.\n\n\n\n\n\n\n.\n\n\nIRIS is a tool for doing semi-automated image segmentation of satellite imagery (or images in general), with a goal of accelerating the creation of ML training datasets for Earth Observation. The user interface provides configurable simultaneous views of the same image for multispectral imagery, along with interactive AI-assisted segmentation.\nUnlike much of the ML we’ll encounter this week, the backend model in this case is a gradient boosted decision tree. The reason this works sufficiently well is that IRIS is geared toward segmenting multispectral imagery into a small number of classes, training from scratch on each image; the model is able to learn the correlation structure between features and labels by leveraging multiple features per pixel after the human-in-the-loop manually segments and labels pixels.\nFor more information, check out the YouTube video with the main creator Alistar Francis.\nThings to try\n\nRun the IRIS demo that comes with the code\nUsing the onboard help widgets, figure out how to navigate the interface\nTry iteratively labeling some pixels and running the AI. How does it do?\nExperiment with changing the views. How does that improve your ability to manually distinguish and label features like clouds?",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-segment-geospatial-samgeo",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-segment-geospatial-samgeo",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.6 Adventure: Segment-Geospatial (samgeo)",
    "text": "5.6 Adventure: Segment-Geospatial (samgeo)\n\n\n\n\n\n\nRequires local installation (Python library), or can be run in a hosted notebook environment (JupyterLab, Google Collab, etc).\nSee Installation notes.\n\n\n\nThis is an open source tool that you can either install locally or run in JupyterLab (or Google colab).\nFirst check out the online Segment Anything Model (SAM) demo. SAM was developed by Meta AI. It is trained as a generalized segmentation model that is able to segment (but not label) arbitrary objects in an image. It is designed as a promptable tool, which means a user can provide initial point(s) or box(es) that roughly localize an object within an image, and SAM will try to fully segment that object. Alternatively, it can automatically segment an entier image, effectively by self-promtping with a complete grid of points, and then intelligently merging the corresponding segments.\nToday, SAM is used by numerous image annotation tools to provide interactive, AI-assisted segmentation capabilities.\nOne such tool is the segment-geospatial Python package, which provides some base functionality for applying SAM to geospatial data, either programatically or interactively.\n\n\nNote that in addition to using segment-geospatial directly using Python in a notebook or other environment, you can also play with SAM-assisted segmentation in QGIS and ArcGIS.\nThings to try\n\nRun one or more examples\nAfter running automated segmentation, what do you think about the results?\nWhen doing interactive segmentation, how does it do, and what do you think about the results?",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#adventure-label-studio",
    "href": "sections/hands-on-lab-data-annotation.html#adventure-label-studio",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.7 Adventure: Label Studio",
    "text": "5.7 Adventure: Label Studio\n\n\n\n\n\n\nPython app must be installed and run locally (unless you pay for an Enterprise cloud account)\nSee Quick start document with instructions for installing with pip and running locally in a web browser.\n\n\n\n\nMulti-type data labeling and annotation tool with standardized output format\nWorks on various data types (text, image, audio)\nHas both open source option and paid cloud service\nSee online playground",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-data-annotation.html#other-things-to-try",
    "href": "sections/hands-on-lab-data-annotation.html#other-things-to-try",
    "title": "5  Hands-On Lab: Data Annotation",
    "section": "5.8 Other things to try",
    "text": "5.8 Other things to try\n\nVGG Image Annotator (VIA)\n\nTry local installation?",
    "crumbs": [
      "<b>Day 1: Introduction to AI and Arctic Science</b>",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hands-On Lab: Data Annotation</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html",
    "href": "sections/the-building-blocks-of-nn-and-dl.html",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "",
    "text": "6.1 Overview\nThis session aims to provide a comprehensive introduction to the fundamental components of neural networks and deep learning. Participants will explore the architecture of neural networks, including layers, neurons, weights, and activation functions, as well as the principles behind training models, such as loss functions and optimizers. The goal is to equip participants with a solid understanding of how neural networks are constructed and how they learn, paving the way for deeper dives into specific neural network models and applications in future sessions.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#outline",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#outline",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.2 Outline",
    "text": "6.2 Outline\n\nFundamentals of neural network: history and evolution\nCore components: neurons, layers, and weights\nArchitecture of neural networks: layers and activation functions\nTraining neural networks: loss functions and optimizers\nConclusion and Q&A",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/the-building-blocks-of-nn-and-dl.html#reference",
    "href": "sections/the-building-blocks-of-nn-and-dl.html#reference",
    "title": "6  The Building Blocks of Neural Networks and Deep Learning",
    "section": "6.3 Reference",
    "text": "6.3 Reference\n\nhttps://cs231n.stanford.edu",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Building Blocks of Neural Networks and Deep Learning</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html",
    "href": "sections/intro-to-pytorch.html",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "",
    "text": "7.1 Goal\nThis session introduces PyTorch, one of the most popular deep learning frameworks, known for its flexibility and ease of use. Participants will gain hands-on experience with PyTorch’s core functionalities and apply them to a sample dataset, setting the stage for a subsequent hands-on session. The goal is to arm participants with the essential knowledge and confidence required to begin utilizing PyTorch in their deep learning endeavors, ensuring they’re well-prepared for practical application and further exploration.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#outline",
    "href": "sections/intro-to-pytorch.html#outline",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.2 Outline",
    "text": "7.2 Outline\n\nIntroduction to PyTorch\nUnderstanding PyTorch’s core functionalities\nWorking with data in PyTorch\nBuilding a simple neural network in PyTorch\nTraining a model\nEvaluating a model\nConclusion and preparing for hands-on session",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-pytorch.html#reference",
    "href": "sections/intro-to-pytorch.html#reference",
    "title": "7  Introduction to PyTorch: Core Functionalities and Advantages",
    "section": "7.3 Reference",
    "text": "7.3 Reference\n\nhttps://pytorch.org\nhttps://pytorch.org/tutorials/",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to PyTorch: Core Functionalities and Advantages</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html",
    "href": "sections/hands-on-lab-pytorch.html",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "",
    "text": "8.1 Overview\nThis hands-on lab session is designed to provide participants with practical experience using PyTorch to build, train, and evaluate neural network models. Participants will work through guided exercises that reinforce the concepts introduced in the previous session, applying PyTorch to real-world datasets relevant to Arctic research. By the end of this session, participants will have a solid understanding of how to implement deep learning models using PyTorch, empowering them to tackle their own projects with confidence.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#outline",
    "href": "sections/hands-on-lab-pytorch.html#outline",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.2 Outline",
    "text": "8.2 Outline\n\nRecap of PyTorch core functionalities\nGuided exercise 1: working with real-world datasets\nGuided exercise 2: building a simple neural network\nGuided exercise 3: training and evaluating the model\nTroubleshooting and optimization tips\nConclusion and Q&A",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-pytorch.html#reference",
    "href": "sections/hands-on-lab-pytorch.html#reference",
    "title": "8  Hands-On Lab: PyTorch",
    "section": "8.3 Reference",
    "text": "8.3 Reference\n\nhttps://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\nhttps://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\nhttps://pytorch.org/tutorials/beginner/vt_tutorial.html",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hands-On Lab: PyTorch</span>"
    ]
  },
  {
    "objectID": "sections/permafrost-discovery-gateway.html",
    "href": "sections/permafrost-discovery-gateway.html",
    "title": "9  Permafrost Discovery Gateway",
    "section": "",
    "text": "We have more satellite imagery data than what we know what to do with. There are many people with different knowledges and common passions for permafrost-affected landscapes. At the same time, Alaska and the Arctic at large are starving for basic geospatial information about it’s permafrost-affected landscape and infrastructure and people may have a hard time finding others to help make visions come true. The Permafrost Discovery Gateway (PDG) is an online free tool meant to empower a) researchers to produce and do science with big geospatial data (think sub-meter resolution maps across Alaska and the entire Arctic) and b) agencies and community leaders in land and infrastructure management that involves permafrost. PDG currently offers easy visual exploration via a regular web-browser of datasets that otherwise crush traditional GIS software due to the file size. In the works are, for example, new big datasets of permafrost thaw features and infrastructure, partial dataset download tool, the incorporation of statistical summaries and AI tools that help the user find interesting stories in the big data products, plug-and-play workflows to help you develop your own big dataset, and the monitoring of permafrost thaw near-real time. The PDG can become a gateway where data and people can connect, where technology enables anyone with an internet connection, no matter your technical skills and resources, to connect, explore, and together create.\nDownload Permafrost Discovery Gateway slides",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Permafrost Discovery Gateway</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html",
    "href": "sections/ai-ethics.html",
    "title": "10  AI Ethics",
    "section": "",
    "text": "Goal\nReview FAIR and CARE Principles, and their relevance to data ethics. Examine how ethical considerations are shared and considered at the Arctic Data Center. Discuss ethical considerations in AI and machine learning.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#intro-to-data-ethics",
    "href": "sections/ai-ethics.html#intro-to-data-ethics",
    "title": "10  AI Ethics",
    "section": "10.1 Intro to Data Ethics",
    "text": "10.1 Intro to Data Ethics\nThe Arctic Data Center is an openly-accessible data repository. The data published through it is open for anyone to reuse, subject to one of two license: CC-0 Public Domain and CC-By Attribution 4.0. As an open access repository, we prioritize long-term preservation and embrace principles from the data stewardship community, which established a set of best practices for open data management. n adherence, two principles adopted by the Arctic Data Center are FAIR Principles (Findable, Accessible, Interoperable, and Reproducible) and CARE Principles for Indigenous Governance (Collective Benefit, Authority to Control, Responsibility, Ethics). Both of which serve as frameworks in how to consider data ethics.\nThe FAIR Principles\nFAIR speaks to how metadata is managed, stored, and shared.\n\nWhat is the difference between FAIR principles and open science?\nFAIR principles and open science are overlapping concepts, but are distinctive from one another. Open science supports a culture of sharing research outputs and data, and FAIR focuses on how to prepare the data. The FAIR principles place emphasis on machine readability, “distinct from peer initiatives that focus on the human scholar” (Wilkinson et al 2016) and as such, do not fully engage with sensitive data considerations and with Indigenous rights and interests (Research Data Alliance International Indigenous Data Sovereignty Interest Group, 2019). Metadata can be FAIR but not open. For example, sensitive data (data that contains personal information) may not be appropriate to share, however sharing the anonymized metadata that is easily understandable will reduce research redundancy.\n\nResearch has historically perpetuated colonialism and represented extractive practices, meaning that the research results were not mutually beneficial. These issues also related to how data was owned, shared, and used. To address issues like these, the Global Indigenous Data Alliance (GIDA) introduced CARE Principles for Indigenous Data Governance to support Indigenous data sovereignty. CARE Principles speak directly to how the data is stored and shared in the context of Indigenous data sovereignty. CARE Principles (Carroll et al. 2020) stand for:\n\nCollective Benefit - Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data\nAuthority to Control - Indigenous Peoples’ rights and interests in Indigenous data must be recognized and their authority to control such data be empowered. Indigenous data governance enables Indigenous Peoples and governing bodies to determine how Indigenous Peoples, as well as Indigenous lands, territories, resources, knowledges and geographical indicators, are represented and identified within data.\nResponsibility - Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Accountability requires meaningful and openly available evidence of these efforts and the benefits accruing to Indigenous Peoples.\nEthics - Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n\nTo many, the FAIR and CARE principles are viewed by many as complementary: CARE aligns with FAIR by outlining guidelines for publishing data that contributes to open-science and at the same time, accounts for Indigenous’ Peoples rights and interests (Carroll et al. 2020).\nIn Arctic-based research, there is a paradigm shift to include more local Indigenous Peoples, their concerns, and knowledge throughout the research process (Loseto 2020). At the 2019 ArcticNet Annual Scientific Meeting (ASM), a 4-hour workshop was held between Indigenous and non-Indigenous participants to address the challenge of peer-reviewed publications arising when there is a lack of co-production and co-management in the research process between both groups. In the context of peer review, involving Indigenous People and Indigenous Knowledge (IK) not only can increase the validity of research findings, but also ensure the research is meaningful to those most impacted by it. Moreover, it gives power back to the appropriate people to decide who can be knowledge holders of Indigenous knowledge (Loseto et al. 2020). This example underscores the advocacy CARE framework for Indigenous sovereignty, emphasizing the essential integration of people and purpose in the peer review publication stage. Failure to do so perpetuates power imbalances between science institutions and Indigenous communities. Hence, an equitable framework would adhere to the idea ‘Not about us without us’. As an Arctic research community, it is important to reflect on ways we can continue to engage and incorporate Indigenous communities and if there are gaps to address. However, it is important there is no ‘one size fits all’ approach.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#ethics-at-the-arctic-data-center",
    "href": "sections/ai-ethics.html#ethics-at-the-arctic-data-center",
    "title": "10  AI Ethics",
    "section": "10.2 Ethics at the Arctic Data Center",
    "text": "10.2 Ethics at the Arctic Data Center\nTransparency in data ethics is a vital part of open science. Regardless of discipline, various ethical concerns are always present, including professional ethics such as plagiarism, false authorship, or falsification of data, to ethics regarding the handling of animals, to concerns relevant to human subjects research. As the primary repository for the Arctic program of the National Science Foundation, the Arctic Data Center accepts Arctic data from all disciplines. Recently, a new submission feature was released which asks researchers to describe the ethical considerations that are apparent in their research. This question is asked to all researchers, regardless of disciplines.\nSharing ethical practices openly, similar in the way that data is shared, enables deeper discussion about data management practices, data reuse, sensitivity, sovereignty and other considerations. Further, such transparency promotes awareness and adoption of ethical practices.\n \nInspired by CARE Principles for Indigenous Data Governance (Collective Benefit, Authority to Control, Responsibility, Ethics) and FAIR Principles (Findable, Accessible, Interoperable, Reproducible), we include a space in the data submission process for researchers to describe their ethical research practices. These statements are published with each dataset, and the purpose of these statements is to promote greater transparency in data collection and to guide other researchers. For more information about the ethical research practices statement, check out this blog.\nTo help guide researchers as they write their ethical research statements, we have listed the following ethical considerations that are available on our website. The concerns are organized first by concerns that should be addressed by all researchers, and then by discipline.\nConsider the following ethical considerations that are relevant for your field of research.\n\n10.2.1 Ethical Considerations for all Arctic Researchers\n\nResearch Planning\n\nWere any permits required for your research?\nWas there a code of conduct for the research team decided upon prior to beginning data collection?\nWas institutional or local permission required for sampling?\nWhat impact will your research have on local communities or nearby communities (meaning the nearest community within a 100 mile radius)?\n\nData Collection\n\nWere any local community members involved at any point of the research process, including study site identification, sampling, camp setup, consultation or synthesis?\nWere the sample sites near or on Indigenous land or communities?\n\nData Sharing and Publication\n\nHow were the following concerns accounted for: misrepresentation of results, misrepresentation of experience, plagiarism, improper authorship, or the falsification or data?\nIf this data is intended for publication, are authorship expectations clear for everyone involved? Other professional ethics can be found here",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#ai-ethics",
    "href": "sections/ai-ethics.html#ai-ethics",
    "title": "10  AI Ethics",
    "section": "10.3 AI Ethics",
    "text": "10.3 AI Ethics\nArtificial Intelligence (AI) can be thought of as the development of computer systems that can perform tasks we usually think require human intelligence, such as image recognition, language translation, or autonomous movement. The rapid development and adoption of AI tools in the past years, particularly machine learning algorithms, has revolutionized how big datasets are analyzed, transforming decision-making in all sectors of society. However, frameworks to examine the ethical considerations of AI are just emerging, and careful consideration of how to best develop and apply AI systems is essential to the responsible use of these new, rapidly changing tools. In this section, we will give an overview of the FAST Principles put forward by the Alan Turing Institute in their guide for the responsible design and implementation of AI systems [1].",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#the-fast-principles",
    "href": "sections/ai-ethics.html#the-fast-principles",
    "title": "10  AI Ethics",
    "section": "10.4 The FAST Principles",
    "text": "10.4 The FAST Principles\nFAST stands for Fairness, Accountability, Sustainability, and Transparency. The FAST principles aim to guide the ethical development of AI projects from their inception to deployment. The continuous involvement and commitment of software developers, domain experts, technical leads, project managers, rightsholders, and collaborators involved in the AI project is crucial to implement these principles successfully. The following is a brief overview of each of the FAST principles, we greatly encourage you to read through the Alan Turing Institute guide to learn more!\n\n\n\nLeslie, 2019",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#fairness",
    "href": "sections/ai-ethics.html#fairness",
    "title": "10  AI Ethics",
    "section": "10.5 Fairness",
    "text": "10.5 Fairness\nBias can enter at any point of a research project, from data collection and preprocessing, to model design and implementation. This is because AI projects, as any other, are created by human beings who (even with the best of intentions) can introduce error, prejudice, or misjudgement into a system. Fairness refers to the active minimization of bias and commitment to not harm others through the outcomes of an AI system. The FAST principles [1] suggest the following baseline for fairness:\n\nThe designers and users of AI systems ensure that the decisions and behaviours of their models do not generate discriminatory or inequitable impacts on affected individuals and communities. This entails that these designers and users ensure that the AI systems they are developing and deploying:\n\nAre trained and tested on properly representative, relevant, accurate, and generalisable datasets (Data Fairness)\nHave model architectures that do not include target variables, features, processes, or analytical structures (correlations, interactions, and inferences) which are unreasonable, morally objectionable, or unjustifiable (Design Fairness)\nDo not have discriminatory or inequitable impacts on the lives of the people they affect (Outcome Fairness)\nAre deployed by users sufficiently trained to implement them responsibly and without bias (Implementation Fairness)\n\n\n\n\n\n\n\n\nReal-life Example : Insufficient Radar Network\n\n\n\n\n\nThe following figure [2] shows coverage of the national Doppler weather network (green and yellow circles) over a demographic map of the Black population in the southeast US. This would be an example of an issue in data fairness, since radar coverage does not represent the population uniformly, leaving out areas with higher Black population. Problems with outcome fairness could ensue if this non-representative biases an AI model to under-predict weather impacts in such populations.\n\n\n\nMcGovern et al., 2022 by courtesy of Jack Sillin (CC BY 4.0).",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#accountability",
    "href": "sections/ai-ethics.html#accountability",
    "title": "10  AI Ethics",
    "section": "10.6 Accountability",
    "text": "10.6 Accountability\nAccountability in AI projects stems from the shared view that isolated AI models used to automate decisions are not morally responsible in the same way as a decision-making human. As outputs from AI models are increasingly used to make decisions that affect the environment and human lives, there is a critical need for competent human authorities to offer explanations and justifications for the development process, outputs, and ensuing decisions made by AI systems. Such answerability assignments can be challenging, as AI implementations are often the product of big development teams where the responsibility to answer for a project’s outcome may not be delineated, creating an issue known as “the problem of many hands.” The FAST principles encourage the following accountability implementation:\nAccountability by Design: All AI systems must be designed to facilitate end-to-end answerability and auditability. This requires both responsible humans-in-the-loop across the entire design and implementation chain as well as activity monitoring protocols that enable end-to-end oversight and review.\n\n\n\n\n\n\nReal-life Example: AI for natural disasters response\n\n\n\n\n\nAccountability and the ability to audit AI methods can be crucial when model outputs support critical decision-making, such as in natural disasters. In 2021, a New York Times investigation [3]] covered a private company’s premature release of outputs about neighborhoods most affected by potential earthquakes in Seattle. While the initial release erroneously did not show threats for non-residential areas, ensuing updated versions showed non-compatible predictions again. Although the company acknowledged that its AI models would not replace the first responder’s judgment, the lack of audibility and opacity in the model development hindered accountability for any party, ultimately eroding the public confidence in the tools and leading to a loss of public resources.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#sustainability",
    "href": "sections/ai-ethics.html#sustainability",
    "title": "10  AI Ethics",
    "section": "10.7 Sustainability",
    "text": "10.7 Sustainability\nSustainability in the FAST principles includes continuous assessment of the social impacts of an AI system and technical sustainability of the AI model. In the first consideration, the FAST principles advocate for performing a Stakeholder Impact Assessment (SIA) at different stages to help build confidence in the project and uncover unexpected risks or biases, among other benefits. The Alan Turing Institute guide shares a prototype of an SIA [1]. The core of technical sustainability is creating safe, accurate, reliable, secure, and robust AI systems. To achieve these technical goals, teams must implement thorough testing, performance metrics, uncertainty quantification, and be aware of changes to the underlying distribution of data, among other essential practices.\n\n\n\n\n\n\nReal-life Example: SpaceCows\n\n\n\n\n\nThe SpaceCows project [4], [5] in northern Australia is a collaboration between scientists, industry leaders, and local indigenous communities developing AI centered platforms to analyze GPS tracking data collected from feral cows alongside satellite imagery and weather data. Indigenous knowledge and traditional land owners have been at the center of the development, providing guidance and ultimately benefiting from the AI tools to protect their land and cultural sites.\n\n\n\nImportant indigenous cultural sites can be damaged by feral cattle. Image from CSIRO, SpaceCows: Using AI to tackle feral herds in the Top End.\n\n\nVideos with more information on SpaceCows:\nCSIRO rolls out world’s largest remote ‘space cows’ herd management system\nSpaceCows: Using AI to tackle feral herds in the Top End",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#transparency",
    "href": "sections/ai-ethics.html#transparency",
    "title": "10  AI Ethics",
    "section": "10.8 Transparency",
    "text": "10.8 Transparency\nUnder the FAST principles, transparency in AI projects refers to transparency about how an AI project was designed and implemented and the content and justification of the outcome produced by the AI model. To ensure process transparency, the project should show how the design and implementation included ethical, safety, and fairness considerations throughout the project. To clarify the content and explain the outcomes of an AI system, the project should offer plain language, non-technical explanations accessible to non-specialists that convey how and why a model performed the way it did. In this direction, it is essential to avoid a ‘mathematical glass box’ where the code and mathematics behind the algorithm are openly available, but there is a lack of rationale about how or why the model goes from input to output. Finally, the explanations about how the outcomes were produced should become the basis to justify the outcomes in terms of ethical permissibility, fairness, and trustworthiness. A careful consideration of the balance between the sustainability and transparency principles is necessary when dealing with protected or private data.\n\n\n\n\n\n\nReal-life Example: France’s Digital Republic Act\n\n\n\n\n\nThe concern for transparency in using personal data is an active space for debate. In 2018, the French government passed a law to protect citizens’ privacy, establishing the citizen’s “right to an explanation” regarding, among other things, how an algorithm contributed to decisions on their persona and which data was processed [6], [7]. Overall, this legislation aims to create a fairer and more transparent digital environment where everyone can enjoy equal opportunities.\n\n\n\nPhoto by Google DeepMind",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#discussion-activity",
    "href": "sections/ai-ethics.html#discussion-activity",
    "title": "10  AI Ethics",
    "section": "10.9 Discussion Activity",
    "text": "10.9 Discussion Activity\nGiven the discussion of the CARE priciples and the FAST principles, let’s discuss what responsible AI considerations might exist in the context of Arctic research, particularly with respect to Indigneous peoples of the Arctic. Geospatial data spanning the Arctic typically includes the traditional lands and waters of Arctic Indigenous peoples, and often intersects with current local communities distributed throughout the Arctic. Recent work by projects like Abundant Intelligences are starting to explore the intersection of Indigineous Knowledge systems, Artifical Intelligence models, and how to guide the “development of AI [to support] a more humane future”.\nLet’s take, for example, a researcher that wants to run an machine learning model to detect changes in environmental features at a large regional or Arctic scale. We’ve seen several of these so far, including 1) AI predictions of the distribution of permafrost ice wedges and retrogressive thaw slumps across the Arctic; 2) use of AI to detect changes in surface water extent and lake drainage events across the Arctic; 3) use of AI in a mechanistic process models that helps understand the global source/sink tradeoff of permafrost loss and its impact on climate.\n\n\n\n\n\n\nDiscussion questions\n\n\n\nDivide into 5 groups of 4, find a comfortable place to sit, and pick a large-scale AI application that is of interest to the group. Let’s discuss some of the following questions, and more…\n\nThinking of CARE, does that model provide Collective Benefit to Indigenous populations that it might impact?\nThinking of FAST, what would researchers need to do to ensure that their research process could meet the goals of the four categories of Fairness (Outcome Fairness, Data Fairness, Design Fairness, and Implementation Fairness) for Indigenous people in their research area?",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#conclusion",
    "href": "sections/ai-ethics.html#conclusion",
    "title": "10  AI Ethics",
    "section": "10.10 Conclusion",
    "text": "10.10 Conclusion\nAs new AI developments and applications rapidly emerge and transform everyday life, we need to pause and ensure these technologies are fair, sustainable, and transparent. We must acknowledge human responsibility in designing and implementing AI systems to use these novel tools fairly and with accountability. Finally, we acknowledge that the information covered here is a lightning introduction to AI’s ethical considerations and implications. Whether you are a researcher interested in using AI for the first time or a seasoned ML practitioner, we urge you to dive into the necessary and ever-expanding AI ethics work to learn how to best incorporate these concepts into your work.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/ai-ethics.html#further-reading",
    "href": "sections/ai-ethics.html#further-reading",
    "title": "10  AI Ethics",
    "section": "10.11 Further Reading",
    "text": "10.11 Further Reading\nAcademic Data Science Alliance (ADSA) (2024) The Data Sciene Ethos https://ethos.academicdatascience.org [8]\nChen, W., & Quan-Haase, A. (2020) Big Data Ethics and Politics: Towards New Understandings. Social Science Computer Review. [9]\nCrawford, K., & Paglen, T. (2019) Excavating AI: The Politics of Training Sets for Machine Learning. [10]\nGray, J., & Witt, A. (2021) A feminist data ethics of care framework for machine learning: The what, why, who and how. First Monday, 26(12), Article number: 11833 [11]\n\n\n\n\n[1] D. Leslie, “Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector,” Zenodo, Jun. 2019. doi: 10.5281/zenodo.3240529.\n\n\n[2] A. McGovern, I. Ebert-Uphoff, D. J. G. Ii, and A. Bostrom, “Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science,” Environmental Data Science, vol. 1, p. e6, Jan. 2022, doi: 10.1017/eds.2022.5.\n\n\n[3] S. Fink, “This High-Tech Solution to Disaster Response May Be Too Good to Be True,” The New York Times, Aug. 2019, Accessed: Oct. 19, 2024. [Online]. Available: https://www.nytimes.com/2019/08/09/us/emergency-response-disaster-technology.html\n\n\n[4] T. Shepherd, “Indigenous rangers to use SpaceCows program to protect sacred sites and rock art from feral herds,” The Guardian, Sep. 2021, Accessed: Oct. 19, 2024. [Online]. Available: https://www.theguardian.com/australia-news/2021/sep/15/indigenous-rangers-to-use-spacecows-program-to-protect-sacred-sites-and-rock-art-from-feral-herds\n\n\n[5] CSIRO, “SpaceCows: Using AI to tackle feral herds in the Top End.” Accessed: Oct. 19, 2024. [Online]. Available: https://www.csiro.au/en/news/All/News/2021/September/SpaceCows-Using-AI-to-tackle-feral-herds-in-the-Top-End\n\n\n[6] L. Edwards and M. Veale, “Enslaving the Algorithm: From a ‘Right to an Explanation’ to a ‘Right to Better Decisions’?” Social Science Research Network, Rochester, NY, 2018. doi: 10.2139/ssrn.3052831.\n\n\n[7] S. Lo Piano, “Ethical principles in machine learning and artificial intelligence: Cases from the field and possible ways forward,” Humanities and Social Sciences Communications, vol. 7, no. 1, pp. 1–7, Jun. 2020, doi: 10.1057/s41599-020-0501-9.\n\n\n[8] A. D. S. A. (ADSA), “The Data Science Ethos - Operationalizing Ethics in Data Science,” The Data Science Ethos. Accessed: Oct. 19, 2024. [Online]. Available: https://ethos.academicdatascience.org/\n\n\n[9] W. Chen and A. Quan-Haase, “Big Data Ethics and Politics: Toward New Understandings,” Social Science Computer Review, vol. 38, no. 1, pp. 3–9, Feb. 2020, doi: 10.1177/0894439318810734.\n\n\n[10] “Excavating AI.” Accessed: Oct. 19, 2024. [Online]. Available: https://excavating.ai/\n\n\n[11] J. Gray and A. Witt, “A feminist data ethics of care for machine learning: The what, why, who and how,” First Monday, Dec. 2021, doi: 10.5210/fm.v26i12.11833.",
    "crumbs": [
      "<b>Day 2: AI Fundamentals and Techniques</b>",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI Ethics</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html",
    "href": "sections/guest-lecture-yili-arts-dataset.html",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "",
    "text": "11.1 Overview\nIn this session, we will introduce and explore the Arctic Retrogressive Thaw Slump (ARTS) dataset. We aim to illuminate the background and motivation behind the ARTS dataset, detail its design elements including functions, metadata, and usage, and underscore its defining features such as scalability, scientific integrity, and the potential for community contribution.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html#outline",
    "href": "sections/guest-lecture-yili-arts-dataset.html#outline",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "11.2 Outline",
    "text": "11.2 Outline\n\nBackground and motivation of the Arctic\nRetrogressive Thaw Slump (ARTS) data set.\nSource data for the ARTS data set\nDesign of the data set - functions, metadata, usage\nFeatures of the data set - scalable, scientific, contributable\nData Curation Framework - standards, protocols\nThe ARTS repository - user and contributor guideline\nQuestions and discussions",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/guest-lecture-yili-arts-dataset.html#reference",
    "href": "sections/guest-lecture-yili-arts-dataset.html#reference",
    "title": "11  Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier",
    "section": "11.3 Reference",
    "text": "11.3 Reference\n\nhttps://github.com/whrc/ARTS",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html",
    "href": "sections/exploring-advanced-neural-networks.html",
    "title": "12  Exploring Advanced Neural Networks: Semantic Segmentation",
    "section": "",
    "text": "12.1 Overview\nThis session focuses on advanced neural networks, specifically targeting semantic segmentation. Participants will delve into models such as Fully Convolutional Networks (FCNs) and U-Net, learning how these networks are structured, how they function, and how they can be applied to accurately segment and label each pixel of an image according to the object it represents. The goal is to deepen participants’ understanding of the technical foundations and practical applications of semantic segmentation, equipping them with the skills needed for hands-on implementation and exploration of its real-world utility, particularly in the context of Arctic research.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Semantic Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#outline",
    "href": "sections/exploring-advanced-neural-networks.html#outline",
    "title": "12  Exploring Advanced Neural Networks: Semantic Segmentation",
    "section": "12.2 Outline",
    "text": "12.2 Outline\n\nIntroduction to semantic segmentation\nOverview of key models: Fully Convolutional Networks (FCNs) and U-Net\nDetailed architecture and functionality\nApplications in Arctic research: case studies\nConclusion and Q&A",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Semantic Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/exploring-advanced-neural-networks.html#reference",
    "href": "sections/exploring-advanced-neural-networks.html#reference",
    "title": "12  Exploring Advanced Neural Networks: Semantic Segmentation",
    "section": "12.3 Reference",
    "text": "12.3 Reference\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks for biomedical image segmentation.” Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer International Publishing, 2015. https://arxiv.org/abs/1505.04597\nMinaee, Shervin, et al. “Image segmentation using deep learning: A survey.” IEEE transactions on pattern analysis and machine intelligence 44.7 (2021): 3523-3542. http://www.arxiv.org/abs/2001.05566",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exploring Advanced Neural Networks: Semantic Segmentation</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "",
    "text": "13.1 Overview\nThis session introduces participants to MMSegmentation, a specialized deep learning library designed for semantic segmentation tasks. Participants will explore the unique capabilities of MMSegmentation in handling sophisticated image analysis projects. The session will cover how to navigate the library, implement advanced features, and apply them to real-world datasets, particularly in the context of Arctic research. While the focus will be on MMSegmentation, Detectron2 will also be mentioned as another powerful tool for image analysis. By the end of this session, participants will have a theoretical understanding of MMSegmentation and be prepared for a more extensive hands-on lab session.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#outline",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#outline",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.2 Outline",
    "text": "13.2 Outline\n\nIntroduction to MMSegmentation: features and capabilities\nNavigating MMSegmentation: tools and techniques\nImplementing semantic segmentation with MMSegmentation\nPractical applications in Arctic research\nBrief overview of Detectron2 for comparison\nConclusion and Q&A",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/intro-to-dl-libraries-for-image-analysis.html#reference",
    "href": "sections/intro-to-dl-libraries-for-image-analysis.html#reference",
    "title": "13  Introduction to Deep Learning Libraries for Image Analysis",
    "section": "13.3 Reference",
    "text": "13.3 Reference\n\nhttps://github.com/open-mmlab/mmsegmentation\nhttps://github.com/facebookresearch/detectron2 (for further exploration)",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Deep Learning Libraries for Image Analysis</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-mmsegmentation.html",
    "href": "sections/hands-on-lab-mmsegmentation.html",
    "title": "14  Hands-On Lab: MMSegmentation",
    "section": "",
    "text": "14.1 Overview\nThis hands-on lab session provides participants with practical experience using MMSegmentation to perform semantic segmentation tasks. Participants will engage in guided exercises that build on the concepts introduced in the previous session, applying MMSegmentation to real-world datasets relevant to Arctic research. By the end of this session, participants will have gained the practical skills necessary to implement and fine-tune semantic segmentation models using MMSegmentation, enabling them to effectively apply these techniques in their own research projects.",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hands-On Lab: MMSegmentation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-mmsegmentation.html#outline",
    "href": "sections/hands-on-lab-mmsegmentation.html#outline",
    "title": "14  Hands-On Lab: MMSegmentation",
    "section": "14.2 Outline",
    "text": "14.2 Outline\n\nRecap of MMSegmentation core functionalities\nGuided exercise 1: preparing and loading data\nGuided exercise 2: building and training a semantic segmentation model\nGuided exercise 3: evaluating and fine-tuning the model\nWorking with real-world datasets: Arctic research applications\nTroubleshooting and optimization tips\nConclusion and Q&A",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hands-On Lab: MMSegmentation</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-mmsegmentation.html#reference",
    "href": "sections/hands-on-lab-mmsegmentation.html#reference",
    "title": "14  Hands-On Lab: MMSegmentation",
    "section": "14.3 Reference",
    "text": "14.3 Reference\n\nhttps://mmsegmentation.readthedocs.io/en/latest/user_guides/index.html",
    "crumbs": [
      "<b>Day 3: Advanced AI Workflows and Models</b>",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hands-On Lab: MMSegmentation</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html",
    "href": "sections/ai-workflows-and-mlops.html",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "",
    "text": "15.1 Instructors\nBen Galewsky, Sr. Research Software Engineer National Center for Supercomputing Applications (NCSA) University of Illinois Urbana-Champaign",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#overview",
    "href": "sections/ai-workflows-and-mlops.html#overview",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "15.2 Overview",
    "text": "15.2 Overview\nMachine learning models have become a vital tool for most branches of science. The process and tools for training these models on the lab’s desktop is often fragile, slow, and not reproducible. In this workshop, we will introduce the concept of MLOps, which is a set of practices that aims to streamline the process of developing, training, and deploying machine learning models. We will use the popular open source MLOps tool, MLflow, to demonstrate how to track experiments, package code, and deploy models. We will also introduce Garden, a tool that allows researchers to publish ML Models as citable objects.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#outline",
    "href": "sections/ai-workflows-and-mlops.html#outline",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "15.3 Outline",
    "text": "15.3 Outline\n\nIntroduction to MLOps\nIntroduction to MLflow\nTracking experiments with MLflow\nPackaging code with MLflow\nDeploying models with MLflow\nPublishing models with Garden",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#three-challenges-for-ml-in-research",
    "href": "sections/ai-workflows-and-mlops.html#three-challenges-for-ml-in-research",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "15.4 Three Challenges for ML in Research",
    "text": "15.4 Three Challenges for ML in Research\n\nTraining productivity\nTraining reproducibility\nModel citability",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#introcution-to-mlops",
    "href": "sections/ai-workflows-and-mlops.html#introcution-to-mlops",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "15.5 Introcution to MLOps",
    "text": "15.5 Introcution to MLOps\nMachine Learning Operations (MLOps) is a set of practices that combines Machine Learning, DevOps, and Data Engineering to reliably and efficiently deploy and maintain ML models in production. Just as DevOps revolutionized software development by streamlining the bridge between development and operations, MLOps brings similar principles to machine learning systems.\nThink of MLOps as the infrastructure and practices that transform ML projects from experimental notebooks into robust, production-ready systems that deliver real reproducible scientific value.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#why-researchers-need-mlops",
    "href": "sections/ai-workflows-and-mlops.html#why-researchers-need-mlops",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "15.6 Why Researchers Need MLOps",
    "text": "15.6 Why Researchers Need MLOps\nAs a researcher, you’ve likely experienced the following challenges:\n\nInability to harness computing resources to robustly search hyperparameter space\nDifficulty reproducing results from six months ago\nRetraining is too painful to consider creating better models after new data becomes available\nTracking experiments becomes unwieldy as projects grow\nCollaboration among researchers in your group is difficult\nAdvisors have little visibility into students’ work\n\nMLOps addresses these pain points by providing:\n\n15.6.1 1. Reproducibility\n\nVersion control for data, code, and models\nAutomated documentation of experiments\nContainerization for consistent environments\n\n\n\n15.6.2 2. Automation\n\nAutomated training pipelines\nContinuous integration and deployment (CI/CD)\nAutomated testing and validation\n\n\n\n15.6.3 3. Production Monitoring\n\nReal-time performance monitoring\nData drift detection\nAutomated model retraining triggers\n\n\n\n15.6.4 4. Governance\n\nModel lineage tracking\n\n\nMLOps isn’t just another buzzword—it’s a crucial evolution in how we develop and deploy ML systems. As models become more complex and requirements for reliability increase, MLOps practices become essential for successful ML implementations.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#what-is-mlflow",
    "href": "sections/ai-workflows-and-mlops.html#what-is-mlflow",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "16.1 What is MLflow?",
    "text": "16.1 What is MLflow?\nMLflow is an open-source platform designed to manage the end-to-end machine learning lifecycle. Created by Databricks, it provides a unified set of tools that address the core challenges in developing, training, and deploying machine learning models. MLflow is language-agnostic and can work with any ML library, algorithm, or deployment tool.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#the-pdg-mlflow-tracking-server",
    "href": "sections/ai-workflows-and-mlops.html#the-pdg-mlflow-tracking-server",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "16.2 The PDG MLFlow Tracking Server",
    "text": "16.2 The PDG MLFlow Tracking Server\nThe Permafrost Discovery Gateway project has a shared MLFlow instance that you can use for your arctic research:\nhttps://pdg.mflow.software.ncsa.illinois.edu",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#key-components-of-mlflow",
    "href": "sections/ai-workflows-and-mlops.html#key-components-of-mlflow",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "16.3 Key Components of MLflow",
    "text": "16.3 Key Components of MLflow\nMLFlow consists of three main components:\n\n\n\n\n\n\nTracking\n\n\n\n\n\nThe tracking server of MLFlow allows researchers to log and compare model parameters, metrics, and artifacts across multiple runs. With MLFlow’s tracking features, users can record hyperparameters, evaluation metrics, model versions, and even source code, making it easier to reproduce results and collaborate with team members. The platform provides a user-friendly interface to visualize and compare different experiments, helping practitioners identify the most promising models and configurations. Additionally, MLFlow’s tracking capabilities integrate seamlessly with popular ML frameworks, enabling users to incorporate experiment logging into their existing workflows with minimal code changes. This comprehensive approach to tracking enhances model development efficiency and facilitates better decision-making throughout the machine learning process.\n\n16.3.1 Key Concepts in Tracking:\n\nParameters: key-value inputs to your code\nMetrics: numeric values (can update over time)\nArtifacts: arbitrary files, including data, models and plots\nSource: training code that ran\nVersion: version of the training code\nTags and Notes: any additional information\n\nimport mlflow\n\nwith mlflow.start_run():\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_metric(\"accuracy\", 0.85)\n    mlflow.log_artifact(\"model.pkl\")\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\n\n\n\nMLFlow Projects provide a standardized format for packaging and organizing machine learning code to make it reproducible and reusable across different environments. A Project in MLFlow is essentially a directory containing code, dependencies, and an MLProject file that specifies the project’s entry points and environment requirements. This structure enables data scientists to share their work with teammates who can reliably execute the same code, regardless of their local setup. The MLProject file can define multiple entry points, each specifying its parameters and command to run, making it flexible for different use cases within the same project. MLFlow supports various environments for project execution, including conda environments, Docker containers, and system environments, ensuring consistency across different platforms. This standardization not only improves collaboration but also simplifies the deployment process, as projects can be easily versioned and moved between development and production environments.\n\n16.3.2 Key Concepts in Projects\n\nPackaging format for reproducible ML runs\n\nAny code folder or GitHub repository\nOptional MLproject file with project configuration\n\nDefines dependencies for reproducibility\n\nConda (+ R, Docker, …) dependencies can be specified in MLproject\nReproducible in (almost) any environment\n\nExecution API for running projects\n\nCLI / Python / R / Java\nSupports local and remote execution\n\n\nname: myproject\n\npython_env: python_env.yaml\n\nentry_points:\n  main:\n    parameters:\n      learning_rate: {type: float, default: 0.01}\n    command: \"python train.py --lr {learning_rate}\"\n\n\n\n\n\n\n\n\n\n\nModels\n\n\n\n\n\nThe Models component of MLFlow provides a standardized way to package and deploy machine learning models across different platforms and serving environments. MLFlow’s model format includes all the code and dependencies required to run the model, making it highly portable and easy to share. The platform supports a variety of popular ML frameworks like scikit-learn, TensorFlow, and PyTorch, allowing models to be saved in a framework-agnostic format using the MLFlow Model Registry. This registry acts as a centralized repository where teams can version their models, transition them through different stages (like staging and production), and maintain a clear lineage of model iterations. MLFlow also provides built-in deployment capabilities to various serving platforms such as Kubernetes, Amazon SageMaker, and Azure ML, streamlining the process of moving models from experimentation to production. Additionally, MLFlow’s model serving feature allows for quick local deployment of models as REST endpoints, enabling easy testing and integration with other applications.\n\n16.3.3 Key Concepts in Models\n\nPackaging format for ML Models\n\nAny directory with MLmodel file\n\nDefines dependencies for reproducibility\n\nConda environment can be specified in MLmodel configuration\n\nModel creation utilities\n\nSave models from any framework in MLflow format\n\nDeployment APIs\n\nCLI / Python / R / Java\n\nModel Versioning and Lifecycle -Model repository",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#scaling-up-training-with-mlflow-slurm",
    "href": "sections/ai-workflows-and-mlops.html#scaling-up-training-with-mlflow-slurm",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "16.4 Scaling Up Training With MLFlow-Slurm",
    "text": "16.4 Scaling Up Training With MLFlow-Slurm\nWith your project defined in MLProject file, it’s easy to scale up your workflows by launching them onto a cluster.\nThere is a plugin for MLFlow developed by NCSA called mlflow-slurm\nTo use, you have to create a json file that tells the plugin how to configure slurm jobs.\n{\n  \"partition\": \"cpu\",\n  \"account\": \"bbmi-delta-cpu\",\n  \"mem\": \"16g\",\n  \"modules\": [\"anaconda3_cpu\"]\n}\nWith this in place you can launch a training run on your cluster with the command\nmlflow run --backend slurm \\\n          --backend-config slurm_config.json \\\n          examples/sklearn_elasticnet_wine",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#how-mlflow-solves-common-mlops-challenges",
    "href": "sections/ai-workflows-and-mlops.html#how-mlflow-solves-common-mlops-challenges",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "16.5 How MLflow Solves Common MLOps Challenges",
    "text": "16.5 How MLflow Solves Common MLOps Challenges\n\n16.5.1 Training productivity\n\nTrack impact of hyperparameter and code changes on model quality\nRun hyperparameter sweeps and find best run\nSwitch between desktop to supercomputer\n\n\n\n16.5.2 Training reproducibility\n\nEnforced use of reproducible runtime environments\nTrace models back to specific runs\n\n\n\n16.5.3 Model citeability\n\nPublish models to repository\nVersioning and lifecycle events",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#hands-on-tutorial",
    "href": "sections/ai-workflows-and-mlops.html#hands-on-tutorial",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "16.6 Hands-On Tutorial",
    "text": "16.6 Hands-On Tutorial\nWe will be using a GPU powered JuypterHub provided by NCSA. Connection instructions will be provided in the classroom.\nThe example code is in the Cyber2A-RTS-ToyModel repo. It has a notebook along with some libraries to keep the notebook focused on the MLOps aspects of the code.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#sharing-models",
    "href": "sections/ai-workflows-and-mlops.html#sharing-models",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "16.7 Sharing Models",
    "text": "16.7 Sharing Models\nNow that we have trained and validated a model we will want to first of all, share it with other members of our research group.\n\n16.7.1 MLFlow Model Repository\nExpert users within our research group will have access to the MLFlow tracking server and model repository. You can test the model as an artifact from an existing run. Once you are satisfied with its performance, you can publish it to the MLFlow model repository with the Register Model button on the tracking server.\nPublished models are given sequential version numbers so colleauges can rely on a stable model for their research. Models in the repository can also follow a lifecycle with MLFlow model aliases. Members of the research group who are not activly involved in model development may just want to use the current best model. The researcher who is training the model can decide which version others should use. MLFlow allows you to pull down the model symbolically.\nmlflow.pyfunc.load_model(\"models:/rts@prod\")\n\n\n16.7.2 Models as Citable Objects\nPublishing your MLProject and training code to a Git repo and making the data publicly readable through a data repository is a way for others to reproduce your models. However, to make your work truly reusable it is better to publish the weights of your trained model in a way that is findable, citable, and usable.\nAt a minimum, you should publish your model on Hugging Face. You can include a README and an notebook demonstrating how to use the model. HuggingFace allows you to mint a DOI that you can cite in your publications.\nHere’s an example with the RTS model:\n\nhttps://huggingface.co/bengal1/RTS/tree/main\n\nA new facility called Garden takes this a step further. Your model is hosted as an endpoint which is a hosted function-as-a-service which allows anyone to perform inference with your model without needing to install anything.\nOur example model is hosted at 10.26311/5fb6-f950\nYou can run a remote inference:\nfrom garden_ai import GardenClient\ngarden_client = GardenClient()\n\nrts_garden = garden_client.get_garden('10.26311/5fb6-f950')\nimage_url=\"https://github.com/cyber2a/Cyber2A-RTS-ToyModel/blob/main/data/images/valtest_yg_055.jpg?raw=true\"\npred = rts_garden.identify_rts(image_url)",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/ai-workflows-and-mlops.html#reference",
    "href": "sections/ai-workflows-and-mlops.html#reference",
    "title": "15  AI Workflows and MLOps: From Development to Deployment",
    "section": "16.8 Reference",
    "text": "16.8 Reference\n\nMLflow\nGarden",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>AI Workflows and MLOps: From Development to Deployment</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html",
    "href": "sections/foundation-models.html",
    "title": "17  Foundation Models: The Cornerstones of Modern AI",
    "section": "",
    "text": "17.1 Overview\nFoundation models (FM) are deep learning models trained on massive raw unlabelled datasets usually through self-supervised learning. FMs enable today’s data scientists to use them as the base and fine-tune using domain specific data to obtain models that can handle a wide range of tasks (language, vision, reasoning etc.). In this chapter, we provide an introduction to FMs, its history, evolution, and go through its key features and categories, and a few examples. We also briefly discuss how foundation models work. This chapter will be a precursor to the hands-on session that follows on the same topic.\nIn this session, we take a closer look at what constitutes a foundation model, a few examples, and some basic principles around how it works.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#overview",
    "href": "sections/foundation-models.html#overview",
    "title": "17  Foundation Models: The Cornerstones of Modern AI",
    "section": "",
    "text": "Fig : Image source- 2021 paper on foundation models by Stanford researchers [1]",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#outline",
    "href": "sections/foundation-models.html#outline",
    "title": "17  Foundation Models: The Cornerstones of Modern AI",
    "section": "17.2 Outline",
    "text": "17.2 Outline\n\nOverview of foundation models\nTypes of foundation models\nArchitecture\nSegment Anything Model (SAM 2)\nRetrieval Augmented Generation",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#introduction",
    "href": "sections/foundation-models.html#introduction",
    "title": "17  Foundation Models: The Cornerstones of Modern AI",
    "section": "17.3 Introduction",
    "text": "17.3 Introduction\n\n17.3.1 Traditional ML vs Deep Learning vs Foundation Models\nTraditional machine learning involves algorithms that learn patterns from structured data. Techniques like decision trees, support vector machines, and linear regression fall under this category. These methods often require feature engineering, where domain knowledge is used to select and transform input features to improve model performance. Traditional machine learning excels in scenarios with limited data and interpretable results.\nDeep learning is a subset of machine learning that employs neural networks with multiple layers (hence “deep”). These models automatically learn features from raw data, making them particularly powerful for complex tasks like image and speech recognition. Deep learning excels with large datasets and can capture intricate patterns but often requires significant computational resources and can be harder to interpret compared to traditional methods.\nFoundation models, such as GPT and BERT, represent a new paradigm in AI. These large-scale models are pre-trained on vast amounts of data and can be fine-tuned for specific tasks with minimal additional training. Earlier neural networks were narrowly tuned for specific tasks. With a little fine-tuning, foundation models can handle jobs from translating text to analyzing medical images. Foundation models generally learn from unlabeled datasets, saving the time and expense of manually describing each item in massive collections. Foundation models leverage transfer learning, allowing them to generalize across different tasks more effectively than traditional machine learning and deep learning models.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#foundation-models",
    "href": "sections/foundation-models.html#foundation-models",
    "title": "17  Foundation Models: The Cornerstones of Modern AI",
    "section": "17.4 Foundation Models",
    "text": "17.4 Foundation Models\nFoundation models, introduced in 2021 by Standford Researchers [1], are characterized by their enormous neural networks trained on vast datasets through self-supervised learning. These models serves as a “foundation” on which many task-specific models can be built by adaptation. Their capabilities improves with more data, requiring substantial computational power for training. These models can be adapted to various downstream tasks and are designed for reuse, leveraging transfer learning to enhance performance across different applications.\n\n\n\n\n\nFig : 2021 paper on foundation models by Stanford researchers [1]\n\n\nWith the start of availability of big data for training, evidence showed that performance improves with size. The field came to the conclusion that scale matters, and with the right model architecture, intelligence comes with large-scale data.\nHere’s a few examples of foundation models and their parameter count:\n\nCLIP [2] - 63 million parameters\nBERT [3] - 345 million parameters\nGPT-3 [3] - 175 billion parameters\n\nWikipedia consists of only 3% of its training data\n\nGPT-4 [4] - 1.8 trillion parameters\n\n\n\n\nFig : Growth in compute power. (Source: GPT-3 paper [3])",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#types-of-foundation-models",
    "href": "sections/foundation-models.html#types-of-foundation-models",
    "title": "17  Foundation Models: The Cornerstones of Modern AI",
    "section": "17.5 Types of foundation models",
    "text": "17.5 Types of foundation models\nFoundation models can be classified on the basis of its modality and its underlying architecture.\n\n\n\nCriteria: Modality\nCriteria: Architecture\n\n\n\n\nLanguage Models\nTransformer Models\n\n\nVision Models\nGenerative Models\n\n\nMultimodal Models\nDiffusion Models\n\n\n\n\n17.5.1 Types of foundation models (Modality)\n\n17.5.1.1 Language models\nLanguage models are trained for natural language processing tasks. The primary training objective for LLMs is often next-token prediction, where the model learns to predict the next word in a sequence given the preceding context. This is achieved through a vast amount of text data, enabling the model to learn grammar, facts, and even some reasoning patterns. LLMs tend to be good at various NLP related tasks, like translation, conversational AI, sentiment analysis, content summarization etc., to name a few.\nHere’s some examples of language models:\n\nGPT-3\nGPT-4\nLlama 3.2 [5]\n\n\n\n17.5.1.2 Vision models\nVision models are trained for computer vision tasks. The primary training objective of vision models is to effectively learn representations that enable accurate predictions or useful transformations based on visual data. Vision models tend to be good at tasks like object detection, segmentation, facial recognition, etc.\nHere’s some examples of vision models:\n\nGPT-4-turbo\nSAM [6]\nCLIP [5]\nSwin-transformer [7]\n\n\n\n17.5.1.3 Multimodal models\nMultimodal models are designed to process and understand multiple types of data modalities, such as text, images, audio, and more. These models can handle various data types simultaneously, allowing them to learn relationships and correlations between different forms of information (e.g., associating text descriptions with images). By training on datasets that include multiple modalities, multimodal foundation models learn to create a unified representation space where different types of data can be compared and processed together. This often involves shared architectures for encoding different modalities. These models can often perform well on tasks they haven’t been specifically trained on, thanks to their ability to leverage learned relationships across modalities. This makes them versatile for applications in various domains. Many multimodal models, like CLIP and DALL-E, use contrastive learning to improve their understanding of how different modalities relate. They aim to maximize similarity between paired data (e.g., an image and its caption) while minimizing similarity between unrelated pairs. These models can often perform well on tasks they haven’t been specifically trained on, thanks to their ability to leverage learned relationships across modalities. This makes them versatile for applications in various domains. Multimodal foundation models are used in diverse areas such as image and video captioning, visual question answering, cross-modal retrieval, and interactive AI systems that require understanding and generating multiple types of content.\nHere’s some examples of multimodal foundation models:\n\nGPT-4o\nDALL-E [8]\nCLIP [5]\nSora [9]\nGemini [10]\n\n\n\n\n17.5.2 Types of foundation models (Architecture)\n\n17.5.2.1 Transformer models\nIntroduced in 2017 by the paper “Attention is all you need” [11], the transformer architecture revolutionized NLP by enabling models to efficiently capture complex relationships in data without the limitations of recurrence. This architecture is known for its ability to handle sequential data efficiently. Its parallel processing capabilities and scalability have made it a foundational model for many state-of-the-art systems in various domains, including image processing and speech recognition. Checkout “The Illustrated Transformer” (blog post)[https://jalammar.github.io/illustrated-transformer/] for a detailed overview of the transformer architecture.\n\n\n\nFig : Transformer architecture\n\n\n\n17.5.2.1.1 Attention Mechanism\nAttention is, to some extent, motivated by how we pay visual attention to different regions of an image or correlate words in one sentence [12]. we can explain the relationship between words in one sentence or close context. When we see “eating”, we expect to encounter a food word very soon. The color term describes the food, but probably not so much with “eating” directly.\n\n\n\nFig : One word attends to other words in the same sentence differently\n\n\nCheck out Lilian Weng’s blog post [12] on detailed overview of attention mechanism.\n\n\n17.5.2.1.2 Key components of transformer architecture:\n\nSelf-Attention Mechanism:\n\n\nPurpose: Allows the model to weigh the importance of different words in a sequence relative to each other, capturing dependencies regardless of their distance.\nFunction: For each input token, self-attention computes a set of attention scores that determine how much focus to place on other tokens. This is done using three vectors: Query (Q), Key (K), and Value (V).\nCalculation: The attention score is computed as a dot product of Q and K, followed by a softmax operation to normalize it. The output is a weighted sum of the V vectors based on these scores.\n\nIn the example below, the self-attention mechanism enables us to learn the correlation between the current words and the previous part of the sentence.\n\n\n\nFig : The current word is in red and the size of the blue shade indicates the activation level [13]\n\n\n\nPositional Encoding:\n\n\nPurpose: Since transformers do not have a built-in notion of sequential order, positional encodings are added to the input embeddings to provide information about the position of tokens in the sequence.\nImplementation: Positional encodings use sine and cosine functions of different frequencies to generate unique values for each position.\n\n\nMulti-Head Attention:\n\n\nFunction: Instead of having a single set of attention weights, the transformer employs multiple attention heads, each learning different aspects of the relationships between tokens.\nProcess: The input is split into multiple sub-vectors, and self-attention is applied in parallel. The outputs of each head are concatenated and linearly transformed.\n\n\nFeed-Forward Networks:\n\n\nPurpose: After the multi-head attention step, each token’s representation is passed through a feed-forward neural network, which applies transformations independently to each position.\nStructure: Typically consists of two linear transformations with a ReLU activation in between.\n\n\nLayer Normalization and Residual Connections:\n\n\nLayer Normalization: Applied to stabilize and speed up training by normalizing the outputs of each layer.\nResidual Connections: Shortcuts are added around sub-layers (e.g., attention and feed-forward) to facilitate the flow of gradients during training, helping to mitigate the vanishing gradient problem.\n\n\nStacking Layers:\n\n\nTransformers consist of multiple layers of multi-head attention and feed-forward networks, allowing for deep representations of the input data.\n\n\nOutput Layer:\n\n\nFor tasks like language modeling or translation, the final layer typically uses a linear transformation followed by a softmax activation to predict the next token or class.\n\nThere are more than 50 major transformer models [14]. The transformer architecture is versatile and can be configured in different ways. The transformer architecture can support both auto-regressive and non-auto-regressive configurations depending on how the self-attention mechanism is applied and how the model is trained.\n\nAuto-Regressive Models: In an auto-regressive setup, like the original GPT (Generative Pre-trained Transformer), the model generates text one token at a time. During training, it predicts the next token in a sequence based on the previously generated tokens, conditioning on all prior context. This means that at each step, the model only attends to the tokens that come before the current position, ensuring that future tokens do not influence the prediction.\nNon-Auto-Regressive Models: Other models, like BERT (Bidirectional Encoder Representations from Transformers) [3], are designed to be non-auto-regressive. BERT processes the entire input sequence simultaneously and is trained using masked language modeling, where some tokens in the input are masked, and the model learns to predict them based on the surrounding context.\n\nGPT-3 and CLIP models utilize transformers as the underlying architecture.\n\n\n\n17.5.2.2 Generative-Adversarial models\nIntroduced in 2014, Generative Adversarial Networks (GANs) [15] involves two neural networks (generator-discriminator network pair) contest with each other in the form of a zero-sum game, where one agent’s gain is another agent’s loss. Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics.\n\n\n\nFig : GAN basic architecture\n\n\nIn a GAN,\n\nthe generator learns to generate plausible data. The generated instances become negative training examples for the discriminator.\nThe discriminator learns to distinguish the generator’s fake data from real data. The discriminator penalizes the generator for producing implausible results.\n\nWhen training begins, the generator produces obviously fake data, and the discriminator quickly learns to tell that it’s fake:\n\n\n\nFig : GAN training - early phase. Image source: Google developers blog\n\n\nAs training progresses, the generator gets closer to producing output that can fool the discriminator:\n\n\n\nFig : GAN training - mid phase\n\n\nFinally, if generator training goes well, the discriminator gets worse at telling the difference between real and fake. It starts to classify fake data as real, and its accuracy decreases. The training procedure for generator is to maximise the probability of discriminator making a mistake.\n\n\n\nFig : GAN training complete\n\n\nHere’s a picture of the whole system:\n\n\n\nFig : GAN architecture\n\n\nA disadvantage of GAN is potentially unstable training and less diversity in generation due to their adversarial training nature. StyleGAN [16] and BigGAN [17] are example of models that utilize GAN as the underlying architecture.\n\n\n17.5.2.3 Diffusion models\nDiffusion models, introduced in 2020 [18], are inspired by non-equilibrium thermodynamics. They define a Markov chain of diffusion steps to slowly add random noise to data and then learn to reverse the diffusion process to construct desired data samples from the noise [19].\n\n17.5.2.3.1 Key components of diffusion models\n\nForward Diffusion Process:\n\n\nThe forward process gradually adds Gaussian noise to the training data over a series of time steps. This process effectively transforms the original data into pure noise.\n\n\nReverse Diffusion Process:\n\n\nThe reverse process aims to denoise the noisy data back into a sample from the data distribution. This process is learned through a neural network.\nAt each time step, the model predicts the mean and variance of the distribution of the previous step conditioned on the current noisy data. The network outputs parameters that help in gradually removing the noise.\n\n\nTraining Objective:\n\n\nThe model is trained to minimize the difference between the predicted clean data and the actual data at each step of the diffusion process. This is often done using a mean squared error (MSE) loss between the predicted noise and the actual noise added during the forward process.\n\n\nSampling:\n\n\nTo generate new samples, the process starts with pure noise and applies the learned reverse diffusion process iteratively. Over multiple time steps, the model denoises the input until it resembles a sample from the training distribution.\n\n\n\n\nFig : Training a diffusion model. Image source : Lilweng’s blog\n\n\nDiffusion models can generate high-resolution and diverse images, often outperforming GANs in certain tasks. They are generally more stable to train compared to GANs, as they do not rely on adversarial training dynamics.\nStable-diffusion [20], DALL-E [8], Sora are some of the most common models utilizing diffusion architecture.\n\n\n\n\n17.5.3 Foundation Models - Applications\nHaving explored the foundational principles and capabilities of foundation models, we can now delve into specific applications that leverage their power. Two prominent techniques that build upon the capabilities of these models are Segment Anything Model (SAM) and Retrieval-Augmented Generation (RAG).",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#segment-anything-model",
    "href": "sections/foundation-models.html#segment-anything-model",
    "title": "17  Foundation Models: The Cornerstones of Modern AI",
    "section": "17.6 Segment Anything Model",
    "text": "17.6 Segment Anything Model\nSegment Anything Model (SAM) is a foundation model for the Promptable Visual Segmentation (PVS) task. PVS inspired from prompt engineering in NLP that user prompts can be a powerful tool for pre-training foundation models and downstream tasks. It is developed by the Fundamental AI Research (FAIR) team at Meta [6]. SAM is a simple and flexible framework that can segment any object in an image by providing a combination of one or more prompts - namely points, bounding boxes, or other segmentation masks. SAM is based on the transformer architecture and is trained on a large-scale dataset of images and their corresponding segmentation masks.\nThe latest version of SAM, SAM 2, can segment images and videos and uses a unified architecture for both tasks [21]. It is designed to handle complex scenes with multiple objects and can generate high-quality segmentations with minimal user input. The model can be used for various applications, including image editing, object detection, and video analysis.\nSince large-scale datasets for segmentation are unavailable, the research team created a data engine to generate segmentation masks, which were then manually annotated when developing SAM. The model was trained on diverse images to improve its generalization capabilities. This model-in-the-loop self-supervised training created two datasets: SA-1B containing 1B segmentation masks from about 11M privacy preserving images and SA-V dataset containing 642.6K masklets (spatio-temporal segmentation masks) from 50.9K videos.\n\n\n\n\n\n\nFigure 17.1: SAM 2 Architecture. Image source: [21]",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/foundation-models.html#retrieval-augmented-generation-rag",
    "href": "sections/foundation-models.html#retrieval-augmented-generation-rag",
    "title": "17  Foundation Models: The Cornerstones of Modern AI",
    "section": "17.7 Retrieval-Augmented Generation (RAG)",
    "text": "17.7 Retrieval-Augmented Generation (RAG)\nLarge pre-trained Language Models (LLMs) have revolutionized natural language processing, but they come with inherent limitations that necessitate the development of techniques like Retrieval-Augmented Generation (RAG). This chapter explores the motivations behind RAG by examining the constraints of traditional LLMs.\n\n\n\n\n\nFig : A typical user interaction with LLM\n\n\n\n17.7.1 Limitations of Large Language Models\n\nLack of Specific Knowledge Access\n\nLLMs, despite their vast training data, cannot access specific knowledge bases or datasets that weren’t part of their original training. This limitation becomes apparent when dealing with specialized domains or recent information.\n\nAbsence of User-Specific Data\n\nLLM has not seen “your” data - the unique, often proprietary information that organizations and individuals possess. This gap can lead to generic responses that lack the nuance and specificity required in many real-world applications.\n\nDomain-Specific Knowledge Challenges\n\nWhen specific domain knowledge is required, the traditional approach has been to fine-tune the LLM. However, this process can be resource-intensive and may not always yield optimal results, especially for niche or rapidly evolving fields.\n\n\n\n\n\nFig : Fine-tuning LLMs. Image source : datacamp blong\n\n\n\nLack of Source Attribution\n\nLLMs generate responses based on patterns learned during training, but they don’t provide sources for their information. This lack of attribution can be problematic in contexts where verifiability and credibility are crucial.\n\nHallucinations\n\nOne of the most significant issues with LLMs is their tendency to produce “hallucinations” - plausible-sounding but factually incorrect or nonsensical information. This phenomenon can undermine the reliability of the model’s outputs. See Lilweng’s blog post [22] on hallucinations for detailed information.\n\n\n\n\n\n\n\nFig : LLM Hallucination examples\n\n\n\n\nOutdated Information\n\nThe knowledge of an LLM is static, frozen at the time of its training. This leads to the problem of outdated information, where the model cannot account for recent events, discoveries, or changes in the world.\nRetrieval-Augmented Generation emerges as a solution to these limitations. By combining the generative capabilities of LLMs with the ability to retrieve and incorporate external, up-to-date information, RAG offers a path to more accurate, current, and verifiable AI-generated content. In the following sections, we will explore how RAG works, its advantages, and its potential applications in various domains.\n\n\n17.7.2 Introduction to RAG\nIntroduced in 2020 [23], RAG framework can be thought of as combining two techniques -\n\nGeneration\n\nDone by LLMs.\nLLM models used are typically tuned for question-answering\nLLM responds to a user query.\n\nRetrieval-Augmented\n\nUse an external database to store specific knowledge\nRetrieve the required information from the provided knowledge base\nProvide this retrieved information to the LLMs as context to answer user question.\n\n\nLet’s now compare the traditional LLM and RAG approaches\n\n17.7.2.1 Traditional LLM approach\n\n\n\nFig : Traditional LLM approach\n\n\n\nUser Input: The process begins with the user submitting a question.\nPrompt Engineering: The user’s question is combined with a pre-defined prompt.\nLLM Processing: The combined prompt and question are fed into the LLM.\nResponse Generation: The LLM generates and returns a response based on its training.\n\n\n\n17.7.2.2 RAG approach\n\n\n\nFig : RAG approach\n\n\n\nUser Input: As before, the user submits a question.\nKnowledge Base Query: The question is used to query a knowledge base.\nDocument Retrieval: Relevant documents are retrieved from the knowledge base.\nPrompt Construction: A prompt is constructed using:\n\nThe original question\nRetrieved relevant documents\nAny additional context or instructions\n\nLLM Processing: The comprehensive prompt is fed into the LLM.\nResponse Generation: The LLM generates a response based on both its pre-trained knowledge and the provided context.\n\n\n\n\n\n\n\n\nWithout RAG\nWith RAG\n\n\n\n\nNo ability to access a specific knowledge/domain\nPoint to a knowledge base\n\n\nNo sources\nSources cited in LLM response\n\n\nHallucinations\nLLM response is grounded by relevant information from knowledge base\n\n\nOut-of-date information\nUpdate the knowledge base with new information\n\n\n\n\n\n\n17.7.3 RAG - Retrieval-Augmented Generation\nWe will focus on the “retrieval” part of RAG for this section.\n\n17.7.3.1 Knowledge database\nIn the age of burgeoning data complexity and high-dimensional information, traditional databases often fall short when it comes to efficiently handling and extracting meaning from intricate datasets. Enter vector databases, a technological innovation that has emerged as a solution to the challenges posed by the ever-expanding landscape of data. (Source: beginner’s blog post on vector DB)\n\n17.7.3.1.1 Vector database\nVector databases have gained significant importance in various fields due to their unique ability to efficiently store, index, and search high-dimensional data points, often referred to as vectors. These databases are designed to handle data where each entry is represented as a vector in a multi-dimensional space. The vectors can represent a wide range of information, such as numerical features, embeddings from text or images, and even complex data like molecular structures.\nAt the heart of vector databases lies the concept of vector embeddings. These are mathematical representations of data points in a high-dimensional space. In the context of natural language processing:\n\nWord Embeddings: Individual words are represented as real-valued vectors in a multi-dimensional space.\nSemantic Capture: These embeddings capture the semantic meaning and relationships of the text.\nSimilarity Principle: Words with similar meanings tend to have similar vector representations.\n\n\n\n\n\n\nFig : Vectors\n\n\n\n17.7.3.1.1.1 How vector databases work\nLet’s start with a simple example of dealing with an LLM such as ChatGPT. The model has large volumes of data with a lot of content, and they provide us with the ChatGPT application.\n\n\n\nFig : VectorDB within RAG. Source: KDnuggets blog post\n\n\nSo let’s go through the steps.\n\nAs the user, you will input your query into the application.\nYour query is then inserted into the embedding model which creates vector embeddings based on the content we want to index.\nThe vector embedding then moves into the vector database, regarding the content that the embedding was made from.\nThe vector database produces an output and sends it back to the user as a query result.\n\nWhen the user continues to make queries, it will go through the same embedding model to create embeddings to query that database for similar vector embeddings. The similarities between the vector embeddings are based on the original content, in which the embedding was created.\nNow lets see how it works in the vector database.\n\n\n\nFig : VectorDB pipeline. Source: pinecone blog post\n\n\nThe three main stages that a vector database query goes through are:\n\nIndexing\n\nAs explained in the example above, once the vector embedding moves into the vector database, it then uses a variety of algorithms to map the vector embedding to data structures for faster searching.\n\nQuerying\n\nOnce it has gone through its search, the vector database compares the queried vector to indexed vectors, applying the similarity metric to find the nearest neighbor.\n\nPost Processing\n\nDepending on the vector database you use, the vector database will post-process the final nearest neighbor to produce a final output to the query. As well as possibly re-ranking the nearest neighbors for future reference.\n\n\n\n\n\n17.7.4 RAG - Retrieval-Augmented Generation\nWe will focus on the “generation” part of RAG for this section. Here most of the heavy-lifting is done by the LLMs. Let’s see how best to communicate/prompt these LLM models for RAG.\n\n17.7.4.1 Prompting\nPrompting is a crucial technique in effectively communicating with Large Language Models (LLMs) to achieve desired outcomes without modifying the underlying model. As LLMs become more sophisticated, the art of crafting effective prompts has emerged as a key skill in natural language processing and AI applications. Checkout LilianWeng blog post [24], medium blog post on prompt engineering.\nPrompting is often an iterative process. It typically requires multiple trial-and-error attempts to achieve the desired effect. Each iteration can provide insights into how the model interprets and responds to different input structures.\n\n17.7.4.1.1 Key Elements of Effective Prompting\n\nDefining a Persona\n\nAssigning the LLM a specific role or behavior can significantly influence its responses. By giving it a defined persona, the model will attempt to respond in a manner that aligns with that role. This can improve the quality and relevance of its answers.\nExample: “You are a helpful research assistant”\nThis prompt frames the model’s responses to be in line with the behavior expected of a research assistant, such as providing accurate information and being resourceful.\n\nSetting Guardrails\n\nGuardrails provide boundaries or conditions within which the model should operate. This is particularly useful to avoid misleading or incorrect information. You can ask the model to refrain from answering if it’s unsure of the response.\nExample: “If you don’t know the final answer, just say ‘I don’t know’.”\nThis instructs the LLM to admit uncertainty instead of generating a potentially incorrect answer, thereby increasing reliability.\n\nProviding Clear Instructions\n\nGiving the LLM specific actions to perform before generating responses ensures that it processes the necessary information correctly. This is important when dealing with tasks like reviewing files or using external data.\nExample: “Read the data file before answering any questions.”\nThis directs the LLM to review relevant materials, improving the quality of the subsequent answers.\n\nSpecifying Response Formats\n\nYou can enhance the usefulness of responses by specifying the desired output format. By doing this, you ensure the model delivers information in a form that aligns with your needs.\nExample: “Respond using markdowns.”\nThis ensures the LLM outputs text in Markdown format, which can be helpful for structured documents or technical writing.\n\n\n\n\n17.7.5 RAG System\nLet’s bring it all together\n\n\n\nFig : RAG system. Image source : blog.demir\n\n\n\nUser Submits Query: The user inputs a query into the system. This is the initial step where the user’s request is captured.\nRAG System Query Relevant Documents: The RAG system processes the user’s query and searches for relevant documents.\nDocument Database Returns Documents: The document database receives the request for relevant documents and returns the documents it finds to the RAG system.\nCombine The Query & The Documents: The RAG system takes the documents provided by the document database and combines them with the original query.\nLLM Returns Answer: The combined query and documents are sent to a Large Language Model (LLM), which generates an answer based on the information provided.\nRAG System Return Answer to User: Finally, the answer generated by the LLM is sent back through the RAG system.\n\n\n\n\n\n[1] R. Bommasani et al., “On the opportunities and risks of foundation models,” ArXiv, 2021, Available: https://crfm.stanford.edu/assets/report.pdf\n\n\n[2] A. Radford et al., “Learning transferable visual models from natural language supervision,” CoRR, vol. abs/2103.00020, 2021, Available: https://arxiv.org/abs/2103.00020\n\n\n[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” CoRR, vol. abs/1810.04805, 2018, Available: http://arxiv.org/abs/1810.04805\n\n\n[4] OpenAI et al., “GPT-4 technical report.” 2024. Available: https://arxiv.org/abs/2303.08774\n\n\n[5] A. Dubey et al., “The llama 3 herd of models.” 2024. Available: https://arxiv.org/abs/2407.21783\n\n\n[6] A. Kirillov et al., “Segment anything.” 2023. Available: https://arxiv.org/abs/2304.02643\n\n\n[7] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using shifted windows.” 2021. Available: https://arxiv.org/abs/2103.14030\n\n\n[8] A. Ramesh et al., “Zero-shot text-to-image generation,” CoRR, vol. abs/2102.12092, 2021, Available: https://arxiv.org/abs/2102.12092\n\n\n[9] Y. Liu et al., “Sora: A review on background, technology, limitations, and opportunities of large vision models.” 2024. Available: https://arxiv.org/abs/2402.17177\n\n\n[10] G. Team et al., “Gemini: A family of highly capable multimodal models.” 2024. Available: https://arxiv.org/abs/2312.11805\n\n\n[11] A. Vaswani et al., “Attention is all you need,” CoRR, vol. abs/1706.03762, 2017, Available: http://arxiv.org/abs/1706.03762\n\n\n[12] L. Weng, “Attention? attention!” lilianweng.github.io, 2018, Available: https://lilianweng.github.io/posts/2018-06-24-attention/\n\n\n[13] J. Cheng, L. Dong, and M. Lapata, “Long short-term memory-networks for machine reading,” CoRR, vol. abs/1601.06733, 2016, Available: http://arxiv.org/abs/1601.06733\n\n\n[14] X. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and M. Kazi, “Transformer models: An introduction and catalog.” 2024. Available: https://arxiv.org/abs/2302.07730\n\n\n[15] I. J. Goodfellow et al., “Generative adversarial networks.” 2014. Available: https://arxiv.org/abs/1406.2661\n\n\n[16] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for generative adversarial networks,” CoRR, vol. abs/1812.04948, 2018, Available: http://arxiv.org/abs/1812.04948\n\n\n[17] A. Brock, J. Donahue, and K. Simonyan, “Large scale GAN training for high fidelity natural image synthesis,” CoRR, vol. abs/1809.11096, 2018, Available: http://arxiv.org/abs/1809.11096\n\n\n[18] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” CoRR, vol. abs/2006.11239, 2020, Available: https://arxiv.org/abs/2006.11239\n\n\n[19] L. Weng, “What are diffusion models?” lilianweng.github.io, Jul. 2021, Available: https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\n\n\n[20] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” CoRR, vol. abs/2112.10752, 2021, Available: https://arxiv.org/abs/2112.10752\n\n\n[21] N. Ravi et al., “SAM 2: Segment anything in images and videos,” arXiv preprint arXiv:2408.00714, 2024, Available: https://arxiv.org/abs/2408.00714\n\n\n[22] L. Weng, “Extrinsic hallucinations in LLMs.” lilianweng.github.io, Jul. 2024, Available: https://lilianweng.github.io/posts/2024-07-07-hallucination/\n\n\n[23] P. S. H. Lewis et al., “Retrieval-augmented generation for knowledge-intensive NLP tasks,” CoRR, vol. abs/2005.11401, 2020, Available: https://arxiv.org/abs/2005.11401\n\n\n[24] L. Weng, “Prompt engineering,” lilianweng.github.io, Mar. 2023, Available: https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Foundation Models: The Cornerstones of Modern AI</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html",
    "href": "sections/hands-on-lab-foundation-models.html",
    "title": "18  Hands-On Lab: Foundation Models",
    "section": "",
    "text": "18.1 Overview\nThe hands-on lab on foundation models will focus on building and applying small-scale foundation models for some example use cases. The main goal of this 1-hour session will be to get more familiarized with foundation models and in interacting with them.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#outline",
    "href": "sections/hands-on-lab-foundation-models.html#outline",
    "title": "18  Hands-On Lab: Foundation Models",
    "section": "18.2 Outline",
    "text": "18.2 Outline\n\nImage Segmentation using Segment Anything Model (SAM)\nChatbot using LLMs + RAG",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#references",
    "href": "sections/hands-on-lab-foundation-models.html#references",
    "title": "18  Hands-On Lab: Foundation Models",
    "section": "18.3 References",
    "text": "18.3 References\n\nSegment Anything\nSegment Anything Notebook",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#sam",
    "href": "sections/hands-on-lab-foundation-models.html#sam",
    "title": "18  Hands-On Lab: Foundation Models",
    "section": "18.4 SAM",
    "text": "18.4 SAM",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/hands-on-lab-foundation-models.html#rag",
    "href": "sections/hands-on-lab-foundation-models.html#rag",
    "title": "18  Hands-On Lab: Foundation Models",
    "section": "18.5 RAG",
    "text": "18.5 RAG",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hands-On Lab: Foundation Models</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html",
    "href": "sections/reproducibility.html",
    "title": "19  Reproducibility",
    "section": "",
    "text": "19.1 Goal\nThis session aims to highlight the importance of reproducibility in AI-driven Arctic research. Participants will learn about the challenges and best practices for ensuring that AI models and their results can be reproduced by other researchers, a cornerstone for building trust and advancing the field. The discussion will cover strategies for documenting experiments, sharing data and code, and using version control systems.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#introduction",
    "href": "sections/reproducibility.html#introduction",
    "title": "19  Reproducibility",
    "section": "19.2 Introduction",
    "text": "19.2 Introduction\nReproducibility is not a new topic when it comes to artificial intelligence and machine learning in science, but is more important than ever as AI research is often criticized for not being reproducible. This becomes particularly problematic when validation of a model requires reproducing it.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#why-is-reproducibility-important",
    "href": "sections/reproducibility.html#why-is-reproducibility-important",
    "title": "19  Reproducibility",
    "section": "19.3 Why is Reproducibility Important?",
    "text": "19.3 Why is Reproducibility Important?\n\n19.3.1 Reproducible vs. Replicable\nReproducibility is important in science because it allows other researchers to validate the results of a study and/or use the same analysis for processing their data, promoting open science and collaboration.\n\nReproducible means that other researchers can take the same data, run the same analysis, and get the same result.\nReplicable means other researchers can take different data, run the same analysis, and get their own result without errors.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#the-reproducibility-checklist",
    "href": "sections/reproducibility.html#the-reproducibility-checklist",
    "title": "19  Reproducibility",
    "section": "19.4 The Reproducibility Checklist",
    "text": "19.4 The Reproducibility Checklist\nThe Reproducibility Checklist was created by Canadian computer scientist, Joelle Pineau, with the goal of facilitating reproducible machine learning algorithms that can be tested and replicated. When publishing your model, it is beneficial to work through this checklist and ensure that you’re including the items on this checklist. The checklist is as follows:\nFor all models and algorithms, check that you include:\n\nA clear description of the mathematical setting, algorithm, and/or model\nAn analysis of the complexity (time, space, sample size) of any algorithm\nA link to a downloadable source code*, with specification of all dependencies, including external libraries\n\nFor any theoretical claim, check that you include:\n\nA statement of the result\nA clear explanation of each assumption\nA complete proof of the claim\n\nFor all figures and tables that include empirical results, check that you include:\n\nA complete description of the data collection process, including sample size\nA link to a downloadable version of the dataset or simulation environment\nAn explanation of any data that was excluded and a description of any preprocessing step\nAn explanation of how samples were allocated for training, validation, and testing\nThe range of hyperparameters considered, method to select the best hyperparameter configuration, and specification of each hyperparameter used to generate results\nThe exact number of evaluation runs\nA description of how experiments were run\nA clear definition of the specific measure of statistics used to report results\nClearly defined error bars\nA description of results with central tendency (e.g., mean) and variation (e.g., standard deviation)\nA description of the computing infrastructure used\n\n*With sensitive data or proprietary code, scientists may not wish to release all of their code and data. In this case, data can be anonymized and/or partial code can be released that won’t run but can be read and reproduced.\nConsider the sensitivity of your data/code when publishing.\n\nSensitive data should be anonymized before publishing\nResearchers or organizations may only release partial code if their code is proprietary\nBe sure that the partial code released can still be read and reproduced",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#sharing-code",
    "href": "sections/reproducibility.html#sharing-code",
    "title": "19  Reproducibility",
    "section": "19.5 Sharing Code",
    "text": "19.5 Sharing Code\nThe first step to solving the problem of reproducibility is sharing the code that was used to generate the model. This allows other researchers to:\n\nValidate the model\nTrack code construction and see any author annotations\nExpand on published work\n\nDespite this, sharing code does not always mean that models are fully reproducible. Many machine learning models are trained on restricted datasets or require extensive computing power for training the model. Because of this, there are a few additional criteria that improve reproducibility including:\n\nData and metadata availability (must be included without question)\nTransparency of the code you’re using and dependencies needed to run the code\nEasily installable computational analysis tools and pipelines\nInstalled software should behave the same on every machine and should have the same runtime\n\n\n19.5.1 Trips and Tricks to Sharing Code\n\nAvoid using absolute file paths when reading in data (and in general the use of slashes, as these differ between operating systems)\n\n\n\n\nAvoid using an absolute path to read in your data as shown here\n\n\n\nClean your data within your code\nAvoid copy/pasting in a spreadsheet\nAlways keep an unedited version of your raw data\n\nA general guide to publishing reproducible work:",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#model-repositories",
    "href": "sections/reproducibility.html#model-repositories",
    "title": "19  Reproducibility",
    "section": "19.6 Model Repositories",
    "text": "19.6 Model Repositories\nPyTorch Hub is a pre-trained model repository designed to facilitate reproducibility and enable new research. It is easily usable with Colab and Papers with Code, but models must be trained on openly accessible data.\n\nPapers with Code is an open source hub for publications that include direct links to GitHub code, no account needed to access datasets.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#version-control",
    "href": "sections/reproducibility.html#version-control",
    "title": "19  Reproducibility",
    "section": "19.7 Version Control",
    "text": "19.7 Version Control\n\nVersion control is the process of keeping track of every individual change by each contributor that’s saved in a version control framework, or a special database. Keeping a history of these changes to track model performance relative to model parameters saves the time you’d spend retraining the model.\nThe three components of version control in machine learning are:\n\nCode: We recommend writing and storing your model code in the same language as your implementation code to make it easier to maintain all code and dependencies\nData: Versioning should link the data to the appropriate metadata and note any changes in either\nModel: The model connects your code and data with your model parameters and analysis\n\nUsing a version control system ensures easier:\n\nCollaboration\n\nCollaborators can easily pull changes from a shared repository, push their own changes, annotate their code, and revert back to previous versions of their model\n\nVersioning\n\nIf your model breaks, you’ll have a log of any changes that were made, allowing you or others to revert back to a stable version\n\nDependency tracking\n\nYou can test more than one model on different branches or repositories, tune the model parameters and hyperparameters, and monitor the accuracy of each implemented change\n\nModel updates\n\nVersion control allows for incrementally released versions while continuing the development of the next release",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#summary",
    "href": "sections/reproducibility.html#summary",
    "title": "19  Reproducibility",
    "section": "19.8 Summary",
    "text": "19.8 Summary\nConsider the following to ensure your model is reproducible:\n\nUse the reproducibility checklist for algorithms, theoretical claims, and figures/tables.\nAnonymize any sensitive data and remove proprietary code before publishing\n\nBUT still provide training data and enough code for others to replicate your model\n\nShare data and metadata, be transparent in any dependencies needed to run your model, use easily installable computational analysis tools and pipelines, and ensure installed software behaves the same on every machine (i.e. runtime)\nUse a pre-trained model repository (ex. PyTorch Hub) and publish to open-source journals/websites (ex. Papers with Code)\nPractice efficient version control (recommend GitHub if working with collaborators)",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#hands-on-activity",
    "href": "sections/reproducibility.html#hands-on-activity",
    "title": "19  Reproducibility",
    "section": "19.9 Hands-On Activity",
    "text": "19.9 Hands-On Activity\nLEGO Metadata: In groups of 3-5 people, take ~15 minutes to create a structure out of LEGO bricks and write instructions for a group who will recreate your structure based on these instructions.\nGroups will then be rotated and given instructions and LEGO pieces from another group where they will have ~15 minutes to attempt to recreate that group’s structure.\nWe will have a closing group discussion about this activity. Some questions include:\n\nWhat were some assumptions you made while writing your instructions?\nWere there any unexpected hurdles you encountered when writing your instructions or trying to replicate another group’s structure?\nWhat did you find most difficult about this activity?\nNow that you see how successful or unsuccessful the other group was in recreating your structure, is there anything you would do differently?\n\nThis activity was adapted from the Lego Metadata for Reproducibility Game Pack (doi: 10.36399/gla.pubs.196477) developed by Mary Donaldson and Matt Mahon at the University of Glasgow.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "sections/reproducibility.html#references-resources",
    "href": "sections/reproducibility.html#references-resources",
    "title": "19  Reproducibility",
    "section": "19.10 References & Resources",
    "text": "19.10 References & Resources\n\nGundersen, Odd Erik, and Sigbjørn Kjensmo. 2018. “State of the Art: Reproducibility in Artificial Intelligence”. Proceedings of the AAAI Conference on Artificial Intelligence 32 (1).\nGundersen, Odd Erik, Yolanda Gil, and David W. Aha. “On Reproducible AI: Towards Reproducible Research, Open Science, and Digital Scholarship in AI Publications.” AI Magazine 39, no. 3 (September 28, 2018): 56–68.\n“How the AI Community Can Get Serious about Reproducibility.” Accessed September 18, 2024.\nAbid, Areeba. “Addressing ML’s Reproducibility Crisis.” Medium, January 7, 2021.\nPyTorch. “Towards Reproducible Research with PyTorch Hub.” Accessed September 18, 2024.\nStojnic, Robert. “ML Code Completeness Checklist.” PapersWithCode (blog), April 8, 2020.\nAkalin, Altuna. “Scientific Data Analysis Pipelines and Reproducibility.” Medium, July 5, 2021.\nHashesh, Ahmed. “Version Control for ML Models: What It Is and How To Implement It.” neptune.ai, July 22, 2022.\nNCEAS Learning Hub\nDonaldson, M. and Mahon, M. 2019. LEGO® Metadata for Reproducibility game pack. Documentation. University of Glasgow.",
    "crumbs": [
      "<b>Day 4: Workflows and Foundation Models</b>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] A.\nK. Liljedahl et al., “Pan-Arctic ice-wedge\ndegradation in warming permafrost and its influence on tundra\nhydrology,” Nature Geoscience, vol. 9, no. 4, pp.\n312–318, Apr. 2016, doi: 10.1038/ngeo2674.\n\n\n[2] A.\nA. Vasiliev, D. S. Drozdov, A. G. Gravis, G. V. Malkova, K. E. Nyland,\nand D. A. Streletskiy, “Permafrost degradation in the\nWestern Russian Arctic,”\nEnvironmental Research Letters, vol. 15, no. 4, p. 045001, Apr.\n2020, doi: 10.1088/1748-9326/ab6f12.\n\n\n[3] S.\nL. Smith, H. B. O’Neill, K. Isaksen, J. Noetzli, and V. E. Romanovsky,\n“The changing thermal state of permafrost,” Nature\nReviews Earth & Environment, vol. 3, no. 1, pp. 10–23, Jan.\n2022, doi: 10.1038/s43017-021-00240-1.\n\n\n[4] T.\nA. Douglas, M. R. Turetsky, and C. D. Koven, “Increased rainfall\nstimulates permafrost thaw across a variety of Interior\nAlaskan boreal ecosystems,” npj Climate and\nAtmospheric Science, vol. 3, no. 1, pp. 1–7, Jul. 2020, doi: 10.1038/s41612-020-0130-4.\n\n\n[5] R.\nÍ. Magnússon et al., “Extremely wet summer events enhance\npermafrost thaw for multiple years in Siberian\ntundra,” Nature Communications, vol. 13, no. 1, p. 1556,\nMar. 2022, doi: 10.1038/s41467-022-29248-x.\n\n\n[6] L.\nM. Farquharson, V. E. Romanovsky, W. L. Cable, D. A. Walker, S. V.\nKokelj, and D. Nicolsky, “Climate Change\nDrives Widespread and Rapid\nThermokarst Development in Very\nCold Permafrost in the Canadian\nHigh Arctic,” Geophysical Research\nLetters, vol. 46, no. 12, pp. 6681–6689, 2019, doi: 10.1029/2019GL082187.\n\n\n[7] D.\nNotz and J. Stroeve, “Observed Arctic sea-ice loss\ndirectly follows anthropogenic CO2 emission,”\nScience, vol. 354, no. 6313, pp. 747–750, Nov. 2016, doi: 10.1126/science.aag2345.\n\n\n[8] D.\nM. Nielsen, M. Dobrynin, J. Baehr, S. Razumov, and M. Grigoriev,\n“Coastal Erosion Variability at the\nSouthern Laptev Sea\nLinked to Winter Sea\nIce and the Arctic\nOscillation,” Geophysical Research Letters,\nvol. 47, no. 5, p. e2019GL086876, 2020, doi: 10.1029/2019GL086876.\n\n\n[9] L.\nBruhwiler, F.-J. W. Parmentier, P. Crill, M. Leonard, and P. I. Palmer,\n“The Arctic Carbon Cycle\nand Its Response to Changing\nClimate,” Current Climate Change Reports,\nvol. 7, no. 1, pp. 14–34, Mar. 2021, doi: 10.1007/s40641-020-00169-5.\n\n\n[10] T.\nK. F. Campbell, T. C. Lantz, R. H. Fraser, and D. Hogan, “High\nArctic Vegetation Change\nMediated by Hydrological\nConditions,” Ecosystems, vol. 24, no. 1,\npp. 106–121, Jan. 2021, doi: 10.1007/s10021-020-00506-7.\n\n\n[11] S.\nC. Davidson et al., “Ecological insights from three\ndecades of animal movement tracking across a changing\nArctic,” Science, vol. 370, no. 6517, pp.\n712–715, Nov. 2020, doi: 10.1126/science.abb7080.\n\n\n[12] L.\nSuter, D. Streletskiy, and N. Shiklomanov, “Assessment of the cost\nof climate change impacts on critical infrastructure in the circumpolar\nArctic,” Polar Geography, vol. 42, no. 4,\npp. 267–286, Oct. 2019, doi: 10.1080/1088937X.2019.1686082.\n\n\n[13] M.\nL. Druckenmiller et al., “The\nArctic,” Bulletin of the American Meteorological\nSociety, vol. 102, no. 8, pp. S263–S316, Aug. 2021, doi: 10.1175/BAMS-D-21-0086.1.\n\n\n[14] M.\nPhilipp, A. Dietz, S. Buchelt, and C. Kuenzer, “Trends in\nSatellite Earth Observation for\nPermafrost Related\nAnalyses—A Review,”\nRemote Sensing, vol. 13, no. 6, p. 1217, Jan. 2021, doi: 10.3390/rs13061217.\n\n\n[15] “Changing state of Arctic\nsea ice across all seasons - IOPscience.” Accessed:\nOct. 18, 2024. [Online]. Available: https://iopscience.iop.org/article/10.1088/1748-9326/aade56\n\n\n[16] “AI in\nAnalytics: Top Use\nCases and Tools.” Accessed: Oct. 18,\n2024. [Online]. Available: https://www.marketingaiinstitute.com/blog/how-to-use-artificial-intelligence-for-analytics\n\n\n[17] M.\nI. Jordan and T. M. Mitchell, “Machine learning:\nTrends, perspectives, and prospects,”\nScience, vol. 349, no. 6245, pp. 255–260, Jul. 2015, doi: 10.1126/science.aaa8415.\n\n\n[18] A.\nVaswani et al., “Attention is all you need,” in\nAdvances in neural information processing systems, I. Guyon, U.\nV. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R.\nGarnett, Eds., Curran Associates, Inc., 2017. Available: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n\n\n[19] C.\nWitharana et al., “An\nObject-Based Approach for\nMapping Tundra\nIce-Wedge Polygon\nTroughs from Very High\nSpatial Resolution Optical\nSatellite Imagery,” Remote\nSensing, vol. 13, no. 4, p. 558, Jan. 2021, doi: 10.3390/rs13040558.\n\n\n[20] C.\nWitharana et al., “Ice-wedge polygon detection in\nsatellite imagery from pan-Arctic regions,\nPermafrost Discovery Gateway,\n2001-2021,” 2023, doi: 10.18739/A2KW57K57.\n\n\n[21] L.\nEdwards and M. Veale, “Enslaving the Algorithm:\nFrom a ‘Right to an\nExplanation’ to a ‘Right to\nBetter Decisions’?” Social\nScience Research Network, Rochester, NY, 2018. doi: 10.2139/ssrn.3052831.\n\n\n[22] S.\nFink, “This High-Tech\nSolution to Disaster Response\nMay Be Too Good to\nBe True,” The New York Times,\nAug. 2019, Accessed: Oct. 19, 2024. [Online]. Available: https://www.nytimes.com/2019/08/09/us/emergency-response-disaster-technology.html\n\n\n[23] D.\nLeslie, “Understanding artificial intelligence ethics and safety:\nA guide for the responsible design and implementation of\nAI systems in the public sector,” Zenodo, Jun. 2019.\ndoi: 10.5281/zenodo.3240529.\n\n\n[24] S.\nLo Piano, “Ethical principles in machine learning and artificial\nintelligence: Cases from the field and possible ways forward,”\nHumanities and Social Sciences Communications, vol. 7, no. 1,\npp. 1–7, Jun. 2020, doi: 10.1057/s41599-020-0501-9.\n\n\n[25] A.\nMcGovern, I. Ebert-Uphoff, D. J. G. Ii, and A. Bostrom, “Why we\nneed to focus on developing ethical, responsible, and trustworthy\nartificial intelligence approaches for environmental science,”\nEnvironmental Data Science, vol. 1, p. e6, Jan. 2022, doi: 10.1017/eds.2022.5.\n\n\n[26] T.\nShepherd, “Indigenous rangers to use SpaceCows\nprogram to protect sacred sites and rock art from feral herds,”\nThe Guardian, Sep. 2021, Accessed: Oct. 19, 2024. [Online].\nAvailable: https://www.theguardian.com/australia-news/2021/sep/15/indigenous-rangers-to-use-spacecows-program-to-protect-sacred-sites-and-rock-art-from-feral-herds\n\n\n[27] CSIRO, “SpaceCows:\nUsing AI to tackle feral herds in the\nTop End.” Accessed: Oct. 19, 2024.\n[Online]. Available: https://www.csiro.au/en/news/All/News/2021/September/SpaceCows-Using-AI-to-tackle-feral-herds-in-the-Top-End\n\n\n[28] A.\nD. S. A. (ADSA), “The Data Science\nEthos - Operationalizing Ethics\nin Data Science,” The Data Science\nEthos. Accessed: Oct. 19, 2024. [Online]. Available: https://ethos.academicdatascience.org/\n\n\n[29] W.\nChen and A. Quan-Haase, “Big Data Ethics\nand Politics: Toward New\nUnderstandings,” Social Science Computer\nReview, vol. 38, no. 1, pp. 3–9, Feb. 2020, doi: 10.1177/0894439318810734.\n\n\n[30] “Excavating AI.”\nAccessed: Oct. 19, 2024. [Online]. Available: https://excavating.ai/\n\n\n[31] J.\nGray and A. Witt, “A feminist data ethics of care for machine\nlearning: The what, why, who and how,” First\nMonday, Dec. 2021, doi: 10.5210/fm.v26i12.11833.\n\n\n[32] “Checklist to Examine\nAI-readiness for Open\nEnvironmental Datasets,”\nfigshare. Jun. 2022. doi: 10.6084/m9.figshare.19983722.v1.\n\n\n[33] S.\nLong and T. Romanoff, “AI-Ready\nOpen Data.”\nAI-Ready Open Data\n Bipartisan Policy\nCenter, 2023. Accessed: Oct. 19, 2024. [Online]. Available:\nhttps://bipartisanpolicy.org/explainer/ai-ready-open-data/\n\n\n[34] O.\nBenjelloun et al., “Croissant Format\nSpecification,” Croissant site. 2024.\nAccessed: Oct. 20, 2024. [Online]. Available: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n\n\n[35] M.\nD. Mahecha et al., “Earth system data cubes unravel\nglobal multivariate dynamics,” Earth System Dynamics,\nvol. 11, no. 1, pp. 201–234, Feb. 2020, doi: 10.5194/esd-11-201-2020.\n\n\n[36] “ERA5 hourly data on single\nlevels from 1940 to present.” doi: https://doi.org/10.24381/cds.adbb2d47.\n\n\n[37] O.\nJ. Reichman, M. B. Jones, and M. P. Schildhauer, “Challenges and\nopportunities of open data in ecology.” Science (New York,\nN.Y.), vol. 331, no. 6018, pp. 703–5, Feb. 2011, doi: 10.1126/science.1197962.\n\n\n[38] I.\nNitze et al., “DARTS:\nMulti-year database of AI detected\nretrogressive thaw slumps (RTS) and active layer detachment\nslides (ALD) in hotspots of the circum-arctic permafrost\nregion - v1,” 2024, doi: 10.18739/A2RR1PP44.\n\n\n[39] A.\nD. of F. Game, D. of C. and Fisheries, and A.-Y.-K. Region,\n“Salmon age, sex, and length data from\nArctic-Yukon-Kuskokwim\nRegion of Alaska, 1960-2017,” 2018,\ndoi: 10.5063/SN07CZ.\n\n\n[40] M.\nD. Wilkinson et al., “The FAIR\nGuiding Principles for scientific data\nmanagement and stewardship,” Scientific Data, vol. 3, p.\n160018, Mar. 2016, doi: 10.1038/sdata.2016.18.\n\n\n[41] M.\nD. Wilkinson, S.-A. Sansone, E. Schultes, P. Doorn, L. O. Bonino da\nSilva Santos, and M. Dumontier, “A design framework and exemplar\nmetrics for FAIRness,” Scientific Data,\nvol. 5, p. 180118, Jun. 2018, doi: 10.1038/sdata.2018.118.\n\n\n[42] G.\nPeng et al., “Harmonizing quality measures of\nFAIRness assessment towards machine-actionable quality\ninformation,” International Journal of Digital Earth,\nvol. 17, no. 1, p. 2390431, Dec. 2024, doi: 10.1080/17538947.2024.2390431.\n\n\n[43] M.\nJones et al., “MetaDIG:\nEngaging Scientists in the\nImprovement of Metadata and\nData,” Figshare, 2016, doi: 10.6084/m9.figshare.4055808.v1.\n\n\n[44] M.\nJones, P. Slaughter, and T. Habermann, “Quantifying\nFAIR: Metadata improvement and guidance in the\nDataONE repository network.” 2019. doi: https://doi.org/10.5063/f1kp80gx.\n\n\n[45] S.\nS. Chong, M. Schildhauer, M. O’Brien, B. Mecum, and M. B. Jones,\n“Enhancing the FAIRness of Arctic\nResearch Data Through\nSemantic Annotation,” Data Science\nJournal, vol. 23, no. 1, Jan. 2024, doi: 10.5334/dsj-2024-002.\n\n\n[46] R.\nBommasani et al., “On the opportunities and risks of\nfoundation models,” ArXiv, 2021, Available: https://crfm.stanford.edu/assets/report.pdf\n\n\n[47] A.\nRadford et al., “Learning transferable visual models from\nnatural language supervision,” CoRR, vol.\nabs/2103.00020, 2021, Available: https://arxiv.org/abs/2103.00020\n\n\n[48] J.\nDevlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of deep bidirectional transformers for language\nunderstanding,” CoRR, vol. abs/1810.04805, 2018,\nAvailable: http://arxiv.org/abs/1810.04805\n\n\n[49] OpenAI et al., “GPT-4 technical\nreport.” 2024. Available: https://arxiv.org/abs/2303.08774\n\n\n[50] A.\nDubey et al., “The llama 3 herd of models.” 2024.\nAvailable: https://arxiv.org/abs/2407.21783\n\n\n[51] A.\nKirillov et al., “Segment anything.” 2023.\nAvailable: https://arxiv.org/abs/2304.02643\n\n\n[52] Z.\nLiu et al., “Swin transformer: Hierarchical vision\ntransformer using shifted windows.” 2021. Available: https://arxiv.org/abs/2103.14030\n\n\n[53] A.\nRamesh et al., “Zero-shot text-to-image\ngeneration,” CoRR, vol. abs/2102.12092, 2021, Available:\nhttps://arxiv.org/abs/2102.12092\n\n\n[54] Y.\nLiu et al., “Sora: A review on background, technology,\nlimitations, and opportunities of large vision models.” 2024.\nAvailable: https://arxiv.org/abs/2402.17177\n\n\n[55] G.\nTeam et al., “Gemini: A family of highly capable\nmultimodal models.” 2024. Available: https://arxiv.org/abs/2312.11805\n\n\n[56] A.\nVaswani et al., “Attention is all you need,”\nCoRR, vol. abs/1706.03762, 2017, Available: http://arxiv.org/abs/1706.03762\n\n\n[57] L.\nWeng, “Attention? attention!”\nlilianweng.github.io, 2018, Available: https://lilianweng.github.io/posts/2018-06-24-attention/\n\n\n[58] J.\nCheng, L. Dong, and M. Lapata, “Long short-term memory-networks\nfor machine reading,” CoRR, vol. abs/1601.06733, 2016,\nAvailable: http://arxiv.org/abs/1601.06733\n\n\n[59] X.\nAmatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and M.\nKazi, “Transformer models: An introduction and catalog.”\n2024. Available: https://arxiv.org/abs/2302.07730\n\n\n[60] I.\nJ. Goodfellow et al., “Generative adversarial\nnetworks.” 2014. Available: https://arxiv.org/abs/1406.2661\n\n\n[61] T.\nKarras, S. Laine, and T. Aila, “A style-based generator\narchitecture for generative adversarial networks,” CoRR,\nvol. abs/1812.04948, 2018, Available: http://arxiv.org/abs/1812.04948\n\n\n[62] A.\nBrock, J. Donahue, and K. Simonyan, “Large scale GAN\ntraining for high fidelity natural image synthesis,”\nCoRR, vol. abs/1809.11096, 2018, Available: http://arxiv.org/abs/1809.11096\n\n\n[63] L.\nWeng, “What are diffusion models?”\nlilianweng.github.io, Jul. 2021, Available: https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\n\n\n[64] J.\nHo, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic\nmodels,” CoRR, vol. abs/2006.11239, 2020, Available: https://arxiv.org/abs/2006.11239\n\n\n[65] R.\nRombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n“High-resolution image synthesis with latent diffusion\nmodels,” CoRR, vol. abs/2112.10752, 2021, Available: https://arxiv.org/abs/2112.10752\n\n\n[66] N.\nRavi et al., “SAM 2: Segment anything in images and\nvideos,” arXiv preprint arXiv:2408.00714, 2024,\nAvailable: https://arxiv.org/abs/2408.00714\n\n\n[67] P.\nS. H. Lewis et al., “Retrieval-augmented generation for\nknowledge-intensive NLP tasks,” CoRR, vol.\nabs/2005.11401, 2020, Available: https://arxiv.org/abs/2005.11401\n\n\n[68] L.\nWeng, “Extrinsic hallucinations in LLMs.”\nlilianweng.github.io, Jul. 2024, Available: https://lilianweng.github.io/posts/2024-07-07-hallucination/\n\n\n[69] L.\nWeng, “Prompt engineering,” lilianweng.github.io,\nMar. 2023, Available: https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n\n\n[70] P.\nNorvig and S. J. Russell, Artificial intelligence: A modern\napproach, 3rd ed. Pearson, 2004. Available: https://books.google.com/books/about/Artificial_Intelligence.html?id=8jZBksh-bUMC\n\n\n[71] D.\nO. Hebb, The organization of behavior: A neuropsychological\ntheory. New York: Wiley, 1949. Available: https://en.wikipedia.org/wiki/The_Organization_of_Behavior\n\n\n[72] F.\nRosenblatt, “The perceptron: A probabilistic model for information\nstorage and organization in the brain,” Psychological\nReview, vol. 65, no. 6, pp. 386–408, 1958, doi: 10.1037/H0042519.\n\n\n[73] M.\nBennett, A brief history of intelligence: Evolution, AI, and the\nfive breakthroughs that made our brains, Hardcover. Harper,\n2023.\n\n\n[74] Inc. PitchBook Data, “Artificial\nintelligence & machine learning report, Q2 2024,” PitchBook,\n2024. Available: https://pitchbook.com/news/reports/q2-2024-artificial-intelligence-machine-learning-report\n\n\n[75] D.\nKawahara, S. Ozeki, and I. Mizuuchi, “A curiosity algorithm for\nrobots based on the free energy principle,” pp. 53–59, 2022, doi:\n10.1109/SII52469.2022.9708819.\n\n\n[76] T.\nWang, F. Wang, Z. Xie, and F. Qin, “Curiosity model policy\noptimization for robotic manipulator tracking control with input\nsaturation in uncertain environment,” Frontiers in\nNeurorobotics, vol. 18, 2024, doi: 10.3389/fnbot.2024.1376215.",
    "crumbs": [
      "References"
    ]
  }
]