# Hands-On Lab: Foundation Models

## Overview
The hands-on lab on foundation models will focus on building and applying foundation models for some example use cases. The main goal of this 1-hour session will be to get more familiarized with foundation models and in interacting with them.

## Outline
1. Image Segmentation using Segment Anything Model (SAM)
2. Chatbot using LLMs + RAG

## References
1. [Segment Anything](https://segment-anything.com/)
2. [Segment Anything Notebook](https://github.com/facebookresearch/segment-anything/tree/main/notebooks)


## SAM




## RAG Hands-On

Will be using [Langchain framework](https://www.langchain.com/)

Suggested code references:
- Langchain RAG from scratch [github](https://github.com/langchain-ai/rag-from-scratch/tree/main)
- Langchain [RAG tutorial](https://python.langchain.com/docs/tutorials/rag/)

Session hands-on code in [github.com/ncsa/cyber2a-workshop](github.com/ncsa/cyber2a-workshop)

Session technical details in course book : [cyber2a.github.io/cyber2a-course/sections/foundation-models.html](https://cyber2a.github.io/cyber2a-course/sections/foundation-models.html)

## Recap 

| Without RAG    | With RAG  |
| -------- | ------- |
| No ability to access a specific knowledge/domain  | Point to a knowledge base   |
| No sources | Sources cited in LLM response   |
| Hallucinations   | LLM response is grounded by relevant information from knowledge base   |
| Out-of-date information | Update the knowledge base with new information  |

![RAG approach](../images/foundation-models/ragpromptframework.png)

RAG system can be thought of as combining two techniques:
1. Retrieval
2. Generation
where the Generation is augmented / improved by the retrieval step.

The retrieval is typically done by a specialized database and the generation part is typically done by an LLM.

## RAG - *Retrieval*-Augmented Generation

We will focus on the "retrieval" part of RAG for this section.

### Knowledge database

In the age of burgeoning data complexity and high-dimensional information, traditional databases often fall short when it comes to efficiently handling and extracting meaning from intricate datasets. Enter vector databases, a technological innovation that has emerged as a solution to the challenges posed by the ever-expanding landscape of data. (Source: beginner's [blog post](https://medium.com/data-and-beyond/vector-databases-a-beginners-guide-b050cbbe9ca0) on vector DB)

#### Vector database

Vector databases have gained significant importance in various fields due to their unique ability to efficiently store, index, and search high-dimensional data points, often referred to as vectors. These databases are designed to handle data where each entry is represented as a vector in a multi-dimensional space. The vectors can represent a wide range of information, such as numerical features, embeddings from text or images, and even complex data like molecular structures.

At the heart of vector databases lies the concept of vector embeddings. These are mathematical representations of data points in a high-dimensional space. In the context of natural language processing:

1. Word Embeddings: Individual words are represented as real-valued vectors in a multi-dimensional space.
2. Semantic Capture: These embeddings capture the semantic meaning and relationships of the text.
3. Similarity Principle: Words with similar meanings tend to have similar vector representations.

::: {.column-margin}
![Fig : Vectors](../images/foundation-models/vectorDB-vectors.png)
:::

##### How vector databases work
Let’s start with a simple example of dealing with an LLM such as ChatGPT. The model has large volumes of data with a lot of content, and they provide us with the ChatGPT application.

![Fig : VectorDB within RAG. Source: KDnuggets [blog post](https://www.kdnuggets.com/2023/06/vector-databases-important-llms.html)](../images/foundation-models/vectorDB.png)

So let’s go through the steps.

1. As the user, you will input your query into the application.
2. Your query is then inserted into the embedding model which creates vector embeddings based on the content we want to index. 
3. The vector embedding then moves into the vector database, regarding the content that the embedding was made from. 
4. The vector database produces an output and sends it back to the user as a query result. 

When the user continues to make queries, it will go through the same embedding model to create embeddings to query that database for similar vector embeddings. The similarities between the vector embeddings are based on the original content, in which the embedding was created. 

Now lets see how it works in the vector database.

![Fig : VectorDB pipeline. Source: pinecone [blog post](https://www.pinecone.io/learn/vector-database/)](../images/foundation-models/vectordb-working.png)

The three main stages that a vector database query goes through are:

1. Indexing

As explained in the example above, once the vector embedding moves into the vector database, it then uses a variety of algorithms to map the vector embedding to data structures for faster searching. 

2. Querying

Once it has gone through its search, the vector database compares the queried vector to indexed vectors, applying the similarity metric to find the nearest neighbor. 

3. Post Processing 

Depending on the vector database you use, the vector database will post-process the final nearest neighbor to produce a final output to the query. As well as possibly re-ranking the nearest neighbors for future reference. 



## RAG - Retrieval-Augmented *Generation*

We will focus on the "generation" part of RAG for this section. Here most of the heavy-lifting is done by the LLMs. Let's see how best to communicate/prompt these LLM models for RAG.

### Prompting
Prompting is a crucial technique in effectively communicating with Large Language Models (LLMs) to achieve desired outcomes without modifying the underlying model. As LLMs become more sophisticated, the art of crafting effective prompts has emerged as a key skill in natural language processing and AI applications. Checkout LilianWeng blog post [@weng2023prompt], medium [blog post](https://medium.com/thedeephub/llm-prompt-engineering-for-beginners-what-it-is-and-how-to-get-started-0c1b483d5d4f#:~:text=In%20essence%2C%20a%20prompt%20is,you%20want%20it%20to%20do) on prompt engineering.

Prompting is often an iterative process. It typically requires multiple trial-and-error attempts to achieve the desired effect. Each iteration can provide insights into how the model interprets and responds to different input structures.

#### Key Elements of Effective Prompting

1. Defining a Persona

Assigning the LLM a specific role or behavior can significantly influence its responses. By giving it a defined persona, the model will attempt to respond in a manner that aligns with that role. This can improve the quality and relevance of its answers.

Example:
“You are a helpful research assistant”

This prompt frames the model's responses to be in line with the behavior expected of a research assistant, such as providing accurate information and being resourceful.

2. Setting Guardrails

Guardrails provide boundaries or conditions within which the model should operate. This is particularly useful to avoid misleading or incorrect information. You can ask the model to refrain from answering if it's unsure of the response.

Example:
“If you don’t know the final answer, just say ‘I don’t know’.”

This instructs the LLM to admit uncertainty instead of generating a potentially incorrect answer, thereby increasing reliability.

3. Providing Clear Instructions

Giving the LLM specific actions to perform before generating responses ensures that it processes the necessary information correctly. This is important when dealing with tasks like reviewing files or using external data.

Example:
“Read the data file before answering any questions.”

This directs the LLM to review relevant materials, improving the quality of the subsequent answers.

4. Specifying Response Formats

You can enhance the usefulness of responses by specifying the desired output format. By doing this, you ensure the model delivers information in a form that aligns with your needs.

Example:
“Respond using markdowns.”

This ensures the LLM outputs text in Markdown format, which can be helpful for structured documents or technical writing.

## RAG System

Let's bring it all together

![Fig : RAG system. Image source : [blog.demir](https://blog.demir.io/hands-on-with-rag-step-by-step-guide-to-integrating-retrieval-augmented-generation-in-llms-ac3cb075ab6f) ](../images/foundation-models/RAGsystem.png)

1. User Submits Query: The user inputs a query into the system. This is the initial step where the user’s request is captured.
2. RAG System Query Relevant Documents: The RAG system processes the user’s query and searches for relevant documents.
3. Document Database Returns Documents: The document database receives the request for relevant documents and returns the documents it finds to the RAG system.
4. Combine The Query & The Documents: The RAG system takes the documents provided by the document database and combines them with the original query.
5. LLM Returns Answer: The combined query and documents are sent to a Large Language Model (LLM), which generates an answer based on the information provided.
6. RAG System Return Answer to User: Finally, the answer generated by the LLM is sent back through the RAG system.