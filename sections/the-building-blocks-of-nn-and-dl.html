<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; The Building Blocks of Neural Networks and Deep Learning – Cyber2A: AI for Arctic Research</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../sections/intro-to-pytorch.html" rel="next">
<link href="../sections/hands-on-lab-data-annotation.html" rel="prev">
<link href="../images/index/arcticlogo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-de070a7b0ab54f8780927367ac907214.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-1bc3b17e7a2c2828f435aa750fb76336.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../sections/the-building-blocks-of-nn-and-dl.html"><b>Day 2: AI Fundamentals and Techniques</b></a></li><li class="breadcrumb-item"><a href="../sections/the-building-blocks-of-nn-and-dl.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The Building Blocks of Neural Networks and Deep Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Cyber2A: AI for Arctic Research</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/cyber2a/cyber2a-course/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 1: Introduction to AI and Arctic Science</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/breaking-the-ice-with-ai-in-arctic-science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Breaking the Ice with AI in Arctic Science</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-for-everyone.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">AI for Everyone</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-ready-data-in-arctic-research.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">AI-Ready Data in Arctic Research</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/data-annotation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Data Annotation: The Foundation of Deep Learning Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-data-annotation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Hands-On Lab: Data Annotation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 2: AI Fundamentals and Techniques</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/the-building-blocks-of-nn-and-dl.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The Building Blocks of Neural Networks and Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/intro-to-pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to PyTorch: Core Functionalities and Advantages</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Hands-On Lab: PyTorch</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/permafrost-discovery-gateway.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Permafrost Discovery Gateway</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-ethics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">AI Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 3: Advanced AI Workflows and Models</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/guest-lecture-yili-arts-dataset.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/exploring-advanced-neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Exploring Advanced Neural Networks: Instance Segmentation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/intro-to-dl-libraries-for-image-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Introduction to Deep Learning Libraries for Image Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 4: Workflows and Foundation Models</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-workflows-and-mlops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">AI Workflows and MLOps: From Development to Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-ai-workflows.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Hands-On Lab: AI Workflows</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/foundation-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Foundation Models: The Cornerstones of Modern AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-foundation-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Hands-On Lab: Foundation Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/reproducibility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Reproducibility</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 5: AI Frontiers</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/the-fun-and-frontiers-of-ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">The Fun and Frontiers of AI: Innovation, Imagination, Interaction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">6.1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#what-does-this-tool-deep-learning-do" id="toc-what-does-this-tool-deep-learning-do" class="nav-link" data-scroll-target="#what-does-this-tool-deep-learning-do"><span class="header-section-number">6.1.1</span> What does this tool (deep learning) do?</a></li>
  <li><a href="#key-questions-and-building-blocks-of-deep-learning" id="toc-key-questions-and-building-blocks-of-deep-learning" class="nav-link" data-scroll-target="#key-questions-and-building-blocks-of-deep-learning"><span class="header-section-number">6.1.2</span> Key questions and building blocks of deep learning</a></li>
  </ul></li>
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data"><span class="header-section-number">6.2</span> Data</a>
  <ul class="collapse">
  <li><a href="#inputs" id="toc-inputs" class="nav-link" data-scroll-target="#inputs"><span class="header-section-number">6.2.1</span> Inputs</a></li>
  <li><a href="#outputs" id="toc-outputs" class="nav-link" data-scroll-target="#outputs"><span class="header-section-number">6.2.2</span> Outputs</a></li>
  <li><a href="#quantity-and-quality" id="toc-quantity-and-quality" class="nav-link" data-scroll-target="#quantity-and-quality"><span class="header-section-number">6.2.3</span> Quantity and quality</a></li>
  </ul></li>
  <li><a href="#models" id="toc-models" class="nav-link" data-scroll-target="#models"><span class="header-section-number">6.3</span> Models</a>
  <ul class="collapse">
  <li><a href="#layers-the-building-blocks-of-models" id="toc-layers-the-building-blocks-of-models" class="nav-link" data-scroll-target="#layers-the-building-blocks-of-models"><span class="header-section-number">6.3.1</span> Layers: The Building Blocks of Models</a></li>
  <li><a href="#common-model-architectures" id="toc-common-model-architectures" class="nav-link" data-scroll-target="#common-model-architectures"><span class="header-section-number">6.3.2</span> Common Model Architectures</a></li>
  <li><a href="#pre-trained-models-and-transfer-learning" id="toc-pre-trained-models-and-transfer-learning" class="nav-link" data-scroll-target="#pre-trained-models-and-transfer-learning"><span class="header-section-number">6.3.3</span> Pre-trained models and transfer learning</a></li>
  <li><a href="#model-customization-for-transfer-learning" id="toc-model-customization-for-transfer-learning" class="nav-link" data-scroll-target="#model-customization-for-transfer-learning"><span class="header-section-number">6.3.4</span> Model Customization (for Transfer Learning)</a></li>
  </ul></li>
  <li><a href="#loss-functions" id="toc-loss-functions" class="nav-link" data-scroll-target="#loss-functions"><span class="header-section-number">6.4</span> Loss Functions</a>
  <ul class="collapse">
  <li><a href="#common-loss-functions" id="toc-common-loss-functions" class="nav-link" data-scroll-target="#common-loss-functions"><span class="header-section-number">6.4.1</span> Common loss functions</a></li>
  <li><a href="#beyond-the-basics-more-tools-in-the-toolbox" id="toc-beyond-the-basics-more-tools-in-the-toolbox" class="nav-link" data-scroll-target="#beyond-the-basics-more-tools-in-the-toolbox"><span class="header-section-number">6.4.2</span> Beyond the Basics: More Tools in the Toolbox</a></li>
  <li><a href="#choosing-your-loss-function-practical-tips" id="toc-choosing-your-loss-function-practical-tips" class="nav-link" data-scroll-target="#choosing-your-loss-function-practical-tips"><span class="header-section-number">6.4.3</span> Choosing Your Loss Function: Practical Tips</a></li>
  <li><a href="#watching-the-score-training-loss-and-validation-loss" id="toc-watching-the-score-training-loss-and-validation-loss" class="nav-link" data-scroll-target="#watching-the-score-training-loss-and-validation-loss"><span class="header-section-number">6.4.4</span> Watching the Score: Training Loss and Validation Loss</a></li>
  <li><a href="#the-final-piece-minimizing-loss-is-the-name-of-the-game" id="toc-the-final-piece-minimizing-loss-is-the-name-of-the-game" class="nav-link" data-scroll-target="#the-final-piece-minimizing-loss-is-the-name-of-the-game"><span class="header-section-number">6.4.5</span> The Final Piece: Minimizing Loss is the Name of the Game</a></li>
  </ul></li>
  <li><a href="#optimization-algorithms" id="toc-optimization-algorithms" class="nav-link" data-scroll-target="#optimization-algorithms"><span class="header-section-number">6.5</span> Optimization Algorithms</a>
  <ul class="collapse">
  <li><a href="#the-core-idea-gradient-descent" id="toc-the-core-idea-gradient-descent" class="nav-link" data-scroll-target="#the-core-idea-gradient-descent"><span class="header-section-number">6.5.1</span> The Core Idea: Gradient Descent</a></li>
  <li><a href="#how-gradients-are-calculated-backpropagation-the-short-story" id="toc-how-gradients-are-calculated-backpropagation-the-short-story" class="nav-link" data-scroll-target="#how-gradients-are-calculated-backpropagation-the-short-story"><span class="header-section-number">6.5.2</span> How Gradients are Calculated: Backpropagation (The Short Story)</a></li>
  <li><a href="#flavors-of-gradient-descent-handling-the-data" id="toc-flavors-of-gradient-descent-handling-the-data" class="nav-link" data-scroll-target="#flavors-of-gradient-descent-handling-the-data"><span class="header-section-number">6.5.3</span> Flavors of Gradient Descent: Handling the Data</a></li>
  <li><a href="#challenges-on-the-way-down" id="toc-challenges-on-the-way-down" class="nav-link" data-scroll-target="#challenges-on-the-way-down"><span class="header-section-number">6.5.4</span> Challenges on the Way Down</a></li>
  <li><a href="#getting-smarter-learning-rate-scheduling" id="toc-getting-smarter-learning-rate-scheduling" class="nav-link" data-scroll-target="#getting-smarter-learning-rate-scheduling"><span class="header-section-number">6.5.5</span> Getting Smarter: Learning Rate Scheduling</a></li>
  <li><a href="#getting-even-smarter-advanced-optimization-algorithms" id="toc-getting-even-smarter-advanced-optimization-algorithms" class="nav-link" data-scroll-target="#getting-even-smarter-advanced-optimization-algorithms"><span class="header-section-number">6.5.6</span> Getting Even Smarter: Advanced Optimization Algorithms</a></li>
  <li><a href="#choosing-your-optimizer-practical-tips" id="toc-choosing-your-optimizer-practical-tips" class="nav-link" data-scroll-target="#choosing-your-optimizer-practical-tips"><span class="header-section-number">6.5.7</span> Choosing Your Optimizer: Practical Tips</a></li>
  </ul></li>
  <li><a href="#training-and-inference" id="toc-training-and-inference" class="nav-link" data-scroll-target="#training-and-inference"><span class="header-section-number">6.6</span> Training and Inference</a>
  <ul class="collapse">
  <li><a href="#training-teaching-the-model" id="toc-training-teaching-the-model" class="nav-link" data-scroll-target="#training-teaching-the-model"><span class="header-section-number">6.6.1</span> Training: Teaching the Model</a></li>
  <li><a href="#inference-using-the-trained-model" id="toc-inference-using-the-trained-model" class="nav-link" data-scroll-target="#inference-using-the-trained-model"><span class="header-section-number">6.6.2</span> Inference: Using the Trained Model</a></li>
  <li><a href="#training-vs.-inference" id="toc-training-vs.-inference" class="nav-link" data-scroll-target="#training-vs.-inference"><span class="header-section-number">6.6.3</span> Training vs.&nbsp;Inference</a></li>
  </ul></li>
  <li><a href="#conclusion-putting-the-blocks-together" id="toc-conclusion-putting-the-blocks-together" class="nav-link" data-scroll-target="#conclusion-putting-the-blocks-together"><span class="header-section-number">6.7</span> Conclusion: Putting the Blocks Together</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../sections/the-building-blocks-of-nn-and-dl.html"><b>Day 2: AI Fundamentals and Techniques</b></a></li><li class="breadcrumb-item"><a href="../sections/the-building-blocks-of-nn-and-dl.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The Building Blocks of Neural Networks and Deep Learning</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The Building Blocks of Neural Networks and Deep Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>Welcome to your first step into the exciting world of deep learning! This lesson will guide you through the fundamentals in a simple and friendly way. Think of this as the start of your journey into a fascinating area of artificial intelligence.</p>
<p>In this session, we’ll explore:</p>
<ul>
<li><strong>Data</strong> and <strong>Models</strong>: Understand why data is crucial and how models act like ‘brains’ to interpret information. &nbsp;</li>
<li><strong>Loss Functions</strong> and <strong>Optimization Algorithms</strong>: Discover how computers learn from their mistakes using these tools to improve over time. &nbsp;</li>
</ul>
<p>By the end of this lesson, you’ll grasp these key concepts. These building blocks will help you understand how AI works and how you can apply these powerful tools.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Good luck learning!
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div style="display: flex; justify-content: center; align-items: center; width: 100%;">
    <div class="tenor-gif-embed" data-postid="1471332848232877888" data-share-method="host" data-aspect-ratio="1" data-width="50%">
        <a href="https://tenor.com/view/balance-wheelie-viralhog-cadillac-lightning-mcqueen-drag-racing-gif-1471332848232877888">Balance Wheelie Viralhog GIF</a>
        from <a href="https://tenor.com/search/balance+wheelie-gifs">Balance Wheelie GIFs</a>
    </div>
</div>
<script type="text/javascript" async="" src="https://tenor.com/embed.js"></script>
</div>
</div>
</div>
</section>
<section id="introduction" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">6.1</span> Introduction</h2>
<p>Think of <strong>deep learning</strong> as a powerful tool, much like a car helps you reach a destination. Just as you don’t need to know every mechanical detail to drive, you can start with deep learning by understanding its essential components. &nbsp;</p>
<section id="what-does-this-tool-deep-learning-do" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="what-does-this-tool-deep-learning-do"><span class="header-section-number">6.1.1</span> What does this tool (deep learning) do?</h3>
<p>At its core, deep learning is about:</p>
<blockquote class="blockquote">
<p><em>Finding a function automatically that maps given inputs to desired outputs</em> <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
</blockquote>
<p>In simpler terms, deep learning enables computers to learn patterns from data to make predictions or decisions, without needing explicit step-by-step rules. It’s like teaching a child to recognize dogs by showing them many pictures, rather than giving a long list of rules defining a dog.</p>
<p>Here are a few examples:</p>
<table class="table-striped caption-top table">
<thead>
<tr class="header">
<th>Inputs</th>
<th>Outputs</th>
<th>Functions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A sentence or prompt</td>
<td>Text completion</td>
<td>LLM (e.g., ChatGPT <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>)</td>
</tr>
<tr class="even">
<td>A caption/description</td>
<td>An image</td>
<td>DALL-E <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></td>
</tr>
<tr class="odd">
<td>Historical weather data</td>
<td>Weather forecasting</td>
<td>GraphCast <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Can you think of a real-world problem you’d like to solve using deep learning? &nbsp;</li>
<li>What kind of data would you need (inputs)? &nbsp;</li>
<li>What results would you want the system to produce (outputs)?</li>
</ul>
</div>
</div>
</section>
<section id="key-questions-and-building-blocks-of-deep-learning" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="key-questions-and-building-blocks-of-deep-learning"><span class="header-section-number">6.1.2</span> Key questions and building blocks of deep learning</h3>
<p>To find the right function using deep learning, we break the process down into four key questions, which correspond to the fundamental building blocks:</p>
<ol type="1">
<li><strong>What are the inputs and outputs?</strong> This relates to the <strong>Data</strong> we use. &nbsp;</li>
<li><strong>What kind of functions could we use?</strong> This involves choosing <strong>Models</strong> (the set of possible functions). &nbsp;</li>
<li><strong>How do we know if a function is good or bad?</strong> This is where <strong>Loss Functions</strong> come in to evaluate performance. &nbsp;</li>
<li><strong>How do we find the best function from our chosen set?</strong> This is achieved through <strong>Optimization Algorithms</strong>.</li>
</ol>
<p>These four elements form the core of deep learning. Two related concepts, <strong>Training</strong> (the process of finding the best function) and <strong>Inference</strong> (using the trained function), tie these blocks together for practical use. &nbsp;</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/dl-blocks.png" class="img-fluid figure-img"></p>
<figcaption>Deep learning building blocks</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember, these building blocks are interconnected. Choices made for one block often affect the others, involving trade-offs. &nbsp;</p>
</div>
</div>
<p><strong>Let’s begin exploring these building blocks.</strong></p>
</section>
</section>
<section id="data" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="data"><span class="header-section-number">6.2</span> Data</h2>
<p><em>Data is the starting point of deep learning. It defines the <strong>inputs</strong> and <strong>outputs</strong> of the function we aim to learn.</em></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>For specific examples related to Arctic research data, please refer to the <a href="../sections/ai-ready-data-in-arctic-research.html">AI-ready Data</a> section.</p>
</div>
</div>
<section id="inputs" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="inputs"><span class="header-section-number">6.2.1</span> Inputs</h3>
<p>Building any deep learning application starts with defining and preparing your input data. Data is the foundation upon which models learn patterns. &nbsp;</p>
<p>Before you start, consider:</p>
<ul>
<li><strong>Data Type</strong>: What kind of data is it (images, text, audio, numerical measurements)? &nbsp;</li>
<li><strong>Data Volume &amp; Representativeness</strong>: How much data do you have? Does it accurately reflect the real-world scenarios your model will encounter? &nbsp;</li>
<li><strong>Task Requirements</strong>: Are there specific needs like granularity (level of detail), temporal consistency (order matters), or spatial coverage? &nbsp;</li>
<li><strong>Data Quality</strong>: Is the data clean? Look out for missing values, outliers (unusual data points), or noise</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>How well do you understand the origin and quality of your data? &nbsp;</li>
<li>Even well-known datasets like ImageNet can have labeling issues or biases. Always critically examine your data source <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</li>
</ul>
</div>
</div>
<p>Key steps for preparing input data:</p>
<section id="data-collection" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="data-collection"><strong>1. Data Collection</strong></h4>
<ul>
<li>Find and gather data from reliable sources. Public datasets or domain-specific repositories can save time <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</li>
<li>Aim for diverse data to cover various scenarios.</li>
<li>Check data quality early, looking for inconsistencies or noise.</li>
</ul>
</section>
<section id="data-preparation" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="data-preparation"><strong>2. Data Preparation</strong></h4>
<ul>
<li>Handle missing or outlier data points appropriately (e.g., fill them in using statistical methods or with your domain knowledge about the data) <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>.</li>
<li>Standardize or normalize numerical data. This puts features on a similar scale, which prevents some features from dominating the learning process and often helps models train faster <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>.</li>
</ul>
</section>
<section id="splitting-data" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="splitting-data"><strong>3. Splitting Data</strong></h4>
<ul>
<li><p>Divide your data into distinct sets:</p>
<ul>
<li><strong>Training Set</strong>: Used to teach the model.</li>
<li><strong>Validation Set</strong>: Used to tune model settings (hyperparameters) and check performance during training.</li>
<li><strong>Testing Set</strong>: Used for a final, unbiased evaluation of the model’s performance after training.</li>
</ul></li>
<li><p>Ensure each set represents the overall data distribution. Avoid “data leakage” where information from the validation or test set accidentally influences the training process. &nbsp;</p></li>
<li><p>Consider the nature of your data when splitting <a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>:</p>
<ul>
<li><strong>Imbalanced Data</strong>: If some categories are rare, use stratified sampling to maintain the correct proportion of categories in each split <a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>.</li>
<li><strong>Temporal Data (Time Series)</strong>: Split based on time (e.g., train on past data, test on future data). Random shuffling usually isn’t appropriate here.</li>
<li><strong>Spatial Data</strong>: Split based on geographic areas to avoid testing on areas very close to training data <a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>.</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>What could go wrong if information from the test set ‘leaks’ into the training process?</p>
<p><em>Hint: Over-optimistic performance estimate, model won’t generalize well.</em></p>
</div>
</div>
</section>
<section id="data-augmentation-optional-but-often-helpful" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="data-augmentation-optional-but-often-helpful"><strong>4. Data Augmentation (Optional but often helpful)</strong></h4>
<ul>
<li>Artificially increase the size and diversity of your training data by applying relevant transformations. &nbsp;</li>
<li>Examples: For images, you might rotate, flip, or change brightness. For text, you might replace words with synonyms <a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>. &nbsp;</li>
<li>Use transformations that make sense for your data type and problem. Don’t create unrealistic data. &nbsp;</li>
<li>Be mindful not to introduce transformations that fundamentally change the meaning or label of the data <a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>. &nbsp;</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>An interesting paper on data augmentation showing simple copy-paste can be a strong augmentation technique even if the output is not realistic <a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/data-augmentation.png" class="img-fluid figure-img"></p>
<figcaption>Source: Ghiasi et al., 2021</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Think about recognizing street signs in images. What’s one type of image augmentation that might be helpful? What’s one that might actually be harmful?</p>
<ul>
<li>Helpful: slight rotation, brightness change.</li>
<li>Harmful: flipping horizontally might change meaning, extreme color shifts.</li>
</ul>
</div>
</div>
</section>
</section>
<section id="outputs" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="outputs"><span class="header-section-number">6.2.2</span> Outputs</h3>
<p>Defining the desired outputs is as critical as preparing the inputs. The output format dictates what the model predicts and must align with your project goals. &nbsp;</p>
<p>Consider:</p>
<ul>
<li><p><strong>Output Type</strong>: What kind of prediction do you need?</p>
<ul>
<li><strong>Classification</strong>: Assigning inputs to predefined categories (e.g., “dog” vs.&nbsp;“cat”, “spam” vs.&nbsp;“not spam”). &nbsp;</li>
<li><strong>Regression</strong>: Predicting a continuous numerical value (e.g., sea ice concentration percentage, temperature). &nbsp;</li>
<li><strong>Structured Output</strong>: Predicting complex outputs like bounding boxes around objects in an image, or generating a sequence of text. &nbsp;</li>
</ul></li>
<li><p><strong>Output Format</strong>: How should the prediction be represented? (e.g., a probability distribution over classes, a single number, coordinates). &nbsp;</p></li>
<li><p><strong>Output Constraints</strong>: Are there specific requirements, like values needing to be within a certain range (e.g., probability between 0 and 1)?</p></li>
</ul>
<p>Key steps for preparing output data:</p>
<section id="identify-output-type" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="identify-output-type"><strong>1. Identify Output Type</strong></h4>
<ul>
<li>Choose the type that matches your problem: classification, regression, etc.. &nbsp;</li>
</ul>
</section>
<section id="format-outputs" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="format-outputs"><strong>2. Format Outputs</strong></h4>
<ul>
<li>Select a format compatible with your chosen model and loss function.</li>
<li><em>One-hot encoding</em> <a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> is common for classification tasks with multiple categories. (e.g., [0, 1, 0] for the second category out of three).</li>
<li>Normalize continuous outputs (regression) if necessary, similar to input normalization.</li>
<li>For complex structured outputs, reparameterizing the output format can make the model’s job easier. In other words: “Don’t just predict what you want — predict something easier that leads to what you want.”</li>
</ul>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced reading for output reparameterization
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In deep learning, how you formulate the output can affect how well the model trains and performs. For example, if you predict the exact coordinates of a bounding box, what happens if we input an image that is much larger than what the model has seen during training?</p>
<p>Even if two output formulations are mathematically equivalent, one might lead to:</p>
<ul>
<li>Better gradient flow</li>
<li>Lower loss variance</li>
<li>Smoother optimization landscape</li>
<li>Easier convergence</li>
</ul>
<p>Some examples:</p>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 25%">
<col style="width: 41%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Naive Approach</th>
<th>Smarter Reparameterization</th>
<th>Why it’s better</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Object detection</td>
<td>Predict absolute coordinates of bounding boxes</td>
<td>Predict offsets from anchor boxes</td>
<td>Easier to learn, more stable</td>
</tr>
<tr class="even">
<td>Language modeling</td>
<td>Predict next word directly</td>
<td>Use token embeddings</td>
<td>Captures context better</td>
</tr>
<tr class="odd">
<td>Depth estimation</td>
<td>Predict raw depth</td>
<td>Predict inverse depth</td>
<td>Handles wide range of depths better</td>
</tr>
<tr class="even">
<td>Keypoint detection</td>
<td>Predict coordinate directly</td>
<td>Predict heatmaps</td>
<td>Converts regression to classification-like problem, easier for CNNs</td>
</tr>
</tbody>
</table>
<p>Read more about this topic in:</p>
<ul>
<li><p><a href="https://www.jmlr.org/papers/volume15/delgado14a/delgado14a.pdf">Fernández-Delgado et al.&nbsp;(2014)</a> discusses how <strong>reformulating outputs using structured prediction</strong> can help deep models generalize better.</p></li>
<li><p><a href="https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">Girshick et al.&nbsp;(2014)</a> introduces <strong>bounding box regression via anchor offsets</strong>.</p></li>
<li><p><a href="https://arxiv.org/pdf/1503.02531">Hinton et al.&nbsp;(2015)</a> introduces the idea that <strong>softening the target distribution</strong> helps model learn better.</p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Here’s an example of label encoding and one-hot encoding for a classification problem with three categories:</p>
<table class="table-striped caption-top table">
<thead>
<tr class="header">
<th>Fruit Name</th>
<th>Label Encoding</th>
<th>One-hot Encoding</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Apple</td>
<td>0</td>
<td>[1, 0, 0]</td>
</tr>
<tr class="even">
<td>Banana</td>
<td>1</td>
<td>[0, 1, 0]</td>
</tr>
<tr class="odd">
<td>Orange</td>
<td>2</td>
<td>[0, 0, 1]</td>
</tr>
</tbody>
</table>
<ul>
<li>Label encoding: Assigns a single number to each category, like 0, 1, 2.</li>
<li>One-hot encoding: Uses a binary vector to represent each category, like [1, 0, 0], [0, 1, 0], [0, 0, 1].</li>
</ul>
<p>In the following two cases, which encoding might be more appropriate?</p>
<ul>
<li><strong>Fruit categories</strong>: Apple, Banana, Orange.</li>
<li><strong>Severity of a storm</strong>: Light, Moderate, Severe.</li>
</ul>
<p><em>Hint: Think about whether the categories have a natural order. For example, does Apple need to come before Banana in the encoding?</em></p>
</div>
</div>
</section>
<section id="data-labeling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="data-labeling"><strong>3. Data Labeling</strong></h4>
<ul>
<li>Assign the correct “ground truth” output (label) to each input data point in your training and evaluation sets. This provides the reference the model learns from. &nbsp;</li>
<li>Refer to the <a href="../sections/data-annotation.html">Data Annotation</a> section for detailed labeling techniques.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Balance the desired detail of your outputs with model complexity, data availability, and computational resources. For instance, predicting exact sea ice percentage (0-100%) is harder than predicting broad categories like “low” (&lt;15%) vs.&nbsp;“high” (&gt;85%).</p>
</div>
</div>
</section>
</section>
<section id="quantity-and-quality" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="quantity-and-quality"><span class="header-section-number">6.2.3</span> Quantity and quality</h3>
<section id="quantity" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="quantity"><strong>Quantity</strong></h4>
<ul>
<li>Does more data always lead to better results? Not necessarily in isolation.</li>
<li>Interestingly, research shows that just having more data isn’t the full story. How much data you need is related to the size of your model and the computing power you have. This plot illustrates that for a fixed amount of computing power (FLOPs), there’s an optimal combination of model size and data amount to get the best results. <a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/dl-data-quantity.png" class="img-fluid figure-img"></p>
<figcaption>Hoﬀmann et al., (2022)</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>For various model sizes, we choose the number of training tokens such that the final FLOPs is a constant. The cosine cycle length is set to match the target FLOP count. We find a clear valley in loss, meaning that for a given FLOP budget there is an optimal model to train</p>
</blockquote>
</section>
<section id="quality" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="quality"><strong>Quality</strong></h4>
<ul>
<li><p>Data quality is often more critical than sheer quantity. High-quality, relevant data is essential for reliable models.</p></li>
<li><p>Common quality issues include:</p>
<ul>
<li>Incorrect or inconsistent labels. &nbsp;</li>
<li>Noise or irrelevant information within the data. &nbsp;</li>
<li>Poorly filtered datasets containing duplicates or near-duplicates.&nbsp;</li>
</ul></li>
<li><p>Research emphasizes that improving dataset quality through careful filtering and cleaning significantly boosts model performance. &nbsp;</p>
<ul>
<li>Rae et al.&nbsp;(2021) <a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>.</li>
</ul>
<blockquote class="blockquote">
<p>Our data pipeline (Section A.1.1) includes text quality filtering, removal of repetitious text, deduplication of similar documents, and removal of documents with significant test-set overlap. We find that successive stages of this pipeline improve language model downstream performance (Section A.3.2), emphasising the importance of dataset quality.</p>
</blockquote>
<ul>
<li>Hoffmann et al., (2022) <a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>.</li>
</ul>
<blockquote class="blockquote">
<p>Nonetheless, large language models face several challenges, including their overwhelming computational requirements (the cost of training and inference increase with model size) (Rae et al., 2021; Thoppilan et al., 2022) and the need for acquiring more high-quality training data. In fact, in this work we find that larger, high quality datasets will play a key role in any further scaling of language models.</p>
</blockquote></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>If your model performs poorly, what should you investigate first: trying a bigger model, or carefully check the quality and relevance of your training data?</p>
<p><em>Hint: Data quality issues are often a primary culprit.</em></p>
</div>
</div>
</section>
</section>
</section>
<section id="models" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="models"><span class="header-section-number">6.3</span> Models</h2>
<p><em>Models in deep learning represent the <strong>set of possible functions</strong> that can map inputs to outputs. Think of a model architecture as defining a family of functions; training then selects the specific function (by finding the right parameters) from that family that best fits the data. </em>&nbsp;</p>
<section id="layers-the-building-blocks-of-models" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="layers-the-building-blocks-of-models"><span class="header-section-number">6.3.1</span> Layers: The Building Blocks of Models</h3>
<p>Deep learning models are constructed by stacking layers. Each layer performs a specific transformation on the data it receives before passing it to the next layer. &nbsp;</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Don’t feel overwhelmed by the variety of layers initially. Focus on understanding their general purpose and how they fit into complete models. You can delve deeper into specific layer mechanics later.</p>
</div>
</div>
<p>Here are some common layer types:</p>
<section id="fully-connected-dense-layer" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="fully-connected-dense-layer"><strong>1. Fully-Connected (Dense) Layer</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/3_fully-connected-layer_0.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Source: <a href="https://builtin.com/machine-learning/fully-connected-layer">BuiltIn</a></figcaption>
</figure>
</div>
<ul>
<li><p><strong>What it does</strong>: Connects every input neuron to every output neuron. Each connection has a learnable weight. It calculates outputs by taking a weighted sum of all inputs <a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>.</p></li>
<li><p><strong>Think of it as</strong>: A system where every input feature can potentially influence every output value.</p></li>
<li><p><strong>Uses</strong>: Often used in the final stages of classification models to make predictions based on learned features, or to adjust the dimensionality (size) of the data representation. &nbsp;</p></li>
<li><p><strong>Pros</strong>: Can learn complex combinations of features. Simple concept. &nbsp;</p></li>
<li><p><strong>Cons</strong>: Many parameters (computationally expensive, prone to overfitting). Doesn’t inherently understand spatial or sequential structures in data (e.g., pixel neighborhoods in images). &nbsp;</p></li>
</ul>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interactive visualization of a fully-connected layer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="vis-fc-layer"></div>
<script type="module" src="../_resources/js/vis-fully-connected-layer.js"></script>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Fully-connected layers are sometimes criticized for not understanding ‘spatial structure’ in images. Why is that?</p>
<p><em>Hint: Fully-connected layers treat a 2D image as a flat vector, ignoring pixel neighborhoods.</em></p>
</div>
</div>
</section>
<section id="convolutional-layer-conv-layer" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="convolutional-layer-conv-layer"><strong>2. Convolutional Layer (Conv Layer)</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.nvidia.com/content/dam/en-zz/Solutions/glossary/data-science/neural-networks/neural-networks-pic1.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>Source: <a href="https://www.nvidia.com/en-in/glossary/convolutional-neural-network/">NVIDIA</a></figcaption>
</figure>
</div>
<ul>
<li><p><strong>What it does</strong>: Applies learnable filters (kernels) across the input data (often images). Each filter detects specific local patterns (like edges, corners, textures) <a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>.</p></li>
<li><p><strong>How it works</strong>: Small filters (also called <em>kernels</em>) slide across the input data. At each position, it computes a weighted sum of the input values in the filter’s receptive field. <em>Stride</em> controls the step size of the filter, and <em>padding</em> adds values (usually zeros) around the border to control output size. <code>Output size = (Input size + 2 × Padding - Kernel size) / Stride + 1</code>.</p></li>
<li><p><strong>Think of it as</strong>: Using a small magnifying glass that slides over an image, looking for specific visual features.</p></li>
<li><p><strong>Uses</strong>: Feature extraction in image processing, video analysis, and sometimes other data types where local patterns are important. &nbsp;</p></li>
<li><p><strong>Pros</strong>: Parameter sharing (filters are reused, reducing parameters compared to fully-connected layers). Captures spatial hierarchies (early layers find simple features, later layers combine them). Translation invariance (can detect a pattern regardless of its position). &nbsp;</p></li>
<li><p><strong>Cons</strong>: Primarily focused on local patterns; may need other mechanisms to capture long-range dependencies. Choosing filter size, stride, and padding requires careful design. &nbsp;</p></li>
</ul>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interactive visualization of a convolutional layer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="vis-conv-layer"></div>
<script type="module" src="../_resources/js/vis-convolutional-layer.js"></script>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>What is the main benefit of ‘parameter sharing’ in convolutional layers compared to fully-connected layers when processing images?</p>
<p><em>Hint: Fewer weights to learn, detects patterns anywhere in the image.</em></p>
</div>
</div>
</section>
<section id="pooling-layer" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="pooling-layer"><strong>3. Pooling Layer</strong></h4>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://computersciencewiki.org/images/9/9e/MaxpoolSample.png" class="img-fluid"></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Source: <a href="https://computersciencewiki.org/index.php?title=Max-pooling_/_Pooling">Computersciencewiki</a></p>
</div>
</div>
</div>
<ul>
<li><p><strong>What it does</strong>: Reduces the spatial dimensions (height/width) of the input, summarizing information within local regions. &nbsp;</p></li>
<li><p><strong>How it works</strong>: Divides the input into regions. For each region, it outputs a single summary value – typically the maximum (<em>Max Pooling</em>) or the average (<em>Average Pooling</em>) of the values in that region. <em>Global Pooling</em> summarizes across the entire feature map <a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>. &nbsp;</p></li>
<li><p><strong>Think of it as</strong>: Downsizing an image while trying to keep the most salient information from each small block.</p></li>
<li><p><strong>Uses</strong>: Reduces computational load and memory usage. Increases robustness to small spatial variations in the input. Helps focus on the most prominent features. &nbsp;</p></li>
<li><p><strong>Pros</strong>: Reduces dimensions significantly. Provides some invariance to minor translations. Computationally efficient. &nbsp;</p></li>
<li><p><strong>Cons</strong>: Discards information (potentially important details). &nbsp;</p></li>
</ul>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interactive visualization of a pooling layer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="vis-pooling-layer"></div>
<script type="module" src="../_resources/js/vis-pooling-layer.js"></script>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>What information is potentially lost when using Max Pooling? Why might we still use it?</p>
<p><em>Hint: Loses details/exact locations within the region; Gains computational efficiency and some robustness to feature location.</em></p>
</div>
</div>
</section>
<section id="activation-layer-non-linearity" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="activation-layer-non-linearity"><strong>4. Activation Layer (Non-linearity)</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i0.wp.com/sefiks.com/wp-content/uploads/2020/02/sample-activation-functions-square.png?resize=1024%2C806&amp;ssl=1" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>Source: <a href="https://sefiks.com/2020/02/02/dance-moves-of-deep-learning-activation-functions/#google_vignette">Sefik Ilkin Serengil</a></figcaption>
</figure>
</div>
<ul>
<li><p><strong>What it does</strong>: Introduces non-linearity into the model. Without it, stacking linear layers (like Conv or Fully-Connected) would just result in another linear function. Non-linearity allows models to learn complex patterns. &nbsp;</p></li>
<li><p><strong>How it works</strong>: Applies a fixed mathematical function element-wise to the output of the previous layer. Common activation functions include:</p>
<ul>
<li>ReLU: <span class="math inline">\(f(x) = \max(0, x)\)</span></li>
<li>Sigmoid: <span class="math inline">\(f(x) = \frac{1}{1 + e^{-x}}\)</span></li>
<li>Tanh: <span class="math inline">\(f(x) = \frac{e^{2x} - 1}{e^{2x} + 1}\)</span></li>
</ul></li>
<li><p><strong>Think of it as</strong>: A gate or switch that modifies the signal passing through, allowing the network to make more complex “decisions”. ReLU, for example, simply outputs the input if it’s positive, and zero otherwise <span class="math inline">\(\max(0,x)\)</span>. &nbsp;</p></li>
<li><p><strong>Uses</strong>: Applied after most convolutional and fully-connected layers to enable learning of complex mappings. &nbsp;</p></li>
<li><p><strong>Pros</strong>: Essential for learning non-trivial functions. Different activations have properties suited for different tasks or network parts (e.g., Sigmoid outputs values between 0 and 1, useful for probabilities) <a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>.</p></li>
<li><p><strong>Cons</strong>: Poor choices can lead to training difficulties like the “vanishing gradient problem” where gradients become too small for effective learning <a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>.</p></li>
</ul>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interactive visualization of an activation layer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="vis-activation-layer"></div>
<script type="module" src="../_resources/js/vis-activation-layer.js"></script>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>If a network only used linear activation functions (or no activations), what kind of relationship could it learn between inputs and outputs, no matter how many layers it had?</p>
<p><em>Hint: only linear relationships.</em></p>
</div>
</div>
</section>
<section id="recurrent-layer-e.g.-lstm-gru" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="recurrent-layer-e.g.-lstm-gru"><strong>5. Recurrent Layer (e.g., LSTM, GRU)</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20241030134529497963/recurrent-neuron.png" class="img-fluid figure-img" style="width:20.0%"></p>
<figcaption>Source: <a href="https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/">geeksforgeeks</a></figcaption>
</figure>
</div>
<ul>
<li><p><strong>What it does</strong>: Processes sequential data (like text, time series, speech) by maintaining an internal “memory” or state that captures information from previous steps in the sequence. &nbsp;</p></li>
<li><p><strong>How it works</strong>: At each step in the sequence, the layer takes the current input and the hidden state from the previous step. It uses these to compute the output for the current step and update its hidden state for the next step. Variants like LSTM (Long Short-Term Memory) <a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> and GRU (Gated Recurrent Unit) <a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> use gating mechanisms to control what information is remembered, forgotten, or passed on, helping them learn long-range dependencies. &nbsp;</p></li>
<li><p><strong>Think of it as</strong>: Reading a sentence word by word, keeping the context from earlier words in mind to understand the current word.</p></li>
<li><p><strong>Uses</strong>: Natural Language Processing (NLP), speech recognition, time series forecasting. &nbsp;</p></li>
<li><p><strong>Pros</strong>: Explicitly designed to handle sequential dependencies. Can process sequences of varying lengths. LSTMs/GRUs mitigate the vanishing gradient problem common in simpler RNNs. &nbsp;</p></li>
<li><p><strong>Cons</strong>: Processing is inherently sequential, making parallelization harder and training potentially slower than feed-forward networks. Can still struggle with extremely long dependencies despite improvements <a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a>.</p></li>
</ul>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Visualization of a LSTM cell
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div style="display: flex; justify-content: center; align-items: center; width: 100%;">
    <video src="https://packaged-media.redd.it/afzlbpt2ncg81/pb/m2-res_480p.mp4?m=DASHPlaylist.mpd&amp;v=1&amp;e=1744837200&amp;s=962a50128d5f94aa8b6812f3f6c2d1119d2fa3e2" controls=""></video>
</div>
<div style="text-align: center; font-size: 0.8em; color: #666; margin-top: 0.5em;">
    Source: <a href="https://www.reddit.com/r/TheInsaneApp/comments/smiln0/long_short_term_memory_cell_visualized/" target="_blank">Reddit r/TheInsaneApp</a>
</div>
<p>The key components of an LSTM cell are:</p>
<ul>
<li><strong>Three gates</strong> (shown as X symbols in circles) from left to right:
<ul>
<li><strong>Forget gate</strong>: This decides what information to throw away or keep from memory.</li>
<li><strong>Input gate</strong>: This decides what new information to add to memory.</li>
<li><strong>Output gate</strong>: This decides what information to share with the next cell.</li>
</ul></li>
<li><strong>Inputs</strong>:
<ul>
<li>Current input <span class="math inline">\(x_t\)</span></li>
<li>Previous hidden state <span class="math inline">\(h_{t-1}\)</span></li>
<li>Previous cell state <span class="math inline">\(C_{t-1}\)</span></li>
</ul></li>
<li><strong>Outputs</strong>:
<ul>
<li>Current hidden state <span class="math inline">\(h_t\)</span></li>
<li>Current cell state <span class="math inline">\(C_t\)</span></li>
</ul></li>
<li>The <strong>blue box</strong> represents the <strong>sigmoid function</strong>, which outputs a value between 0 and 1. It controls how much information passes through each gate, like a filter that can be partially open or closed.</li>
<li>The <strong>purple box</strong> represents the <strong>tanh function</strong>, which outputs a value between -1 and 1. It scales the input values.</li>
</ul>
<p>The LSTM cell works as follows:</p>
<ol type="1">
<li><span class="math inline">\(h_{t-1}\)</span> and <span class="math inline">\(x_t\)</span> are combined together and passed through sigmoid functions as gate control signals.</li>
<li>The forget gate determines how much of the previous cell state <span class="math inline">\(C_{t-1}\)</span> (the previous memory) is passed to the next cell. Think of this like deciding which old memories to keep or discard.</li>
<li>The input gate determines how much of the current input <span class="math inline">\(x_t\)</span> is added to the cell state. This is like deciding which new information is worth remembering.</li>
<li>The output gate determines how much of the current cell state <span class="math inline">\(C_t\)</span> is passed to the next hidden state <span class="math inline">\(h_t\)</span>. This is like deciding which parts of your memory to actively think about right now.</li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>What is the main benefit of using an LSTM over a basic RNN?</p>
<p><em>Hint: Better handling of long-term dependencies / avoiding the vanishing gradient problem.</em></p>
</div>
</div>
</section>
<section id="attention-layer" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="attention-layer"><strong>6. Attention Layer</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://theaisummer.com/static/e9145585ddeed479c482761fe069518d/ea64c/attention.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>Convolution and fully connected layers use fixed connection patterns—the same weights apply no matter the input. Attention layers, on the other hand, compute weights based on the input itself, so the connections can change depending on what the model sees. All these weights are learned and updated during training. Source: <a href="https://theaisummer.com/attention/">The AI Summer</a></figcaption>
</figure>
</div>
<ul>
<li><p><strong>What it does</strong>: Allows the model to dynamically focus on the most relevant parts of the input sequence when producing an output at a particular step. Instead of relying solely on the final hidden state (like in basic RNNs), it can “look back” at different parts of the input. &nbsp;</p></li>
<li><p><strong>How it works</strong>: Calculates “attention scores” indicating the relevance of each input element (e.g., each word in a source sentence for translation) to the current output element (e.g., the word being translated). It then computes a weighted sum of the input representations based on these scores, effectively highlighting the important parts <a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a>.</p></li>
<li><p><strong>Think of it as</strong>: When translating a sentence, focusing your attention on specific source words relevant to the target word you are about to write.</p></li>
<li><p><strong>Uses</strong>: Machine translation, text summarization, question answering, image captioning. Core component of Transformer models. &nbsp;</p></li>
<li><p><strong>Pros</strong>: Significantly improves performance on tasks requiring alignment between input and output sequences. Can handle long-range dependencies effectively. Provides interpretability by showing where the model “attends”. &nbsp;</p></li>
<li><p><strong>Cons</strong>: Can be computationally intensive, especially <em>self-attention</em> (where elements within the same sequence attend to each other), which scales quadratically with sequence length.</p></li>
</ul>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Visualization of image attention mechanism (conceptual flow)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Attention mechanism is originally proposed for natural language processing tasks. The following visualization shows how it works for images. You can check how it works for text here: <a href="https://www.youtube.com/watch?v=wjZofJX0v4M">How LLMs work</a> and <a href="https://www.youtube.com/watch?v=eMlx5fFNoYc">Attention in transformers</a>.</p>
<div id="vis-attention-layer"></div>
<script type="module" src="../_resources/js/vis-attention-layer.js"></script>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the context of translating a long sentence, how does attention help the model generate a more accurate translation compared to a model without attention, e.g., LSTM?</p>
<p><em>Hint: Allows focusing on relevant source words for each target word being generated. This is more accurate than using the last hidden state of the LSTM, especially for long sentences.</em></p>
</div>
</div>
</section>
</section>
<section id="common-model-architectures" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="common-model-architectures"><span class="header-section-number">6.3.2</span> Common Model Architectures</h3>
<p>Layers are assembled into architectures. Different architectures excel at different types of data and tasks because they embody different assumptions about the data (inductive biases). Often, models serve primarily as powerful <strong>feature extractors</strong>. Once meaningful features are extracted, they can be fed into simpler “head” layers for specific tasks (like classification or regression). &nbsp;</p>
<p>This modularity allows flexibility:</p>
<ul>
<li><strong>One Model, Many Tasks</strong>: The same base feature extractor (e.g., a CNN) can be used for image classification, object detection, or segmentation by swapping out the final layers. &nbsp;</li>
<li><strong>Many Models, One Task</strong>: Different architectures (CNN, Transformer) might be applied to the same task (e.g., image classification), each leveraging different strengths.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>See <a href="#model-customization">Model customization</a> for more details on how to customize models for different data and tasks.</p>
</div>
</div>
<p>Why different models? Data and tasks vary fundamentally: &nbsp;</p>
<ul>
<li><strong>CNNs</strong>: Great for grid-like data (images) due to spatial pattern recognition. &nbsp;</li>
<li><strong>LSTMs/RNNs</strong>: Suited for sequential data, capturing temporal dependencies. &nbsp;</li>
<li><strong>Transformers</strong>: Excel at modeling relationships between elements in a sequence using attention, regardless of distance. &nbsp;</li>
<li><strong>GNNs</strong>: Designed for graph-structured data, modeling relationships between connected entities.</li>
</ul>
<p>Choosing the right architecture matches the structure of your data and task. &nbsp;</p>
<p>Here’s a look at some prominent model architectures:</p>
<section id="convolutional-neural-networks-cnns" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="convolutional-neural-networks-cnns"><strong>1. Convolutional Neural Networks (CNNs)</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20250207123959732912/Working-of-CNN_.webp" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption>A typical CNN architecture. Source: <a href="https://www.geeksforgeeks.org/convolutional-neural-network-cnn-in-machine-learning/">geeksforgeeks</a></figcaption>
</figure>
</div>
<ul>
<li><p><strong>Best for</strong>: Grid-like data, especially images.</p></li>
<li><p><strong>Core idea</strong>: Uses convolutional layers to automatically learn hierarchies of spatial features (edges -&gt; textures -&gt; parts -&gt; objects).</p></li>
<li><p><strong>Architecture</strong>: A typical CNN architecture combines several types of layers:</p>
<ul>
<li><strong>Convolutional layers</strong>: Extract features like edges, textures, and shapes.</li>
<li><strong>Activation layers</strong>: Add non-linearity to learn complex patterns.</li>
<li><strong>Pooling layers</strong>: Reduce data dimensions while keeping important information.</li>
<li><strong>Batch normalization layers <a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a></strong>: Stabilize and accelerate training.</li>
<li><strong>Dropout layers <a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a></strong>: Prevent overfitting by randomly dropping some neurons.</li>
<li><strong>Fully-connected layers</strong>: Combine extracted features for predictions.</li>
</ul></li>
</ul>
<ul>
<li><p><strong>Strengths</strong>:</p>
<ul>
<li><strong>Parameter efficiency</strong>: Instead of learning a separate weight for each pixel (as in a fully-connected layer), CNNs use small filters that slide across the image, sharing weights.</li>
<li><strong>Translation invariance</strong>: Since the same filter is applied everywhere, CNNs can recognize patterns regardless of their location in the image.</li>
<li><strong>Hierarchical learning <a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a></strong>: CNNs build understanding from simple to complex. Early layers detect edges, corners, and textures. Middle layers combine these into shapes and textures, and deep layers assemble these into recognizable objects or concepts.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.cs.cmu.edu/~epxing/Class/10708-19/assets/img/notes/lecture-16/cnn_hierarchy.png" class="img-fluid figure-img"></p>
<figcaption>Hierarchical features in CNNs. Source: <a href="https://www.cs.cmu.edu/~epxing/Class/10708-19/notes/lecture-16/">CMU 10-708 Probabilistic Graphical Models</a></figcaption>
</figure>
</div></li>
</ul>
<ul>
<li><p><strong>Examples</strong>:</p>
<ul>
<li><strong>LeNet <a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a></strong>: Early successful CNN for handwritten digit recognition.</li>
<li><strong>AlexNet <a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a></strong>: Breakthrough performance on ImageNet (2012).</li>
<li><strong>VGG <a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a></strong>: Simple and deep architecture.</li>
<li><strong>ResNet <a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a></strong>: Introduced residual connections to enable very deep networks.</li>
<li><strong>Inception/GoogLeNet <a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a></strong>: Used parallel convolutions of different sizes to capture multi-scale features.</li>
<li><strong>EfficientNet <a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a></strong>: Scaled depth/width/resolution systematically.</li>
</ul></li>
</ul>
<ul>
<li><p><strong>Applications</strong>:</p>
<ul>
<li><strong>Image classification <a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a></strong>: Identify objects and scenes in images (used in search engines).</li>
<li><strong>Medical image analysis <a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a></strong>: Classify diseases in medical imaging, including radiology, MRI, and CT scans.</li>
<li><strong>Facial recognition <a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a></strong>: Create facial embeddings for recognition tasks.</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>What are the two main advantages of using convolutional layers over fully-connected layers for image classification?</p>
<p><em>Hint: Parameter sharing/efficiency and translation invariance.</em></p>
</div>
</div>
</section>
<section id="long-short-term-memory-networks-lstms-lstm-4" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="long-short-term-memory-networks-lstms-lstm-4"><strong>2. Long Short-Term Memory Networks (LSTMs) <a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a></strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption>Source: <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTMs by Christopher Olah</a></figcaption>
</figure>
</div>
<ul>
<li><p><strong>Best for</strong>: Sequential data (text, time series, speech).</p></li>
<li><p><strong>Core idea</strong>: A type of RNN specifically designed with internal “gates” (<strong>forget</strong>, <strong>input</strong>, <strong>output</strong>) to control information flow, allowing it to remember relevant information over long sequences and forget irrelevant details. Addresses the vanishing gradient problem of simple RNNs. &nbsp;</p></li>
<li><p><strong>Architecture</strong>: See <a href="#recurrent-layer">Visualization of a LSTM cell</a> for the introduction of a LSTM cell and its operation.</p></li>
<li><p><strong>Strengths</strong>:</p>
<ul>
<li><strong>Long-term memory</strong>: Can remember information for extended sequences, solving the vanishing gradient problem plaguing simple RNNs <a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a>.</li>
<li><strong>Selective memory management</strong>: Can learn which information to remember and which to forget.</li>
<li><strong>Flexible sequence handling</strong>: Can process inputs of variable length and maintain contextual understanding.</li>
<li><strong>Gradient stability</strong>: Special architecture prevents gradients from vanishing or exploding during backpropagation <a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a>.</li>
</ul></li>
</ul>
<ul>
<li><p><strong>Variants</strong>:</p>
<ul>
<li><strong>Vanilla LSTM <a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a></strong>: The original architecture introduced by Hochreiter &amp; Schmidhuber.</li>
<li><strong>GRU (Gated Recurrent Unit) <a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a></strong>: Simplified version with fewer parameters that merges cell and hidden states.</li>
<li><strong>Bidirectional LSTM <a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a></strong>: Processes sequences in both forward and backward directions, i.e., using both past and future context.</li>
<li><strong>ConvLSTM <a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a></strong>: Combines convolutional operations with LSTM for spatiotemporal data, e.g., video classification.</li>
</ul></li>
</ul>
<ul>
<li><p><strong>Applications</strong>:</p>
<ul>
<li><strong>Natural language processing <a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a></strong>: Machine translation, text generation, and sentiment analysis.</li>
<li><strong>Time series prediction <a href="#fn50" class="footnote-ref" id="fnref50" role="doc-noteref"><sup>50</sup></a></strong>: Stock market forecasting, weather prediction, and energy consumption.</li>
<li><strong>Speech recognition <a href="#fn51" class="footnote-ref" id="fnref51" role="doc-noteref"><sup>51</sup></a></strong>: Converting spoken language to text with contextual understanding.</li>
<li><strong>Music generation <a href="#fn52" class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a></strong>: Creating original musical compositions with temporal coherence.</li>
<li><strong>Anomaly detection <a href="#fn53" class="footnote-ref" id="fnref53" role="doc-noteref"><sup>53</sup></a></strong>: Identifying unusual patterns in sequential data like network traffic.</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>What is the high-level purpose of the ‘gates’ in an LSTM cell?</p>
<p><em>Hint: To control the flow of information - deciding what information to remember, forget, and output.</em></p>
</div>
</div>
</section>
<section id="transformers" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="transformers"><strong>3. Transformers</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.datacamp.com/legacy/v1704797298/image_7b08f474e7.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Diagram of the Transformer architecture showing Encoder and Decoder stacks. Source: <a href="https://www.datacamp.com/tutorial/how-transformers-work">DataCamp: How Transformers work</a></figcaption>
</figure>
</div>
<ul>
<li><p><strong>Best for</strong>: Sequential data (dominant in modern NLP: text translation, generation, understanding), but increasingly adapted for images (Vision Transformers) and other data types.</p></li>
<li><p><strong>Core idea</strong>: Revolutionized sequence modeling by relying almost entirely on <strong>self-attention</strong> mechanisms instead of recurrence (like LSTMs). This allows the model to weigh the importance of <em>all</em> other elements in the sequence when processing a given element, capturing context effectively regardless of distance. This design is highly parallelizable (faster training!) but requires <strong>positional encodings</strong> to retain sequence order information since it doesn’t process word-by-word.</p></li>
<li><p><strong>Architecture</strong>: The foundational “Attention Is All You Need” paper <a href="#fn54" class="footnote-ref" id="fnref54" role="doc-noteref"><sup>54</sup></a> introduced an <strong>Encoder-Decoder</strong> structure:</p>
<ul>
<li><strong>Encoder</strong>: Processes the input sequence (e.g., source language sentence) and builds rich, context-aware representations.</li>
<li><strong>Decoder</strong>: Generates the output sequence (e.g., target language sentence) based on the encoder’s representations and the output generated so far.</li>
</ul>
<p>Both Encoder and Decoder are typically stacks of identical layers, each containing key sub-components:</p>
<ul>
<li><p><strong>Self-Attention Layers (often Multi-Head)</strong>: The core mechanism! Allows each position to ‘attend’ to all other positions (within the encoder, or previous positions in the decoder) to understand context. Multi-head attention allows attending to different types of information simultaneously. <em>(See the <a href="#attention-layer">Attention Layer</a> section for more details/visualizations)</em>.</p></li>
<li><p><strong>Position-wise Feed-Forward Networks</strong>: Process information independently at each position in the sequence.</p></li>
<li><p><strong>Residual Connections &amp; Layer Normalization</strong> <a href="#fn55" class="footnote-ref" id="fnref55" role="doc-noteref"><sup>55</sup></a>: These are crucial techniques applied around the sub-layers to help gradients flow during training and stabilize the learning process, enabling deeper networks.</p></li>
<li><p><strong>Positional Encodings</strong> <a href="#fn56" class="footnote-ref" id="fnref56" role="doc-noteref"><sup>56</sup></a>: Added to the input to give the model information about the position of each element in the sequence.</p></li>
</ul></li>
</ul>
<ul>
<li><strong>Strengths</strong>:
<ul>
<li><strong>Parallelization</strong>: Unlike sequential RNNs, attention calculations can be performed largely in parallel across the sequence, enabling much faster training on modern hardware (GPUs/TPUs).</li>
<li><strong>Long-Range Dependencies</strong> <a href="#fn57" class="footnote-ref" id="fnref57" role="doc-noteref"><sup>57</sup></a>: Self-attention directly connects all positions, making it easier to capture relationships between distant elements in a sequence compared to RNNs/LSTMs.</li>
<li><strong>Context-Aware Representations</strong>: Each element’s final representation is influenced by its entire context, leading to deeper understanding.</li>
<li><strong>Scalability</strong> <a href="#fn58" class="footnote-ref" id="fnref58" role="doc-noteref"><sup>58</sup></a>: Performance generally improves predictably with more data and larger model sizes, following established “scaling laws.”</li>
</ul></li>
</ul>
<ul>
<li><p><strong>Popular Transformer architectures</strong>: (Many models build on the original Transformer ideas)</p>
<ul>
<li><strong>BERT</strong> <a href="#fn59" class="footnote-ref" id="fnref59" role="doc-noteref"><sup>59</sup></a>: Encoder-only model, highly effective for language <em>understanding</em> tasks (classification, question answering) via pre-training.</li>
<li><strong>GPT (Generative Pre-trained Transformer)</strong> <a href="#fn60" class="footnote-ref" id="fnref60" role="doc-noteref"><sup>60</sup></a> <a href="#fn61" class="footnote-ref" id="fnref61" role="doc-noteref"><sup>61</sup></a>: Decoder-only model, powerful for text <em>generation</em> tasks, known for scaling to very large sizes.</li>
<li><strong>ViT (Vision Transformer)</strong> <a href="#fn62" class="footnote-ref" id="fnref62" role="doc-noteref"><sup>62</sup></a>: Adapted the Transformer for computer <em>vision</em> by treating image patches like sequence tokens.</li>
<li><strong>BART</strong> <a href="#fn63" class="footnote-ref" id="fnref63" role="doc-noteref"><sup>63</sup></a>: Full Encoder-Decoder model often used for sequence-to-sequence tasks like summarization.</li>
<li><strong>Swin Transformer</strong> <a href="#fn64" class="footnote-ref" id="fnref64" role="doc-noteref"><sup>64</sup></a>: A hierarchical Vision Transformer using shifted windows for efficiency.</li>
</ul></li>
</ul>
<ul>
<li><p><strong>Applications</strong>: Virtually all modern NLP tasks (translation, summarization, Q&amp;A), increasingly used in computer vision, biology, and other domains.</p>
<ul>
<li><strong>Natural language processing <a href="#fn65" class="footnote-ref" id="fnref65" role="doc-noteref"><sup>65</sup></a></strong>: Machine translation, text summarization, question answering, and text generation, outperforming LSTM-based approaches.</li>
<li><strong>Computer vision <a href="#fn66" class="footnote-ref" id="fnref66" role="doc-noteref"><sup>66</sup></a></strong>: Image classification, object detection, segmentation, and image generation, increasingly surpassing CNN performance.</li>
<li><strong>Speech recognition <a href="#fn67" class="footnote-ref" id="fnref67" role="doc-noteref"><sup>67</sup></a></strong>: End-to-end speech-to-text.</li>
<li><strong>Multimodal learning <a href="#fn68" class="footnote-ref" id="fnref68" role="doc-noteref"><sup>68</sup></a></strong>: Models combining text, images, and/or audio (e.g., image captioning, text-to-image generation).</li>
<li><strong>Computational biology <a href="#fn69" class="footnote-ref" id="fnref69" role="doc-noteref"><sup>69</sup></a></strong>: Protein structure prediction (AlphaFold2), genomic sequence analysis.</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Transformers have largely surpassed LSTMs for state-of-the-art performance in NLP and are becoming competitive alternatives to CNNs in vision, especially when large datasets are available. The original “Attention Is All You Need” paper <a href="#fn70" class="footnote-ref" id="fnref70" role="doc-noteref"><sup>70</sup></a> is a highly recommended read for understanding the foundation. Research on scaling laws <a href="#fn71" class="footnote-ref" id="fnref71" role="doc-noteref"><sup>71</sup></a> suggests performance will likely continue to improve with increased model size and data, though this requires significant computational resources.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Why are ‘Positional Encodings’ necessary in Transformers?</li>
</ul>
<p><em>Hint: Self-attention itself doesn’t know the order/position of elements; these encodings add that information.</em></p>
<ul>
<li>What advantage does self-attention provide for understanding long sentences compared to the hidden state passed along in an LSTM?</li>
</ul>
<p><em>Hint: Allows direct connections/comparisons between any two words, regardless of distance.</em></p>
</div>
</div>
</section>
<section id="autoencoders" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="autoencoders"><strong>4. Autoencoders</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20231130152144/Autoencoder.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>Autoencoder structure: Input -&gt; Hidden Layer (Bottleneck) -&gt; Output (Reconstruction). Source: <a href="https://www.geeksforgeeks.org/auto-encoders/">geeksforgeeks</a></figcaption>
</figure>
</div>
<ul>
<li><p><strong>Best for</strong>: <strong>Unsupervised learning</strong> tasks like dimensionality reduction, feature learning, data denoising, and generative modeling. Unlike models needing explicit labels, autoencoders learn directly from the input data itself.</p></li>
<li><p><strong>Core idea</strong>: An unsupervised network trained to <strong>reconstruct its input</strong>. It consists of an <strong>Encoder</strong> that compresses the input into a lower-dimensional latent space (the <strong>bottleneck</strong>), and a <strong>Decoder</strong> that tries to reconstruct the original input from this compressed representation. The bottleneck forces the network to learn the most important, efficient features of the data.</p></li>
<li><p><strong>Architecture</strong>: A typical autoencoder consists of three main components:</p>
<ul>
<li><strong>Encoder</strong>: Compresses/encodes the input data into a compact representation.</li>
<li><strong>Bottleneck/Latent space</strong>: The compressed, lower-dimensional representation of the input data.</li>
<li><strong>Decoder</strong>: Reconstructs/decodes the original input data from the bottleneck representation.</li>
</ul></li>
<li><p><strong>Variants</strong>: (Building on the basic idea)</p>
<ul>
<li><strong>Vanilla autoencoders</strong> <a href="#fn72" class="footnote-ref" id="fnref72" role="doc-noteref"><sup>72</sup></a>: The basic architecture described above.</li>
<li><strong>Sparse autoencoders</strong> <a href="#fn73" class="footnote-ref" id="fnref73" role="doc-noteref"><sup>73</sup></a>: Encourage sparsity in the bottleneck for potentially more robust features.</li>
<li><strong>Denoising autoencoders</strong> <a href="#fn74" class="footnote-ref" id="fnref74" role="doc-noteref"><sup>74</sup></a>: Trained specifically to reconstruct <em>clean</em> inputs from intentionally <em>corrupted</em> versions, making them good for noise removal and learning robust features.</li>
<li><strong>Variational autoencoders (VAEs)</strong> <a href="#fn75" class="footnote-ref" id="fnref75" role="doc-noteref"><sup>75</sup></a>: A generative variant that learns a probability distribution in the latent space, allowing you to <em>generate new</em> data samples similar to the training data.</li>
<li><strong>Convolutional autoencoders</strong> <a href="#fn76" class="footnote-ref" id="fnref76" role="doc-noteref"><sup>76</sup></a>: Use convolutional layers in the encoder/decoder, suitable for image data.</li>
<li><strong>Adversarial autoencoders</strong> <a href="#fn77" class="footnote-ref" id="fnref77" role="doc-noteref"><sup>77</sup></a>: Combine autoencoders with ideas from Generative Adversarial Networks (GANs).</li>
</ul></li>
</ul>
<ul>
<li><p><strong>Applications</strong>:</p>
<ul>
<li><strong>Anomaly detection</strong> <a href="#fn78" class="footnote-ref" id="fnref78" role="doc-noteref"><sup>78</sup></a>: Identify unusual data points – they typically have high reconstruction error when passed through an AE trained on normal data.</li>
<li><strong>Image denoising and restoration</strong> <a href="#fn79" class="footnote-ref" id="fnref79" role="doc-noteref"><sup>79</sup></a>: Clean up noisy or corrupted images (especially Denoising AEs).</li>
<li><strong>Dimensionality reduction</strong> <a href="#fn80" class="footnote-ref" id="fnref80" role="doc-noteref"><sup>80</sup></a>: Compress high-dimensional data to a lower dimension while preserving important structure (similar to PCA, but non-linear).</li>
<li><strong>Feature learning</strong> <a href="#fn81" class="footnote-ref" id="fnref81" role="doc-noteref"><sup>81</sup></a>: The encoder part can be used to extract meaningful features for downstream supervised tasks.</li>
<li><strong>Recommender systems</strong> <a href="#fn82" class="footnote-ref" id="fnref82" role="doc-noteref"><sup>82</sup></a>: Learn latent representations (embeddings) of users and items.</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>How does the training goal of an autoencoder differ from the supervised classification models (like CNNs) we saw earlier?</p>
<p><em>Hint: Autoencoders are unsupervised, meaning they don’t need labeled data. They learn to compress data into a lower-dimensional representation, then reconstruct it from that compressed representation.</em></p>
</div>
</div>
</section>
<section id="choosing-the-right-architecture" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="choosing-the-right-architecture"><strong>Choosing the Right Architecture</strong></h4>
<p>As highlighted, different models can often tackle the same application (e.g., CNNs and Vision Transformers for image classification). The choice depends on factors like:</p>
<ul>
<li><strong>Your Data:</strong> Is it grid-like (images), sequential (text/time), graph-structured, or something else?</li>
<li><strong>Your Task:</strong> Are you classifying, regressing, generating, clustering?</li>
<li><strong>Available Resources:</strong> How much data and computing power do you have? Some models are more data-hungry or computationally expensive than others.</li>
<li><strong>Performance Needs:</strong> Do you need state-of-the-art accuracy, or is speed/efficiency more critical?</li>
</ul>
<p>Understanding the core strengths and inductive biases of each architecture helps in selecting the most appropriate starting point for your problem.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Based on the architectures discussed (CNNs, LSTMs, Transformers, Autoencoders):</p>
<ul>
<li>If you wanted to classify sentiment (positive/negative) based on customer reviews (text data), which 1-2 architectures seem most appropriate to start with? Why?</li>
<li>If you wanted to predict tomorrow’s temperature based on historical weather data (time series), which architecture(s) might you consider?</li>
<li>If you wanted to detect fraudulent transactions in a network of users and payments, which architecture fits best?</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Although there are many types of deep learning models, some research reveals interesting relationships between them, demonstrating that many models can be viewed as special cases of others. Here are some examples for you to explore:</p>
<ul>
<li><a href="https://graphdeeplearning.github.io/post/transformers-are-gnns/">Transformers are Graph Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/2309.10713">Interpret Vision Transformers as ConvNets with Dynamic Convolutions</a></li>
<li><a href="https://arxiv.org/abs/2301.08210">Everything is Connected: Graph Neural Networks</a></li>
<li><a href="https://arxiv.org/pdf/2008.02217">Hopfield Networks is All You Need</a></li>
</ul>
</div>
</div>
</section>
</section>
<section id="pre-trained-models-and-transfer-learning" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="pre-trained-models-and-transfer-learning"><span class="header-section-number">6.3.3</span> Pre-trained models and transfer learning</h3>
<p>In deep learning, creating models from scratch requires lots of time and computing power, e.g., collecting and labeling a large dataset, designing good network architectures, and training for days, weeks, or months on powerful GPUs. So, how can <em>we</em>, as learners or small teams leverage the power of deep learning without needing Google-level resources?</p>
<p><strong>Pre-trained models</strong> and <strong>transfer learning</strong> offer a simpler approach. Instead of starting from zero, researchers can use models that have already been constructed and learned from massive datasets. This lets them build effective systems faster and with less data, like standing on the shoulders of giants rather than climbing the mountain alone.</p>
<section id="pre-trained-models" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="pre-trained-models"><strong>Pre-trained models</strong></h4>
<p><em>A <strong>pre-trained model</strong> is a neural network that has already been trained on a large benchmark dataset to solve a specific task (usually a general one). They have learned valuable features and can be used as-is for their original task or as a starting point for other tasks.</em></p>
<p><strong>Example</strong>:</p>
<ul>
<li><strong>Image classification</strong>: Pre-trained models like ResNet are trained on the ImageNet dataset, which contains over 14 million images across 1000 categories <a href="#fn83" class="footnote-ref" id="fnref83" role="doc-noteref"><sup>83</sup></a> <a href="#fn84" class="footnote-ref" id="fnref84" role="doc-noteref"><sup>84</sup></a>.</li>
<li><strong>Text classification</strong>: Pre-trained models like BERT are trained on large text corpora (like Wikipedia and BooksCorpus) <a href="#fn85" class="footnote-ref" id="fnref85" role="doc-noteref"><sup>85</sup></a>.</li>
</ul>
</section>
<section id="transfer-learning" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="transfer-learning"><strong>Transfer learning</strong></h4>
<p><em><strong>Transfer Learning</strong> is the technique of taking a pre-trained model (which learned on Task A / Dataset A) and adapting it for your own, usually different but related, Task B / Dataset B. The machine exploits the knowledge gained from the pre-training stage to improve its performance on the new task.</em></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://data-science-blog.com/wp-content/uploads/2022/04/re-use-of-pre-trained-models-in-transfer-machine-learning.png" class="img-fluid figure-img"></p>
<figcaption>Transfer learning strategies: Feature Extraction (Strategy 1) and Fine-tuning (Strategy 2). Source: <a href="https://data-science-blog.com/blog/2022/04/11/how-to-choose-the-best-pre-trained-model-for-your-convolutional-neural-network/">data-science-blog</a></figcaption>
</figure>
</div>
<p><strong>Common approaches</strong>:</p>
<ul>
<li><strong>Feature Extraction</strong>: Use the pre-trained model (mostly frozen, i.e., weights not updated) as a fixed feature extractor. You typically replace or add only the final layer(s) (“head”) to suit your specific task and train only those new layers on your data. Good for smaller datasets or tasks very similar to the original. Faster training. &nbsp;</li>
<li><strong>Fine-tuning</strong>: Start with the pre-trained model’s weights, but allow some or all of them (usually the later layers) to be updated during training on your new dataset, typically using a low learning rate. This adapts the learned features more specifically to your task. Better for larger datasets where you have enough data to tune without overfitting. Can lead to better performance but requires more careful training. &nbsp;</li>
<li><strong>Prompt Engineering</strong>: (Mainly for large language models) Guide the pre-trained model’s behavior by carefully designing the input text (prompt), often without changing the model’s weights at all.</li>
</ul>
</section>
<section id="benefits-of-using-pre-trained-models-and-transfer-learning" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="benefits-of-using-pre-trained-models-and-transfer-learning"><strong>Benefits of using pre-trained models and transfer learning</strong></h4>
<ul>
<li><strong>Less data needed</strong>: You can often get good results with much smaller datasets for your specific task because the model already has a head start.</li>
<li><strong>Faster development &amp; training</strong>: Training takes less time because you’re training fewer parameters (feature extraction) or only tuning existing ones for a shorter period (fine-tuning).</li>
<li><strong>Better performance</strong>: Often achieves higher accuracy than training a model from scratch, especially with limited data, because the pre-trained features provide a better starting point.</li>
<li><strong>Accessibility</strong>: Makes powerful deep learning accessible even without massive computational resources or huge datasets. Democratizes AI!</li>
</ul>
</section>
<section id="general-guidelines-for-using-pre-trained-models-and-transfer-learning" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="general-guidelines-for-using-pre-trained-models-and-transfer-learning"><strong>General guidelines for using pre-trained models and transfer learning</strong></h4>
<ul>
<li><p>The pre-trained model’s task should be similar to your task (e.g., ImageNet models for classifying different types of flowers, medical images, or satellite images; BERT for sentiment analysis or question answering; etc.)</p></li>
<li><p>You have limited data for your task</p></li>
<li><p>You have limited computational resources</p></li>
<li><p>Be cautious when:</p>
<ul>
<li>Your task is very different from the pre-trained model’s task (e.g., using an ImageNet model for audio analysis - the learned features might not be relevant)</li>
<li>You have a massive, high-quality dataset for your task (training from scratch might eventually yield better results, but transfer learning can still be a good starting point!)</li>
<li>The input data format is completely different</li>
</ul></li>
</ul>
</section>
<section id="how-to-implement-transfer-learning-conceptual" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="how-to-implement-transfer-learning-conceptual"><strong>How to implement transfer learning? (Conceptual)</strong></h4>
<ol type="1">
<li><strong>Choose a pre-trained model</strong>: Select one appropriate for your data type (image, text) and task complexity (e.g., MobileNet for speed, ResNet for accuracy)</li>
<li><strong>Load the model</strong>: Use libraries like PyTorch, TensorFlow/Keras, or Hugging Face Transformers. They make it easy!</li>
<li><strong>Adapt the model</strong>: Customize the model to your specific task (e.g., replace the final classification layer). See <a href="#model-customization">Model customization</a> for more details.</li>
<li><strong>Choose strategy</strong>: Decide between feature extraction, fine-tuning, or a hybrid approach (freezing some layers and fine-tuning others).</li>
<li><strong>Train the model</strong>: Train the adaptable parts of the model on your data.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We will cover practical implementation of transfer learning later.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>When performing transfer learning, what is the key difference between the <em>feature extraction</em> approach and the <em>fine-tuning</em> approach?</p>
<p><em>Hint: Whether the weights of the original pre-trained layers are kept frozen or allowed to be updated.</em></p>
</div>
</div>
</section>
</section>
<section id="model-customization-for-transfer-learning" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="model-customization-for-transfer-learning"><span class="header-section-number">6.3.4</span> Model Customization (for Transfer Learning)</h3>
<p>When applying transfer learning, we rarely use the pre-trained model exactly as is. We need to customize it to fit our specific data and task. Conceptually, we can think of the model as having three main parts to consider for customization, as shown in the diagram: <strong>Input Adaptation</strong>, the <strong>Feature Extractor</strong>, and <strong>Output Adaptation</strong>. <img src="../images/dl-pytorch/models.png" class="img-fluid"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this section, you just need to get a sense of what different parts of the model are for. We will cover the customization in practice in the later sections.</p>
</div>
</div>
<section id="input-adaptation-data-preprocessing" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="input-adaptation-data-preprocessing"><strong>Input Adaptation (Data Preprocessing)</strong></h4>
<ul>
<li><p><strong>What it is</strong>: This involves preparing your input data so that it’s compatible with the feature extractor. Pre-trained models have specific expectations about their input.</p></li>
<li><p><strong>Why needed?</strong>: Your data might have a different size, format, or distribution than the data the model was originally trained on.</p></li>
<li><p><strong>Customization/Actions</strong>: This is typically handled during data preprocessing before feeding data to the model:</p>
<ul>
<li><strong>Images</strong>: Resizing to the expected input dimensions (e.g., 224x224 pixels), ensuring the correct number of color channels (e.g., converting grayscale to RGB if needed), normalizing pixel values using the same mean and standard deviation the original model was trained with.</li>
<li><strong>Text</strong>: Using the exact same tokenizer the pre-trained model used, converting text to numerical IDs, padding or truncating sequences to a consistent length expected by the model.</li>
<li><strong>Model</strong>: You may need to adjust the model’s input layer to accept your data type (e.g., adding a new channel dimension for images, or using a different tokenizer for text).</li>
</ul></li>
</ul>
<p>Ensuring input compatibility is crucial for the pre-trained features to be meaningful.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/input-adaptation.png" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption>Input adaptation example. (a) Original 6-band input architecture. (b) Adapting 3-band(RGB) data to 6-band input by zero-padding the extra bands. (c) Adapting 3-band(RGB) data to 6-band input by duplicating the RGB bands. (d) Adapting the input layer to accept 3-band(RGB) data. Source: <a href="https://www.tandfonline.com/doi/abs/10.1080/13658816.2024.2397441">Hsu et al., 2024</a></figcaption>
</figure>
</div>
</section>
<section id="feature-extractor-the-pre-trained-basebody" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="feature-extractor-the-pre-trained-basebody"><strong>Feature Extractor (The Pre-trained Base/Body)</strong></h4>
<ul>
<li><p><strong>What it is</strong>: This is the main body of the pre-trained model, containing most of the layers (e.g., convolutional layers in ResNet, transformer layers in BERT).</p></li>
<li><p><strong>Its knowledge</strong>: It holds the rich, general-purpose features learned from the original large dataset (like visual patterns or language structures). This is the core knowledge we want to transfer.</p></li>
<li><p><strong>Customization</strong>: The main decision here is whether to keep its weights fixed (frozen) or allow them to be updated (unfrozen) during training on your new task.</p>
<ul>
<li><strong>Frozen (Feature Extraction strategy)</strong>: The weights are not changed. The model acts purely as a fixed feature extractor. This is safer with small datasets.</li>
<li><strong>Unfrozen (Fine-tuning strategy)</strong>: Some or all layers (usually the later ones) are allowed to update their weights slightly, adapting the learned features more closely to your specific task. Requires more data and careful training (low learning rate!).</li>
</ul></li>
</ul>
</section>
<section id="output-adaptation-the-task-specific-head" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="output-adaptation-the-task-specific-head"><strong>Output Adaptation (The Task-Specific Head)</strong></h4>
<ul>
<li><p><strong>What it is</strong>: This involves modifying the final part of the model so it produces the correct output format for your specific task.</p></li>
<li><p><strong>Why needed?</strong>: The original model’s final layer(s) were designed for its task (e.g., classifying 1000 ImageNet categories). Your task (e.g., classifying 2 categories, predicting a single value) requires a different output structure.</p></li>
<li><p><strong>Customization/Actions</strong>:</p>
<ul>
<li><strong>Remove the old head</strong>: Discard the original model’s final classification layer(s).</li>
<li><strong>Add a new head</strong>: Attach one or more new layers tailored to your task. This new head must be trained on your data.</li>
<li><strong>Example (Classification)</strong>: Add a fully connected layer where the number of output units equals the number of your classes, often followed by a suitable activation function (like softmax or sigmoid).</li>
<li><strong>Example (Regression)</strong>: Add a fully connected layer with a single output unit (or multiple if predicting multiple values).</li>
<li><strong>Example (Instance segmentation)</strong>: Add Mask R-CNN <a href="#fn86" class="footnote-ref" id="fnref86" role="doc-noteref"><sup>86</sup></a> head for predicting masks, bounding boxes, and class labels.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/output-adaptation.png" class="img-fluid figure-img"></p>
<figcaption>Output adaptation example. The dashed box indicates the output adaptation part (task-specific head). They are heads for classification (left) and instance segmentation (right) respectively.</figcaption>
</figure>
</div>
</section>
<section id="considerations-for-model-customization" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="considerations-for-model-customization"><strong>Considerations for model customization</strong></h4>
<ul>
<li><strong>Task Similarity</strong>: How close is your task to the original pre-training task? Very similar tasks might only need output adaptation (feature extraction). Dissimilar tasks might benefit more from fine-tuning the feature extractor too.</li>
<li><strong>Data Amount</strong>: With little data, stick closer to feature extraction (freeze the base) to prevent overfitting. With more data, fine-tuning becomes a viable option to potentially gain more performance.</li>
<li><strong>Resources</strong>: Feature extraction is computationally cheaper than fine-tuning.</li>
<li><strong>Experimentation is key</strong>: Often, the best approach involves trying different levels of fine-tuning (e.g., unfreezing just the last block vs.&nbsp;multiple blocks) and seeing what works best for your specific problem.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>If you’re adapting a pre-trained image model for a new classification task with 5 classes, what part of the model absolutely must you change or replace?</li>
</ul>
<p><em>Hint: The Output Adaptation / final classification layer/head.</em></p>
<ul>
<li>Why is it generally important to use the same normalization (mean/standard deviation) for your input images as was used to train the original pre-trained model?</li>
</ul>
<p><em>Hint: The learned features expect data in that specific numerical range/distribution.</em></p>
</div>
</div>
</section>
</section>
</section>
<section id="loss-functions" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="loss-functions"><span class="header-section-number">6.4</span> Loss Functions</h2>
<p>Okay, we have our <strong>data</strong> (inputs and desired outputs) and a <strong>model</strong> (a set of possible functions). But how do we know if a specific function chosen by the model is actually any good? How do we measure its mistakes? That’s where the <strong>Loss Function</strong> comes in.</p>
<p><em>A loss function (also called a cost function or objective function) evaluates how well our model’s predictions match the actual target values (the ground truth) from our data. It calculates a single number, the “loss,” which represents the “cost” or “error” of the model’s current performance.</em></p>
<p><strong>Think of it like this</strong>:</p>
<ul>
<li>Imagine you’re learning to shoot arrows at a target. The loss function is like a scoring system.</li>
<li>An arrow hitting the bullseye gets a score of 0 (no error, perfect prediction).</li>
<li>Arrows landing further away get higher scores (higher error, worse prediction).</li>
<li>Your goal is to adjust your aim (train the model) to get the lowest score possible (minimize the loss).</li>
</ul>
<p><strong>The core process</strong>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/loss-function-process.png" class="img-fluid figure-img"></p>
<figcaption>Loss function process</figcaption>
</figure>
</div>
<ol type="1">
<li>The model takes an input from your data.</li>
<li>It makes a prediction.</li>
<li>The loss function compares this prediction to the true target value (the label).</li>
<li>It outputs a loss score:
<ul>
<li><strong>Low Loss</strong>: Prediction is close to the target. Good!</li>
<li><strong>High Loss</strong>: Prediction is far from the target. Bad!</li>
</ul></li>
</ol>
<p>This loss score is crucial because it provides a signal for how the model needs to adjust itself during training. The overall goal of training is to find the model parameters (weights and biases) that minimize the average loss across the entire training dataset.</p>
<section id="common-loss-functions" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="common-loss-functions"><span class="header-section-number">6.4.1</span> Common loss functions</h3>
<p>The choice of loss function depends heavily on the type of task your model is performing. The two most common types are <strong>Regression</strong> (predicting numbers) and <strong>Classification</strong> (predicting categories).</p>
<section id="loss-functions-for-regression-tasks-predicting-numbers" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="loss-functions-for-regression-tasks-predicting-numbers"><strong>1. Loss Functions for Regression Tasks (Predicting Numbers)</strong></h4>
<p>Here, we’re predicting continuous values (like temperature, house prices, etc.). The loss measures the <em>distance</em> between the predicted and actual values.</p>
<ul>
<li><p><strong>Mean Squared Error (MSE)</strong>:</p>
<ul>
<li><strong>What it does</strong>: Calculates the <em>average</em> of the <em>squared</em> differences between predicttions (<span class="math inline">\(y_{pred}\)</span>) and true values (<span class="math inline">\(y_{true}\)</span>).</li>
<li><strong>Formula</strong>: <span class="math inline">\(L_{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_{pred,i} - y_{true,i})^2\)</span> (<span class="math inline">\(n\)</span> is the number of samples).</li>
<li><strong>Intuition</strong>: Squaring the difference (<span class="math inline">\(y_{pred} - y_{true}\)</span>) has a big impact. Small errors become tiny, but large errors become <em>huge</em>. This means MSE really hates large errors and pushes the model hard to avoid them. It’s quite sensitive to outliers (data points that are wildly different from the rest).</li>
</ul></li>
<li><p><strong>Mean Absolute Error (MAE)</strong>:</p>
<ul>
<li><strong>What it does</strong>: Calculates the <em>average</em> of the <em>absolute</em> differences between predicttions and true values.</li>
<li><strong>Formula</strong>: <span class="math inline">\(L_{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_{pred,i} - y_{true,i}|\)</span> (<span class="math inline">\(n\)</span> is the number of samples).</li>
<li><strong>Intuition</strong>: Takes the absolute value <span class="math inline">\(|y_{pred} - y_{true}|\)</span>. It doesn’t dramatically penalize large errors like MSE does. The makes MAE less sensitive to outlier.</li>
</ul></li>
</ul>
<p><strong>Visualization of MSE vs.&nbsp;MAE</strong></p>
<p>Hover over the plot to see the loss values for different error magnitudes.</p>
<p>Some insights:</p>
<ul>
<li>MSE grows quadratically (<span class="math inline">\(|error|^2\)</span>) with error magnitude while MAE grows linearly (<span class="math inline">\(|error|\)</span>).</li>
<li>MSE penalizes larger errors much more severely than MAE.</li>
<li>MAE is more robust to outliers than MSE.</li>
<li>For small error magnitudes, MAE and MSE are almost identical.</li>
<li>MSE has nicer mathematical properties (differentiable everywhere)</li>
</ul>
<div id="vis-mse-mae"></div>
<script type="module" src="../_resources/js/vis-mse-mae.js"></script>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Imagine you’re predicting house prices. Your dataset mostly has houses between $100k and $500k, but there are a couple of mega-mansions listed at $10 million (outliers!). If you want your model to find a good general trend without being overly skewed by those mansions, would MSE or MAE be a safer first choice for your loss function? Why?</p>
<p><em>(Hint: Think about how each function treats those potentially huge errors).</em></p>
</div>
</div>
</section>
<section id="loss-functions-for-classification-tasks-predicting-categories" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="loss-functions-for-classification-tasks-predicting-categories"><strong>2. Loss Functions for Classification Tasks (Predicting Categories)</strong></h4>
<p>Here, we’re assigning inputs to distinct categories (e.g., “dog” vs.&nbsp;“cat”, “spam” vs.&nbsp;“not spam”, “digit 0-9”). Loss functions here often work with probabilities. The model usually outputs a probability for each possible class, and the loss function checks how well these probabilities align with the true class.</p>
<p><strong>The Go-To: Cross-Entropy Loss (Log Loss)</strong></p>
<p>Cross-Entropy sounds fancy, but the core idea is simple: <strong>it measures how different the model’s predicted probability distribution is from the true probability distribution</strong>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definitions
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>True Probability Distribution</strong>: For a given input, the correct class has a probability of 1 (or 100%), and all other classes have a probability of 0.</li>
<li><strong>Model’s Estimated Probability Distribution</strong>: The model outputs its <em>estimated</em> probabilities for each class (e.g., 70% cat, 25% dog, 5% bird).</li>
<li><strong>Cross-Entropy</strong>: Calculates a loss based on how much probability the model assigned to the correct class. It heavily penalizes models that are confidently wrong (e.g., predicting 95% “dog” when it’s actually a “cat”).</li>
</ul>
</div>
</div>
<ul>
<li><p><strong>Binary Cross-Entropy (BCE Loss)</strong></p>
<ul>
<li><p><strong>Used For</strong>: Binary classification (only two classes, like 0/1, True/False, Spam/Not Spam).</p></li>
<li><p><strong>Works with</strong>: Models whose final layer outputs a single probability (usually using a <strong>Sigmoid</strong> activation function). This probability typically represents the likelihood of belonging to the “positive” class (class 1).</p></li>
<li><p><strong>Formula</strong>: (Don’t stress over memorizing this now, focus on when to use it!) <span class="math inline">\(L_{BCE} = - \frac{1}{n} \sum_{i=1}^{n} [y_{true}^{(i)} \log(p_{pred}^{(i)}) + (1 - y_{true}^{(i)}) \log(1 - p_{pred}^{(i)})]\)</span> (where <span class="math inline">\(p_{pred}\)</span> is the predicted probability for the positive class, and <span class="math inline">\(y_{true}\)</span> is 0 or 1).</p></li>
<li><p><strong>Intuition</strong>: Compares the model’s predicted probability (e.g., 0.8 probability of being spam) to the true label (1 if it is spam, 0 if it’s not). The closer the prediction is to the true label, the lower the loss. The logarithm part is key to heavily penalizing confident wrong answers.</p></li>
</ul></li>
<li><p><strong>Categorical Cross-Entropy (CCE Loss)</strong></p>
<ul>
<li><p><strong>Used For</strong>: Multi-class classification (more than two classes, e.g., classifying images into “dog”, “cat”, “bird”, “fish”).</p></li>
<li><p><strong>Works With</strong>: Models whose final layer outputs a probability distribution across all classes (usually using a <strong>Softmax</strong> activation function). These probabilities all sum up to 1.</p></li>
<li><p><strong>Formula Concept</strong>: (Again, focus on the application!) <span class="math inline">\(L_{CCE} = - \frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{C} y_{true}^{(i,j)} \log(p_{pred}^{(i,j)})\)</span> (where C is the number of classes, <span class="math inline">\(y_{true}^{(i,j)}\)</span> is 1 for the true class j and 0 otherwise, and <span class="math inline">\(p_{pred}^{(i,j)}\)</span> is the predicted probability for class j).</p></li>
<li><p><strong>Intuition</strong>: Compares the model’s predicted probability list (e.g., [0.1 dog, 0.7 cat, 0.1 bird, 0.1 fish]) to the true list (which would be [0, 1, 0, 0] if it’s actually a cat, often called one-hot encoding). It gives a lower loss if the highest probability is assigned to the correct class.</p></li>
<li><p><strong>Example Walkthrough</strong>:</p>
<ul>
<li>Task: Classify an image into one of 3 classes (cat, dog, bird).</li>
<li>True Label: It’s a ‘cat’. The target output (one-hot encoded) is [1, 0, 0].</li>
<li>Model Prediction (Probabilities): The model outputs [0.7, 0.2, 0.1] (70% chance it’s a cat, 20% dog, 10% bird).</li>
<li>CCE Loss Calculation: It primarily focuses on the probability assigned to the correct class. Using the formula, the loss involves <span class="math inline">\(−log(0.7)\)</span>, which is about 0.36.</li>
</ul></li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Using the Categorical Cross-Entropy example above (True label [1, 0, 0]):</p>
<ol type="1">
<li>What if the model was more confident and correct, predicting [0.95, 0.03, 0.02]? Would the calculated CCE loss be higher or lower than the original loss (which was ~0.36)?</li>
<li>What if the model was confident but wrong, predicting [0.1, 0.8, 0.1] (thinking it’s a dog)? Would the CCE loss be higher or lower than ~0.36?</li>
</ol>
<p><em>(Hint: Think about the −log(p) part. What happens to −log(p) when the probability p for the correct class gets closer to 1? What happens when it gets closer to 0?)</em></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Key Takeaway on Cross-Entropy</strong>: Don’t let the formulas intimidate you! Just remember: Cross-Entropy is the standard, effective way to measure error for classification tasks. It pushes the model to assign high probability to the correct answer and low probability to the wrong answers.</p>
</div>
</div>
</section>
</section>
<section id="beyond-the-basics-more-tools-in-the-toolbox" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="beyond-the-basics-more-tools-in-the-toolbox"><span class="header-section-number">6.4.2</span> Beyond the Basics: More Tools in the Toolbox</h3>
<p>While MSE, MAE, and Cross-Entropy are your workhorses, many other loss functions exist for specialized situations. You don’t need to know them all starting out, but it’s good to be aware they exist!</p>
<ul>
<li><p><strong>For Regression with Outliers</strong>:</p>
<ul>
<li><strong>Huber Loss</strong> <a href="#fn87" class="footnote-ref" id="fnref87" role="doc-noteref"><sup>87</sup></a> <a href="#fn88" class="footnote-ref" id="fnref88" role="doc-noteref"><sup>88</sup></a>: A hybrid of MSE (for small errors) and MAE (for large errors). Tries to get the best of both worlds – less sensitive to outliers than MSE, but still smooth around the minimum.</li>
<li><strong>Log-Cosh Loss</strong> <a href="#fn89" class="footnote-ref" id="fnref89" role="doc-noteref"><sup>89</sup></a>: Another smooth function that acts like MSE for small errors but is less steep for large ones, offering some robustness.</li>
</ul></li>
<li><p><strong>For Classification</strong>:</p>
<ul>
<li><strong>Hinge Loss</strong> <a href="#fn90" class="footnote-ref" id="fnref90" role="doc-noteref"><sup>90</sup></a> <a href="#fn91" class="footnote-ref" id="fnref91" role="doc-noteref"><sup>91</sup></a>: Often used with Support Vector Machines (SVMs). Cares about whether the prediction is correctly classified with a certain margin of confidence.</li>
</ul></li>
<li><p><strong>For Imbalanced Data</strong> (where some classes are much rarer than others):</p>
<ul>
<li><strong>Weighted Cross-Entropy</strong> <a href="#fn92" class="footnote-ref" id="fnref92" role="doc-noteref"><sup>92</sup></a>: Gives more importance (weight) to errors made on the rare classes.</li>
<li><strong>Focal Loss</strong> <a href="#fn93" class="footnote-ref" id="fnref93" role="doc-noteref"><sup>93</sup></a> <a href="#fn94" class="footnote-ref" id="fnref94" role="doc-noteref"><sup>94</sup></a>: Modifies cross-entropy to focus training more on hard-to-classify examples, often helping with both imbalance and distinguishing tricky classes.</li>
</ul></li>
<li><p><strong>For Specific Tasks</strong>:</p>
<ul>
<li><strong>CTC Loss</strong> <a href="#fn95" class="footnote-ref" id="fnref95" role="doc-noteref"><sup>95</sup></a> <a href="#fn96" class="footnote-ref" id="fnref96" role="doc-noteref"><sup>96</sup></a>: Used in sequence tasks like speech recognition where the exact alignment between input audio and output text isn’t known.</li>
<li><strong>Dice Loss</strong> <a href="#fn97" class="footnote-ref" id="fnref97" role="doc-noteref"><sup>97</sup></a> / <strong>IoU Loss</strong> <a href="#fn98" class="footnote-ref" id="fnref98" role="doc-noteref"><sup>98</sup></a>: Common in computer vision for image segmentation and object detection, directly measuring the overlap between predicted regions and true regions.</li>
</ul></li>
<li><p><strong>For Multi-Task Learning</strong>: Sometimes a model needs to do several things at once (e.g., classify an object and draw a box around it). You might combine multiple loss functions.</p></li>
</ul>
</section>
<section id="choosing-your-loss-function-practical-tips" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="choosing-your-loss-function-practical-tips"><span class="header-section-number">6.4.3</span> Choosing Your Loss Function: Practical Tips</h3>
<ol type="1">
<li><p><strong>Start Standard</strong>: Use MSE or MAE for regression, and Binary/Categorical Cross-Entropy for classification. These cover most common scenarios.</p></li>
<li><p><strong>Match Your Output</strong>: What does your model’s final layer produce?</p>
<ul>
<li>A single number? -&gt; Regression (MSE, MAE).</li>
<li>A single probability (0 to 1)? -&gt; Binary Classification (BCE Loss).</li>
<li>Probabilities for multiple classes (summing to 1)? -&gt; Multi-class Classification (CCE Loss).</li>
</ul></li>
<li><p><strong>Consider Your Data/Problem</strong>: Do you have significant outliers? (Maybe lean MAE/Huber over MSE). Is your classification data heavily imbalanced? (Maybe look into Weighted CE or Focal Loss later).</p></li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Your goal is to predict house prices (a single dollar value). Which loss function would Tip #1 and #2 suggest you start with? Now, if you know your dataset includes a few multi-million dollar mansions (outliers) mixed in with mostly average homes, which tip becomes relevant, and what might you consider?”</p>
</div>
</div>
</section>
<section id="watching-the-score-training-loss-and-validation-loss" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4" class="anchored" data-anchor-id="watching-the-score-training-loss-and-validation-loss"><span class="header-section-number">6.4.4</span> Watching the Score: Training Loss and Validation Loss</h3>
<p>As your model trains, you need to track its performance not just on the data it’s learning from (<strong>Training Set</strong>) but also on data it hasn’t seen before (<strong>Validation Set</strong>).</p>
<ul>
<li><strong>Training Loss</strong>: Calculated on the data the model is currently training on. We expect this to go down steadily as the model learns.</li>
<li><strong>Validation Loss</strong>: Calculated on the separate validation dataset (the model doesn’t learn from this data, it’s just used for evaluation). This tells us how well the model is generalizing to new, unseen examples.</li>
</ul>
<p>Plotting both losses over time gives you crucial insights into how the training is going (click the image to see the full size):</p>
<p><a href="../images/dl-pytorch/loss_curves.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="../images/dl-pytorch/loss_curves.png" class="img-fluid" style="width:100.0%"></a></p>
<ul>
<li><p><strong>The Dream Scenario (Good Fit)</strong>: Both training loss and validation loss decrease and flatten out at a low value. Your model is learning useful patterns and generalizing well! 🎉</p></li>
<li><p><strong>The Memorizer (Overfitting)</strong>: Training loss keeps going down, but validation loss starts to creep back up (or stays high while training loss plummets). The model has learned the training data too well, including its noise and quirks. It fails to perform well on new data. Uh oh! 😟</p>
<ul>
<li><em>Common Fixes</em>: Get more diverse data (augmentation), simplify the model, add regularization (like Dropout or L1/L2), stop training earlier (early stopping).</li>
</ul></li>
<li><p><strong>The Slacker (Underfitting)</strong>: Both training and validation loss remain high or decrease very slowly to a high value. The model isn’t complex enough, hasn’t trained long enough, or isn’t structured well enough to capture the underlying patterns in the data. 😴</p>
<ul>
<li><em>Common Fixes</em>: Train longer, use a more complex model (more layers/neurons), engineer better input features, try a different model architecture, reduce regularization if it’s too strong.</li>
</ul></li>
</ul>
<p>Monitoring these curves is like checking your car’s dashboard while driving – it helps you understand what’s happening and make necessary adjustments!</p>
</section>
<section id="the-final-piece-minimizing-loss-is-the-name-of-the-game" class="level3" data-number="6.4.5">
<h3 data-number="6.4.5" class="anchored" data-anchor-id="the-final-piece-minimizing-loss-is-the-name-of-the-game"><span class="header-section-number">6.4.5</span> The Final Piece: Minimizing Loss is the Name of the Game</h3>
<p>No matter which loss function you choose, the fundamental goal of the training process remains the same: <strong>adjust the model’s internal parameters (weights and biases) to make the calculated loss as low as possible.</strong></p>
<p>How does the model actually do that adjustment? That’s the job of the <strong>Optimization Algorithm</strong> (like Gradient Descent), which uses the output of the loss function to figure out which way to tweak the parameters. We’ll cover optimizers next!</p>
</section>
</section>
<section id="optimization-algorithms" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="optimization-algorithms"><span class="header-section-number">6.5</span> Optimization Algorithms</h2>
<p>In the previous section, we’ve figured out how to measure our model’s mistakes using a <strong>Loss Function</strong>. We know a lower loss score means the model is doing better. But how does the model actually learn from these mistakes and improve? How does it adjust its internal knobs (parameters like weights and biases) to reach that coveted low-loss state? &nbsp;</p>
<p>That’s where <strong>Optimization Algorithms</strong> step in! They are the engines that drive the learning process.</p>
<p><em>An optimization algorithm uses the information from the loss function to systematically update the model’s parameters in a way that minimizes the loss.</em></p>
<p><strong>Think of it like this:</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/serrano/v-14/Figures/Bimage001.png" class="img-fluid figure-img"></p>
<figcaption>Descending the loss landscape. Source:<a href="https://livebook.manning.com/book/grokking-machine-learning/appendix-b/v-14/13">Grokking Machine Learning</a></figcaption>
</figure>
</div>
<ul>
<li>Imagine you’re on a foggy mountain (<strong>the loss landscape</strong>), and you want to get to the lowest point in the valley (<strong>minimum loss</strong>).</li>
<li>You can only feel the slope of the ground right under your feet (<strong>the gradient</strong> calculated from the loss function).</li>
<li>The optimization algorithm is your strategy for taking steps downhill based on that slope, hoping to eventually reach the bottom.</li>
</ul>
<section id="the-core-idea-gradient-descent" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="the-core-idea-gradient-descent"><span class="header-section-number">6.5.1</span> The Core Idea: Gradient Descent</h3>
<p>The most fundamental optimization algorithm is <strong>Gradient Descent</strong>. Its strategy is beautifully simple:</p>
<ol type="1">
<li><strong>Calculate the Slope</strong>: Figure out the slope (the <strong>gradient</strong>) of the loss landscape at your current position (current model parameters). The gradient tells you the direction of the steepest ascent (uphill).</li>
<li><strong>Take a Step Downhill</strong>: Take a small step in the exact opposite direction of the gradient (downhill). This step adjusts the model’s parameters.</li>
<li><strong>Repeat</strong>: Keep calculating the gradient and taking steps downhill until you can’t go any lower (you’ve hopefully reached a minimum).</li>
</ol>
<p>The “size” of the step you take is controlled by a crucial parameter called the <strong>Learning Rate</strong>.</p>
<ul>
<li><p><strong>Learning Rate (α)</strong>: This small number determines how big of a step you take downhill.</p>
<ul>
<li><strong>Too Large</strong>: You might overshoot the minimum, bouncing around wildly or even going uphill!</li>
<li><strong>Too Small</strong>: You’ll take tiny, tiny steps, and it might take forever to reach the bottom (or get stuck). Finding a good learning rate is often a key part of training a model effectively.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png" class="img-fluid figure-img"></p>
<figcaption>Descending the loss landscape with different learning rates. Source: <a href="https://www.jeremyjordan.me/nn-learning-rate/">Jeremy Jordan</a></figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>A good article on how to choose a good learning rate: <a href="https://www.jeremyjordan.me/nn-learning-rate/">Jeremy Jordan: How to choose a good learning rate</a></p>
</div>
</div>
</section>
<section id="how-gradients-are-calculated-backpropagation-the-short-story" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="how-gradients-are-calculated-backpropagation-the-short-story"><span class="header-section-number">6.5.2</span> How Gradients are Calculated: Backpropagation (The Short Story)</h3>
<p>You might wonder, “How does the algorithm calculate this ‘slope’ or gradient across potentially millions of parameters in a deep network?” The answer is a clever and fundamental process called <strong>Backpropagation</strong>.</p>
<p>Don’t worry about the deep math right now! The key is to understand the concept of how the error signal flows backward through the network. Conceptually, backpropagation works like this:</p>
<ol type="1">
<li><p><strong>Forward Pass</strong>: Input data goes forward through the network, layer by layer, using the current parameters (weights and biases) to eventually produce an output prediction.</p></li>
<li><p><strong>Calculate Loss</strong>: The loss function compares the model’s prediction to the true target value, calculating the overall error (the loss score).</p></li>
<li><p><strong>Backward Pass</strong>: This is where the magic happens! The error signal is propagated <em>backward</em> from the output layer through the network towards the input layer. Using the chain rule from calculus (essentially, a way to calculate derivatives of composite functions), the algorithm efficiently calculates how much each individual parameter (every weight and bias in the network) contributed to the final error. This calculated contribution for each parameter <em>is</em> its gradient.</p></li>
</ol>
<p>This efficient process gives us the gradient for all parameters, telling the optimization algorithm exactly which way is “downhill” (i.e., how to adjust each specific knob) to reduce the loss on the next iteration.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>What problem does backpropagation solve for us during training?</p>
<p><em>Hint: It efficiently calculates the gradient of the loss with respect to <strong>all</strong> model parameters.</em></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you’re interested in a more visual or in-depth understanding of backpropagation without heavy math, resources like <a href="https://www.youtube.com/watch?v=Ilg3gGewQ5U">3Blue1Brown’s videos</a> or blog posts on sites like <a href="https://colah.github.io/posts/2015-08-Backprop/">colah’s blog</a> can be very helpful.</p>
</div>
</div>
</section>
<section id="flavors-of-gradient-descent-handling-the-data" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3" class="anchored" data-anchor-id="flavors-of-gradient-descent-handling-the-data"><span class="header-section-number">6.5.3</span> Flavors of Gradient Descent: Handling the Data</h3>
<p>Calculating the gradient using <em>every single data point</em> in your training set for <em>every single step</em> can be very computationally expensive, especially with large datasets. This leads to different variations of Gradient Descent:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FXHp55rpDM0tkaD5oz3Dvg.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Gradient Descent Variants. BGD: smooth, direct. SGD: very noisy, zig-zagging. Mini-Batch: moderate, balanced. Source: <a href="https://medium.com/analytics-vidhya/gradient-descent-vs-stochastic-gd-vs-mini-batch-sgd-fbd3a2cb4ba4">Medium</a></figcaption>
</figure>
</div>
<ul>
<li><p><strong>Batch Gradient Descent (BGD) <a href="#fn99" class="footnote-ref" id="fnref99" role="doc-noteref"><sup>99</sup></a>:</strong></p>
<ul>
<li><strong>How</strong>: Calculates the gradient using the entire training dataset for each parameter update.</li>
<li><strong>Pros</strong>: Gives a very accurate estimate of the true gradient. Steps are stable and directly towards the minimum.</li>
<li><strong>Cons</strong>: Extremely slow and memory-intensive for large datasets. Doesn’t allow for online learning (updating as new data arrives).</li>
</ul></li>
<li><p><strong>Stochastic Gradient Descent (SGD) <a href="#fn100" class="footnote-ref" id="fnref100" role="doc-noteref"><sup>100</sup></a> <a href="#fn101" class="footnote-ref" id="fnref101" role="doc-noteref"><sup>101</sup></a> <a href="#fn102" class="footnote-ref" id="fnref102" role="doc-noteref"><sup>102</sup></a>:</strong></p>
<ul>
<li><strong>How</strong>: Calculates the gradient and updates parameters using only one randomly chosen training example at a time.</li>
<li><strong>Pros</strong>: Much faster updates. Requires less memory. Can escape shallow local minima more easily due to the noisy steps. Allows for online learning.</li>
<li><strong>Cons</strong>: Updates are very noisy and bounce around a lot. The path towards the minimum is erratic, and it might never settle perfectly at the minimum. Often requires careful tuning of the learning rate.</li>
</ul></li>
<li><p><strong>Mini-Batch Gradient Descent <a href="#fn103" class="footnote-ref" id="fnref103" role="doc-noteref"><sup>103</sup></a>:</strong></p>
<ul>
<li><strong>How</strong>: Calculates the gradient and updates parameters using a small, random subset (a “mini-batch”) of the training data (e.g., 32, 64, 128 samples) at each step.</li>
<li><strong>Pros</strong>: The Goldilocks solution! Balances the stability of BGD with the speed and efficiency of SGD. Takes advantage of hardware optimizations for matrix calculations. Reduces noise compared to SGD.</li>
<li><strong>Cons</strong>: Introduces another hyperparameter (batch size) to tune. Still has some noise compared to BGD.</li>
<li><strong>Status</strong>: This is the most common approach used in deep learning today. When people just say “SGD”, they often actually mean Mini-Batch SGD.</li>
</ul></li>
</ul>
<section id="batch-size" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="batch-size"><strong>Batch Size</strong></h4>
<p>When using Mini-Batch Gradient Descent (explained next), this is the number of training samples used in one iteration (one parameter update).</p>
<ul>
<li><em>Smaller batch sizes</em>: Lead to faster updates (more updates per epoch) but can be noisier (less stable gradient estimate). Can sometimes help escape local minima. Require less memory per step.</li>
<li><em>Larger batch sizes</em>: Provide smoother, more stable updates (more accurate gradient estimate) but require more computational resources (memory) per step and take longer per update. Can sometimes get stuck in sharper minima which might generalize less well.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Imagine training on a dataset with 1,000,000 images.</p>
<ul>
<li>How many gradient calculations does Batch GD do per update?</li>
<li>How many does Stochastic GD (SGD) do per update?</li>
<li>If you use Mini-Batch GD with a batch size of 100, how many gradient calculations (considering the batch average) lead to one parameter update? Which approach seems like the most practical compromise between accuracy and speed?</li>
</ul>
</div>
</div>
</section>
</section>
<section id="challenges-on-the-way-down" class="level3" data-number="6.5.4">
<h3 data-number="6.5.4" class="anchored" data-anchor-id="challenges-on-the-way-down"><span class="header-section-number">6.5.4</span> Challenges on the Way Down</h3>
<p>The journey to the minimum loss isn’t always smooth. Optimization algorithms face challenges navigating the complex loss landscape:</p>
<ul>
<li><strong>Learning Rate Selection</strong>: As mentioned, choosing the right learning rate is critical and often requires experimentation. If it’s too large, the algorithm might overshoot the minimum and diverge; if it’s too small, training can be incredibly slow. Techniques like learning rate schedules (gradually decreasing the learning rate during training) are common strategies to help manage this. We will cover this in the next section.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZC9qItK9wI0F6BwSVYMQGg.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Local minimum and saddle point. Source: <a href="https://medium.com/analytics-vidhya/journey-of-gradient-descent-from-local-to-global-c851eba3d367">Medium</a></figcaption>
</figure>
</div>
<ul>
<li><p><strong>Local Minima</strong>: The loss landscape might have many valleys. Simple gradient descent just follows the slope downwards from its starting point, so it might lead you to the bottom of a nearby, smaller valley (a <strong>local minimum</strong>) instead of finding the deepest valley overall (the <strong>global minimum</strong>). If the algorithm gets stuck in a local minimum, the model’s performance might be suboptimal.</p></li>
<li><p><strong>Saddle Points</strong>: Imagine the shape of a horse saddle – it curves up in one direction and down in another. A saddle point on the loss landscape is similar: the slope (gradient) might be zero or very close to zero, but it’s not actually a minimum. Gradient descent can slow down drastically or get stuck oscillating around these points because there isn’t a clear single direction downhill. <em>(In practice, especially for high-dimensional deep learning problems, saddle points can often be more problematic than local minima)</em>.</p></li>
</ul>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2018/05/patho.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2018/06/patho2-1.png" class="img-fluid"></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Visualization of ravine and gradient descent path. Source: <a href="https://www.digitalocean.com/community/tutorials/intro-to-optimization-momentum-rmsprop-adam">Digital Ocean</a></p>
</div>
</div>
</div>
<ul>
<li><strong>Ravines/Narrow Valleys</strong>: The landscape might feature long, narrow valleys where the slope is very steep <em>across</em> the valley but very gentle <em>along</em> the valley floor. Simple gradient descent can struggle here, oscillating back and forth across the steep walls instead of moving efficiently along the bottom of the valley towards the minimum. This oscillation can make convergence very slow.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What’s the potential problem with getting stuck in a ‘local minimum’?</li>
</ul>
<p><em>Hint: The model’s performance might be okay, but not the best possible, because parameters aren’t optimal.</em></p>
<ul>
<li>What does a ‘saddle point’ look like on the loss landscape? Why is it problematic for gradient descent?</li>
</ul>
<p><em>Hint: Flat in some directions, curved in others; gradient is zero or near-zero, so the optimizer doesn’t know where to go or moves very slowly.</em></p>
<ul>
<li>If you see your loss decreasing very slowly, but oscillating significantly between iterations, what landscape feature might be causing this?</li>
</ul>
<p><em>Hint: A narrow ravine or valley.</em></p>
</div>
</div>
</section>
<section id="getting-smarter-learning-rate-scheduling" class="level3" data-number="6.5.5">
<h3 data-number="6.5.5" class="anchored" data-anchor-id="getting-smarter-learning-rate-scheduling"><span class="header-section-number">6.5.5</span> Getting Smarter: Learning Rate Scheduling</h3>
<p>Manually finding the <em>perfect</em> fixed learning rate can be tough. Often, it’s beneficial to adjust the learning rate during training. This is called <strong>Learning Rate Scheduling</strong>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/learning-rate-scheduling.png" class="img-fluid figure-img"></p>
<figcaption>Learning Rate Scheduling Comparison</figcaption>
</figure>
</div>
<ul>
<li><strong>Fixed Scheduling</strong>: The simplest approach - maintain a constant learning rate throughout training. Easy, but might not be optimal.</li>
<li><strong>Step Decay</strong>: Reduce the learning rate by a certain factor at predefined epochs (e.g., cut it in half every 10 epochs). Allows for larger steps early on and finer adjustments later.</li>
<li><strong>Exponential Decay</strong>: Gradually decrease the learning rate exponentially over time. Provides a smoother decrease than step decay.</li>
<li><strong>Cyclical Learning Rates (CLR)</strong>: Vary the learning rate cyclically between a lower and upper bound. The idea is that periodically increasing the rate can help the model jump out of poor local minima or saddle points.</li>
</ul>
<p>Using a schedule often helps the model converge faster and reach a better final solution compared to a fixed learning rate.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Why is it often beneficial to decrease the learning rate gradually during training (like in Step or Exponential Decay)?”</li>
</ul>
<p><em>Hint: Allows larger steps early on when far from the minimum, and smaller, more precise steps later when closer).</em></p>
<ul>
<li>What is the potential advantage of periodically increasing the learning rate, as seen in Cyclical Learning Rates?</li>
</ul>
<p><em>Hint: May help the optimizer jump out of poor local minima or saddle points.</em></p>
</div>
</div>
</section>
<section id="getting-even-smarter-advanced-optimization-algorithms" class="level3" data-number="6.5.6">
<h3 data-number="6.5.6" class="anchored" data-anchor-id="getting-even-smarter-advanced-optimization-algorithms"><span class="header-section-number">6.5.6</span> Getting Even Smarter: Advanced Optimization Algorithms</h3>
<p>Because of these challenges, researchers developed more sophisticated optimization algorithms that often work better and require less manual tuning than basic SGD. They adapt the update rule based on the history of the gradients.</p>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif" class="img-fluid figure-img"></p>
<figcaption>Loss contours for different optimizers.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif" class="img-fluid figure-img"></p>
<figcaption>Saddle point for different optimizers.</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Source: <a href="https://www.ruder.io/optimizing-gradient-descent/">Ruder’s Blog</a></p>
</div>
</div>
</div>
<ul>
<li><p><strong>SGD with Momentum</strong> <a href="#fn104" class="footnote-ref" id="fnref104" role="doc-noteref"><sup>104</sup></a>:</p>
<ul>
<li><strong>Idea</strong>: Adds inertia to the updates. Imagine a ball rolling downhill – it accumulates momentum and doesn’t just stop instantly if the slope becomes flat or slightly uphill.</li>
<li><strong>How</strong>: It considers the direction of previous updates. This helps accelerate movement along consistent directions (down the valley floor) and dampens oscillations across narrow ravines.</li>
<li><strong>Benefit</strong>: Often converges faster than basic SGD and navigates tricky landscapes more effectively.</li>
</ul></li>
<li><p><strong>RMSprop (Root Mean Square Propagation)</strong> <a href="#fn105" class="footnote-ref" id="fnref105" role="doc-noteref"><sup>105</sup></a>:</p>
<ul>
<li><strong>Idea</strong>: Adapts the learning rate for each parameter individually. If a parameter’s gradient has been consistently large, it reduces its effective learning rate; if it’s been small, it increases it.</li>
<li><strong>How</strong>: Keeps a moving average of the squared gradients for each parameter. Divides the learning rate by the square root of this average.</li>
<li><strong>Benefit</strong>: Helps handle situations where different parameters need different step sizes. Good for non-stationary objectives (where the “shape” of the loss landscape changes).</li>
</ul></li>
<li><p><strong>Adam (Adaptive Moment Estimation)</strong> <a href="#fn106" class="footnote-ref" id="fnref106" role="doc-noteref"><sup>106</sup></a>:</p>
<ul>
<li><strong>Idea</strong>: The current rockstar! Combines the best of both worlds: Momentum + RMSprop.</li>
<li><strong>How</strong>: Keeps track of both the momentum (like SGD with Momentum) and the per-parameter scaling of gradients (like RMSprop).</li>
<li><strong>Benefit</strong>: Often works very well across a wide range of problems with relatively little tuning (though the default learning rate might still need adjustment). It’s frequently the default, go-to optimizer for many deep learning tasks.</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Here’s a great article on various optimization algorithms: <a href="https://www.ruder.io/optimizing-gradient-descent/">An overview of gradient descent optimization algorithms</a></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>While Adam is often a great starting point, sometimes simpler optimizers like SGD with Momentum can achieve slightly better final performance with careful tuning, especially in computer vision. Don’t be afraid to experiment!</p>
</div>
</div>
</section>
<section id="choosing-your-optimizer-practical-tips" class="level3" data-number="6.5.7">
<h3 data-number="6.5.7" class="anchored" data-anchor-id="choosing-your-optimizer-practical-tips"><span class="header-section-number">6.5.7</span> Choosing Your Optimizer: Practical Tips</h3>
<ol type="1">
<li><strong>Start with Adam</strong>: For most problems, Adam is a robust and effective choice to begin with. Use common default settings (e.g., learning rate of 0.001).</li>
<li><strong>Consider SGD with Momentum</strong>: If Adam isn’t giving optimal results, or if you’re working in a domain where SGD+Momentum is known to perform well (like some vision tasks), give it a try. It might require more careful learning rate tuning and scheduling.</li>
<li><strong>Experiment</strong>: The best optimizer and its settings (like learning rate, momentum parameters) can depend on the specific model architecture, dataset, and task. Monitoring your training and validation loss curves is key to diagnosing issues and guiding your choices.</li>
</ol>
<section id="now-we-have-all-the-core-building-blocks" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="now-we-have-all-the-core-building-blocks"><strong>Now we have all the core building blocks!</strong></h4>
<ul>
<li><strong>Data</strong>: The inputs and desired outputs. &nbsp;</li>
<li><strong>Model</strong>: The family of functions that can map inputs to outputs. &nbsp;</li>
<li><strong>Loss Function</strong>: Measures how bad the model’s predictions are. &nbsp;</li>
<li><strong>Optimization Algorithm</strong>: Adjusts the model’s parameters to minimize the loss. &nbsp;</li>
</ul>
<p>Together, these elements allow a computer to learn from data through a process called <strong>Training</strong>. Once trained, the model can be used to make predictions on new, unseen data, which is called <strong>Inference</strong>.</p>
</section>
</section>
</section>
<section id="training-and-inference" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="training-and-inference"><span class="header-section-number">6.6</span> Training and Inference</h2>
<p>We’ve now assembled all the core building blocks needed for deep learning: &nbsp;</p>
<ul>
<li><strong>Data</strong>: The fuel for learning (inputs and target outputs).</li>
<li><strong>Model</strong>: The potential functions we can learn.</li>
<li><strong>Loss Function</strong>: The scorekeeper measuring model errors.</li>
<li><strong>Optimization Algorithm</strong>: The engine that adjusts the model to reduce errors.</li>
</ul>
<p>Now, how do these pieces work together in practice? They come together in two main phases: <strong>Training</strong> and <strong>Inference</strong>. &nbsp;</p>
<p><strong>Think of it like learning to cook:</strong></p>
<ul>
<li><p><strong>Training</strong> is like practicing a recipe. You gather <strong>ingredients (Data)</strong>, have a basic <strong>recipe structure (Model)</strong>, taste your dish and see how far it is from the ideal flavour (<strong>Loss Function</strong>), and then adjust your cooking technique or ingredient amounts (<strong>Optimization Algorithm</strong>) based on the taste test. You repeat this process until your dish tastes great.</p></li>
<li><p><strong>Inference</strong> is when you confidently cook the finalized recipe for your guests, serving them the delicious dish you perfected during practice.</p></li>
</ul>
<section id="training-teaching-the-model" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="training-teaching-the-model"><span class="header-section-number">6.6.1</span> Training: Teaching the Model</h3>
<p>Training is the phase where the model <em>learns</em> from the data. It’s an iterative process where we repeatedly show the model examples, measure its mistakes, and adjust its internal parameters (weights and biases) to get better. This iterative process happens in what’s called the <strong>Training Loop</strong>.</p>
<section id="the-training-loop-a-cycle-of-learning" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-training-loop-a-cycle-of-learning"><strong>The Training Loop: A Cycle of Learning</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/training-loop.png" class="img-fluid figure-img"></p>
<figcaption>Training loop</figcaption>
</figure>
</div>
<p>Imagine the training loop as one full cycle of studying a chapter in a textbook:</p>
<ol type="1">
<li><p><strong>Forward Pass</strong>: The model takes a batch of input data (e.g., a few images or sentences) and passes it through its layers to make a prediction. (Like reading a section of the chapter). &nbsp;</p></li>
<li><p><strong>Loss Calculation</strong>: The Loss Function compares the model’s predictions for that batch with the actual target values (ground truth). It calculates a loss score representing the error for that batch. (Like answering practice questions and seeing how many you got wrong). &nbsp;</p></li>
<li><p><strong>Backward Pass (Backpropagation)</strong>: This is the crucial learning step! The calculated loss is used to compute the gradients – determining how much each parameter in the model contributed to the error. It figures out the “direction of blame” for the mistakes. (Like reviewing your wrong answers to understand why they were wrong). &nbsp;</p></li>
<li><p><strong>Parameter Update</strong>: The Optimization Algorithm (like Adam or SGD) takes the gradients and the learning rate, and updates the model’s weights and biases. It nudges the parameters in the direction that should reduce the loss for the next time. (Like correcting your understanding based on the review). &nbsp;</p></li>
</ol>
<p>This entire cycle (Forward -&gt; Loss -&gt; Backward -&gt; Update) repeats over and over, using different batches of data each time.</p>
</section>
<section id="epochs-batches-and-iterations" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="epochs-batches-and-iterations"><strong>Epochs, Batches, and Iterations</strong></h4>
<p>You’ll often hear these terms:</p>
<ul>
<li><p><strong>Batch (or Mini-Batch)</strong>: A small subset of the total training data used in one iteration of the training loop. Using batches makes the training process computationally efficient and often more stable than using single examples. Common batch sizes are 32, 64, 128, etc.. &nbsp;</p></li>
<li><p><strong>Iteration</strong>: One run through the training loop cycle using a single batch (one forward pass, one backward pass, one parameter update).</p></li>
<li><p><strong>Epoch</strong>: One complete pass through the entire training dataset. If your dataset has 1000 examples and your batch size is 100, one epoch consists of 10 iterations (1000 / 100 = 10).</p></li>
</ul>
<p>Training usually involves running for multiple epochs, allowing the model to see the entire dataset several times and gradually refine its parameters.</p>
</section>
<section id="the-goal-of-training" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-goal-of-training"><strong>The Goal of Training</strong></h4>
<p>The ultimate goal isn’t just to minimize the loss on the <em>training</em> data (which can lead to overfitting, as we saw with the loss curves ). The real goal is to minimize the training loss <em>while ensuring the model performs well on new, unseen data</em>. This ability to perform well on unseen data is called <strong>generalization</strong>, and we monitor it using the <strong>Validation Loss</strong>. We aim for that “Good Fit” scenario where both training and validation loss go down and stabilize.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why is ‘generalization’ considered the true goal of training, rather than just minimizing the loss on the training data?”</p>
<p><em>Hint: We want the model to perform well on new, unseen data it will encounter in the real world, not just memorize the data it was trained on.</em></p>
</div>
</div>
</section>
</section>
<section id="inference-using-the-trained-model" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="inference-using-the-trained-model"><span class="header-section-number">6.6.2</span> Inference: Using the Trained Model</h3>
<p>Once the training process is complete (e.g., the validation loss stops improving), we have a <strong>trained model</strong>. Now we can use it for its intended purpose – making predictions on new data it has never seen before. This is the <strong>Inference</strong> phase.</p>
<section id="the-inference-process" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-inference-process"><strong>The Inference Process</strong></h4>
<p>Inference is typically much simpler than training:</p>
<ol type="1">
<li><strong>Input New Data</strong>: Feed new data (e.g., an image you want to classify, a sentence you want to translate) into the trained model.</li>
<li><strong>Forward Pass Only</strong>: The data flows through the network’s layers, using the learned parameters (weights and biases) to calculate an output.</li>
<li><strong>Get Prediction</strong>: The model outputs its prediction (e.g., the class label “cat”, the translated sentence, the predicted temperature).</li>
</ol>
<p><strong>Crucially, during inference:</strong></p>
<ul>
<li>There’s no <strong>loss calculation</strong> (we don’t usually have the “true” answers for new real-world data).</li>
<li>There’s no <strong>backpropagation</strong>.</li>
<li>There are no <strong>parameter updates</strong>. The model’s learned weights are fixed or “frozen”.</li>
</ul>
<p>Inference is just about using the knowledge the model gained during training.</p>
</section>
<section id="evaluating-performance" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="evaluating-performance"><strong>Evaluating Performance</strong></h4>
<p>After training, and before deploying the model for real-world use, we perform a final evaluation using the <strong>Test Set</strong> – data that was completely held aside and never used during training or validation. This gives an unbiased estimate of how the model will likely perform in the real world. &nbsp;</p>
<p>Common ways to measure performance (metrics) depend on the task:</p>
<ul>
<li><p><strong>Classification</strong>: Accuracy (overall percentage correct), Precision (of the positive predictions, how many were actually positive?), Recall (of all the actual positives, how many did we find?), F1-Score (a balance of Precision and Recall) <a href="#fn107" class="footnote-ref" id="fnref107" role="doc-noteref"><sup>107</sup></a> <a href="#fn108" class="footnote-ref" id="fnref108" role="doc-noteref"><sup>108</sup></a>.</p></li>
<li><p><strong>Regression</strong>: Mean Squared Error (MSE) or Mean Absolute Error (MAE) on the test set predictions.</p></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>What are the three key steps/processes from the training loop that are absent during inference?</p>
<p><em>Hint: Loss Calculation, Backward Pass/Backpropagation, Parameter Update</em></p>
</div>
</div>
</section>
</section>
<section id="training-vs.-inference" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="training-vs.-inference"><span class="header-section-number">6.6.3</span> Training vs.&nbsp;Inference</h3>
<p>Here’s a quick summary:</p>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 39%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Training Phase</th>
<th>Inference Phase</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Goal</td>
<td>Learn parameters from data</td>
<td>Make predictions on new data</td>
</tr>
<tr class="even">
<td>Input Data</td>
<td>Training &amp; Validation Sets</td>
<td>New, unseen data (or Test Set)</td>
</tr>
<tr class="odd">
<td>Process</td>
<td>Forward Pass + Loss + Backward Pass + Update</td>
<td>Forward Pass Only</td>
</tr>
<tr class="even">
<td>Parameters</td>
<td>Continuously Updated</td>
<td>Fixed / Frozen</td>
</tr>
<tr class="odd">
<td>Output</td>
<td>Updated Model Parameters</td>
<td>Predictions</td>
</tr>
<tr class="even">
<td>Computation Cost</td>
<td>High</td>
<td>Lower</td>
</tr>
</tbody>
</table>
<section id="putting-it-all-together" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="putting-it-all-together"><strong>Putting it all together</strong></h4>
<p>With Training and Inference, we can now put all the building blocks together to build a complete deep learning life cycle.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/training-inference.png" class="img-fluid figure-img"></p>
<figcaption>Training and Inference</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Imagine you’ve trained a model to detect spam emails.</p>
<ul>
<li>When you run your massive dataset of labeled emails through the training loop for 10 epochs, is that Training or Inference?</li>
<li>When your email service uses the finished model to decide if a new incoming email is spam or not, is that Training or Inference?</li>
<li>Which phase requires more computational power (GPU time, etc.)? Why?</li>
</ul>
</div>
</div>
</section>
</section>
</section>
<section id="conclusion-putting-the-blocks-together" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="conclusion-putting-the-blocks-together"><span class="header-section-number">6.7</span> Conclusion: Putting the Blocks Together</h2>
<p>Congratulations! You’ve journeyed through the essential building blocks of neural networks and deep learning. We’ve seen how these fundamental pieces fit together to enable machines to learn from data.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-54-contents" aria-controls="callout-54" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proud of you!
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-54" class="callout-54-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div style="display: flex; justify-content: center; align-items: center; width: 100%;">
    <div class="tenor-gif-embed" data-postid="8650293081808857112" data-share-method="host" data-aspect-ratio="0.564257" data-width="50%">
        <a href="https://tenor.com/view/kitty-meow-meow-hi-five-well-done-proud-of-you-gif-8650293081808857112">Kitty Meow GIF</a>
        from <a href="https://tenor.com/search/kitty-gifs">Kitty GIFs</a>
    </div>
</div>
<script type="text/javascript" async="" src="https://tenor.com/embed.js"></script>
</div>
</div>
</div>
<p>Let’s quickly recap what we’ve covered:</p>
<ol type="1">
<li><p><strong>Data</strong>: The crucial starting point. We learned about preparing inputs and outputs, the importance of data quality over sheer quantity, and how data defines the problem we want the model to solve.</p></li>
<li><p><strong>Models</strong>: These are the ‘brains’ that learn patterns. We explored fundamental layers (like Dense, Convolutional, Recurrent, Attention) and how they assemble into powerful architectures (CNNs, LSTMs, Transformers, GNNs, Autoencoders), each suited for different types of data and tasks. We also saw the practical power of using <strong>pre-trained models</strong> and <strong>transfer learning</strong> to build effective systems efficiently.</p></li>
<li><p><strong>Loss Functions</strong>: The ‘scorekeeper’ that tells us how well the model is doing. We discussed how to choose appropriate functions based on the task (MSE/MAE for regression, Cross-Entropy for classification) and the importance of monitoring <strong>training and validation loss</strong> to diagnose issues like overfitting or underfitting.</p></li>
<li><p><strong>Optimization Algorithms</strong>: The ‘engine’ that drives learning. We explored the core idea of <strong>Gradient Descent</strong>, how <strong>Backpropagation</strong> calculates the necessary updates, and how advanced optimizers (like Adam) and techniques (like learning rate scheduling) help navigate the complex ‘loss landscape’ more effectively.</p></li>
</ol>
<p>These four components work in concert during the <strong>Training</strong> phase to adjust the model’s parameters, minimizing the loss and maximizing performance. The ultimate goal is <strong>generalization</strong> – creating a model that performs well not just on the data it learned from, but on new, unseen data during the <strong>Inference</strong> phase.</p>
<p>Understanding these building blocks provides you with a solid conceptual foundation. You can now better appreciate how different deep learning systems work under the hood and make informed decisions when approaching new problems.</p>
<p><strong>Next Steps:</strong></p>
<p>We’ve focused on the core <em>concepts</em> so far. In the upcoming sections, we’ll transition from theory to practice. We’ll introduce <strong>PyTorch</strong>, a powerful deep learning framework, and dive into <strong>hands-on labs</strong> where you’ll apply these building blocks to build and train your own neural networks. Get ready to bring these ideas to life!</p>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><a href="https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/ML%20basic%20(v8).pdf"><em>https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/ML%20basic%20(v8).pdf</em></a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://openai.com/chatgpt/overview/" class="uri">https://openai.com/chatgpt/overview/</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><a href="https://openai.com/index/dall-e/" class="uri">https://openai.com/index/dall-e/</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><a href="https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/" class="uri">https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><a href="https://arxiv.org/pdf/2006.07159">Beyer, L., Hénaff, O. J., Kolesnikov, A., Zhai, X., &amp; Oord, A. V. D. (2020). Are we done with imagenet?. arXiv preprint arXiv:2006.07159.</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><a href="https://arcticdata.io/" class="uri">https://arcticdata.io/</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><a href="https://arctic.noaa.gov/data/" class="uri">https://arctic.noaa.gov/data/</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><a href="https://www.geeksforgeeks.org/ml-handling-missing-values/" class="uri">https://www.geeksforgeeks.org/ml-handling-missing-values/</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><a href="https://www.freecodecamp.org/news/how-to-detect-outliers-in-machine-learning/" class="uri">https://www.freecodecamp.org/news/how-to-detect-outliers-in-machine-learning/</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p><a href="https://developers.google.com/machine-learning/crash-course/numerical-data/normalization" class="uri">https://developers.google.com/machine-learning/crash-course/numerical-data/normalization</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p><a href="https://datascience.stanford.edu/news/splitting-data-randomly-can-ruin-your-model" class="uri">https://datascience.stanford.edu/news/splitting-data-randomly-can-ruin-your-model</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p><a href="https://en.wikipedia.org/wiki/Stratified_sampling" class="uri">https://en.wikipedia.org/wiki/Stratified_sampling</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p><a href="https://www.tandfonline.com/doi/abs/10.1080/24694452.2024.2373787">Li, W., Hsu, C. Y., Wang, S., &amp; Kedron, P. (2024). GeoAI Reproducibility and Replicability: a computational and spatial perspective. Annals of the American Association of Geographers, 114(9), 2085-2103.</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p><a href="https://www.datacamp.com/tutorial/complete-guide-data-augmentation" class="uri">https://www.datacamp.com/tutorial/complete-guide-data-augmentation</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p><a href="https://proceedings.neurips.cc/paper/2017/hash/f26dab9bf6a137c3b6782e562794c2f2-Abstract.html">Ratner, A. J., Ehrenberg, H., Hussain, Z., Dunnmon, J., &amp; Ré, C. (2017). Learning to compose domain-specific transformations for data augmentation. Advances in neural information processing systems, 30.</a><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ghiasi_Simple_Copy-Paste_Is_a_Strong_Data_Augmentation_Method_for_Instance_CVPR_2021_paper.pdf">Ghiasi, Golnaz, et al.&nbsp;“Simple copy-paste is a strong data augmentation method for instance segmentation.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.</a><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p><a href="https://www.geeksforgeeks.org/ml-one-hot-encoding/" class="uri">https://www.geeksforgeeks.org/ml-one-hot-encoding/</a><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p><a href="https://arxiv.org/abs/2203.15556">Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., … &amp; Sifre, L. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.</a><a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p><a href="https://arxiv.org/abs/2112.11446">Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., … &amp; Irving, G. (2021). Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</a><a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p><a href="https://arxiv.org/abs/2203.15556">Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., … &amp; Sifre, L. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.</a><a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p><a href="https://adamharley.com/nn_vis/mlp/3d.html" class="uri">https://adamharley.com/nn_vis/mlp/3d.html</a><a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p><a href="https://cs231n.github.io/convolutional-networks/" class="uri">https://cs231n.github.io/convolutional-networks/</a><a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p><a href="https://arxiv.org/abs/1312.4400">Lin, M., Chen, Q., &amp; Yan, S. (2013). Network in network. arXiv preprint arXiv:1312.4400.</a><a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p><a href="https://www.v7labs.com/blog/neural-networks-activation-functions" class="uri">https://www.v7labs.com/blog/neural-networks-activation-functions</a><a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p><a href="https://www.geeksforgeeks.org/tanh-vs-sigmoid-vs-relu/" class="uri">https://www.geeksforgeeks.org/tanh-vs-sigmoid-vs-relu/</a><a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a><a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p><a href="https://arxiv.org/abs/1406.1078">Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</a><a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p><a href="https://www.superdatascience.com/blogs/recurrent-neural-networks-rnn-the-vanishing-gradient-problem" class="uri">https://www.superdatascience.com/blogs/recurrent-neural-networks-rnn-the-vanishing-gradient-problem</a><a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29"><p><a href="https://www.youtube.com/watch?v=eMlx5fFNoYc" class="uri">https://www.youtube.com/watch?v=eMlx5fFNoYc</a><a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30"><p><a href="http://proceedings.mlr.press/v37/ioffe15.pdf">Ioffe, Sergey, and Christian Szegedy. “Batch normalization: Accelerating deep network training by reducing internal covariate shift.” International conference on machine learning. PMLR, 2015.</a><a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31"><p><a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">Srivastava, Nitish, et al.&nbsp;“Dropout: a simple way to prevent neural networks from overfitting.” The journal of machine learning research 15.1 (2014): 1929-1958.</a><a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32"><p><a href="https://filelist.tudelft.nl/EWI/Over%20de%20faculteit/Afdelingen/Intelligent%20Systems/Pattern%20Recognition%20Laboratory/PR/Reading%20Group/Zeiler%2C%20Fergus%20-%202013%20-%20Visualizing%20and%20Understanding%20Convolutional%20Networks.pdf">Zeiler, Matthew D., and Rob Fergus. “Visualizing and understanding convolutional networks.” Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13. Springer International Publishing, 2014.</a><a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33"><p><a href="https://hal.science/hal-03926082/document">LeCun, Yann, et al.&nbsp;“Gradient-based learning applied to document recognition.” Proceedings of the IEEE 86.11 (1998): 2278-2324.</a><a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34"><p><a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems 25 (2012).</a><a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35"><p><a href="https://arxiv.org/pdf/1409.1556">Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014).</a><a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36"><p><a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">He, Kaiming, et al.&nbsp;“Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</a><a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn37"><p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">Szegedy, Christian, et al.&nbsp;“Going deeper with convolutions.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</a><a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn38"><p><a href="http://proceedings.mlr.press/v97/tan19a/tan19a.pdf">Tan, Mingxing, and Quoc Le. “Efficientnet: Rethinking model scaling for convolutional neural networks.” International conference on machine learning. PMLR, 2019.</a><a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn39"><p><a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems 25 (2012).</a><a href="#fnref39" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn40"><p><a href="https://arxiv.org/pdf/1702.05747">Litjens, Geert, et al.&nbsp;“A survey on deep learning in medical image analysis.” Medical image analysis 42 (2017): 60-88.</a><a href="#fnref40" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn41"><p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf">Schroff, Florian, Dmitry Kalenichenko, and James Philbin. “Facenet: A unified embedding for face recognition and clustering.” Proceedings of the IEEE conference on computer vision and pattern re(cognition. 2015.</a><a href="#fnref41" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn42"><p><a href="https://glossary.midtown.ai/assets/l/long_short_term_memory_paper.pdf">Hochreiter, Sepp, and Jürgen Schmidhuber. “Long short-term memory.” Neural computation 9.8 (1997): 1735-1780.</a><a href="#fnref42" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn43"><p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTMs</a><a href="#fnref43" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn44"><p><a href="https://proceedings.mlr.press/v28/pascanu13.html">Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. “On the difficulty of training recurrent neural networks.” International conference on machine learning. Pmlr, 2013.</a><a href="#fnref44" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn45"><p><a href="https://glossary.midtown.ai/assets/l/long_short_term_memory_paper.pdf">Hochreiter, Sepp, and Jürgen Schmidhuber. “Long short-term memory.” Neural computation 9.8 (1997): 1735-1780.</a><a href="#fnref45" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn46"><p><a href="https://arxiv.org/pdf/1406.1078">Cho, Kyunghyun, et al.&nbsp;“Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078 (2014).</a><a href="#fnref46" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn47"><p><a href="https://mediatum.ub.tum.de/doc/1290194/document.pdf">Graves, Alex, and Jürgen Schmidhuber. “Framewise phoneme classification with bidirectional LSTM and other neural network architectures.” Neural networks 18.5-6 (2005): 602-610.</a><a href="#fnref47" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn48"><p><a href="https://proceedings.neurips.cc/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf">Shi, Xingjian, et al.&nbsp;“Convolutional LSTM network: A machine learning approach for precipitation nowcasting.” Advances in neural information processing systems 28 (2015).</a><a href="#fnref48" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn49"><p><a href="https://arxiv.org/pdf/1708.02709">Young, Tom, et al.&nbsp;“Recent trends in deep learning based natural language processing.” ieee Computational intelligenCe magazine 13.3 (2018): 55-75.</a><a href="#fnref49" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn50"><p><a href="https://par.nsf.gov/servlets/purl/10186768">Siami-Namini, Sima, Neda Tavakoli, and Akbar Siami Namin. “A comparison of ARIMA and LSTM in forecasting time series.” 2018 17th IEEE international conference on machine learning and applications (ICMLA). Ieee, 2018.</a><a href="#fnref50" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn51"><p><a href="https://arxiv.org/pdf/1303.5778">Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. “Speech recognition with deep recurrent neural networks.” 2013 IEEE international conference on acoustics, speech and signal processing. Ieee, 2013.</a><a href="#fnref51" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn52"><p><a href="https://sferics.idsia.ch/pub/juergen/2002_ieee.pdf">Eck, Douglas, and Juergen Schmidhuber. “Finding temporal structure in music: Blues improvisation with LSTM recurrent networks.” Proceedings of the 12th IEEE workshop on neural networks for signal processing. IEEE, 2002.</a><a href="#fnref52" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn53"><p><a href="https://www.researchgate.net/profile/Mohamed-Mourad-Lafifi/post/Optimizing-Long-Short-Term-Memory-Model-CNN-for-anomaly-detection/attachment/5f46fa6bce377e00016f45e8/AS%3A928935898542080%401598486985261/download/Long+Short+Term+Memory+Networks+for+Anomaly+Detection+in+Time+Series.pdf">Malhotra, Pankaj, et al.&nbsp;“Long short term memory networks for anomaly detection in time series.” Proceedings. Vol. 89. No.&nbsp;9. 2015.</a><a href="#fnref53" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn54"><p><a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Vaswani, Ashish, et al.&nbsp;“Attention is all you need.” Advances in neural information processing systems 30 (2017).</a><a href="#fnref54" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn55"><p><a href="https://arxiv.org/pdf/1607.06450">Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. “Layer normalization.” arXiv preprint arXiv:1607.06450 (2016).</a><a href="#fnref55" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn56"><p><a href="http://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf">Gehring, Jonas, et al.&nbsp;“Convolutional sequence to sequence learning.” International conference on machine learning. PMLR, 2017.</a><a href="#fnref56" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn57"><p><a href="https://arxiv.org/pdf/1901.02860">Dai, Zihang, et al.&nbsp;“Transformer-xl: Attentive language models beyond a fixed-length context.” arXiv preprint arXiv:1901.02860 (2019).</a><a href="#fnref57" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn58"><p><a href="https://arxiv.org/pdf/2001.08361/1000">Kaplan, Jared, et al.&nbsp;“Scaling laws for neural language models.” arXiv preprint arXiv:2001.08361 (2020).</a><a href="#fnref58" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn59"><p><a href="https://aclanthology.org/N19-1423.pdf">Devlin, Jacob, et al.&nbsp;“Bert: Pre-training of deep bidirectional transformers for language understanding.” Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers). 2019.</a><a href="#fnref59" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn60"><p><a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown, Tom, et al.&nbsp;“Language models are few-shot learners.” Advances in neural information processing systems 33 (2020): 1877-1901.</a><a href="#fnref60" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn61"><p><a href="https://arxiv.org/pdf/2303.08774">Achiam, Josh, et al.&nbsp;“Gpt-4 technical report.” arXiv preprint arXiv:2303.08774 (2023).</a><a href="#fnref61" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn62"><p><a href="https://arxiv.org/pdf/2010.11929/1000">Dosovitskiy, Alexey, et al.&nbsp;“An image is worth 16x16 words: Transformers for image recognition at scale.” arXiv preprint arXiv:2010.11929 (2020).</a><a href="#fnref62" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn63"><p><a href="https://arxiv.org/pdf/1910.13461">Lewis, Mike, et al.&nbsp;“Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.” arXiv preprint arXiv:1910.13461 (2019).</a><a href="#fnref63" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn64"><p><a href="http://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf">Liu, Ze, et al.&nbsp;“Swin transformer: Hierarchical vision transformer using shifted windows.” Proceedings of the IEEE/CVF international conference on computer vision. 2021.</a><a href="#fnref64" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn65"><p><a href="https://aclanthology.org/2020.emnlp-demos.6.pdf">Wolf, Thomas, et al.&nbsp;“Transformers: State-of-the-art natural language processing.” Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations. 2020.</a><a href="#fnref65" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn66"><p><a href="https://arxiv.org/pdf/2101.01169">Khan, Salman, et al.&nbsp;“Transformers in vision: A survey.” ACM computing surveys (CSUR) 54.10s (2022): 1-41.</a><a href="#fnref66" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn67"><p><a href="https://arxiv.org/pdf/2005.08100">Gulati, Anmol, et al.&nbsp;“Conformer: Convolution-augmented transformer for speech recognition.” arXiv preprint arXiv:2005.08100 (2020).</a><a href="#fnref67" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn68"><p><a href="http://proceedings.mlr.press/v139/radford21a/radford21a.pdf">Radford, Alec, et al.&nbsp;“Learning transferable visual models from natural language supervision.” International conference on machine learning. PmLR, 2021.</a><a href="#fnref68" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn69"><p><a href="https://www.nature.com/articles/s41586-021-03819-2%3C/p%3E%3Cp%3E-AlphaFold">Jumper, John, et al.&nbsp;“Highly accurate protein structure prediction with AlphaFold.” nature 596.7873 (2021): 583-589.</a><a href="#fnref69" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn70"><p><a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Vaswani, Ashish, et al.&nbsp;“Attention is all you need.” Advances in neural information processing systems 30 (2017).</a><a href="#fnref70" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn71"><p><a href="https://arxiv.org/pdf/2001.08361/1000">Kaplan, Jared, et al.&nbsp;“Scaling laws for neural language models.” arXiv preprint arXiv:2001.08361 (2020).</a><a href="#fnref71" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn72"><p><a href="https://dbirman.github.io/learn/hierarchy/pdfs/Hinton2006.pdf">Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. “Reducing the dimensionality of data with neural networks.” science 313.5786 (2006): 504-507.</a><a href="#fnref72" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn73"><p><a href="https://graphics.stanford.edu/courses/cs233-21-spring/ReferencedPapers/SAE.pdf">Ng, Andrew. “Sparse autoencoder.” CS294A Lecture notes 72.2011 (2011): 1-19.</a><a href="#fnref73" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn74"><p><a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=d8004ccce5ab7dc89f968ca0226d6be5c8697bdf">Vincent, Pascal, et al.&nbsp;“Extracting and composing robust features with denoising autoencoders.” Proceedings of the 25th international conference on Machine learning. 2008.</a><a href="#fnref74" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn75"><p><a href="http://web2.cs.columbia.edu/~blei/fogm/2018F/materials/KingmaWelling2013.pdf">Kingma, Diederik P., and Max Welling. “Auto-encoding variational bayes.” 20 Dec.&nbsp;2013.</a><a href="#fnref75" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn76"><p><a href="https://people.idsia.ch/~ciresan/data/icann2011.pdf">Masci, Jonathan, et al.&nbsp;“Stacked convolutional auto-encoders for hierarchical feature extraction.” Artificial neural networks and machine learning–ICANN 2011: 21st international conference on artificial neural networks, espoo, Finland, June 14-17, 2011, proceedings, part i 21. Springer Berlin Heidelberg, 2011.</a><a href="#fnref76" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn77"><p><a href="https://arxiv.org/pdf/1511.05644">Makhzani, Alireza, et al.&nbsp;“Adversarial autoencoders.” arXiv preprint arXiv:1511.05644 (2015).</a><a href="#fnref77" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn78"><p><a href="https://dl.acm.org/doi/abs/10.1145/2689746.2689747">Sakurada, Mayu, and Takehisa Yairi. “Anomaly detection using autoencoders with nonlinear dimensionality reduction.” Proceedings of the MLSDA 2014 2nd workshop on machine learning for sensory data analysis. 2014.</a><a href="#fnref78" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn79"><p><a href="https://proceedings.neurips.cc/paper/2012/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf">Xie, Junyuan, Linli Xu, and Enhong Chen. “Image denoising and inpainting with deep neural networks.” Advances in neural information processing systems 25 (2012).</a><a href="#fnref79" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn80"><p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W15/papers/Wang_Generalized_Autoencoder_A_2014_CVPR_paper.pdf">Wang, Wei, et al.&nbsp;“Generalized autoencoder: A neural network framework for dimensionality reduction.” Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2014.</a><a href="#fnref80" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn81"><p><a href="https://proceedings.neurips.cc/paper/2006/file/5da713a690c067105aeb2fae32403405-Paper.pdf">Bengio, Yoshua, et al.&nbsp;“Greedy layer-wise training of deep networks.” Advances in neural information processing systems 19 (2006).</a><a href="#fnref81" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn82"><p><a href="http://users.cecs.anu.edu.au/~u5098633/papers/www15.pdf">Sedhain, Suvash, et al.&nbsp;“Autorec: Autoencoders meet collaborative filtering.” Proceedings of the 24th international conference on World Wide Web. 2015.</a><a href="#fnref82" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn83"><p><a href="https://pytorch.org/vision/main/models.html">https://pytorch.org/vision/main/models.html</a><a href="#fnref83" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn84"><p><a href="https://github.com/huggingface/pytorch-image-models">https://github.com/huggingface/pytorch-image-models</a><a href="#fnref84" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn85"><p><a href="https://huggingface.co/models?pipeline_tag=text-classification&amp;sort=downloads">https://huggingface.co/models?pipeline_tag=text-classification&amp;sort=downloads</a><a href="#fnref85" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn86"><p><a href="https://arxiv.org/pdf/1703.06870">He, Kaiming, et al.&nbsp;“Mask r-cnn.” Proceedings of the IEEE international conference on computer vision. 2017.</a><a href="#fnref86" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn87"><p><a href="https://en.wikipedia.org/wiki/Huber_loss">Huber Loss</a><a href="#fnref87" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn88"><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html">Huber Loss in PyTorch</a><a href="#fnref88" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn89"><p><a href="https://arxiv.org/pdf/2208.04564">Saleh, Resve A., and A. K. Saleh. “Statistical properties of the log-cosh loss function used in machine learning.” arXiv preprint arXiv:2208.04564 (2022).</a><a href="#fnref89" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn90"><p><a href="https://en.wikipedia.org/wiki/Hinge_loss">Hinge Loss</a><a href="#fnref90" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn91"><p><a href="https://lightning.ai/docs/torchmetrics/stable/classification/hinge_loss.html">Hinge Loss in PyTorch</a><a href="#fnref91" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn92"><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">Weighted Cross-Entropy Loss in PyTorch</a><a href="#fnref92" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn93"><p><a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf">Lin, Tsung-Yi, et al.&nbsp;“Focal loss for dense object detection.” Proceedings of the IEEE international conference on computer vision. 2017.</a><a href="#fnref93" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn94"><p><a href="https://pytorch.org/vision/main/generated/torchvision.ops.sigmoid_focal_loss.html">Focal Loss in PyTorch</a><a href="#fnref94" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn95"><p><a href="https://mediatum.ub.tum.de/doc/1292048/document.pdf">Graves, Alex, et al.&nbsp;“Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.” Proceedings of the 23rd international conference on Machine learning. 2006.</a><a href="#fnref95" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn96"><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html">CTC Loss in PyTorch</a><a href="#fnref96" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn97"><p><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7610921/pdf/EMS126388.pdf">Sudre, Carole H., et al.&nbsp;“Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations.” Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Québec City, QC, Canada, September 14, Proceedings 3. Springer International Publishing, 2017.</a><a href="#fnref97" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn98"><p><a href="https://arxiv.org/pdf/1908.03851">Zhou, Dingfu, et al.&nbsp;“Iou loss for 2d/3d object detection.” 2019 international conference on 3D vision (3DV). IEEE, 2019.</a><a href="#fnref98" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn99"><p><a href="https://www.geeksforgeeks.org/gradient-descent-algorithm-and-its-variants/">Batch Gradient Descent</a><a href="#fnref99" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn100"><p><a href="https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/">Stochastic Gradient Descent</a><a href="#fnref100" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn101"><p><a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">Stochastic Gradient Descent in PyTorch</a><a href="#fnref101" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn102"><p><a href="https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/">Difference between Batch and Stochastic Gradient Descent</a><a href="#fnref102" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn103"><p><a href="https://www.geeksforgeeks.org/mini-batch-gradient-descent-in-deep-learning/">Mini-Batch Gradient Descent</a><a href="#fnref103" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn104"><p><a href="https://proceedings.neurips.cc/paper/2020/file/d3f5d4de09ea19461dab00590df91e4f-Paper.pdf">Liu, Yanli, Yuan Gao, and Wotao Yin. “An improved analysis of stochastic gradient descent with momentum.” Advances in Neural Information Processing Systems 33 (2020): 18261-18271.</a><a href="#fnref104" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn105"><p><a href="http://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf">Hinton, Geoffrey, Nitish Srivastava, and Kevin Swersky. “Neural networks for machine learning lecture 6a overview of mini-batch gradient descent.” Cited on 14.8 (2012): 2.</a><a href="#fnref105" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn106"><p><a href="https://arxiv.org/pdf/1412.6980">Kingma, Diederik P., and Jimmy Ba. “Adam: A method for stochastic optimization.” arXiv preprint arXiv:1412.6980 (2014).</a><a href="#fnref106" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn107"><p><a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision and Recall</a><a href="#fnref107" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn108"><p><a href="https://en.wikipedia.org/wiki/F-score">F-Score</a><a href="#fnref108" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../sections/hands-on-lab-data-annotation.html" class="pagination-link" aria-label="Hands-On Lab: Data Annotation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Hands-On Lab: Data Annotation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../sections/intro-to-pytorch.html" class="pagination-link" aria-label="Introduction to PyTorch: Core Functionalities and Advantages">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to PyTorch: Core Functionalities and Advantages</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<!-- Default Statcounter code for cyber2a online course
http://cyber2a.github.io/cyber2a-course/ -->
<script type="text/javascript">
    var sc_project=13129980; 
    var sc_invisible=1; 
    var sc_security="fa33fcfd"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async=""></script>
    <noscript><div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img class="statcounter" src="https://c.statcounter.com/13129980/0/fa33fcfd/1/" alt="Web Analytics" referrerpolicy="no-referrer-when-downgrade"></a></div></noscript>
    <!-- End of Statcounter Code -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>