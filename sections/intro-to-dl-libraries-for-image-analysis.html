<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>13&nbsp; Introduction to Deep Learning Libraries for Image Analysis – Cyber2A: AI for Arctic Research</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../sections/ai-workflows-and-mlops.html" rel="next">
<link href="../sections/exploring-advanced-neural-networks.html" rel="prev">
<link href="../images/index/arcticlogo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-3584025bd2775ba93d20b6dabf7256d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../sections/guest-lecture-yili-arts-dataset.html"><b>Day 3: Advanced AI Workflows and Models</b></a></li><li class="breadcrumb-item"><a href="../sections/intro-to-dl-libraries-for-image-analysis.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Introduction to Deep Learning Libraries for Image Analysis</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Cyber2A: AI for Arctic Research</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/cyber2a/cyber2a-course/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 1: Introduction to AI and Arctic Science</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/breaking-the-ice-with-ai-in-arctic-science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Breaking the Ice with AI in Arctic Science</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-for-everyone.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">AI for Everyone</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-ready-data-in-arctic-research.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">AI-Ready Data in Arctic Research</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/data-annotation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Data Annotation: The Foundation of Deep Learning Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-data-annotation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Hands-On Lab: Data Annotation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 2: AI Fundamentals and Techniques</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/the-building-blocks-of-nn-and-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The Building Blocks of Neural Networks and Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/intro-to-pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to PyTorch: Core Functionalities and Advantages</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Hands-On Lab: PyTorch</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/permafrost-discovery-gateway.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Permafrost Discovery Gateway</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-ethics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">AI Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 3: Advanced AI Workflows and Models</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/guest-lecture-yili-arts-dataset.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/exploring-advanced-neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Exploring Advanced Neural Networks: Instance Segmentation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/intro-to-dl-libraries-for-image-analysis.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Introduction to Deep Learning Libraries for Image Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 4: Workflows and Foundation Models</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-workflows-and-mlops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">AI Workflows and MLOps: From Development to Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-ai-workflows.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Hands-On Lab: AI Workflows</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/foundation-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Foundation Models: The Cornerstones of Modern AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-foundation-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Hands-On Lab: Foundation Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/reproducibility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Reproducibility</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 5: AI Frontiers</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/the-fun-and-frontiers-of-ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">The Fun and Frontiers of AI: Innovation, Imagination, Interaction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#from-wrenches-to-power-tools-why-libraries" id="toc-from-wrenches-to-power-tools-why-libraries" class="nav-link" data-scroll-target="#from-wrenches-to-power-tools-why-libraries"><span class="header-section-number">13.1</span> From Wrenches to Power Tools: Why Libraries?</a></li>
  <li><a href="#where-do-libraries-fit-in-the-deep-learning-workflow" id="toc-where-do-libraries-fit-in-the-deep-learning-workflow" class="nav-link" data-scroll-target="#where-do-libraries-fit-in-the-deep-learning-workflow"><span class="header-section-number">13.2</span> Where Do Libraries Fit in the Deep Learning Workflow?</a></li>
  <li><a href="#the-landscape-at-a-glance" id="toc-the-landscape-at-a-glance" class="nav-link" data-scroll-target="#the-landscape-at-a-glance"><span class="header-section-number">13.3</span> The Landscape at a Glance</a>
  <ul class="collapse">
  <li><a href="#high-level-libraries-the-task-experts" id="toc-high-level-libraries-the-task-experts" class="nav-link" data-scroll-target="#high-level-libraries-the-task-experts"><strong>High-Level Libraries: The Task Experts</strong></a></li>
  <li><a href="#glue-layers-and-trainer-engines" id="toc-glue-layers-and-trainer-engines" class="nav-link" data-scroll-target="#glue-layers-and-trainer-engines"><strong>Glue Layers and Trainer Engines</strong></a></li>
  </ul></li>
  <li><a href="#model-zoos-transfer-learning-superpowers" id="toc-model-zoos-transfer-learning-superpowers" class="nav-link" data-scroll-target="#model-zoos-transfer-learning-superpowers"><span class="header-section-number">13.4</span> Model Zoos &amp; Transfer Learning Superpowers</a>
  <ul class="collapse">
  <li><a href="#what-is-transfer-learning" id="toc-what-is-transfer-learning" class="nav-link" data-scroll-target="#what-is-transfer-learning">What Is Transfer Learning?</a></li>
  <li><a href="#what-is-a-model-zoo" id="toc-what-is-a-model-zoo" class="nav-link" data-scroll-target="#what-is-a-model-zoo">What Is a Model Zoo?</a></li>
  <li><a href="#how-transfer-learning-works-in-practice" id="toc-how-transfer-learning-works-in-practice" class="nav-link" data-scroll-target="#how-transfer-learning-works-in-practice">How Transfer Learning Works in Practice</a></li>
  <li><a href="#why-this-matters-for-researchers" id="toc-why-this-matters-for-researchers" class="nav-link" data-scroll-target="#why-this-matters-for-researchers">Why This Matters for Researchers</a></li>
  </ul></li>
  <li><a href="#configuration-styles-without-tears" id="toc-configuration-styles-without-tears" class="nav-link" data-scroll-target="#configuration-styles-without-tears"><span class="header-section-number">13.5</span> Configuration Styles Without Tears</a>
  <ul class="collapse">
  <li><a href="#whats-a-configuration-system" id="toc-whats-a-configuration-system" class="nav-link" data-scroll-target="#whats-a-configuration-system">What’s a Configuration System?</a></li>
  <li><a href="#configuration-in-detectron2" id="toc-configuration-in-detectron2" class="nav-link" data-scroll-target="#configuration-in-detectron2">Configuration in Detectron2</a></li>
  <li><a href="#configuration-in-mmsegmentation" id="toc-configuration-in-mmsegmentation" class="nav-link" data-scroll-target="#configuration-in-mmsegmentation">Configuration in MMSegmentation</a></li>
  </ul></li>
  <li><a href="#data-datasets-and-formats-youll-actually-meet" id="toc-data-datasets-and-formats-youll-actually-meet" class="nav-link" data-scroll-target="#data-datasets-and-formats-youll-actually-meet"><span class="header-section-number">13.6</span> Data, Datasets, and Formats You’ll Actually Meet</a>
  <ul class="collapse">
  <li><a href="#why-registration-matters" id="toc-why-registration-matters" class="nav-link" data-scroll-target="#why-registration-matters">Why Registration Matters</a></li>
  <li><a href="#detectron2-dataset-catalogs-and-metadata" id="toc-detectron2-dataset-catalogs-and-metadata" class="nav-link" data-scroll-target="#detectron2-dataset-catalogs-and-metadata">Detectron2: Dataset Catalogs and Metadata</a></li>
  <li><a href="#mmsegmentation-dataset-wrappers-and-pipelines" id="toc-mmsegmentation-dataset-wrappers-and-pipelines" class="nav-link" data-scroll-target="#mmsegmentation-dataset-wrappers-and-pipelines">MMSegmentation: Dataset Wrappers and Pipelines</a></li>
  <li><a href="#common-dataset-formats" id="toc-common-dataset-formats" class="nav-link" data-scroll-target="#common-dataset-formats">Common Dataset Formats</a></li>
  </ul></li>
  <li><a href="#training-evaluating-and-logging-the-happy-path" id="toc-training-evaluating-and-logging-the-happy-path" class="nav-link" data-scroll-target="#training-evaluating-and-logging-the-happy-path"><span class="header-section-number">13.7</span> Training, Evaluating, and Logging (The Happy Path)</a>
  <ul class="collapse">
  <li><a href="#training-without-the-tears" id="toc-training-without-the-tears" class="nav-link" data-scroll-target="#training-without-the-tears">Training Without the Tears</a></li>
  <li><a href="#detectron2-the-defaulttrainer" id="toc-detectron2-the-defaulttrainer" class="nav-link" data-scroll-target="#detectron2-the-defaulttrainer">Detectron2: The DefaultTrainer</a></li>
  <li><a href="#mmsegmentation-the-runner-system" id="toc-mmsegmentation-the-runner-system" class="nav-link" data-scroll-target="#mmsegmentation-the-runner-system">MMSegmentation: The Runner System</a></li>
  <li><a href="#evaluation-knowing-how-youre-doing" id="toc-evaluation-knowing-how-youre-doing" class="nav-link" data-scroll-target="#evaluation-knowing-how-youre-doing">Evaluation: Knowing How You’re Doing</a></li>
  <li><a href="#logging-see-your-model-learn" id="toc-logging-see-your-model-learn" class="nav-link" data-scroll-target="#logging-see-your-model-learn">Logging: See Your Model Learn</a></li>
  </ul></li>
  <li><a href="#extending-when-you-outgrow-the-defaults" id="toc-extending-when-you-outgrow-the-defaults" class="nav-link" data-scroll-target="#extending-when-you-outgrow-the-defaults"><span class="header-section-number">13.8</span> Extending When You Outgrow the Defaults</a>
  <ul class="collapse">
  <li><a href="#how-modularity-works" id="toc-how-modularity-works" class="nav-link" data-scroll-target="#how-modularity-works">How Modularity Works</a></li>
  <li><a href="#custom-components-in-detectron2" id="toc-custom-components-in-detectron2" class="nav-link" data-scroll-target="#custom-components-in-detectron2">Custom Components in Detectron2</a></li>
  <li><a href="#custom-components-in-mmsegmentation" id="toc-custom-components-in-mmsegmentation" class="nav-link" data-scroll-target="#custom-components-in-mmsegmentation">Custom Components in MMSegmentation</a></li>
  <li><a href="#custom-training-loops" id="toc-custom-training-loops" class="nav-link" data-scroll-target="#custom-training-loops">Custom Training Loops</a></li>
  </ul></li>
  <li><a href="#reproducibility-project-structure-that-wont-haunt-you-later" id="toc-reproducibility-project-structure-that-wont-haunt-you-later" class="nav-link" data-scroll-target="#reproducibility-project-structure-that-wont-haunt-you-later"><span class="header-section-number">13.9</span> Reproducibility &amp; Project Structure That Won’t Haunt You Later</a>
  <ul class="collapse">
  <li><a href="#why-reproducibility-matters" id="toc-why-reproducibility-matters" class="nav-link" data-scroll-target="#why-reproducibility-matters">Why Reproducibility Matters</a></li>
  <li><a href="#organize-your-project-like-a-scientist" id="toc-organize-your-project-like-a-scientist" class="nav-link" data-scroll-target="#organize-your-project-like-a-scientist">Organize Your Project Like a Scientist</a></li>
  <li><a href="#save-the-essentials-every-time" id="toc-save-the-essentials-every-time" class="nav-link" data-scroll-target="#save-the-essentials-every-time">Save the Essentials Every Time</a></li>
  <li><a href="#example-making-experiments-self-contained" id="toc-example-making-experiments-self-contained" class="nav-link" data-scroll-target="#example-making-experiments-self-contained">Example: Making Experiments Self-Contained</a></li>
  <li><a href="#version-control-isnt-just-for-code" id="toc-version-control-isnt-just-for-code" class="nav-link" data-scroll-target="#version-control-isnt-just-for-code">Version Control Isn’t Just for Code</a></li>
  <li><a href="#track-results-like-a-scientist" id="toc-track-results-like-a-scientist" class="nav-link" data-scroll-target="#track-results-like-a-scientist">Track Results Like a Scientist</a></li>
  </ul></li>
  <li><a href="#common-pitfalls-and-friendly-fixes" id="toc-common-pitfalls-and-friendly-fixes" class="nav-link" data-scroll-target="#common-pitfalls-and-friendly-fixes"><span class="header-section-number">13.10</span> Common Pitfalls (and Friendly Fixes)</a>
  <ul class="collapse">
  <li><a href="#cuda-out-of-memory-the-classic" id="toc-cuda-out-of-memory-the-classic" class="nav-link" data-scroll-target="#cuda-out-of-memory-the-classic"><span class="header-section-number">13.10.1</span> “CUDA out of memory” — The Classic</a></li>
  <li><a href="#shape-mismatch-errors" id="toc-shape-mismatch-errors" class="nav-link" data-scroll-target="#shape-mismatch-errors"><span class="header-section-number">13.10.2</span> Shape Mismatch Errors</a></li>
  <li><a href="#wrong-class-counts" id="toc-wrong-class-counts" class="nav-link" data-scroll-target="#wrong-class-counts"><span class="header-section-number">13.10.3</span> Wrong Class Counts</a></li>
  <li><a href="#dataset-not-found-or-misregistered" id="toc-dataset-not-found-or-misregistered" class="nav-link" data-scroll-target="#dataset-not-found-or-misregistered"><span class="header-section-number">13.10.4</span> Dataset Not Found or Misregistered</a></li>
  <li><a href="#non-reproducible-results" id="toc-non-reproducible-results" class="nav-link" data-scroll-target="#non-reproducible-results"><span class="header-section-number">13.10.5</span> Non-Reproducible Results</a></li>
  <li><a href="#overfitting-the-silent-enemy" id="toc-overfitting-the-silent-enemy" class="nav-link" data-scroll-target="#overfitting-the-silent-enemy"><span class="header-section-number">13.10.6</span> Overfitting — The Silent Enemy</a></li>
  <li><a href="#getting-lost-in-configs" id="toc-getting-lost-in-configs" class="nav-link" data-scroll-target="#getting-lost-in-configs"><span class="header-section-number">13.10.7</span> Getting Lost in Configs</a></li>
  <li><a href="#final-thought-debugging-is-part-of-discovery" id="toc-final-thought-debugging-is-part-of-discovery" class="nav-link" data-scroll-target="#final-thought-debugging-is-part-of-discovery">Final Thought: Debugging is Part of Discovery</a></li>
  </ul></li>
  <li><a href="#where-to-go-next" id="toc-where-to-go-next" class="nav-link" data-scroll-target="#where-to-go-next">Where to Go Next</a>
  <ul class="collapse">
  <li><a href="#parting-thought" id="toc-parting-thought" class="nav-link" data-scroll-target="#parting-thought">Parting Thought</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../sections/guest-lecture-yili-arts-dataset.html"><b>Day 3: Advanced AI Workflows and Models</b></a></li><li class="breadcrumb-item"><a href="../sections/intro-to-dl-libraries-for-image-analysis.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Introduction to Deep Learning Libraries for Image Analysis</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Introduction to Deep Learning Libraries for Image Analysis</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>In the previous chapters, we built our own neural networks step by step, from the basic building blocks to full-fledged models using PyTorch. You’ve seen how much power PyTorch gives us: we can design layers, build custom architectures, and train deep networks for image analysis almost from scratch.</p>
<p>But in real-world research, especially when projects need to move fast or integrate complex models (like object detection, instance segmentation, or multi-class land cover mapping), building everything from scratch quickly becomes overwhelming. This is where <strong>deep learning libraries</strong> step in.</p>
<p><img src="../images/dl-pytorch/libraries.png" class="img-fluid"></p>
<p>Think of PyTorch as a <strong>box of electronic components</strong>, resistors, capacitors, wires, chips. You can build anything if you’re patient and skilled. Deep learning libraries, on the other hand, are like <strong>modular circuit boards</strong>, ready-made systems you can plug together, modify, and extend. They free you from re-implementing the same boilerplate code so you can focus on research questions, data, and innovation.</p>
<p>In this chapter, we’ll explore these “power tools” of deep learning, libraries that sit on top of PyTorch and make tasks like <strong>object detection</strong>, <strong>semantic segmentation</strong>, or <strong>instance segmentation</strong> much easier. We’ll look closely at <strong>Detectron2</strong> (developed by Meta AI Research) and <strong>MMSegmentation</strong> (developed by OpenMMLab), both widely used in scientific and industrial image analysis.</p>
<p>By the end of this chapter, you’ll understand:</p>
<ul>
<li>Why these libraries exist and how they simplify your workflow</li>
<li>How configurations, datasets, and training systems work behind the scenes</li>
<li>How to quickly train and evaluate your own custom models using Detectron2 and MMSegmentation</li>
</ul>
<p>This session is less about “coding from zero” and more about <strong>standing on the shoulders of giants</strong> – leveraging the collective work of open-source communities to make your own research more efficient, reproducible, and powerful.</p>
</section>
<section id="from-wrenches-to-power-tools-why-libraries" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="from-wrenches-to-power-tools-why-libraries"><span class="header-section-number">13.1</span> From Wrenches to Power Tools: Why Libraries?</h2>
<p>When we first learned deep learning, we did everything the “manual” way: defining layers, activation functions, loss functions, and optimizers from scratch. That process was essential: it helped us understand what really happens inside a neural network.</p>
<p>But now imagine you’re starting a new research project, say, mapping melt ponds in Arctic imagery or detecting wildfires from satellite data. You know convolutional networks can help, but do you really want to write every part of the training pipeline again: the dataset loader, data augmentations, evaluation metrics, checkpoints, logging…?</p>
<p>That’s like fixing a car with just a wrench set, technically possible, but slow and exhausting. Deep learning libraries such as <strong>Detectron2</strong> and <strong>MMSegmentation</strong> are like <strong>power tools</strong>: they handle the repetitive engineering so you can focus on design and experimentation.</p>
<p>Here’s what they bring to the table:</p>
<ul>
<li><strong>Reusable blueprints</strong>: Predefined architectures like Mask R-CNN, U-Net, or SegFormer are ready to use. You can still customize them, but you don’t need to rebuild them from scratch.</li>
<li><strong>Experiment organization</strong>: Config files (often YAML or Python-based) store hyperparameters, datasets, and model details in one place, making it easy to reproduce experiments.</li>
<li><strong>Scalable training</strong>: Multi-GPU training, mixed precision, and checkpointing are built-in.</li>
<li><strong>Community wisdom</strong>: These libraries encode years of experience from researchers who’ve already solved the same headaches you’re likely to face.</li>
</ul>
<p>Using such frameworks doesn’t mean losing flexibility. In fact, most allow deep customization: you can plug in your own backbone, add a new loss function, or modify the evaluation logic. Think of them as scaffolding, a structure that supports your ideas while keeping everything stable and organized.</p>
<p>In short:</p>
<blockquote class="blockquote">
<p><strong>PyTorch teaches you how to build; Detectron2 and MMSegmentation teach you how to scale.</strong></p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Note: Power Tools Still Need Skilled Hands
</div>
</div>
<div class="callout-body-container callout-body">
<p>Deep learning libraries can save you time, but they won’t think for you. They won’t fix poor data quality, unbalanced labels, or the wrong model choice. The best researchers use these tools <strong>because they understand what’s inside them</strong>. That’s why the earlier PyTorch foundations are so important: they give you the mechanical intuition to use high-level tools wisely.</p>
</div>
</div>
</section>
<section id="where-do-libraries-fit-in-the-deep-learning-workflow" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="where-do-libraries-fit-in-the-deep-learning-workflow"><span class="header-section-number">13.2</span> Where Do Libraries Fit in the Deep Learning Workflow?</h2>
<p>By now, you’ve seen that deep learning libraries are not replacements for PyTorch, they are <strong>extensions</strong> of it. Think of them as layers of abstraction built on top of your foundation, automating the tedious parts while leaving room for creativity.</p>
<p>Let’s recall the typical workflow you’ve been practicing:</p>
<ol type="1">
<li><strong>Define your data</strong>: collecting, labeling, and preprocessing images</li>
<li><strong>Build your model</strong>: choosing an architecture and writing its layers</li>
<li><strong>Train and evaluate</strong>: running experiments, tuning hyperparameters, analyzing results</li>
<li><strong>Deploy or reuse</strong>: applying the trained model to new images or datasets</li>
</ol>
<p><img src="../images/dl-pytorch/workflow.png" class="img-fluid"></p>
<p>Deep learning libraries step in mainly during <strong>steps 2 and 3</strong>, though some also help with data handling. They provide:</p>
<ul>
<li>Ready-made model definitions (so you don’t need to code every convolution and skip connection),</li>
<li>Pre-configured training pipelines (so you don’t manually manage learning rates or checkpoints),</li>
<li>Built-in evaluators and loggers (so you get standardized metrics out of the box).</li>
</ul>
<p>You can still open the hood, but the engine runs much more smoothly.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If PyTorch is like driving a manual car, full control over every gear and pedal, libraries like Detectron2 and MMSegmentation are your <strong>self-driving research assistants</strong>.</p>
<p>You set the destination (the research goal), adjust the route when needed, and let the framework handle the technical road rules of training, batching, and evaluation.</p>
</div>
</div>
</section>
<section id="the-landscape-at-a-glance" class="level2" data-number="13.3">
<h2 data-number="13.3" class="anchored" data-anchor-id="the-landscape-at-a-glance"><span class="header-section-number">13.3</span> The Landscape at a Glance</h2>
<p>Once you start exploring deep learning tools, it can feel like walking into a bustling workshop, every bench has its own specialized equipment.</p>
<p>Some tools are small and flexible (good for tinkering), while others are industrial-grade systems built for large-scale experiments.</p>
<p>Let’s take a quick tour of this landscape and see how the pieces fit together.</p>
<p>At the very bottom sits <strong>PyTorch</strong>, the foundation of almost everything in this ecosystem. It provides the essential machinery: tensors, automatic differentiation, neural network layers, and optimizers.</p>
<p>Alongside it, <strong>torchvision</strong> and <strong>timm</strong> (short for <em>PyTorch Image Models</em>) offer a growing library of pre-implemented models and utilities. If PyTorch is the language, these are your ready-made sentences: common CNNs, ViTs, and ResNets you can borrow and adapt.</p>
<section id="high-level-libraries-the-task-experts" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="high-level-libraries-the-task-experts"><strong>High-Level Libraries: The Task Experts</strong></h3>
<p>Building on top of PyTorch are specialized frameworks that focus on <em>particular types of image understanding tasks</em>. Each one comes with optimized models, data pipelines, and evaluation tools designed for a specific research goal.</p>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 40%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Library</th>
<th>Focus</th>
<th>Example Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Detectron2</strong> (Meta AI)</td>
<td>Object detection &amp; instance segmentation</td>
<td>Detecting wildlife or human-made structures in Arctic imagery</td>
</tr>
<tr class="even">
<td><strong>MMSegmentation</strong> (OpenMMLab)</td>
<td>Semantic segmentation</td>
<td>Land cover mapping, permafrost zone classification</td>
</tr>
<tr class="odd">
<td><strong>Ultralytics YOLO</strong></td>
<td>Real-time detection</td>
<td>Drone-based monitoring or field data collection</td>
</tr>
<tr class="even">
<td><strong>MONAI</strong></td>
<td>Medical image analysis</td>
<td>Cross-domain transfer for geospatial raster segmentation</td>
</tr>
</tbody>
</table>
<p>Each of these libraries is <strong>built on PyTorch</strong>, but adds powerful components:</p>
<ul>
<li><strong>Config systems</strong> to control every detail of the model and experiment</li>
<li><strong>Data loaders</strong> that follow standard dataset formats (e.g., COCO, Cityscapes)</li>
<li><strong>Evaluation modules</strong> that calculate common metrics automatically</li>
<li><strong>Model zoos</strong> that store pretrained weights and reference baselines</li>
</ul>
</section>
<section id="glue-layers-and-trainer-engines" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="glue-layers-and-trainer-engines"><strong>Glue Layers and Trainer Engines</strong></h3>
<p>Underneath these libraries are <strong>training engines</strong> like <em>mmengine</em>, <em>PyTorch Lightning</em>, or <em>Lightning Fabric</em>, which handle distributed training, checkpointing, and logging. They ensure reproducibility and scalability across GPUs or clusters.</p>
<p>In research practice, you may not need to use these directly, but knowing they exist helps you understand what’s running behind the scenes when you launch a training job.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/ecosystem.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>The deep learning ecosystem is layered. Each level builds on PyTorch to offer more automation, specialization, and reproducibility.</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>You don’t have to learn all of these at once. A good way to start is to pick <strong>one library</strong> that aligns with your research question. For example:</p>
<ul>
<li><strong>Detectron2</strong> if you’re identifying or counting objects in images (ice floes, wildlife, vehicles).</li>
<li><strong>MMSegmentation</strong> if you’re labeling every pixel (land cover, permafrost, water vs.&nbsp;ice).</li>
</ul>
<p>Later, once you’re comfortable, you’ll see that most frameworks share similar ideas, configuration files, model registries, and trainer loops, just with slightly different flavors.</p>
</div>
</div>
</section>
</section>
<section id="model-zoos-transfer-learning-superpowers" class="level2" data-number="13.4">
<h2 data-number="13.4" class="anchored" data-anchor-id="model-zoos-transfer-learning-superpowers"><span class="header-section-number">13.4</span> Model Zoos &amp; Transfer Learning Superpowers</h2>
<p>One of the most exciting parts of using modern deep learning libraries is that you don’t need to start from zero. Instead of training a model entirely from scratch, you can <strong>borrow the knowledge of models already trained on massive datasets</strong>, a concept known as <strong>transfer learning</strong>.</p>
<section id="what-is-transfer-learning" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="what-is-transfer-learning">What Is Transfer Learning?</h3>
<p>In essence, transfer learning means starting with a model that already “knows” general visual features, edges, textures, shapes, learned from millions of images (for example, ImageNet or COCO). You then fine-tune it on your own, smaller dataset (for example, Arctic satellite scenes).</p>
</section>
<section id="what-is-a-model-zoo" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="what-is-a-model-zoo">What Is a Model Zoo?</h3>
<p>A <strong>Model Zoo</strong> is a curated collection of pretrained models that you can load instantly. Each model comes with:</p>
<ul>
<li>Pretrained weights</li>
<li>A ready-to-use configuration file</li>
<li>Example scripts for training and inference</li>
<li>Documented accuracy and benchmark results</li>
</ul>
<p>Most major libraries have their own Model Zoo:</p>
<ul>
<li><strong>Detectron2:</strong> Models like Faster R-CNN, Mask R-CNN, RetinaNet, and ViTDet trained on COCO and LVIS.</li>
<li><strong>MMSegmentation:</strong> Dozens of segmentation networks, U-Net, DeepLabV3+, SegFormer, HRNet, pretrained on ADE20K, Cityscapes, and more.</li>
<li><strong>Ultralytics YOLO:</strong> Multiple versions (YOLOv5, YOLOv8) optimized for size and speed.</li>
<li><strong>MONAI:</strong> 2D and 3D models pretrained on medical datasets, sometimes transferable to geoscientific data.</li>
</ul>
</section>
<section id="how-transfer-learning-works-in-practice" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="how-transfer-learning-works-in-practice">How Transfer Learning Works in Practice</h3>
<p>Here’s the basic recipe you’ll see repeated across libraries:</p>
<ol type="1">
<li><p><strong>Load a pretrained model</strong>: Start with a checkpoint trained on a large benchmark.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>cfg.MODEL.WEIGHTS <span class="op">=</span> <span class="st">"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Adapt the model to your dataset</strong>: Adjust the number of output classes and dataset paths.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>cfg.MODEL.ROI_HEADS.NUM_CLASSES <span class="op">=</span> <span class="dv">3</span>  <span class="co"># for your 3 Arctic land cover types</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Fine-tune on your own data</strong>: Instead of random initialization, the model begins with knowledge from the pretrained checkpoint, faster convergence, better accuracy, and less data required.</p></li>
<li><p><strong>Save and reuse</strong>: Once trained, you can store your model as your own “mini zoo” for future experiments.</p></li>
</ol>
</section>
<section id="why-this-matters-for-researchers" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="why-this-matters-for-researchers">Why This Matters for Researchers</h3>
<p>Transfer learning is especially powerful for <strong>scientific and environmental data</strong>, where labeled examples are scarce. A model trained on everyday photos (like COCO) might not have seen icebergs or tundra vegetation, but its early layers already know how to detect <strong>edges, shapes, and patterns</strong>. Those skills transfer remarkably well to geospatial imagery with only minimal retraining.</p>
<blockquote class="blockquote">
<p><strong>In practice:</strong> Fine-tuning a COCO-pretrained Mask R-CNN can outperform a model trained from scratch on your small Arctic dataset, often in a fraction of the time.</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Don’t think of pretrained models as shortcuts; think of them as <strong>foundations</strong>. They let you start at the <em>second floor of knowledge</em> instead of the basement. Just remember: a good researcher still tests, validates, and adapts, never copies blindly.</p>
</div>
</div>
</section>
</section>
<section id="configuration-styles-without-tears" class="level2" data-number="13.5">
<h2 data-number="13.5" class="anchored" data-anchor-id="configuration-styles-without-tears"><span class="header-section-number">13.5</span> Configuration Styles Without Tears</h2>
<p>By now, you know that every deep learning project involves a lot of details, learning rates, datasets, model names, checkpoint paths, augmentation settings, and more. If you hard-code all of that into Python scripts, your experiment directory will soon look like a spaghetti bowl of slightly different files.</p>
<p>That’s where <strong>configuration systems</strong> come in.</p>
<section id="whats-a-configuration-system" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="whats-a-configuration-system">What’s a Configuration System?</h3>
<p>A <strong>configuration file</strong> is simply a structured way to describe how your experiment should run. Instead of changing numbers inside code, you define them in a single, human-readable file and let the training engine read it.</p>
<p>It’s like giving your experiment a <em>recipe card</em>:</p>
<blockquote class="blockquote">
<p>“Use a ResNet-50” backbone, train for 10,000 iterations, on this dataset, with that learning rate.”</p>
</blockquote>
<p>This not only keeps things tidy, it also makes your research <strong>reproducible</strong>. A month from now (or a reviewer six months later), you’ll be able to re-run your model exactly as before.</p>
<p><img src="../images/dl-pytorch/config-system.png" class="img-fluid"></p>
</section>
<section id="configuration-in-detectron2" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="configuration-in-detectron2">Configuration in Detectron2</h3>
<p>Detectron2 offers <strong>two flavors</strong> of configuration systems, both widely used in research:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Type</th>
<th>Format</th>
<th>Strength</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Default Config</strong></td>
<td>YAML-based</td>
<td>Simple and traditional</td>
</tr>
<tr class="even">
<td><strong>LazyConfig</strong></td>
<td>Python-based</td>
<td>Flexible and programmatic</td>
</tr>
</tbody>
</table>
<section id="default-config-yaml" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="default-config-yaml"><strong>1. Default Config (YAML)</strong></h4>
<p>This older system uses YAML files that look like this:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">MODEL</span><span class="kw">:</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">WEIGHTS</span><span class="kw">:</span><span class="at"> </span><span class="st">"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">ROI_HEADS</span><span class="kw">:</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">NUM_CLASSES</span><span class="kw">:</span><span class="at"> </span><span class="dv">3</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="fu">SOLVER</span><span class="kw">:</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">BASE_LR</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.002</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">MAX_ITER</span><span class="kw">:</span><span class="at"> </span><span class="dv">10000</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="fu">DATASETS</span><span class="kw">:</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">TRAIN</span><span class="kw">:</span><span class="at"> ("my_dataset_train",)</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">TEST</span><span class="kw">:</span><span class="at"> ("my_dataset_val",)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>You can modify these settings in Python:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.config <span class="im">import</span> get_cfg</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>cfg <span class="op">=</span> get_cfg()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>cfg.merge_from_file(<span class="st">"path/to/config.yaml"</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>cfg.MODEL.ROI_HEADS.NUM_CLASSES <span class="op">=</span> <span class="dv">3</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then train:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>python train_net.py <span class="op">--</span>config<span class="op">-</span><span class="bu">file</span> path<span class="op">/</span>to<span class="op">/</span>config.yaml</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="lazyconfig-python" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="lazyconfig-python"><strong>2. LazyConfig (Python)</strong></h4>
<p>Newer versions of Detectron2 introduce <strong>LazyConfig</strong>, which uses Python itself as the configuration language. You can define dynamic logic (loops, function calls) directly inside your config.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.config <span class="im">import</span> LazyCall</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.modeling <span class="im">import</span> build_model</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>LazyCall(build_model)(</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        backbone<span class="op">=</span><span class="bu">dict</span>(<span class="bu">type</span><span class="op">=</span><span class="st">"ResNet"</span>, depth<span class="op">=</span><span class="dv">50</span>),</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        num_classes<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    solver<span class="op">=</span><span class="bu">dict</span>(base_lr<span class="op">=</span><span class="fl">0.002</span>, max_iter<span class="op">=</span><span class="dv">10000</span>),</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then load and run:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.config <span class="im">import</span> LazyConfig</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>cfg <span class="op">=</span> LazyConfig.load(<span class="st">"my_config.py"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why it’s useful:</strong> LazyConfig makes it easy to reuse pieces of configs, automate parameter sweeps, and integrate logic while keeping experiments reproducible.</p>
</section>
</section>
<section id="configuration-in-mmsegmentation" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="configuration-in-mmsegmentation">Configuration in MMSegmentation</h3>
<p>MMSegmentation follows a similar philosophy but uses a <strong>hierarchical Python-based config system</strong> (shared across all OpenMMLab libraries). Each config file <em>inherits</em> from a base file — so you can easily stack settings:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>_base_ <span class="op">=</span> [</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'../_base_/models/segformer_mit-b0.py'</span>,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'../_base_/datasets/ade20k.py'</span>,</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'../_base_/default_runtime.py'</span>,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'../_base_/schedules/schedule_20k.py'</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then you can override parameters for your custom experiment:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    decode_head<span class="op">=</span><span class="bu">dict</span>(num_classes<span class="op">=</span><span class="dv">5</span>),</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    pretrained<span class="op">=</span><span class="st">'pretrained/segformer_mit-b0.pth'</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why it’s useful:</strong> This modular structure helps you swap datasets, models, or training schedules without rewriting code, ideal for researchers comparing multiple configurations.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Note: Keep Configs as Research Records
</div>
</div>
<div class="callout-body-container callout-body">
<p>Treat your configuration files like lab notebooks:</p>
<ul>
<li>Always <strong>save a copy</strong> of the exact config used for each experiment.</li>
<li><strong>Name it clearly</strong> (e.g., <code>segformer_lr2e-3_bs16_20k.py</code>).</li>
<li><strong>Store it with your results</strong>. It’s your evidence of what was tested.</li>
</ul>
<p>When a collaborator asks, “Which hyperparameters did you use?”, your answer should be one command away.</p>
</div>
</div>
</section>
</section>
<section id="data-datasets-and-formats-youll-actually-meet" class="level2" data-number="13.6">
<h2 data-number="13.6" class="anchored" data-anchor-id="data-datasets-and-formats-youll-actually-meet"><span class="header-section-number">13.6</span> Data, Datasets, and Formats You’ll Actually Meet</h2>
<p>Every deep learning model lives or dies by its data. In earlier chapters, we loaded images manually and passed them through DataLoaders. That gave us flexibility, but it also meant writing lots of boilerplate code, for example, file paths, annotations, transforms, batching.</p>
<p>High-level libraries like <strong>Detectron2</strong> and <strong>MMSegmentation</strong> take care of much of that work. They expect data to follow <em>common formats</em> and provide tools to register and transform datasets automatically.</p>
<section id="why-registration-matters" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="why-registration-matters">Why Registration Matters</h3>
<p>Before a library can use your dataset, it needs to know three things:</p>
<ol type="1">
<li><strong>Where</strong> your images are stored</li>
<li><strong>How</strong> annotations are structured (bounding boxes, polygons, masks, etc.)</li>
<li><strong>What</strong> each label means</li>
</ol>
<p>Registration tells the framework:</p>
<blockquote class="blockquote">
<p>“Here’s my dataset’s name, here’s how to load it, and here’s what each class represents.”</p>
</blockquote>
<p>After registration, the library can treat your dataset just like COCO or Cityscapes, even if it’s custom Arctic imagery or wildfire maps.</p>
</section>
<section id="detectron2-dataset-catalogs-and-metadata" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="detectron2-dataset-catalogs-and-metadata">Detectron2: Dataset Catalogs and Metadata</h3>
<p>Detectron2 uses two registries:</p>
<ul>
<li><strong><code>DatasetCatalog</code></strong>: keeps track of how to load your data.</li>
<li><strong><code>MetadataCatalog</code></strong>: stores extra info like class names and color palettes.</li>
</ul>
<p>Here’s a simple example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.data <span class="im">import</span> DatasetCatalog, MetadataCatalog</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_arctic_dataset():</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Returns a list of dictionaries, one per image</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dataset_dicts</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>DatasetCatalog.register(<span class="st">"arctic_train"</span>, get_arctic_dataset)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>MetadataCatalog.get(<span class="st">"arctic_train"</span>).<span class="bu">set</span>(thing_classes<span class="op">=</span>[<span class="st">"ice"</span>, <span class="st">"water"</span>, <span class="st">"rock"</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>After this, you can simply say <code>cfg.DATASETS.TRAIN = ("arctic_train",)</code>, Detectron2 knows where to look and how to interpret it.</p>
<p><em>Built-in bonus:</em> If your data already follows the COCO format (images + JSON annotations), you can skip the function and register directly:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.data.datasets <span class="im">import</span> register_coco_instances</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>register_coco_instances(<span class="st">"arctic_coco"</span>, {}, <span class="st">"annotations.json"</span>, <span class="st">"images/"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="mmsegmentation-dataset-wrappers-and-pipelines" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="mmsegmentation-dataset-wrappers-and-pipelines">MMSegmentation: Dataset Wrappers and Pipelines</h3>
<p>MMSegmentation uses a <strong>dataset registry</strong> too, but it focuses on <em>semantic segmentation</em> (labeling each pixel). Datasets are defined in configuration files as <strong>pipelines</strong>, sequences of steps that load, resize, normalize, and augment images.</p>
<p>Example (from <code>config.py</code>):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>dataset_type <span class="op">=</span> <span class="st">'CustomDataset'</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>data_root <span class="op">=</span> <span class="st">'data/arctic/'</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>train_pipeline <span class="op">=</span> [</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(<span class="bu">type</span><span class="op">=</span><span class="st">'LoadImageFromFile'</span>),</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(<span class="bu">type</span><span class="op">=</span><span class="st">'LoadAnnotations'</span>),</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(<span class="bu">type</span><span class="op">=</span><span class="st">'Resize'</span>, scale<span class="op">=</span>(<span class="dv">512</span>, <span class="dv">512</span>)),</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(<span class="bu">type</span><span class="op">=</span><span class="st">'RandomFlip'</span>, prob<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(<span class="bu">type</span><span class="op">=</span><span class="st">'PackSegInputs'</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">type</span><span class="op">=</span>dataset_type,</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        data_root<span class="op">=</span>data_root,</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        img_dir<span class="op">=</span><span class="st">'images/train'</span>,</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>        ann_dir<span class="op">=</span><span class="st">'masks/train'</span>,</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>        pipeline<span class="op">=</span>train_pipeline</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This configuration tells MMSegmentation exactly <strong>how to read</strong> and <strong>how to transform</strong> the data without writing a single loader in Python.</p>
</section>
<section id="common-dataset-formats" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="common-dataset-formats">Common Dataset Formats</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 20%">
<col style="width: 26%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Format</th>
<th>Used In</th>
<th>Structure</th>
<th>Typical Task</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>COCO</strong></td>
<td>Detectron2, YOLO</td>
<td>Images + JSON annotations</td>
<td>Object detection, instance segmentation</td>
</tr>
<tr class="even">
<td><strong>Cityscapes</strong></td>
<td>MMSegmentation</td>
<td>Folder per split + label masks</td>
<td>Semantic segmentation</td>
</tr>
<tr class="odd">
<td><strong>Pascal VOC</strong></td>
<td>MMSegmentation</td>
<td>XML annotations + masks</td>
<td>Detection &amp; segmentation</td>
</tr>
<tr class="even">
<td><strong>Custom (GeoTIFF, PNG)</strong></td>
<td>Research data</td>
<td>Often adapted to COCO or Cityscapes format</td>
<td>Any</td>
</tr>
</tbody>
</table>
<p>Most libraries assume your dataset follows one of these. If not, you can write a small converter, once, and use it across all your experiments.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Note: Think “Format First”
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before you annotate a single image, decide <strong>which format</strong> your library expects. Converting later is possible but painful.</p>
<p>Starting in COCO or Cityscapes format saves enormous time and makes your data immediately usable in most frameworks.</p>
</div>
</div>
</section>
</section>
<section id="training-evaluating-and-logging-the-happy-path" class="level2" data-number="13.7">
<h2 data-number="13.7" class="anchored" data-anchor-id="training-evaluating-and-logging-the-happy-path"><span class="header-section-number">13.7</span> Training, Evaluating, and Logging (The Happy Path)</h2>
<p>If building a neural network from scratch is like cooking in a lab kitchen, measuring every gram and washing every beaker, then using a deep learning library is like stepping into a professional kitchen: everything’s prepped, labeled, and ready for you to create.</p>
<p>Libraries like <strong>Detectron2</strong> and <strong>MMSegmentation</strong> come with <strong>trainers</strong> and <strong>evaluators</strong> that handle most of the behind-the-scenes work. All you need to do is point them to the right dataset, choose a model, and press “run.”</p>
<section id="training-without-the-tears" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="training-without-the-tears">Training Without the Tears</h3>
<p>In raw PyTorch, you need to manage the training loop yourself:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> model(batch)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>In Detectron2 or MMSegmentation, this loop is built in.</p>
</section>
<section id="detectron2-the-defaulttrainer" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="detectron2-the-defaulttrainer">Detectron2: The DefaultTrainer</h3>
<p>Detectron2 provides a <strong>DefaultTrainer</strong>, an all-in-one tool that sets up the model, optimizer, data loader, evaluator, and logging for you.</p>
<p><strong>Step 1: Prepare the config</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.config <span class="im">import</span> get_cfg</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>cfg <span class="op">=</span> get_cfg()</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>cfg.merge_from_file(<span class="st">"configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>cfg.DATASETS.TRAIN <span class="op">=</span> (<span class="st">"arctic_train"</span>,)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>cfg.DATASETS.TEST <span class="op">=</span> (<span class="st">"arctic_val"</span>,)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>cfg.SOLVER.MAX_ITER <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>cfg.MODEL.ROI_HEADS.NUM_CLASSES <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>cfg.OUTPUT_DIR <span class="op">=</span> <span class="st">"./output"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Step 2: Start training</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.engine <span class="im">import</span> DefaultTrainer</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> DefaultTrainer(cfg)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>trainer.resume_or_load(resume<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>And that’s it! Detectron2 handles everything from checkpointing to TensorBoard logging.</p>
<p>When training finishes, you’ll find your model weights, metrics, and logs neatly stored in <code>./output</code>.</p>
</section>
<section id="mmsegmentation-the-runner-system" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="mmsegmentation-the-runner-system">MMSegmentation: The Runner System</h3>
<p>MMSegmentation uses a <strong>runner</strong> system (from <code>mmengine</code>) that works similarly.</p>
<p>Just define your config, then run:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> tools/train.py configs/segformer/segformer_mit-b0_512x512_20k_arctic.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The runner:</p>
<ul>
<li>Builds your dataset</li>
<li>Initializes the model</li>
<li>Handles mixed precision and multi-GPU training</li>
<li>Logs everything to a timestamped folder</li>
</ul>
<p>It even tracks best checkpoints automatically, saving the model with the highest validation mIoU.</p>
</section>
<section id="evaluation-knowing-how-youre-doing" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="evaluation-knowing-how-youre-doing">Evaluation: Knowing How You’re Doing</h3>
<p>Both libraries have built-in evaluators that know how to measure standard metrics.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 41%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th>Library</th>
<th>Common Metrics</th>
<th>Typical Tasks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Detectron2</strong></td>
<td>mAP, AP50, AP75</td>
<td>Object detection, instance segmentation</td>
</tr>
<tr class="even">
<td><strong>MMSegmentation</strong></td>
<td>mIoU, F1-score, pixel accuracy</td>
<td>Semantic segmentation</td>
</tr>
</tbody>
</table>
<p>To evaluate a model in Detectron2:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.evaluation <span class="im">import</span> COCOEvaluator, inference_on_dataset</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.data <span class="im">import</span> build_detection_test_loader</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>evaluator <span class="op">=</span> COCOEvaluator(<span class="st">"arctic_val"</span>, cfg, <span class="va">False</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> build_detection_test_loader(cfg, <span class="st">"arctic_val"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>inference_on_dataset(trainer.model, val_loader, evaluator)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>In MMSegmentation, evaluation happens automatically after each epoch, metrics appear in your terminal and in the log files.</p>
</section>
<section id="logging-see-your-model-learn" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="logging-see-your-model-learn">Logging: See Your Model Learn</h3>
<p>Detectron2 and MMSegmentation both record training progress using event loggers. You can visualize them in <strong>TensorBoard</strong> or <strong>WandB</strong>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ex">tensorboard</span> <span class="at">--logdir</span> ./output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>You’ll see curves for loss, learning rate, and validation accuracy — a powerful way to detect overfitting or convergence.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Note: Automate but Verify
</div>
</div>
<div class="callout-body-container callout-body">
<p>These trainers handle most details for you, but <strong>don’t stop watching</strong>. Keep an eye on:</p>
<ul>
<li>Loss not decreasing (maybe learning rate too high)</li>
<li>Validation metrics not improving (maybe overfitting)</li>
<li>GPU memory usage (batch size too large)</li>
</ul>
<p>Even in an automated system, the best results come from <strong>curious supervision</strong>.</p>
</div>
</div>
</section>
</section>
<section id="extending-when-you-outgrow-the-defaults" class="level2" data-number="13.8">
<h2 data-number="13.8" class="anchored" data-anchor-id="extending-when-you-outgrow-the-defaults"><span class="header-section-number">13.8</span> Extending When You Outgrow the Defaults</h2>
<p>At some point, every researcher reaches a moment of curiosity:</p>
<blockquote class="blockquote">
<p>“What if I change this part of the model?”</p>
</blockquote>
<p>That’s when you know you’re ready to go beyond using the defaults, to start <em>extending</em> the framework. The good news is, libraries like <strong>Detectron2</strong> and <strong>MMSegmentation</strong> are designed for that. They’re modular, meaning every major component (model, dataset, loss, trainer) can be swapped or redefined without rewriting everything else.</p>
<section id="how-modularity-works" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="how-modularity-works">How Modularity Works</h3>
<p>Think of these frameworks as Lego sets:</p>
<ul>
<li>The <strong>backbone</strong> extracts features (e.g., ResNet, Swin Transformer).</li>
<li>The <strong>head</strong> interprets those features for detection or segmentation.</li>
<li>The <strong>loss function</strong> guides learning.</li>
<li>The <strong>trainer</strong> controls how the model updates.</li>
</ul>
<p>Each part connects through a clear interface. You can remove one block and insert your own as long as it fits the same shape.</p>
</section>
<section id="custom-components-in-detectron2" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="custom-components-in-detectron2">Custom Components in Detectron2</h3>
<p>Detectron2 uses <strong>registries</strong>, basically dictionaries that store all available modules.</p>
<p>To create your own backbone:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.modeling <span class="im">import</span> BACKBONE_REGISTRY, Backbone, ShapeSpec</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="at">@BACKBONE_REGISTRY.register</span>()</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ArcticBackbone(Backbone):</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, cfg, input_shape):</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">"stage1"</span>: <span class="va">self</span>.conv(x)}</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_shape(<span class="va">self</span>):</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">"stage1"</span>: ShapeSpec(channels<span class="op">=</span><span class="dv">64</span>, stride<span class="op">=</span><span class="dv">2</span>)}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then, in your config:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>cfg.MODEL.BACKBONE.NAME <span class="op">=</span> <span class="st">"ArcticBackbone"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>That’s it! Detectron2 will use your custom module in the next training run.</p>
<blockquote class="blockquote">
<p><strong>Tip:</strong> You can register <em>anything</em> — custom ROI heads, new data mappers, or loss functions. It’s how most research papers built on Detectron2 extend its functionality.</p>
</blockquote>
</section>
<section id="custom-components-in-mmsegmentation" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="custom-components-in-mmsegmentation">Custom Components in MMSegmentation</h3>
<p>MMSegmentation achieves the same flexibility through <strong>Python modules + registry decorators</strong>, powered by <code>mmengine</code>.</p>
<p>To define your own segmentation head:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mmseg.registry <span class="im">import</span> MODELS</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mmseg.models.decode_heads <span class="im">import</span> BaseDecodeHead</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="at">@MODELS.register_module</span>()</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ArcticHead(BaseDecodeHead):</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, channels, num_classes, <span class="op">**</span>kwargs):</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(in_channels, channels, num_classes, <span class="op">**</span>kwargs)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv <span class="op">=</span> nn.Conv2d(in_channels, num_classes, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>._transform_inputs(inputs)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.conv(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then you can reference it directly in your config:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> <span class="bu">dict</span>(decode_head<span class="op">=</span><span class="bu">dict</span>(<span class="bu">type</span><span class="op">=</span><span class="st">'ArcticHead'</span>, num_classes<span class="op">=</span><span class="dv">4</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Behind the scenes, MMSegmentation automatically recognizes and builds it. There is no need to touch the core codebase.</p>
</section>
<section id="custom-training-loops" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="custom-training-loops">Custom Training Loops</h3>
<p>If you ever need total control (for example, new scheduling logic or domain adaptation), both frameworks let you write your own training loop.</p>
<p><strong>Detectron2 example:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.engine <span class="im">import</span> SimpleTrainer</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> SimpleTrainer(model, data_loader, optimizer)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>MMSeg example:</strong> You can subclass <code>IterBasedRunner</code> or <code>EpochBasedRunner</code> from <code>mmengine</code> to modify training behavior.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Note: Customize Gradually
</div>
</div>
<div class="callout-body-container callout-body">
<p>Customization doesn’t mean reinventing everything at once. Try the <strong>“one-change rule”</strong>: make one small modification (e.g., a new backbone or loss) while keeping the rest default.</p>
<p>This keeps your experiments stable and interpretable, crucial for scientific work.</p>
</div>
</div>
</section>
</section>
<section id="reproducibility-project-structure-that-wont-haunt-you-later" class="level2" data-number="13.9">
<h2 data-number="13.9" class="anchored" data-anchor-id="reproducibility-project-structure-that-wont-haunt-you-later"><span class="header-section-number">13.9</span> Reproducibility &amp; Project Structure That Won’t Haunt You Later</h2>
<p>Once you’ve trained your first few models, it’s tempting to celebrate and move on. But a few weeks later, someone (maybe even future you) will ask:</p>
<blockquote class="blockquote">
<p>“Which config file produced this model again?”</p>
</blockquote>
<p>If you can’t answer that question confidently, it’s time to talk about <strong>reproducibility</strong>, the unsung hero of good research.</p>
<section id="why-reproducibility-matters" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="why-reproducibility-matters">Why Reproducibility Matters</h3>
<p>Deep learning experiments are notoriously easy to lose track of. A change of one parameter, random seed, or data split can subtly alter your results. To make your work credible, shareable, and expandable, you want others, and your future self, to be able to <strong>recreate your exact run</strong>.</p>
</section>
<section id="organize-your-project-like-a-scientist" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="organize-your-project-like-a-scientist">Organize Your Project Like a Scientist</h3>
<p>A clean folder structure makes all the difference. Here’s a simple yet scalable template used in many research labs:</p>
<pre><code>project_name/
 ├── configs/          # All config files (YAML or Python)
 ├── data/             # Datasets or symbolic links
 ├── notebooks/        # Exploration, visualization
 ├── scripts/          # Custom scripts (e.g., data converters)
 ├── output/           # Model checkpoints, logs, results
 │    ├── run_2025_10_20_ice_mask/
 │    └── run_2025_10_21_swin_large/
 ├── README.md         # Notes and experiment summaries
 └── environment.yml   # Conda environment or pip requirements</code></pre>
<p><strong>Rule of thumb:</strong> everything you need to reproduce an experiment should live under one directory or be referenced clearly in version control (e.g., GitHub).</p>
</section>
<section id="save-the-essentials-every-time" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="save-the-essentials-every-time">Save the Essentials Every Time</h3>
<ol type="1">
<li><strong>Configuration file</strong>: the recipe of your experiment.</li>
<li><strong>Random seed</strong>: ensures identical initialization.</li>
<li><strong>Model checkpoint</strong>: weights after training.</li>
<li><strong>Logs</strong>: losses, accuracies, and metrics.</li>
<li><strong>Environment info</strong>: PyTorch version, CUDA version, GPU type.</li>
</ol>
<p>Most frameworks can log these automatically, just don’t delete them when cleaning up!</p>
</section>
<section id="example-making-experiments-self-contained" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="example-making-experiments-self-contained">Example: Making Experiments Self-Contained</h3>
<p>In Detectron2, you can do:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.utils.env <span class="im">import</span> collect_env_info</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"output/env.txt"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    f.write(collect_env_info())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>In MMSegmentation, this happens by default in <code>work_dirs/</code>. You can even set:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>log_level <span class="op">=</span> <span class="st">'INFO'</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>default_hooks <span class="op">=</span> <span class="bu">dict</span>(logger<span class="op">=</span><span class="bu">dict</span>(interval<span class="op">=</span><span class="dv">50</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>to ensure everything important is recorded.</p>
</section>
<section id="version-control-isnt-just-for-code" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="version-control-isnt-just-for-code">Version Control Isn’t Just for Code</h3>
<p>Treat your configs like research notebooks, commit them to Git:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> add configs/segformer_arctic.py</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> commit <span class="at">-m</span> <span class="st">"Experiment: SegFormer lr=2e-3 batch=16 20k iters"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Now, every experiment has a timestamp and description. If a result looks surprising, you can trace exactly which settings produced it.</p>
<blockquote class="blockquote">
<p><strong>Tip:</strong> You can even tag commits with the model name and metric (e.g., <code>v1.3_mIoU78.4</code>).</p>
</blockquote>
</section>
<section id="track-results-like-a-scientist" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="track-results-like-a-scientist">Track Results Like a Scientist</h3>
<p>Many researchers use lightweight experiment trackers like:</p>
<ul>
<li><strong>TensorBoard</strong> (visual learning curves)</li>
<li><strong>Weights &amp; Biases (W&amp;B)</strong> (interactive dashboards)</li>
<li><strong>MLflow</strong> or <strong>Comet</strong> (experiment management)</li>
</ul>
<p>They integrate easily with Detectron2 and MMSegmentation. You just add a few lines of code or CLI flags.</p>
<p>The goal isn’t flashy charts; it’s traceability.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Note: The “One-Command Rule”
</div>
</div>
<div class="callout-body-container callout-body">
<p>Aim for this ideal:</p>
<blockquote class="blockquote">
<p>Any trained model in your project can be reproduced by running one command.</p>
</blockquote>
<p>Example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> train.py <span class="at">--config</span> configs/segformer_arctic.py <span class="at">--seed</span> 42</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>If that single command can rebuild your result, your workflow is healthy.</p>
</div>
</div>
</section>
</section>
<section id="common-pitfalls-and-friendly-fixes" class="level2" data-number="13.10">
<h2 data-number="13.10" class="anchored" data-anchor-id="common-pitfalls-and-friendly-fixes"><span class="header-section-number">13.10</span> Common Pitfalls (and Friendly Fixes)</h2>
<p>No matter how polished the library or how carefully you follow tutorials, sooner or later, something will go wrong. That’s not failure, that’s <strong>deep learning reality</strong>.</p>
<p>Every researcher, from beginner to expert, runs into broken imports, dimension mismatches, and mysterious “CUDA out of memory” errors.</p>
<p>The key is not avoiding problems, it’s <strong>learning how to debug calmly and systematically.</strong></p>
<section id="cuda-out-of-memory-the-classic" class="level3" data-number="13.10.1">
<h3 data-number="13.10.1" class="anchored" data-anchor-id="cuda-out-of-memory-the-classic"><span class="header-section-number">13.10.1</span> “CUDA out of memory” — The Classic</h3>
<p><strong>Symptom:</strong> Training crashes with messages like:</p>
<pre><code>RuntimeError: CUDA out of memory.</code></pre>
<p><strong>Fix:</strong></p>
<ul>
<li>Lower your <code>batch_size</code>.</li>
<li>Reduce image size or crop patches.</li>
<li>Disable unnecessary augmentations.</li>
<li>If using multiple GPUs, set <code>cfg.SOLVER.IMS_PER_BATCH</code> or <code>samples_per_gpu</code> accordingly.</li>
<li>Clear the cache: <code>torch.cuda.empty_cache()</code></li>
</ul>
<blockquote class="blockquote">
<p><strong>Tip:</strong> When experimenting, always start small. Fast iteration beats huge runs.</p>
</blockquote>
</section>
<section id="shape-mismatch-errors" class="level3" data-number="13.10.2">
<h3 data-number="13.10.2" class="anchored" data-anchor-id="shape-mismatch-errors"><span class="header-section-number">13.10.2</span> Shape Mismatch Errors</h3>
<p><strong>Symptom:</strong></p>
<pre><code>RuntimeError: The size of tensor a (256) must match the size of tensor b (128)</code></pre>
<p><strong>Fix:</strong></p>
<ul>
<li>Check that your <strong>input channels</strong> match the model’s expected channels (e.g., 3 vs 14-band imagery).</li>
<li>In MMSeg, update <code>in_channels</code> in the config.</li>
<li>In Detectron2, confirm your custom backbone’s output shape matches the head input.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Tip:</strong> Print shapes often, it’s the simplest sanity check in deep learning.</p>
</blockquote>
</section>
<section id="wrong-class-counts" class="level3" data-number="13.10.3">
<h3 data-number="13.10.3" class="anchored" data-anchor-id="wrong-class-counts"><span class="header-section-number">13.10.3</span> Wrong Class Counts</h3>
<p><strong>Symptom:</strong> Model trains, but outputs random colors or wrong labels.</p>
<p><strong>Fix:</strong></p>
<ul>
<li>Make sure <code>NUM_CLASSES</code> (Detectron2) or <code>num_classes</code> (MMSeg) equals the number of classes <strong>excluding background</strong>, if required.</li>
<li>Verify label encoding: 0 might be background or the first class.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Tip:</strong> Plot one sample mask with its predicted output early, visual sanity beats numeric confusion.</p>
</blockquote>
</section>
<section id="dataset-not-found-or-misregistered" class="level3" data-number="13.10.4">
<h3 data-number="13.10.4" class="anchored" data-anchor-id="dataset-not-found-or-misregistered"><span class="header-section-number">13.10.4</span> Dataset Not Found or Misregistered</h3>
<p><strong>Symptom:</strong></p>
<pre><code>KeyError: 'Dataset not registered: my_dataset_train'</code></pre>
<p><strong>Fix:</strong></p>
<ul>
<li>Ensure dataset registration runs <strong>before</strong> training starts.</li>
<li>Double-check that paths are absolute or correctly relative.</li>
<li>In Detectron2, print the list of registered datasets to verify registration.</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> detectron2.data <span class="im">import</span> DatasetCatalog</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(DatasetCatalog.<span class="bu">list</span>())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<blockquote class="blockquote">
<p><strong>Tip:</strong> Registration is your library’s handshake, if it doesn’t know your dataset, it can’t talk to it.</p>
</blockquote>
</section>
<section id="non-reproducible-results" class="level3" data-number="13.10.5">
<h3 data-number="13.10.5" class="anchored" data-anchor-id="non-reproducible-results"><span class="header-section-number">13.10.5</span> Non-Reproducible Results</h3>
<p><strong>Symptom:</strong> Running the same code twice gives slightly different results.</p>
<p><strong>Fix:</strong></p>
<ul>
<li><p>Set seeds everywhere:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch, random, numpy <span class="im">as</span> np</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)<span class="op">;</span> random.seed(<span class="dv">42</span>)<span class="op">;</span> np.random.seed(<span class="dv">42</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>In configs: <code>cfg.SEED = 42</code> or <code>random_seed = 42</code>.</p></li>
<li><p>Keep software versions fixed in <code>environment.yml</code>.</p></li>
</ul>
<blockquote class="blockquote">
<p><strong>Tip:</strong> Randomness is great for nature, not for reproducibility.</p>
</blockquote>
</section>
<section id="overfitting-the-silent-enemy" class="level3" data-number="13.10.6">
<h3 data-number="13.10.6" class="anchored" data-anchor-id="overfitting-the-silent-enemy"><span class="header-section-number">13.10.6</span> Overfitting — The Silent Enemy</h3>
<p><strong>Symptom:</strong> Training accuracy skyrockets, validation accuracy flatlines.</p>
<p><strong>Fix:</strong></p>
<ul>
<li>Add augmentations.</li>
<li>Collect more data or use transfer learning.</li>
<li>Apply early stopping or smaller models.</li>
<li>Monitor validation loss, not just training loss.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Tip:</strong> Overfitting isn’t failure, it’s your model saying, “I memorized too well.”</p>
</blockquote>
</section>
<section id="getting-lost-in-configs" class="level3" data-number="13.10.7">
<h3 data-number="13.10.7" class="anchored" data-anchor-id="getting-lost-in-configs"><span class="header-section-number">13.10.7</span> Getting Lost in Configs</h3>
<p><strong>Symptom:</strong> Too many files, too many versions, can’t remember what changed.</p>
<p><strong>Fix:</strong></p>
<ul>
<li>Use <strong>clear names</strong> (<code>segformer_lr2e-3_bs16.py</code>).</li>
<li>Add comments for each modification.</li>
<li>Keep a short experiment log (even a CSV or Markdown table).</li>
</ul>
<blockquote class="blockquote">
<p><strong>Tip:</strong> Future you will thank present you for organized notes.</p>
</blockquote>
</section>
<section id="final-thought-debugging-is-part-of-discovery" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="final-thought-debugging-is-part-of-discovery">Final Thought: Debugging is Part of Discovery</h3>
<p>Every red error message is an opportunity to understand your tools more deeply. The more you debug, the more intuitive deep learning becomes. Even top researchers spend as much time fixing and refining as they do innovating, it’s all part of the craft.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/debugging.png" class="img-fluid figure-img"></p>
<figcaption>Every error solved adds one more piece of intuition to your researcher’s toolkit</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Takeaway
</div>
</div>
<div class="callout-body-container callout-body">
<p>Deep learning libraries make complex research possible but not effortless. With patience, good habits, and clear structure, every challenge becomes a learning moment.</p>
<p>Soon you’ll not only be <em>using</em> these frameworks, you’ll be <em>extending</em> them to build your own.</p>
</div>
</div>
</section>
</section>
<section id="where-to-go-next" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="where-to-go-next">Where to Go Next</h2>
<p>Congratulations! You’ve just completed one of the biggest transitions in your deep learning journey: from building everything by hand to mastering <strong>the modern research toolkit</strong>.</p>
<p>You now know how to:</p>
<ul>
<li>Understand what deep learning libraries are and why they exist,</li>
<li>Choose the right framework for your task,</li>
<li>Configure, train, and evaluate models using prebuilt systems,</li>
<li>Customize them safely when you want to innovate, and</li>
<li>Keep your research organized, reproducible, and trustworthy.</li>
</ul>
<p>That’s a huge step. With these tools, you can now shift your focus from “how do I make this run?” to “what scientific questions can I explore with it?”</p>
<section id="parting-thought" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="parting-thought">Parting Thought</h3>
<p>Frameworks like Detectron2 and MMSegmentation are more than just code. They’re a shared language that connects thousands of researchers worldwide. By learning to use them thoughtfully, you’ve joined that conversation.</p>
<p>Keep your curiosity alive, your experiments tidy, and your GPU cool. The best discoveries often begin with a well-documented <code>config.py</code>.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../sections/exploring-advanced-neural-networks.html" class="pagination-link" aria-label="Exploring Advanced Neural Networks: Instance Segmentation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Exploring Advanced Neural Networks: Instance Segmentation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../sections/ai-workflows-and-mlops.html" class="pagination-link" aria-label="AI Workflows and MLOps: From Development to Deployment">
        <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">AI Workflows and MLOps: From Development to Deployment</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<!-- Default Statcounter code for cyber2a online course
http://cyber2a.github.io/cyber2a-course/ -->
<script type="text/javascript">
    var sc_project=13129980; 
    var sc_invisible=1; 
    var sc_security="fa33fcfd"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async=""></script>
    <noscript><div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img class="statcounter" src="https://c.statcounter.com/13129980/0/fa33fcfd/1/" alt="Web Analytics" referrerpolicy="no-referrer-when-downgrade"></a></div></noscript>
    <!-- End of Statcounter Code -->




</body></html>