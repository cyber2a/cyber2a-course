<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>12&nbsp; Exploring Advanced Neural Networks: Instance Segmentation – Cyber2A: AI for Arctic Research</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../sections/intro-to-dl-libraries-for-image-analysis.html" rel="next">
<link href="../sections/guest-lecture-yili-arts-dataset.html" rel="prev">
<link href="../images/index/arcticlogo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-de070a7b0ab54f8780927367ac907214.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-09ce80c0277fc360936e517f68e5459d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../sections/guest-lecture-yili-arts-dataset.html"><b>Day 3: Advanced AI Workflows and Models</b></a></li><li class="breadcrumb-item"><a href="../sections/exploring-advanced-neural-networks.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Exploring Advanced Neural Networks: Instance Segmentation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Cyber2A: AI for Arctic Research</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/cyber2a/cyber2a-course/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 1: Introduction to AI and Arctic Science</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/breaking-the-ice-with-ai-in-arctic-science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Breaking the Ice with AI in Arctic Science</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-for-everyone.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">AI for Everyone</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-ready-data-in-arctic-research.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">AI-Ready Data in Arctic Research</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/data-annotation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Data Annotation: The Foundation of Deep Learning Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-data-annotation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Hands-On Lab: Data Annotation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 2: AI Fundamentals and Techniques</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/the-building-blocks-of-nn-and-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The Building Blocks of Neural Networks and Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/intro-to-pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to PyTorch: Core Functionalities and Advantages</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Hands-On Lab: PyTorch</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/permafrost-discovery-gateway.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Permafrost Discovery Gateway</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-ethics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">AI Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 3: Advanced AI Workflows and Models</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/guest-lecture-yili-arts-dataset.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/exploring-advanced-neural-networks.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Exploring Advanced Neural Networks: Instance Segmentation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/intro-to-dl-libraries-for-image-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Introduction to Deep Learning Libraries for Image Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 4: Workflows and Foundation Models</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-workflows-and-mlops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">AI Workflows and MLOps: From Development to Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-ai-workflows.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Hands-On Lab: AI Workflows</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/foundation-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Foundation Models: The Cornerstones of Modern AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-foundation-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Hands-On Lab: Foundation Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/reproducibility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Reproducibility</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 5: AI Frontiers</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/the-fun-and-frontiers-of-ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">The Fun and Frontiers of AI: Innovation, Imagination, Interaction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#what-is-instance-segmentation" id="toc-what-is-instance-segmentation" class="nav-link" data-scroll-target="#what-is-instance-segmentation"><span class="header-section-number">12.1</span> What is instance segmentation?</a></li>
  <li><a href="#importance-and-applications-of-instance-segmentation" id="toc-importance-and-applications-of-instance-segmentation" class="nav-link" data-scroll-target="#importance-and-applications-of-instance-segmentation"><span class="header-section-number">12.2</span> Importance and applications of instance segmentation</a></li>
  <li><a href="#transitioning-from-backbone-networks-to-classification-networks" id="toc-transitioning-from-backbone-networks-to-classification-networks" class="nav-link" data-scroll-target="#transitioning-from-backbone-networks-to-classification-networks"><span class="header-section-number">12.3</span> Transitioning from backbone networks to classification networks</a></li>
  <li><a href="#a-step-by-step-guide-to-instance-segmentation" id="toc-a-step-by-step-guide-to-instance-segmentation" class="nav-link" data-scroll-target="#a-step-by-step-guide-to-instance-segmentation"><span class="header-section-number">12.4</span> A step-by-step guide to instance segmentation</a>
  <ul class="collapse">
  <li><a href="#the-first-step-r-cnn" id="toc-the-first-step-r-cnn" class="nav-link" data-scroll-target="#the-first-step-r-cnn"><span class="header-section-number">12.4.1</span> The First Step: R-CNN</a></li>
  <li><a href="#evolution-to-fast-r-cnn" id="toc-evolution-to-fast-r-cnn" class="nav-link" data-scroll-target="#evolution-to-fast-r-cnn"><span class="header-section-number">12.4.2</span> Evolution to Fast R-CNN</a></li>
  <li><a href="#introducing-faster-r-cnn" id="toc-introducing-faster-r-cnn" class="nav-link" data-scroll-target="#introducing-faster-r-cnn"><span class="header-section-number">12.4.3</span> Introducing Faster R-CNN</a></li>
  <li><a href="#the-innovation-of-faster-r-cnn" id="toc-the-innovation-of-faster-r-cnn" class="nav-link" data-scroll-target="#the-innovation-of-faster-r-cnn"><span class="header-section-number">12.4.4</span> The Innovation of Faster R-CNN</a></li>
  </ul></li>
  <li><a href="#transition-to-instance-segmentation-mask-r-cnn" id="toc-transition-to-instance-segmentation-mask-r-cnn" class="nav-link" data-scroll-target="#transition-to-instance-segmentation-mask-r-cnn"><span class="header-section-number">12.5</span> Transition to Instance Segmentation: Mask R-CNN</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">12.6</span> Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../sections/guest-lecture-yili-arts-dataset.html"><b>Day 3: Advanced AI Workflows and Models</b></a></li><li class="breadcrumb-item"><a href="../sections/exploring-advanced-neural-networks.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Exploring Advanced Neural Networks: Instance Segmentation</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Exploring Advanced Neural Networks: Instance Segmentation</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>In this section, we will explore the journey of the evolution of neural networks, starting with basic classification networks and progressing to more complex tasks of object detection and instance segmentation.</p>
<p>We’ll begin with a recap of classification networks, which are designed to identify the presence of objects in images and assign class labels. From there, we’ll move to the realm of object detection, exploring the development of <strong>R-CNN <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></strong>, <strong>Fast R-CNN <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></strong>, and <strong>Faster R-CNN <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></strong>. Each of these models represents a step forward in detecting and localizing objects within images, with Faster R-CNN introducing a more efficient and integrated approach.</p>
<p>Finally, we’ll delve into <strong>Mask R-CNN <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></strong>, which extends the capabilities of Faster R-CNN by adding a branch for predicting segmentation masks. This enhancement allows us to not only detect objects but also delineate their exact shapes, enabling precise instance segmentation.</p>
<p>Our goal is to build on your existing understanding of classification networks, providing you with practical insights into how these networks can be adapted and expanded for more advanced computer vision applications. By the end of this section, you will be equipped with the knowledge to understand and apply these techniques to real-world challenges.</p>
</section>
<section id="what-is-instance-segmentation" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="what-is-instance-segmentation"><span class="header-section-number">12.1</span> What is instance segmentation?</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/ins-seg.png" class="img-fluid figure-img"></p>
<figcaption>Source: https://manipulation.csail.mit.edu/segmentation.html</figcaption>
</figure>
</div>
<p>To grasp the concept of instance segmentation, let’s compare it with other related tasks:</p>
<ul>
<li><strong>Image recognition</strong>: This is the most basic task, where the model identifies the presence of objects in an image and assigns class probabilities. It’s like saying, “There are sheep and a dog here,” without specifying their locations.</li>
<li><strong>Semantic segmentation</strong>: This advances to classifying each pixel into a category, effectively coloring the entire image based on object classes. However, it doesn’t distinguish between individual instances of the same class. For example, all sheep might be colored the same, treating them as a single entity.</li>
<li><strong>Object detection</strong>: This task detects objects and distinguishes them by drawing bounding boxes around each, providing more detail than image recognition. It identifies individual objects but lacks fine-grained, pixel-level detail.</li>
<li><strong>Instance segmentation</strong>: This is the most detailed task, combining the strengths of object detection and semantic segmentation. It not only distinguishes individual objects but also delineates their exact shapes at the pixel level. It allows for precise analysis and manipulation.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Instance segmentation is more challenging and computationally intensive than other tasks, and it’s not always necessary for every application.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider which of these tasks would be most useful for your current projects or applications.</p>
</div>
</div>
</section>
<section id="importance-and-applications-of-instance-segmentation" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="importance-and-applications-of-instance-segmentation"><span class="header-section-number">12.2</span> Importance and applications of instance segmentation</h2>
<p>Instance segmentation plays a crucial role in various fields. Its importance stems from the following factors:</p>
<ul>
<li><strong>Fine-grained precision</strong>: Unlike object detection, which only provides bounding boxes, instance segmentation offers pixel-level accuracy. This precision is essential for tasks where understanding the exact shape and position of objects matters.</li>
<li><strong>Object differentiation</strong>: It not only identifies the presence of objects but also distinguishes between individual instances of the same class. This capability is valuable in scenarios where multiple objects of the same type need to be analyzed separately.</li>
<li><strong>Integration in complex systems</strong>: By providing detailed object masks, instance segmentation enables more sophisticated interactions with objects in the environment, such as robotics and augmented reality applications.</li>
</ul>
<p><strong>Applications:</strong></p>
<ul>
<li><strong>Medical imaging</strong>: In medical applications, instance segmentation can help identify and analyze specific structures within images, such as tumors or organs <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</li>
<li><strong>Robotics</strong>: In robotics, instance segmentation enables robots to perceive and interact with their environment more effectively, distinguishing between different objects and obstacles <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</li>
<li><strong>Environmental monitoring</strong>: Instance segmentation can be used in environmental mapping to detect and classify natural features from satellite imagery. This approach enhances the ability to monitor changes in landscapes across large areas with high accuracy <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a><a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider how the ability to identify not just the presence of objects, but also their precise contours, might enhance applications you are familiar with.</p>
</div>
</div>
</section>
<section id="transitioning-from-backbone-networks-to-classification-networks" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="transitioning-from-backbone-networks-to-classification-networks"><span class="header-section-number">12.3</span> Transitioning from backbone networks to classification networks</h2>
<p><img src="../images/dl-pytorch/models.png" class="img-fluid"> In the previous section, we discussed how deep learning models consist of three core components: <strong>input adaptation</strong>, <strong>feature extractor</strong>, and <strong>output adaptation</strong>. Now, let’s explore how these components are configured starting from a backbone network, like ResNet-50, to create a classification network.</p>
<p><strong>ResNet-50 as a backbone network</strong></p>
<p>Using a ResNet-50 backbone network as an example, let’s first examine the structure of the model:</p>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code snippet to load a pre-trained ResNet-50 model
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the pre-trained ResNet-50 model</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the model architecture</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>The output of the code snippet above provides the architecture of the ResNet-50 model, which includes a series of convolutional and batch normalization layers, typically used for feature extraction:</p>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Output: ResNet-50 architecture
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>ResNet(</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">64</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>(maxpool): MaxPool2d(kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, dilation<span class="op">=</span><span class="dv">1</span>, ceil_mode<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>(layer1): Sequential(</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): BasicBlock(</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">64</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">64</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BasicBlock(</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">64</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">64</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>(layer2): Sequential(</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): BasicBlock(</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">128</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">128</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">128</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>(downsample): Sequential(</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BatchNorm2d(<span class="dv">128</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BasicBlock(</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">128</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">128</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">128</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">128</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>(layer3): Sequential(</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): BasicBlock(</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">128</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">256</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">256</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">256</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>(downsample): Sequential(</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): Conv2d(<span class="dv">128</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BatchNorm2d(<span class="dv">256</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BasicBlock(</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">256</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">256</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">256</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">256</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>(layer4): Sequential(</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): BasicBlock(</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">256</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">512</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">512</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">512</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>(downsample): Sequential(</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): Conv2d(<span class="dv">256</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BatchNorm2d(<span class="dv">512</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BasicBlock(</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">512</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">512</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">512</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">512</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>(avgpool): AdaptiveAvgPool2d(output_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>(fc): Linear(in_features<span class="op">=</span><span class="dv">512</span>, out_features<span class="op">=</span><span class="dv">1000</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>The output above is a liitle verbose, so let’s summarize the key components of the ResNet-50 model:</p>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Summary: ResNet-50 architecture
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>ResNet(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">64</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>(maxpool): MaxPool2d(kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, dilation<span class="op">=</span><span class="dv">1</span>, ceil_mode<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>(layer1): Sequential()</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>(layer2): Sequential()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>(layer3): Sequential()</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>(layer4): Sequential()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>(avgpool): AdaptiveAvgPool2d(output_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>(fc): Linear(in_features<span class="op">=</span><span class="dv">512</span>, out_features<span class="op">=</span><span class="dv">1000</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>where <code>layerx</code> represents the convolutional blocks in the network.</p>
</div>
</div>
</div>
<p><strong>Modifying the ResNet-50 model to perform classification</strong></p>
<p>To modify the ResNet-50 model to perform classification, we first need to define the task. For this example, we will classify images into 10 classes, e.g., 10 different types of food. So, for the data, we have</p>
<ul>
<li>input: an image</li>
<li>output: a 10-dimensional vector of class probabilities</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>How would you modify the ResNet-50 model to perform classification?</p>
</div>
</div>
<p>Let’s examine the three components of the model and see how we can modify them to perform classification.</p>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Input Adaptation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>No changes needed. The ResNet-50 model is designed to accept images as input, which is suitable for our task.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Feature Extractor
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>No changes needed. The pre-trained ResNet-50 model’s layers are retained for feature extraction, leveraging its ability to learn complex patterns from images.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Output Adaptation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>From the summary of the ResNet-50 architecture, we can see that the output layer is a linear layer with 1000 output features.</p>
<p><code>(fc): Linear(in_features=512, out_features=1000, bias=True)</code></p>
<p>It is designed to output 1000 class probabilities, and we need to modify it to output 10 class probabilities for our task. To do this, we can replace the linear layer with a new linear layer that has 10 output features.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace the last linear layer with a new linear layer that has 10 output features</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>model.fc <span class="op">=</span> nn.Linear(<span class="dv">512</span>, <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>Done! We have modified the ResNet-50 model to perform classification. Here is the complete code:</p>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Complete code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the pre-trained ResNet-50 model</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace the last linear layer with a new linear layer that has 10 output features</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>model.fc <span class="op">=</span> nn.Linear(<span class="dv">512</span>, <span class="dv">10</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the model architecture</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Using the ResNet-50 model as a backbone network, is there other way to modify the model to perform classification?</li>
<li>With the classification task as an example, try to think of how to modify the model to perform instance segmentation.</li>
</ul>
</div>
</div>
</section>
<section id="a-step-by-step-guide-to-instance-segmentation" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="a-step-by-step-guide-to-instance-segmentation"><span class="header-section-number">12.4</span> A step-by-step guide to instance segmentation</h2>
<p>To achieve instance segmentation, let’s begin from object detection and progress to instance segmentation since object detection is like a simplified version of instance segmentation. Below, we list the input and output of object detection and instance segmentation.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 33%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Input</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Object Detection</td>
<td>Image</td>
<td>Bounding boxes and class labels for each instance</td>
</tr>
<tr class="even">
<td>Instance Segmentation</td>
<td>Image</td>
<td>Bounding boxes, masks and class labels for each instance</td>
</tr>
</tbody>
</table>
<section id="the-first-step-r-cnn" class="level3" data-number="12.4.1">
<h3 data-number="12.4.1" class="anchored" data-anchor-id="the-first-step-r-cnn"><span class="header-section-number">12.4.1</span> The First Step: R-CNN</h3>
<p>Our journey begins with the introduction of Region-based Convolutional Neural Networks (R-CNN).</p>
<p>Imagine you are tasked with finding specific items in a crowded room (the image). The first step you need is a strategy to pinpoint potential areas where the items might be located. This is where <strong>Selective Search <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></strong> comes into play. It acts like a diligent assistant that systematically scans the image, suggesting potential regions that might contain the items. Once you have these potential regions, the next step is to understand what each region contains. In our example, we use <strong>ResNet-50</strong> to extract features from each region. With the features in hand, we then use a linear <strong>Support Vector Machine (SVM)</strong> to classify each region into one of the 10 classes. But recognizing objects is just part of the story. To ensure precision in locating these objects, we employ a <strong>linear regression model</strong> to refine the coordinates of the bounding boxes.</p>
<p><strong>Summary of R-CNN components:</strong></p>
<ul>
<li><strong>Region proposal (selective search)</strong>: Generate potential object regions.</li>
<li><strong>Feature extraction (ResNet-50)</strong>: Extract features from each region.</li>
<li><strong>Classification (linear SVM)</strong>: Classify each region into one of the 10 classes.</li>
<li><strong>Bounding box regression (linear regression)</strong>: Refine the coordinates of the bounding boxes.</li>
</ul>
<p><strong>Limitations of R-CNN:</strong></p>
<ul>
<li><strong>Slow inference</strong>: Processes each region independently, leading to inefficiency.</li>
<li><strong>Complex training</strong>: Requires multiple models and stages.</li>
<li><strong>Inefficient feature extraction</strong>: Redundant computations for overlapping regions.</li>
</ul>
</section>
<section id="evolution-to-fast-r-cnn" class="level3" data-number="12.4.2">
<h3 data-number="12.4.2" class="anchored" data-anchor-id="evolution-to-fast-r-cnn"><span class="header-section-number">12.4.2</span> Evolution to Fast R-CNN</h3>
<p>Continuing our journey towards instance segmentation, we move from the foundational R-CNN to a more advanced and efficient model: Fast R-CNN. This evolution is necessary to overcome the challenges faced by R-CNN.</p>
<p>In our quest for faster and more efficient object detection, Fast R-CNN addresses the limitations of its predecessor, R-CNN. Imagine now that instead of examining each potential object region individually, you first take a comprehensive view of the entire scene. This panoramic approach forms the basis of Fast R-CNN.</p>
<ul>
<li><p><strong>Feature extraction (single pass)</strong>: Fast R-CNN starts by processing the entire image in a single forward pass through the network, using a shared convolutional feature map. This method not only accelerates the process but also eliminates the need to repeatedly extract features for overlapping regions, directly addressing the inefficiency of R-CNN.</p></li>
<li><p><strong>Region proposal (selective search)</strong>: Although it continues to use Selective Search to suggest possible object regions, the way these proposals are handled is integrated into a more streamlined workflow.</p></li>
<li><p><strong>RoI pooling</strong>: To efficiently zoom into specific regions of interest (RoIs), Fast R-CNN introduces RoI Pooling. This component crops and resizes the shared feature map for each proposed region, much like focusing a camera lens on the areas of interest. RoI Pooling ensures that each region is presented in a consistent shape to the classifier, optimizing the use of features extracted in the initial pass.</p></li>
<li><p><strong>Classification and bounding box regression (fully connected layers)</strong>: Fast R-CNN employs fully connected layers to perform both classification and bounding box regression simultaneously. This integration simplifies the training pipeline and ensures that both tasks benefit from the shared feature representation, in contrast to the separate models used in R-CNN.</p></li>
</ul>
<p><strong>Summary of Fast R-CNN Components:</strong></p>
<ul>
<li><strong>Feature extraction (single pass)</strong>: Processes the entire image once, reducing redundancy and speeding up inference.</li>
<li><strong>Region proposal (selective search)</strong>: Still used, but proposals are more efficiently integrated into the workflow.</li>
<li><strong>RoI pooling</strong>: Efficiently refines regions of interest for classification and bounding box adjustments.</li>
<li><strong>Classification and bounding box regression (fully connected layers)</strong>: Conducts these tasks concurrently, simplifying the model architecture.</li>
</ul>
<p><strong>Solving R-CNN Limitations:</strong></p>
<ul>
<li><strong>Slow inference</strong>: Addressed by processing the entire image in a single pass, reducing computational load.</li>
<li><strong>Complex training pipeline</strong>: Simplified by integrating classification and regression within a unified model.</li>
<li><strong>Inefficient feature extraction</strong>: Eliminated by using shared feature maps, reducing redundant processing.</li>
</ul>
<p><strong>Limitations of Fast R-CNN:</strong></p>
<ul>
<li><strong>Dependence on slow region proposal methods</strong>: The reliance on Selective Search for region proposals limits the speed of the overall system.</li>
<li><strong>Separate proposal and detection stages</strong>: Although integrated more efficiently than R-CNN, the use of Selective Search still represents a bottleneck.</li>
<li><strong>Potential for further speed improvements</strong>: While faster than R-CNN, Fast R-CNN does not fully utilize potential speed optimizations through end-to-end training of the proposal mechanism.</li>
</ul>
</section>
<section id="introducing-faster-r-cnn" class="level3" data-number="12.4.3">
<h3 data-number="12.4.3" class="anchored" data-anchor-id="introducing-faster-r-cnn"><span class="header-section-number">12.4.3</span> Introducing Faster R-CNN</h3>
<p>As we progress in our journey towards instance segmentation, we arrive at Faster R-CNN, which further refines the processes established by Fast R-CNN. This model addresses the bottlenecks that persisted in previous approaches, particularly regarding region proposal efficiency.</p>
</section>
<section id="the-innovation-of-faster-r-cnn" class="level3" data-number="12.4.4">
<h3 data-number="12.4.4" class="anchored" data-anchor-id="the-innovation-of-faster-r-cnn"><span class="header-section-number">12.4.4</span> The Innovation of Faster R-CNN</h3>
<p>Faster R-CNN represents a significant leap forward in object detection by introducing a component that eliminates the dependence on slow external region proposal methods. This innovation stems from the integration of a Region Proposal Network (RPN), which transforms the object detection process into a unified, end-to-end trainable system.</p>
<ul>
<li><p><strong>Feature extraction (single pass)</strong>: Like Fast R-CNN, Faster R-CNN begins by processing the entire image in a single pass through a convolutional network to produce a shared feature map. This ensures efficient feature extraction and serves as the foundation for both region proposal and object detection tasks.</p></li>
<li><p><strong>Region proposal network (RPN)</strong>: The key innovation of Faster R-CNN is the RPN, which is integrated directly into the convolutional feature map. The RPN efficiently generates region proposals by sliding a small network over the shared feature map. This network outputs a set of objectness scores and refined bounding box coordinates for anchors at each spatial location, effectively replacing the slow Selective Search process used in Fast R-CNN.</p></li>
<li><p><strong>RoI pooling</strong>: After generating region proposals, Faster R-CNN employs RoI Pooling to extract fixed-size feature maps from the shared feature map for each proposal. This ensures that each region of interest is consistently prepared for the subsequent classification and bounding box refinement stages.</p></li>
<li><p><strong>Classification and bounding box regression (fully connected layers)</strong>: The proposals are then fed into fully connected layers for simultaneous classification and bounding box regression, refining both the class labels and bounding box coordinates.</p></li>
</ul>
<p><strong>Summary of Faster R-CNN Components:</strong></p>
<ul>
<li><strong>Feature extraction (single pass)</strong>: Processes the entire image once, providing a shared feature map for all subsequent tasks.</li>
<li><strong>Region proposal network (RPN)</strong>: Generates high-quality region proposals directly from the feature map, replacing slower methods.</li>
<li><strong>RoI pooling</strong>: Standardizes proposal feature maps for further processing.</li>
<li><strong>Classification and bounding box regression (fully connected layers)</strong>: Conducts these tasks effectively and efficiently, using the refined proposals.</li>
</ul>
<p><strong>Addressing Fast R-CNN Limitations:</strong></p>
<ul>
<li><strong>Dependence on slow region proposal methods</strong>: Solved by integrating the RPN, which provides fast and efficient region proposals.</li>
<li><strong>Separate proposal and detection stages</strong>: Unified into a single, end-to-end trainable model.</li>
<li><strong>Potential for further speed improvements</strong>: Realized through the RPN, which allows for significant speed gains and paves the way for real-time object detection capabilities.</li>
</ul>
</section>
</section>
<section id="transition-to-instance-segmentation-mask-r-cnn" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="transition-to-instance-segmentation-mask-r-cnn"><span class="header-section-number">12.5</span> Transition to Instance Segmentation: Mask R-CNN</h2>
<p>Having refined the processes of object detection with Faster R-CNN, we now transition to the realm of instance segmentation with Mask R-CNN. This model not only detects objects but also delineates each instance with precise pixel-level masks, taking the capabilities of Faster R-CNN a step further.</p>
<p>Mask R-CNN builds upon the robust framework of Faster R-CNN, adding a crucial component for instance segmentation. It retains the speed and efficiency of Faster R-CNN while introducing new functionality without compromising performance.</p>
<ul>
<li><p><strong>Feature extraction (single pass)</strong>: Mask R-CNN, like its predecessor, begins by processing the entire image in a single pass through a convolutional network to produce a shared feature map. This feature map serves as the foundation for both region proposal and instance segmentation tasks.</p></li>
<li><p><strong>Region proposal network (RPN)</strong>: The RPN continues to play a pivotal role in generating high-quality region proposals directly from the shared feature map. This component ensures that only the most promising regions are considered for further processing.</p></li>
<li><p><strong>RoI Align</strong>: To address the spatial misalignment introduced by RoI Pooling in Faster R-CNN, Mask R-CNN introduces RoI Align. This new component improves the precision of feature extraction by avoiding quantization errors, ensuring that the extracted feature maps for each region of interest are perfectly aligned with the input image.</p></li>
<li><p><strong>Classification, bounding box regression, and mask prediction (parallel heads)</strong>: Mask R-CNN extends the two-head structure of Faster R-CNN by adding a third branch specifically for mask prediction. This additional branch predicts a binary mask for each region of interest, allowing the model to not only classify objects and refine bounding boxes but also generate accurate pixel-level masks.</p></li>
</ul>
<p><strong>Summary of Mask R-CNN Components:</strong></p>
<ul>
<li><strong>Feature extraction (single pass)</strong>: Processes the entire image once, ensuring efficient use of computational resources.</li>
<li><strong>Region proposal network (RPN)</strong>: Continues to provide fast and efficient region proposals.</li>
<li><strong>RoI Align</strong>: Refines feature extraction for precise spatial alignment.</li>
<li><strong>Classification, bounding box regression, and mask prediction (parallel heads)</strong>: Conducts these tasks simultaneously, enabling comprehensive instance segmentation.</li>
</ul>
<p><strong>Enhancements from Faster R-CNN:</strong></p>
<ul>
<li><strong>Pixel-level precision</strong>: Achieved through RoI Align, improving spatial alignment and mask accuracy.</li>
<li><strong>Instance segmentation capability</strong>: Added through the mask prediction branch, enabling the model to generate detailed masks for each object instance.</li>
</ul>
</section>
<section id="conclusion" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">12.6</span> Conclusion</h2>
<p>We’ve explored the evolution from R-CNN to Mask R-CNN, showing how each step builds on the last to improve performance. This process demonstrates how you can apply similar improvements to tackle a wide range of advanced tasks, not just instance segmentation. By understanding these developments, you’re equipped to enhance existing models and create innovative solutions for various challenges in computer vision.</p>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><a href="https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">R. Girshick, J. Donahue, T. Darrell, J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” CVPR 2014.</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">R. Girshick, “Fast R-CNN,” ICCV 2015.</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><a href="https://proceedings.neurips.cc/paper_files/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf">S. Ren, K. He, R. Girshick, J. Sun, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” NIPS 2015.</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf">K. He, G. Gkioxari, P. Dollár, R. Girshick, “Mask R-CNN,” ICCV 2017.</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><a href="https://www.linkedin.com/pulse/medical-image-diagnosis-roles-object-detection-segmentation-egvcc">https://www.linkedin.com/pulse/medical-image-diagnosis-roles-object-detection-segmentation-egvcc</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><a href="https://www.youtube.com/watch?v=tNLtXb04i3w">https://www.youtube.com/watch?v=tNLtXb04i3w</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><a href="https://www.mdpi.com/2072-4292/10/9/1487">Zhang, W., Witharana, C., Liljedahl, A. K., &amp; Kanevskiy, M. (2018). Deep convolutional neural networks for automated characterization of arctic ice-wedge polygons in very high spatial resolution aerial imagery. Remote Sensing, 10(9), 1487.</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><a href="https://dl.acm.org/doi/abs/10.1145/3557918.3565869">Li, W., Hsu, C. Y., Wang, S., Witharana, C., &amp; Liljedahl, A. (2022, November). Real-time GeoAI for high-resolution mapping and segmentation of arctic permafrost features: the case of ice-wedge polygons. In Proceedings of the 5th ACM SIGSPATIAL international workshop on AI for geographic knowledge discovery (pp.&nbsp;62-65).</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><a href="https://link.springer.com/article/10.1007/s11263-013-0620-5">J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders, “Selective search for object recognition,” International Journal of Computer Vision, vol.&nbsp;104, no. 2, pp.&nbsp;154-171, 2013.</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../sections/guest-lecture-yili-arts-dataset.html" class="pagination-link" aria-label="Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../sections/intro-to-dl-libraries-for-image-analysis.html" class="pagination-link" aria-label="Introduction to Deep Learning Libraries for Image Analysis">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Introduction to Deep Learning Libraries for Image Analysis</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<!-- Default Statcounter code for cyber2a online course
http://cyber2a.github.io/cyber2a-course/ -->
<script type="text/javascript">
    var sc_project=13129980; 
    var sc_invisible=1; 
    var sc_security="fa33fcfd"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async=""></script>
    <noscript><div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img class="statcounter" src="https://c.statcounter.com/13129980/0/fa33fcfd/1/" alt="Web Analytics" referrerpolicy="no-referrer-when-downgrade"></a></div></noscript>
    <!-- End of Statcounter Code -->




</body></html>