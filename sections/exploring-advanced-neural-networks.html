<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>12&nbsp; Exploring Advanced Neural Networks: Instance Segmentation – Cyber2A: AI for Arctic Research</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../sections/intro-to-dl-libraries-for-image-analysis.html" rel="next">
<link href="../sections/guest-lecture-yili-arts-dataset.html" rel="prev">
<link href="../images/index/arcticlogo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-3584025bd2775ba93d20b6dabf7256d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../sections/guest-lecture-yili-arts-dataset.html"><b>Day 3: Advanced AI Workflows and Models</b></a></li><li class="breadcrumb-item"><a href="../sections/exploring-advanced-neural-networks.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Exploring Advanced Neural Networks: Instance Segmentation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Cyber2A: AI for Arctic Research</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/cyber2a/cyber2a-course/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 1: Introduction to AI and Arctic Science</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/breaking-the-ice-with-ai-in-arctic-science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Breaking the Ice with AI in Arctic Science</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-for-everyone.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">AI for Everyone</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-ready-data-in-arctic-research.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">AI-Ready Data in Arctic Research</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/data-annotation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Data Annotation: The Foundation of Deep Learning Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-data-annotation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Hands-On Lab: Data Annotation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 2: AI Fundamentals and Techniques</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/the-building-blocks-of-nn-and-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The Building Blocks of Neural Networks and Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/intro-to-pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to PyTorch: Core Functionalities and Advantages</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Hands-On Lab: PyTorch</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/permafrost-discovery-gateway.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Permafrost Discovery Gateway</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-ethics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">AI Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 3: Advanced AI Workflows and Models</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/guest-lecture-yili-arts-dataset.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/exploring-advanced-neural-networks.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Exploring Advanced Neural Networks: Instance Segmentation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/intro-to-dl-libraries-for-image-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Introduction to Deep Learning Libraries for Image Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 4: Workflows and Foundation Models</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/ai-workflows-and-mlops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">AI Workflows and MLOps: From Development to Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-ai-workflows.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Hands-On Lab: AI Workflows</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/foundation-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Foundation Models: The Cornerstones of Modern AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/hands-on-lab-foundation-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Hands-On Lab: Foundation Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/reproducibility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Reproducibility</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text"><b>Day 5: AI Frontiers</b></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sections/the-fun-and-frontiers-of-ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">The Fun and Frontiers of AI: Innovation, Imagination, Interaction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#what-is-instance-segmentation" id="toc-what-is-instance-segmentation" class="nav-link" data-scroll-target="#what-is-instance-segmentation"><span class="header-section-number">12.1</span> What is instance segmentation?</a></li>
  <li><a href="#why-does-it-matter" id="toc-why-does-it-matter" class="nav-link" data-scroll-target="#why-does-it-matter"><span class="header-section-number">12.2</span> Why Does It Matter?</a>
  <ul class="collapse">
  <li><a href="#fine-grained-precision" id="toc-fine-grained-precision" class="nav-link" data-scroll-target="#fine-grained-precision">1. Fine-grained precision</a></li>
  <li><a href="#distinguishing-look-alikes" id="toc-distinguishing-look-alikes" class="nav-link" data-scroll-target="#distinguishing-look-alikes">2. Distinguishing look-alikes</a></li>
  <li><a href="#enabling-interaction" id="toc-enabling-interaction" class="nav-link" data-scroll-target="#enabling-interaction">3. Enabling interaction</a></li>
  <li><a href="#applications-in-the-wild" id="toc-applications-in-the-wild" class="nav-link" data-scroll-target="#applications-in-the-wild">Applications in the Wild</a></li>
  </ul></li>
  <li><a href="#from-building-blocks-to-advanced-networks" id="toc-from-building-blocks-to-advanced-networks" class="nav-link" data-scroll-target="#from-building-blocks-to-advanced-networks"><span class="header-section-number">12.3</span> From Building Blocks to Advanced Networks</a></li>
  <li><a href="#starting-point-classification-networks" id="toc-starting-point-classification-networks" class="nav-link" data-scroll-target="#starting-point-classification-networks"><span class="header-section-number">12.4</span> Starting Point: Classification Networks</a>
  <ul class="collapse">
  <li><a href="#revisiting-our-three-building-blocks" id="toc-revisiting-our-three-building-blocks" class="nav-link" data-scroll-target="#revisiting-our-three-building-blocks">Revisiting our three building blocks</a></li>
  <li><a href="#a-quick-look-under-the-hood" id="toc-a-quick-look-under-the-hood" class="nav-link" data-scroll-target="#a-quick-look-under-the-hood">A quick look under the hood</a></li>
  <li><a href="#customizing-for-your-own-task" id="toc-customizing-for-your-own-task" class="nav-link" data-scroll-target="#customizing-for-your-own-task">Customizing for your own task</a></li>
  </ul></li>
  <li><a href="#the-evolution-of-detection-models" id="toc-the-evolution-of-detection-models" class="nav-link" data-scroll-target="#the-evolution-of-detection-models"><span class="header-section-number">12.5</span> The Evolution of Detection Models</a>
  <ul class="collapse">
  <li><a href="#r-cnn-the-first-step" id="toc-r-cnn-the-first-step" class="nav-link" data-scroll-target="#r-cnn-the-first-step"><span class="header-section-number">12.5.1</span> R-CNN: The First Step</a></li>
  <li><a href="#fast-r-cnn-sharing-features" id="toc-fast-r-cnn-sharing-features" class="nav-link" data-scroll-target="#fast-r-cnn-sharing-features"><span class="header-section-number">12.5.2</span> Fast R-CNN: Sharing Features</a></li>
  <li><a href="#faster-r-cnn-end-to-end-detection" id="toc-faster-r-cnn-end-to-end-detection" class="nav-link" data-scroll-target="#faster-r-cnn-end-to-end-detection"><span class="header-section-number">12.5.3</span> Faster R-CNN: End-to-End Detection</a></li>
  </ul></li>
  <li><a href="#mask-r-cnn-from-detection-to-segmentation" id="toc-mask-r-cnn-from-detection-to-segmentation" class="nav-link" data-scroll-target="#mask-r-cnn-from-detection-to-segmentation"><span class="header-section-number">12.6</span> Mask R-CNN: From Detection to Segmentation</a>
  <ul class="collapse">
  <li><a href="#the-key-idea-add-a-mask-branch" id="toc-the-key-idea-add-a-mask-branch" class="nav-link" data-scroll-target="#the-key-idea-add-a-mask-branch">The key idea: Add a mask branch</a></li>
  <li><a href="#step-by-step-walkthrough-2" id="toc-step-by-step-walkthrough-2" class="nav-link" data-scroll-target="#step-by-step-walkthrough-2">Step-by-step walkthrough</a></li>
  </ul></li>
  <li><a href="#putting-it-all-together" id="toc-putting-it-all-together" class="nav-link" data-scroll-target="#putting-it-all-together"><span class="header-section-number">12.7</span> Putting It All Together</a>
  <ul class="collapse">
  <li><a href="#from-classification-to-instance-segmentation" id="toc-from-classification-to-instance-segmentation" class="nav-link" data-scroll-target="#from-classification-to-instance-segmentation"><strong>From classification to instance segmentation</strong></a></li>
  <li><a href="#visual-timeline" id="toc-visual-timeline" class="nav-link" data-scroll-target="#visual-timeline"><strong>Visual timeline</strong></a></li>
  <li><a href="#a-recurring-pattern" id="toc-a-recurring-pattern" class="nav-link" data-scroll-target="#a-recurring-pattern"><strong>A recurring pattern</strong></a></li>
  <li><a href="#a-unifying-view" id="toc-a-unifying-view" class="nav-link" data-scroll-target="#a-unifying-view"><strong>A unifying view</strong></a></li>
  </ul></li>
  <li><a href="#reflection-and-discussion" id="toc-reflection-and-discussion" class="nav-link" data-scroll-target="#reflection-and-discussion"><span class="header-section-number">12.8</span> Reflection and Discussion</a>
  <ul class="collapse">
  <li><a href="#looking-back" id="toc-looking-back" class="nav-link" data-scroll-target="#looking-back"><strong>Looking back</strong></a></li>
  <li><a href="#reflection-prompts" id="toc-reflection-prompts" class="nav-link" data-scroll-target="#reflection-prompts"><strong>Reflection Prompts</strong></a></li>
  <li><a href="#key-takeaway" id="toc-key-takeaway" class="nav-link" data-scroll-target="#key-takeaway"><strong>Key takeaway</strong></a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../sections/guest-lecture-yili-arts-dataset.html"><b>Day 3: Advanced AI Workflows and Models</b></a></li><li class="breadcrumb-item"><a href="../sections/exploring-advanced-neural-networks.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Exploring Advanced Neural Networks: Instance Segmentation</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Exploring Advanced Neural Networks: Instance Segmentation</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="overview" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="overview">Overview</h2>
<p>In the previous chapters, we learned how neural networks can recognize patterns in data, classify images, and even make pixel-wise predictions. But so far, our models could only tell <strong>what</strong> is in an image, but not <strong>where</strong> it is or <strong>what exact shape</strong> it has.</p>
<p>This chapter takes the next step. We’ll explore how neural networks evolved from simple classifiers into advanced models that can detect and outline every object in a scene, a task known as <strong>instance segmentation</strong>.</p>
<p>Think of it as moving from <strong>naming</strong> objects to <strong>understanding</strong> them in context. <br>For example, if a photo shows three dogs playing in the park, a classification network can tell you <em>“there are dogs”</em>. <br>An object-detection model goes further: <em>“there are three dogs, each at these locations”</em>. <br>An instance-segmentation model goes even further: <em>“here are the three dogs and here are the exact pixels that belong to each one”</em>.</p>
<p>In this section, you will:</p>
<ul>
<li>Understand the difference between <strong>classification</strong>, <strong>object detection</strong>, <strong>semantic segmentation</strong>, and <strong>instance segmentation</strong>.</li>
<li>Learn how all these tasks share the same building blocks, <strong>input</strong>, <strong>feature extractor</strong>, and <strong>output</strong>, but differ in how those blocks are connected.</li>
<li>Follow the historical path of neural networks from <strong>R-CNN<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> → Fast R-CNN<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> → Faster R-CNN<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> → Mask R-CNN<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></strong>, seeing how each new design solved a real problem from the previous one.</li>
</ul>
<p>By the end, you’ll see that advanced models aren’t mysterious or built from scratch. They’re the result of <strong>asking new questions</strong> and <strong>tweaking familiar parts</strong> of the same network structure you already know.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Every time researchers faced a limitation, for example, “it’s too slow,” “it misses small objects,” “it can’t tell shapes apart”, they modified one of the three core components: <strong>input</strong>, <strong>feature extractor</strong>, or <strong>output</strong>.</p>
<p>As you read, try guessing <em>which</em> component each innovation changes!</p>
</div>
</div>
</section>
<section id="what-is-instance-segmentation" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="what-is-instance-segmentation"><span class="header-section-number">12.1</span> What is instance segmentation?</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/ins-seg.png" class="img-fluid figure-img"></p>
<figcaption>Source: https://manipulation.csail.mit.edu/segmentation.html</figcaption>
</figure>
</div>
<p>Before diving into architectures, let’s first understand the concepts.</p>
<p>Computer vision has several “levels of understanding”, and each answers a slightly different question about an image.</p>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>What it asks</th>
<th>What the model outputs</th>
<th>Example thought</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Image Classification</strong></td>
<td><em>What kinds of objects are in this image?</em></td>
<td>One label per image</td>
<td>“This picture contains sheep.”</td>
</tr>
<tr class="even">
<td><strong>Object Detection</strong></td>
<td><em>Where are those objects?</em></td>
<td>Bounding boxes + labels</td>
<td>“There are three sheep there and a dog here.”</td>
</tr>
<tr class="odd">
<td><strong>Semantic Segmentation</strong></td>
<td><em>Which class does each pixel belong to?</em></td>
<td>Pixel-wise class map</td>
<td>“Color every sheep pixel blue, every dog pixel red.”</td>
</tr>
<tr class="even">
<td><strong>Instance Segmentation</strong></td>
<td><em>Which exact pixels belong to each individual object?</em></td>
<td>A mask + label for each instance</td>
<td>“Here are three separate sheep and a dog, each with its own outline.”</td>
</tr>
</tbody>
</table>
<p>Instance segmentation therefore <strong>combines the strengths of object detection and semantic segmentation</strong>. It both <strong>detects</strong> objects (like detection) and <strong>delineates</strong> them (like segmentation). The result is a collection of <em>colored masks</em>, each describing one distinct instance of an object.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Key Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Instance segmentation tells the model: <em>Don’t just find all the cats, tell me which pixels belong to Cat #1, Cat #2, and Cat #3.</em></p>
</div>
</div>
<p>Behind the scenes, this requires the model to learn fine-grained spatial details while still recognizing object categories. It’s a bit like tracing a outline after recognizing the object’s identity.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Think of an example in your field:</p>
<p>Would you care the distinction between the different instances of the same object, or just the presence of the object? Would you rather have a box around a region, or the exact outline of that region’s shape?</p>
<p>That choice usually determines which task you need to solve.</p>
</div>
</div>
</section>
<section id="why-does-it-matter" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="why-does-it-matter"><span class="header-section-number">12.2</span> Why Does It Matter?</h2>
<p>Now that we know what instance segmentation is, let’s ask a simple question: <strong>why go through all that extra effort?</strong></p>
<p>After all, drawing precise masks for every object sounds expensive and complex. But for many real-world problems, those extra pixels make a big difference.</p>
<section id="fine-grained-precision" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="fine-grained-precision">1. Fine-grained precision</h3>
<p>Bounding boxes only tell <em>where</em> an object roughly is. Instance segmentation tells <em>exactly which pixels</em> belong to it. That’s crucial whenever <strong>shape, size, or boundaries</strong> matter. For example, measuring a tumor, estimating crop coverage, or mapping glacial melt ponds.</p>
</section>
<section id="distinguishing-look-alikes" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="distinguishing-look-alikes">2. Distinguishing look-alikes</h3>
<p>Semantic segmentation colors all sheep the same color, as if they’re one big flock. Instance segmentation separates each sheep into its own mask. This distinction lets us <strong>count, track, and compare</strong> individuals, which is essential for crowd analysis, wildlife monitoring, or object-by-object statistics.</p>
</section>
<section id="enabling-interaction" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="enabling-interaction">3. Enabling interaction</h3>
<p>In robotics, augmented reality, or autonomous driving, systems must know <em>where one object ends and another begins</em>. With pixel-accurate masks, robots can grasp, avoid, or label objects more safely and reliably. Likewise, AR apps can anchor digital objects precisely on real-world surfaces.</p>
</section>
<section id="applications-in-the-wild" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="applications-in-the-wild">Applications in the Wild</h3>
<table class="table-striped caption-top table">
<thead>
<tr class="header">
<th>Field</th>
<th>How instance segmentation helps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>🧬 Medical imaging</td>
<td>Identifies and measures individual organs, lesions, or cells with pixel-level accuracy <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</td>
</tr>
<tr class="even">
<td>🤖 Robotics &amp; automation</td>
<td>Allows robots to perceive and interact with separate objects in cluttered scenes <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</td>
</tr>
<tr class="odd">
<td>🌍 Environmental monitoring</td>
<td>Maps terrain features such as permafrost polygons or vegetation patches for change detection <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>.</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>In your own research or projects, where would <strong>pixel-level detail</strong> unlock new insight?</p>
<p>Try to picture the same task using only bounding boxes. What information would you lose?</p>
</div>
</div>
<p>Next, we’ll peek under the hood: how do models actually achieve these capabilities?</p>
<p>To answer that, we’ll revisit our three familiar friends: <strong>input</strong>, <strong>feature extractor</strong>, and <strong>output</strong>, and see how every “advanced” network is just a creative remix of those parts.</p>
</section>
</section>
<section id="from-building-blocks-to-advanced-networks" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="from-building-blocks-to-advanced-networks"><span class="header-section-number">12.3</span> From Building Blocks to Advanced Networks</h2>
<p>So far, we’ve seen <em>what</em> instance segmentation is and <em>why</em> it matters. Now, let’s look at <strong>how</strong> we can make a neural network actually do it.</p>
<p>The good news?</p>
<p>Every deep learning model, no matter how complex its name sounds, is built from the same three ingredients you’ve already met:</p>
<ol type="1">
<li><strong>Input adaptation:</strong> how data is prepared and passed into the network</li>
<li><strong>Feature extractor:</strong> the main body of the model that learns patterns</li>
<li><strong>Output adaptation:</strong> how the extracted information is translated into predictions</li>
</ol>
<p><img src="../images/dl-pytorch/models.png" class="img-fluid"></p>
<p>Think of it like a pipeline:</p>
<blockquote class="blockquote">
<p><strong>Input → Feature extractor → Output</strong></p>
</blockquote>
<p>Each part has its own job. When we move from <em>classification</em> to <em>detection</em> to <em>segmentation</em>, we don’t reinvent the pipeline; we just <strong>add new capabilities</strong> or <strong>modify one of the blocks</strong> to serve a different purpose.</p>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Input</th>
<th>Feature Extractor</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Classification</td>
<td>Image</td>
<td>CNN backbone (e.g., ResNet-50)</td>
<td>Class probabilities</td>
</tr>
<tr class="even">
<td>Object Detection</td>
<td>Image</td>
<td>Same backbone</td>
<td>Bounding boxes + class labels</td>
</tr>
<tr class="odd">
<td>Instance Segmentation</td>
<td>Image</td>
<td>Same backbone</td>
<td>Masks + class labels</td>
</tr>
</tbody>
</table>
<p>Notice how the <strong>feature extractor</strong> often stays the same, for example, ResNet-50, which you’ve already seen. What changes is mainly <strong>how we interpret</strong> those extracted features at the output.</p>
<p>This insight is powerful:</p>
<p>when researchers realized classification networks could already “see” complex features, they began reusing them for localization tasks: first for boxes, then for masks.</p>
<p>Each breakthrough (R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN) simply tweaked one or more of these three blocks.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Key Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Advanced networks aren’t mysterious new inventions; they’re <strong>incremental upgrades</strong> built on the same foundation: input, features, output.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Imagine you want your network to predict <em>depth maps</em> or <em>object edges.</em> Which component, input, feature extractor, or output, would you modify first?</p>
</div>
</div>
<p>Next, we’ll start from the most familiar example: a <strong>classification network</strong>, and see how small architectural changes gradually lead us toward <strong>instance segmentation</strong>.</p>
</section>
<section id="starting-point-classification-networks" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="starting-point-classification-networks"><span class="header-section-number">12.4</span> Starting Point: Classification Networks</h2>
<p>Before we leap into detection and segmentation, let’s anchor ourselves in something familiar: <strong>image classification.</strong></p>
<p>At its core, a classification network answers a simple question:</p>
<blockquote class="blockquote">
<p>“What category does this image belong to?”</p>
</blockquote>
<p>For example, given a picture of fruit, the network might output:</p>
<pre class="text"><code>apple: 0.85, banana: 0.10, orange: 0.05</code></pre>
<section id="revisiting-our-three-building-blocks" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="revisiting-our-three-building-blocks">Revisiting our three building blocks</h3>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 50%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Role in classification</th>
<th>Example (ResNet-50)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Input</td>
<td>Accept an image (e.g., 3 RGB channels) and resize/normalize it for the model.</td>
<td>224 × 224 × 3 tensor</td>
</tr>
<tr class="even">
<td>Feature Extractor</td>
<td>Learn rich visual patterns: edges → textures → shapes that describe the content.</td>
<td>Convolutional backbone of ResNet-50</td>
</tr>
<tr class="odd">
<td>Output Adaptation</td>
<td>Convert those features into class probabilities.</td>
<td>Final fully-connected layer + softmax</td>
</tr>
</tbody>
</table>
<p>So, when we say <em>“ResNet-50 backbone”</em>, we’re really talking about the <strong>feature extractor</strong> block.</p>
</section>
<section id="a-quick-look-under-the-hood" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="a-quick-look-under-the-hood">A quick look under the hood</h3>
<p>Let’s load a pre-trained ResNet-50 from PyTorch and peek at its structure:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the pre-trained ResNet-50 model</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.resnet50(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Caution</span>Click to view the whole architecture of ResNet-50
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>ResNet(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">64</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>(maxpool): MaxPool2d(kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, dilation<span class="op">=</span><span class="dv">1</span>, ceil_mode<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>(layer1): Sequential(</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): BasicBlock(</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">64</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">64</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BasicBlock(</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">64</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">64</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>(layer2): Sequential(</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): BasicBlock(</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">128</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">128</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">128</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>(downsample): Sequential(</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BatchNorm2d(<span class="dv">128</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BasicBlock(</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">128</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">128</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">128</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">128</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>(layer3): Sequential(</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): BasicBlock(</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">128</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">256</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">256</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">256</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>(downsample): Sequential(</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): Conv2d(<span class="dv">128</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BatchNorm2d(<span class="dv">256</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BasicBlock(</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">256</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">256</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">256</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">256</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>(layer4): Sequential(</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): BasicBlock(</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">256</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">512</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">512</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">512</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>(downsample): Sequential(</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>(<span class="dv">0</span>): Conv2d(<span class="dv">256</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BatchNorm2d(<span class="dv">512</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span>): BasicBlock(</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>(conv1): Conv2d(<span class="dv">512</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>(bn1): BatchNorm2d(<span class="dv">512</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>(relu): ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>(conv2): Conv2d(<span class="dv">512</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>(bn2): BatchNorm2d(<span class="dv">512</span>, eps<span class="op">=</span><span class="fl">1e-05</span>, momentum<span class="op">=</span><span class="fl">0.1</span>, affine<span class="op">=</span><span class="va">True</span>, track_running_stats<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>(avgpool): AdaptiveAvgPool2d(output_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>(fc): Linear(in_features<span class="op">=</span><span class="dv">512</span>, out_features<span class="op">=</span><span class="dv">1000</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
<p>You’ll see a long list of convolutional layers grouped into four stages (<code>layer1</code> to <code>layer4</code>), followed by:</p>
<pre class="text"><code>(avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
(fc): Linear(in_features=2048, out_features=1000, bias=True)</code></pre>
<p>That last line, the fully connected (<code>fc</code>) layer, is the <strong>output adaptation</strong>. It converts the learned features into 1,000 ImageNet class scores.</p>
</section>
<section id="customizing-for-your-own-task" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="customizing-for-your-own-task">Customizing for your own task</h3>
<p>Suppose we want to classify images into 10 classes (say, 10 types of flowers). We keep the same input and feature extractor, but swap the output head:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace the last layer with one suited for 10 classes</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>model.fc <span class="op">=</span> nn.Linear(<span class="dv">2048</span>, <span class="dv">10</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>That’s it! You’ve adapted a world-class backbone for your own dataset.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Key Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Classification models like ResNet-50 form the <strong>starting point</strong> for many advanced vision systems. Later, object-detection and segmentation models will reuse this exact feature extractor and simply modify what happens <strong>after</strong> it.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>If classification outputs a single label per image, what kind of modification might you need if you wanted to <strong>locate multiple objects</strong> instead?</p>
<p><em>Hint: the output block will need more than one prediction!</em></p>
</div>
</div>
<p>Next, we’ll follow that hint: evolving our classification network into models that can <em>find</em> and <em>label</em> multiple objects at once.</p>
<p>That journey begins with the first landmark in detection history: <strong>R-CNN</strong>.</p>
</section>
</section>
<section id="the-evolution-of-detection-models" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="the-evolution-of-detection-models"><span class="header-section-number">12.5</span> The Evolution of Detection Models</h2>
<p>Once we know how to classify an image, a natural next question appears:</p>
<blockquote class="blockquote">
<p>“Can the model also tell me where each object is?”</p>
</blockquote>
<p>This question led to one of the most important breakthroughs in computer vision: the evolution from simple classification to <strong>object detection</strong>, and eventually, <strong>instance segmentation</strong>.</p>
<p>Let’s trace this evolution through four key milestones:</p>
<p><strong>R-CNN → Fast R-CNN → Faster R-CNN → Mask R-CNN.</strong></p>
<p>Each model solves one major bottleneck of its predecessor.</p>
<section id="r-cnn-the-first-step" class="level3" data-number="12.5.1">
<h3 data-number="12.5.1" class="anchored" data-anchor-id="r-cnn-the-first-step"><span class="header-section-number">12.5.1</span> R-CNN: The First Step</h3>
<p>Imagine you’re a detective asked to locate every cat in a giant city photograph. You could inspect every pixel one by one, but that would take forever. Instead, you might first mark <em>possible locations</em> (windows, rooftops, gardens) and then zoom into each to decide whether a cat is really there.</p>
<p>That’s exactly the idea behind <strong>R-CNN (Region-based Convolutional Neural Network)</strong>, proposed by Ross Girshick et al.&nbsp;in 2014 <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>.</p>
<p>It was the first deep-learning model to detect <em>and</em> localize objects successfully.</p>
<section id="step-1-propose-candidate-regions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-1-propose-candidate-regions"><strong>Step 1: Propose candidate regions</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/home/step3-660x304.PNG" class="img-fluid figure-img"></p>
<figcaption>Illustration of Selective Search generating region proposals over an image. Source:<a href="https://www.geeksforgeeks.org/machine-learning/selective-search-for-object-detection-r-cnn/">geeksforgeeks</a></figcaption>
</figure>
</div>
<p>Before any neural network runs, R-CNN uses a separate algorithm called <strong>Selective Search<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></strong> to guess where objects might be. Think of it as a diligent assistant drawing ~2000 rectangles around promising areas of the image based on color, texture, and edges.</p>
<p>Each rectangle, called a <em>region proposal</em>, is a possible object.</p>
</section>
<section id="step-2-extract-features-for-each-region" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-2-extract-features-for-each-region"><strong>Step 2: Extract features for each region</strong></h4>
<p>For every proposed region, we crop that portion of the image, resize it to a standard size, and feed it through a CNN (for example, AlexNet or ResNet).</p>
<p>The CNN acts as the <strong>feature extractor</strong>, producing a numerical fingerprint that describes the visual content.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pseudocode for R-CNN feature extraction</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> region <span class="kw">in</span> selective_search(image):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    crop <span class="op">=</span> resize(image[region], (<span class="dv">224</span>, <span class="dv">224</span>))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> cnn(crop)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Because there are ~2000 regions, this step runs the CNN thousands of times per image. This is a key limitation we’ll revisit soon.</p>
</section>
<section id="step-3-classify-and-refine-boxes" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-3-classify-and-refine-boxes"><strong>Step 3: Classify and refine boxes</strong></h4>
<p>Once we have the feature vector for each region:</p>
<ul>
<li>A <strong>Support Vector Machine (SVM)</strong> predicts which class the region belongs to (cat, dog, car, …).</li>
<li>A <strong>linear regression model</strong> fine-tunes the coordinates of the bounding box to fit the object more tightly.</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>cls_score <span class="op">=</span> svm(features)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>bbox <span class="op">=</span> regressor(features)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>So R-CNN’s output consists of <strong>bounding boxes and class labels</strong>.</p>
</section>
<section id="step-4-combine-the-results" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-4-combine-the-results"><strong>Step 4: Combine the results</strong></h4>
<p>Finally, overlapping boxes are merged using <em>Non-Maximum Suppression (NMS)</em> <a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> so only the best one remains for each object.</p>
<p>The full pipeline looks like this:</p>
<p><img src="../images/dl-pytorch/r-cnn-pipeline.png" class="img-fluid"></p>
</section>
<section id="how-r-cnn-fits-into-our-three-block-framework" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="how-r-cnn-fits-into-our-three-block-framework"><strong>How R-CNN fits into our three-block framework</strong></h4>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 70%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>What R-CNN does</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Input adaptation</strong></td>
<td>Image + region proposals from Selective Search</td>
</tr>
<tr class="even">
<td><strong>Feature extractor</strong></td>
<td>CNN applied separately to each region</td>
</tr>
<tr class="odd">
<td><strong>Output adaptation</strong></td>
<td>SVM + regressor for class + bounding box</td>
</tr>
</tbody>
</table>
</section>
<section id="what-made-r-cnn-revolutionary" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="what-made-r-cnn-revolutionary"><strong>What made R-CNN revolutionary</strong></h4>
<ul>
<li>First to prove that <strong>deep CNN features</strong> outperform hand-crafted features (like HOG or SIFT) for object recognition.</li>
<li>Showed that <strong>localizing</strong> multiple objects could be framed as multiple independent classification problems.</li>
<li>Inspired the whole family of “Region-based CNNs” that followed.</li>
</ul>
</section>
<section id="but-it-was-painfully-slow" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="but-it-was-painfully-slow"><strong>But… it was painfully slow</strong></h4>
<ul>
<li>~2000 regions × 1 CNN forward pass ≈ minutes per image.</li>
<li>Multi-stage training: CNN, then SVM, then regressor, then fine-tuning.</li>
<li>Heavy disk usage: each cropped region saved separately to disk before training.</li>
</ul>
<p>The model worked, but not for real-time applications. Researchers loved its accuracy but craved speed.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>If every proposed region needs its own CNN pass, what could you change in the <strong>feature extractor</strong> to make this process faster?</p>
</div>
</div>
<p>Up next, we’ll see how <strong>Fast R-CNN</strong> answered exactly that question: by realizing we can share computation across all regions instead of repeating it thousands of times.</p>
</section>
</section>
<section id="fast-r-cnn-sharing-features" class="level3" data-number="12.5.2">
<h3 data-number="12.5.2" class="anchored" data-anchor-id="fast-r-cnn-sharing-features"><span class="header-section-number">12.5.2</span> Fast R-CNN: Sharing Features</h3>
<p>R-CNN worked, but it was <strong>painfully slow</strong>. Imagine our detective from earlier: instead of scanning one high-resolution map, they visit every single building in the city one by one. It works, but it’s exhausting.</p>
<p>Ross Girshick, the same researcher who created R-CNN, asked a simple but brilliant question in 2015:</p>
<blockquote class="blockquote">
<p>“What if we <strong>extract features just once</strong> for the whole image, and reuse them for all regions?”</p>
</blockquote>
<p>That idea became <strong>Fast R-CNN</strong> – a version that was not only faster but also simpler and more elegant.</p>
<section id="the-key-idea-share-the-feature-map" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-key-idea-share-the-feature-map"><strong>The key idea: share the feature map</strong></h4>
<p>Instead of running a CNN 2000 times, Fast R-CNN runs it <strong>once</strong> on the entire image to create a <strong>shared feature map</strong>. Each proposed region (from Selective Search) is then projected onto this feature map rather than cropped directly from the raw image.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/fast-rcnn-diagram.png" class="img-fluid figure-img"></p>
<figcaption>Fast R-CNN idea: shared feature map + RoI pooling.</figcaption>
</figure>
</div>
<p>So, instead of thousands of redundant CNN computations, we reuse the same deep features everywhere.</p>
</section>
<section id="step-by-step-walkthrough" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-by-step-walkthrough"><strong>Step-by-step walkthrough</strong></h4>
<ol type="1">
<li><p><strong>Input</strong>: Feed the <em>entire image</em> into the CNN backbone (e.g., VGG16 or ResNet).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>feature_map <span class="op">=</span> cnn(image)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Region proposals</strong>: Use <em>Selective Search</em> to get ~2000 candidate regions (same as R-CNN). These are now rectangles in <strong>image coordinates</strong>.</p></li>
<li><p><strong>Map regions onto the feature map</strong>: Each region is projected from image coordinates to the corresponding area on the feature map.</p></li>
<li><p><strong>RoI Pooling</strong>: This is Fast R-CNN’s most clever trick. Because proposed regions vary in size, we can’t feed them directly into a fully connected layer. <strong>RoI Pooling</strong> crops the portion of the feature map corresponding to each region and <em>pools</em> it into a fixed spatial size (for example, 7×7).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> roi <span class="kw">in</span> proposals:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    roi_feat <span class="op">=</span> roi_pool(feature_map, roi, output_size<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Classification and bounding box regression</strong>: Each RoI feature (7×7×C) is flattened and sent through two fully connected layers, producing:</p>
<ul>
<li>Class scores</li>
<li>Bounding box refinements</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>cls_score, bbox <span class="op">=</span> head(roi_feat)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ol>
</section>
<section id="unified-training-and-loss" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="unified-training-and-loss"><strong>Unified training and loss</strong></h4>
<p>Unlike R-CNN, which trained separate models (CNN → SVM → regressor), Fast R-CNN trains everything together using a <strong>multi-task loss</strong>:</p>
<p><span class="math display">\[
L = L_\text{cls} + \lambda L_\text{bbox}
\]</span></p>
<ul>
<li><span class="math inline">\(L_\text{cls}\)</span>: classification loss (softmax cross-entropy)</li>
<li><span class="math inline">\(L_\text{bbox}\)</span>: bounding box regression loss (smooth L1)</li>
</ul>
<p>This end-to-end training makes the model both faster to train and more accurate.</p>
</section>
<section id="what-improved-from-r-cnn" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="what-improved-from-r-cnn"><strong>What improved from R-CNN</strong></h4>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>R-CNN</th>
<th>Fast R-CNN</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Feature extraction</td>
<td>Separate CNN per region</td>
<td>One CNN per image (shared)</td>
<td>🚀 10–20× faster</td>
</tr>
<tr class="even">
<td>Training</td>
<td>Multi-stage (CNN, SVM, regressor)</td>
<td>Unified, single-stage</td>
<td>Simpler</td>
</tr>
<tr class="odd">
<td>Storage</td>
<td>Pre-compute region features</td>
<td>On-the-fly computation</td>
<td>No extra disk space</td>
</tr>
<tr class="even">
<td>Accuracy</td>
<td>High</td>
<td>Higher (better end-to-end learning)</td>
<td>✔️</td>
</tr>
</tbody>
</table>
</section>
<section id="but-one-bottleneck-remains" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="but-one-bottleneck-remains"><strong>But one bottleneck remains…</strong></h4>
<p>Even though feature extraction became efficient, <strong>region proposals</strong> were still generated by the external <em>Selective Search</em> algorithm. Selective Search was CPU-based and slow. It took nearly two seconds per image.</p>
<p>So, while inference was much faster, it wasn’t yet <em>real-time</em>.</p>
</section>
<section id="how-fast-r-cnn-fits-into-our-three-block-framework" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="how-fast-r-cnn-fits-into-our-three-block-framework"><strong>How Fast R-CNN fits into our three-block framework</strong></h4>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 70%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>What changed from R-CNN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Input adaptation</strong></td>
<td>Still uses region proposals (Selective Search), but processes one shared image feature map.</td>
</tr>
<tr class="even">
<td><strong>Feature extractor</strong></td>
<td>Shared CNN backbone computes features once.</td>
</tr>
<tr class="odd">
<td><strong>Output adaptation</strong></td>
<td>Unified head with RoI Pooling + fully connected layers for classification and box regression.</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Key Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Fast R-CNN taught us that most of the heavy computation in object detection could be <strong>shared</strong>.</p>
<p>It’s like realizing you don’t need to bake a separate cake for each guest. You just slice one big cake into pieces.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <em>Selective Search</em> is now the slowest part, what if we teach the network itself to generate region proposals automatically?</p>
</div>
</div>
<p>That question led directly to the next major leap: <strong>Faster R-CNN</strong>: a model that learns to propose its own regions, eliminating Selective Search altogether.</p>
</section>
</section>
<section id="faster-r-cnn-end-to-end-detection" class="level3" data-number="12.5.3">
<h3 data-number="12.5.3" class="anchored" data-anchor-id="faster-r-cnn-end-to-end-detection"><span class="header-section-number">12.5.3</span> Faster R-CNN: End-to-End Detection</h3>
<p>Our detective has gotten smarter.</p>
<p>In R-CNN, they searched every building; in Fast R-CNN, they looked at one satellite map and zoomed in on areas of interest.</p>
<p>But they’re still relying on a <strong>human assistant</strong> (Selective Search) to tell them where to look next and that assistant is slow.</p>
<p>So, in 2015, a new idea arrived:</p>
<blockquote class="blockquote">
<p>“What if the <strong>network itself</strong> could learn where objects are likely to appear?”</p>
</blockquote>
<p>This question gave birth to <strong>Faster R-CNN</strong> by Shaoqing Ren et al.&nbsp;(2015) <a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>, the first truly <strong>end-to-end</strong> object-detection framework. Its secret weapon: the <strong>Region Proposal Network (RPN)</strong>.</p>
<section id="the-key-innovation-region-proposal-network-rpn" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-key-innovation-region-proposal-network-rpn"><strong>The key innovation: Region Proposal Network (RPN)</strong></h4>
<p>The RPN is a tiny neural network that slides over the shared feature map (produced by the CNN backbone) and proposes potential object regions automatically. In other words, the network learns to generate its own “Select me!” boxes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/faster-rcnn-diagram.png" class="img-fluid figure-img"></p>
<figcaption>Faster R-CNN pipeline: shared feature map feeding both the RPN and the detection head.</figcaption>
</figure>
</div>
</section>
<section id="step-by-step-walkthrough-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-by-step-walkthrough-1"><strong>Step-by-step walkthrough</strong></h4>
<ol type="1">
<li><p><strong>Feature extraction</strong>: As before, we start with a CNN backbone (e.g., ResNet-50 or VGG-16) that processes the entire image once:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>feature_map <span class="op">=</span> cnn(image)      <span class="co"># shared backbone</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This feature map is now shared by <strong>two heads</strong>: the RPN (proposal generator) and the detection head (classifier + box regressor).</p></li>
<li><p><strong>Generate anchors</strong>: At each location on the feature map, the model places a small set of <strong>anchor boxes</strong> – templates of different sizes and aspect ratios. You can think of them as little “nets” cast over the image, waiting to be adjusted.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>anchors <span class="op">=</span> generate_anchors(feature_map, scales<span class="op">=</span>[<span class="dv">128</span>,<span class="dv">256</span>,<span class="dv">512</span>], ratios<span class="op">=</span>[<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Each anchor will be tested: “Does this anchor contain an object?”</p></li>
<li><p><strong>Predict objectness and box offsets</strong>: The RPN slides a 3×3 convolutional window over the feature map. For each anchor, it outputs:</p>
<ul>
<li>an <strong>objectness score</strong> (probability that an object exists here), and</li>
<li>a <strong>bounding-box offset</strong> to refine the anchor’s shape and position.</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> anchor <span class="kw">in</span> anchors:</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    obj_score, bbox_offset <span class="op">=</span> rpn_head(feature_map, anchor)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>After this step, the RPN has thousands of proposals with confidence scores.</p></li>
<li><p><strong>Select top proposals</strong>: A post-processing step called <strong>Non-Maximum Suppression (NMS)</strong> removes redundant boxes. Typically, the top ~2000 highest-scoring proposals (300 in practice) are kept.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>proposals <span class="op">=</span> nms(anchors, obj_score, top_k<span class="op">=</span><span class="dv">300</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>These proposals replace the old Selective Search regions.</p></li>
<li><p><strong>RoI Pooling → Detection Head</strong>: Each proposed region is cropped (via RoI Pooling or RoI Align) from the shared feature map and fed into the detection head — the same classification + box regression module introduced in Fast R-CNN:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> roi <span class="kw">in</span> proposals:</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    roi_feat <span class="op">=</span> roi_pool(feature_map, roi)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    cls_score, bbox <span class="op">=</span> head(roi_feat)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ol>
</section>
<section id="training-as-one-network" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="training-as-one-network"><strong>Training as one network</strong></h4>
<p>Both the RPN and the detection head share the same convolutional features and are trained <strong>jointly</strong>. The total loss combines RPN loss + detection loss:</p>
<p><span class="math display">\[
L = L_\text{RPN-cls} + L_\text{RPN-bbox} + L_\text{Det-cls} + L_\text{Det-bbox}
\]</span></p>
<p>This means the entire system, from pixels to final boxes, learns together.</p>
</section>
<section id="why-it-matters" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="why-it-matters"><strong>Why it matters</strong></h4>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Fast R-CNN</th>
<th>Faster R-CNN</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Region proposals</td>
<td>Hand-crafted Selective Search (CPU)</td>
<td>Learned Region Proposal Network (GPU)</td>
<td>🚀 100× faster</td>
</tr>
<tr class="even">
<td>End-to-end training</td>
<td>Partial (Selective Search external)</td>
<td>Fully integrated</td>
<td>Unified</td>
</tr>
<tr class="odd">
<td>Speed</td>
<td>~2 FPS on GPU</td>
<td>~10 FPS on GPU</td>
<td>Real-time possibility</td>
</tr>
<tr class="even">
<td>Accuracy</td>
<td>High</td>
<td>Higher (learned proposals fit better)</td>
<td>✔️</td>
</tr>
</tbody>
</table>
<p>The RPN made object detection practical for real-world systems and set the foundation for everything that came after, including instance segmentation.</p>
</section>
<section id="how-faster-r-cnn-fits-into-our-three-block-framework" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="how-faster-r-cnn-fits-into-our-three-block-framework"><strong>How Faster R-CNN fits into our three-block framework</strong></h4>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 70%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>What changed from Fast R-CNN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Input adaptation</strong></td>
<td>Added Region Proposal Network to generate anchors and proposals internally.</td>
</tr>
<tr class="even">
<td><strong>Feature extractor</strong></td>
<td>Shared backbone features used by both RPN and detection head.</td>
</tr>
<tr class="odd">
<td><strong>Output adaptation</strong></td>
<td>Two branches (classification + regression) refine proposals end-to-end.</td>
</tr>
</tbody>
</table>
</section>
<section id="a-mental-picture" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="a-mental-picture"><strong>A mental picture</strong></h4>
<p>It’s as if our detective now works with a drone that automatically highlights potential objects from above. The detective still classifies them (cat, dog, car…), but no longer wastes time choosing <em>where</em> to look.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Key Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Faster R-CNN solved the last major bottleneck by teaching the network to propose its own regions. The result is a fully trainable system that detects objects with both speed and precision.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the model can now predict object locations and labels efficiently, what extra branch would you add to also predict each object’s <strong>exact shape</strong>?</p>
</div>
</div>
<p>Up next is exactly that extension: <strong>Mask R-CNN</strong>, which builds on Faster R-CNN to perform full <strong>instance segmentation</strong>, adding a mask prediction head for pixel-level detail.</p>
</section>
</section>
</section>
<section id="mask-r-cnn-from-detection-to-segmentation" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="mask-r-cnn-from-detection-to-segmentation"><span class="header-section-number">12.6</span> Mask R-CNN: From Detection to Segmentation</h2>
<p>At this point, our detective is lightning-fast. Thanks to Faster R-CNN, they can spot every object in a scene almost in real time. But there’s still one question left unanswered:</p>
<blockquote class="blockquote">
<p>“Can you <strong>draw the precise outline</strong> of each object, not just a box around it?”</p>
</blockquote>
<p>This challenge led to <strong>Mask R-CNN</strong> by Kaiming He et al.&nbsp;(2017) <a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>, a direct successor to Faster R-CNN that added the ability to perform <strong>instance segmentation</strong>, predicting a <strong>mask</strong> for every detected object.</p>
<p>Mask R-CNN didn’t reinvent the wheel; it simply bolted one more well-designed component onto the same framework.</p>
<section id="the-key-idea-add-a-mask-branch" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="the-key-idea-add-a-mask-branch">The key idea: Add a mask branch</h3>
<p>Faster R-CNN already produced <strong>bounding boxes</strong> and <strong>class labels</strong>. Mask R-CNN adds a <strong>third parallel branch</strong> that predicts a binary mask, a small image showing which pixels belong to the detected object.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/mask-rcnn-diagram.png" class="img-fluid figure-img"></p>
<figcaption>Mask R-CNN: shared backbone → RPN → RoI Align → three heads (cls / bbox / mask).</figcaption>
</figure>
</div>
<p>So for each proposed region, the model now outputs:</p>
<ol type="1">
<li><strong>Class label</strong></li>
<li><strong>Bounding box</strong></li>
<li><strong>Segmentation mask</strong></li>
</ol>
<p>All three are trained together in one network.</p>
</section>
<section id="step-by-step-walkthrough-2" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="step-by-step-walkthrough-2">Step-by-step walkthrough</h3>
<section id="shared-feature-extraction" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="shared-feature-extraction"><strong>1. Shared feature extraction</strong></h4>
<p>As before, the entire image is passed through a backbone (ResNet-50, ResNet-101, etc.) to produce a feature map shared by all heads.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>feature_map <span class="op">=</span> backbone(image)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The RPN (Region Proposal Network) then proposes candidate boxes from this feature map — identical to Faster R-CNN.</p>
</section>
<section id="roi-align-precise-region-cropping" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="roi-align-precise-region-cropping"><strong>2. RoI Align: precise region cropping</strong></h4>
<p>Fast / Faster R-CNN used <strong>RoI Pooling</strong> to crop regions of interest, but pooling involved rounding coordinates to integers, which introduced slight misalignments. Those pixel-level shifts are small for detection, but catastrophic for mask accuracy.</p>
<p><strong>RoI Align</strong> fixes this by using <strong>bilinear interpolation</strong> instead of quantization, preserving exact spatial locations.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>roi_feat <span class="op">=</span> roi_align(feature_map, roi, output_size<span class="op">=</span>(<span class="dv">14</span>,<span class="dv">14</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Each RoI now corresponds perfectly to its true image region.</p>
</section>
<section id="three-parallel-heads" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="three-parallel-heads"><strong>3. Three parallel heads</strong></h4>
<p>Once features are extracted, they’re sent into three separate branches:</p>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 23%">
<col style="width: 53%">
</colgroup>
<thead>
<tr class="header">
<th>Head</th>
<th>Output</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Classification head</strong></td>
<td>Class scores</td>
<td>Identify what the object is</td>
</tr>
<tr class="even">
<td><strong>Bounding box head</strong></td>
<td>Box offsets</td>
<td>Refine position + size</td>
</tr>
<tr class="odd">
<td><strong>Mask head</strong></td>
<td>Pixel mask (28×28)</td>
<td>Predict which pixels belong to the object</td>
</tr>
</tbody>
</table>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>cls_score, bbox <span class="op">=</span> det_head(roi_feat)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>mask_logits <span class="op">=</span> mask_head(roi_feat)        <span class="co"># small conv decoder</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> sigmoid(mask_logits)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Each mask head is a lightweight CNN (often 4 conv layers) that outputs one binary mask per class.</p>
</section>
<section id="training-the-mask-branch" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="training-the-mask-branch"><strong>4. Training the mask branch</strong></h4>
<p>Only the RoIs corresponding to foreground objects are used for mask training. The loss is a pixel-wise <strong>binary cross-entropy</strong> between predicted and true masks:</p>
<p><span class="math display">\[
L_\text{mask} = \frac{1}{N} \sum_{i} \text{BCE}(M_i^\text{pred}, M_i^\text{gt})
\]</span></p>
<p>The total loss combines all three branches:</p>
<p><span class="math display">\[
L = L_\text{cls} + L_\text{bbox} + L_\text{mask}
\]</span></p>
<p>Everything is trained together, end-to-end.</p>
</section>
<section id="how-mask-r-cnn-fits-into-our-three-block-framework" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="how-mask-r-cnn-fits-into-our-three-block-framework"><strong>How Mask R-CNN fits into our three-block framework</strong></h4>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 70%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>What changed from Faster R-CNN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Input adaptation</strong></td>
<td>Same as Faster R-CNN (image → feature map → RPN proposals)</td>
</tr>
<tr class="even">
<td><strong>Feature extractor</strong></td>
<td>Shared backbone + RoI Align for precise spatial features</td>
</tr>
<tr class="odd">
<td><strong>Output adaptation</strong></td>
<td>Added a mask branch (28×28 pixel-wise prediction per instance)</td>
</tr>
</tbody>
</table>
</section>
<section id="code-peek-using-mask-r-cnn-in-pytorch" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="code-peek-using-mask-r-cnn-in-pytorch"><strong>Code peek: using Mask R-CNN in PyTorch</strong></h4>
<p>PyTorch makes this architecture accessible with just a few lines:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.models.detection <span class="im">as</span> detection</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load a pretrained Mask R-CNN (ResNet-50 backbone)</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> detection.maskrcnn_resnet50_fpn(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model([image])     <span class="co"># image: Tensor [C,H,W]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Each prediction contains:</p>
<ul>
<li><code>boxes</code> – bounding boxes</li>
<li><code>labels</code> – class IDs</li>
<li><code>scores</code> – confidence values</li>
<li><code>masks</code> – [ N × 1 × H × W ] tensor of pixel-level masks</li>
</ul>
<p>You can threshold and overlay these masks to visualize segmentation results.</p>
</section>
<section id="why-it-matters-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="why-it-matters-1"><strong>Why it matters</strong></h4>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 28%">
<col style="width: 28%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Faster R-CNN</th>
<th>Mask R-CNN</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Output</td>
<td>Boxes + labels</td>
<td>Boxes + labels + masks</td>
<td>🟢 Pixel-level precision</td>
</tr>
<tr class="even">
<td>Alignment</td>
<td>RoI Pooling (rounding)</td>
<td>RoI Align (interpolation)</td>
<td>🟢 No quantization error</td>
</tr>
<tr class="odd">
<td>Capability</td>
<td>Object detection</td>
<td>Instance segmentation</td>
<td>🟢 New dimension of understanding</td>
</tr>
</tbody>
</table>
</section>
<section id="a-mental-picture-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="a-mental-picture-1"><strong>A mental picture</strong></h4>
<p>Our detective can now not only say</p>
<blockquote class="blockquote">
<p>“There’s a cat at these coordinates,” but also trace its exact fur outline, whiskers and all. No more rough boxes — just precise boundaries.</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Key Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Mask R-CNN is Faster R-CNN with one extra branch and a more careful alignment step.</p>
<p>A tiny change in architecture enabled a huge leap in capability, from detecting objects to understanding their shapes.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Which of the three components, input, feature extractor, or output, did Mask R-CNN change most significantly?</p>
<p>How might you extend the same idea to tasks like keypoint detection or depth estimation?</p>
</div>
</div>
<p>Mask R-CNN set a template for modern segmentation and instance-aware vision models.</p>
<p>Next, we’ll step back and see how all these models, from R-CNN to Mask R-CNN, fit together in one unified story.</p>
</section>
</section>
</section>
<section id="putting-it-all-together" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="putting-it-all-together"><span class="header-section-number">12.7</span> Putting It All Together</h2>
<p>We’ve traveled quite a distance from recognizing <em>what</em> is in an image to outlining <em>where</em> and <em>what shape</em> each object takes. Let’s step back and see how every step in this journey fits together.</p>
<section id="from-classification-to-instance-segmentation" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="from-classification-to-instance-segmentation"><strong>From classification to instance segmentation</strong></h3>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Generation</th>
<th>Year</th>
<th>Key Idea</th>
<th>Input Adaptation</th>
<th>Feature Extractor</th>
<th>Output Adaptation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Classification Network</td>
<td>2012 (AlexNet)</td>
<td>Recognize categories in whole images.</td>
<td>Full image</td>
<td>CNN backbone (e.g., ResNet)</td>
<td>Class probabilities</td>
</tr>
<tr class="even">
<td>R-CNN</td>
<td>2014</td>
<td>Detect objects using region proposals + independent CNNs.</td>
<td>Selective Search regions</td>
<td>Separate CNN per region</td>
<td>SVM + regressor</td>
</tr>
<tr class="odd">
<td>Fast R-CNN</td>
<td>2015</td>
<td>Share convolutional features + RoI Pooling.</td>
<td>Selective Search</td>
<td>One shared CNN per image</td>
<td>Unified head for classification + bbox</td>
</tr>
<tr class="even">
<td>Faster R-CNN</td>
<td>2015</td>
<td>Learn region proposals via Region Proposal Network (RPN).</td>
<td>RPN (anchors + proposals)</td>
<td>Shared backbone (CNN + FPN)</td>
<td>End-to-end classification + bbox</td>
</tr>
<tr class="odd">
<td>Mask R-CNN</td>
<td>2017</td>
<td>Add mask prediction head for pixel-level segmentation.</td>
<td>Same as Faster R-CNN</td>
<td>Shared backbone + RoI Align</td>
<td>Three parallel heads (cls, bbox, mask)</td>
</tr>
</tbody>
</table>
</section>
<section id="visual-timeline" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="visual-timeline"><strong>Visual timeline</strong></h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/detection-timeline.png" class="img-fluid figure-img"></p>
<figcaption>Visual timeline of the evolution of detection models.</figcaption>
</figure>
</div>
<p>Each new architecture didn’t replace the old; it <strong>refined one of the three building blocks</strong> to solve a real bottleneck:</p>
<table class="table-striped caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 36%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Component Modified</th>
<th>Examples</th>
<th>Goal</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Input adaptation</td>
<td>R-CNN (Selective Search), Faster R-CNN (RPN)</td>
<td>Generate or refine region proposals</td>
</tr>
<tr class="even">
<td>Feature extractor</td>
<td>Fast R-CNN (shared CNN)</td>
<td>Reuse computation efficiently</td>
</tr>
<tr class="odd">
<td>Output adaptation</td>
<td>Mask R-CNN (mask head)</td>
<td>Predict more detailed outputs</td>
</tr>
</tbody>
</table>
</section>
<section id="a-recurring-pattern" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="a-recurring-pattern"><strong>A recurring pattern</strong></h3>
<blockquote class="blockquote">
<p><strong>Every innovation begins by asking:</strong> <em>“What can’t the current model do well?”</em> and then tweaking one of the three parts to fix it.</p>
</blockquote>
<p>This pattern defines much of deep learning progress:</p>
<ul>
<li>Better <strong>inputs</strong> → more context (multi-scale images, temporal frames, extra bands).</li>
<li>Stronger <strong>feature extractors</strong> → deeper, more efficient backbones (ResNet, Swin, ViT).</li>
<li>Richer <strong>outputs</strong> → new tasks (masks, depth, keypoints, motion).</li>
</ul>
<p>Once you start thinking in this modular way, “advanced models” stop feeling like magic, they become <strong>creative recombinations</strong> of familiar ideas.</p>
</section>
<section id="a-unifying-view" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="a-unifying-view"><strong>A unifying view</strong></h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dl-pytorch/rcnn-family-summary.png" class="img-fluid figure-img"></p>
<figcaption>Unified view of R-CNN family — single backbone feeding multiple heads.</figcaption>
</figure>
</div>
<p>All four models share the same skeleton:</p>
<ul>
<li>A <strong>backbone</strong> that learns features.</li>
<li>A <strong>proposal mechanism</strong> (external or learned).</li>
<li>One or more <strong>heads</strong> that interpret those features for different tasks.</li>
</ul>
<p>The Mask R-CNN simply made that skeleton multi-talented — detecting, classifying, and segmenting all at once.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Key Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>From R-CNN to Mask R-CNN, progress wasn’t about adding complexity for its own sake. It was about <strong>removing redundancy</strong>, <strong>integrating stages</strong>, and <strong>expanding output richness</strong> all while keeping the same underlying logic.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you were to design “the next R-CNN,” what might you change? Would you feed richer inputs (e.g., multispectral data)? Use a new feature extractor (e.g., transformer backbone)? Or predict new outputs (e.g., elevation maps, uncertainty masks)?</p>
</div>
</div>
</section>
</section>
<section id="reflection-and-discussion" class="level2" data-number="12.8">
<h2 data-number="12.8" class="anchored" data-anchor-id="reflection-and-discussion"><span class="header-section-number">12.8</span> Reflection and Discussion</h2>
<p>Let’s pause and reflect on what we’ve learned.</p>
<p>From R-CNN to Mask R-CNN, each step wasn’t random, it was a <strong>logical response</strong> to a real problem. This mindset is what separates <em>using</em> deep learning from <em>understanding</em> it.</p>
<section id="looking-back" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="looking-back"><strong>Looking back</strong></h3>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 40%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Challenge it solved</th>
<th>Core idea introduced</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>R-CNN</td>
<td>How can a CNN detect multiple objects?</td>
<td>Apply CNN to region proposals (Selective Search)</td>
</tr>
<tr class="even">
<td>Fast R-CNN</td>
<td>Why is R-CNN so slow?</td>
<td>Share one feature map for all regions</td>
</tr>
<tr class="odd">
<td>Faster R-CNN</td>
<td>Can we remove the slow proposal step?</td>
<td>Learn proposals via Region Proposal Network (RPN)</td>
</tr>
<tr class="even">
<td>Mask R-CNN</td>
<td>Can we see object shapes, not just boxes?</td>
<td>Add a mask head + precise RoI Align</td>
</tr>
</tbody>
</table>
<p>Notice how each question leads to an architectural insight and how every model keeps the same foundation:</p>
<p><strong>input adaptation → feature extractor → output adaptation</strong>.</p>
</section>
<section id="reflection-prompts" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="reflection-prompts"><strong>Reflection Prompts</strong></h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Quick Thought
</div>
</div>
<div class="callout-body-container callout-body">
<p>Imagine you are designing your own vision model.</p>
<ul>
<li>Which component (input, feature, output) would you modify first?</li>
<li>What kind of task would that change enable (for example, predicting object <em>depth</em> or <em>motion</em>)?</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Challenge
</div>
</div>
<div class="callout-body-container callout-body">
<p>Sketch your own “R-CNN++”:</p>
<ul>
<li>Keep the same three blocks.</li>
<li>Add one new branch or data source.</li>
<li>Write a one-sentence explanation of <em>why</em> it improves the model. You don’t need code, just reasoning.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Discussion
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why do you think so many breakthroughs (R-CNN → Mask R-CNN → modern detectors) reused the same ideas instead of starting from scratch?</p>
<p>What does this tell us about <strong>how innovation happens in deep learning</strong>?</p>
</div>
</div>
</section>
<section id="key-takeaway" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="key-takeaway"><strong>Key takeaway</strong></h3>
<p>Deep learning progress often follows a simple pattern:</p>
<blockquote class="blockquote">
<p><strong>Question → Limitation → Innovation</strong></p>
</blockquote>
<p>Each new model asks:</p>
<blockquote class="blockquote">
<p>“What can’t the current system do well?” and then modifies <strong>one</strong> of the core building blocks to fix it.</p>
</blockquote>
<p>When you look at architectures through this lens, they stop feeling like black boxes, and start looking like <strong>creative engineering solutions</strong> to everyday problems.</p>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><a href="https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">R. Girshick, J. Donahue, T. Darrell, J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” CVPR 2014.</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">R. Girshick, “Fast R-CNN,” ICCV 2015.</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><a href="https://proceedings.neurips.cc/paper_files/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf">S. Ren, K. He, R. Girshick, J. Sun, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” NIPS 2015.</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf">K. He, G. Gkioxari, P. Dollár, R. Girshick, “Mask R-CNN,” ICCV 2017.</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><a href="https://www.linkedin.com/pulse/medical-image-diagnosis-roles-object-detection-segmentation-egvcc">https://www.linkedin.com/pulse/medical-image-diagnosis-roles-object-detection-segmentation-egvcc</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><a href="https://www.youtube.com/watch?v=tNLtXb04i3w">https://www.youtube.com/watch?v=tNLtXb04i3w</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><a href="https://www.mdpi.com/2072-4292/10/9/1487">Zhang, W., Witharana, C., Liljedahl, A. K., &amp; Kanevskiy, M. (2018). Deep convolutional neural networks for automated characterization of arctic ice-wedge polygons in very high spatial resolution aerial imagery. Remote Sensing, 10(9), 1487.</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><a href="https://dl.acm.org/doi/abs/10.1145/3557918.3565869">Li, W., Hsu, C. Y., Wang, S., Witharana, C., &amp; Liljedahl, A. (2022, November). Real-time GeoAI for high-resolution mapping and segmentation of arctic permafrost features: the case of ice-wedge polygons. In Proceedings of the 5th ACM SIGSPATIAL international workshop on AI for geographic knowledge discovery (pp.&nbsp;62-65).</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><a href="https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">R. Girshick, J. Donahue, T. Darrell, J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” CVPR 2014.</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p><a href="https://link.springer.com/article/10.1007/s11263-013-0620-5">J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders, “Selective search for object recognition,” International Journal of Computer Vision, vol.&nbsp;104, no. 2, pp.&nbsp;154-171, 2013.</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p><a href="https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/">Non-Maximum Suppression Theory and Implementation in PyTorch</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p><a href="https://proceedings.neurips.cc/paper_files/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf">S. Ren, K. He, R. Girshick, J. Sun, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” NIPS 2015.</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p><a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf">K. He, G. Gkioxari, P. Dollár, R. Girshick, “Mask R-CNN,” ICCV 2017.</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../sections/guest-lecture-yili-arts-dataset.html" class="pagination-link" aria-label="Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Guest Lecture - Unveiling the ARTS Dataset for a Thawing Frontier</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../sections/intro-to-dl-libraries-for-image-analysis.html" class="pagination-link" aria-label="Introduction to Deep Learning Libraries for Image Analysis">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Introduction to Deep Learning Libraries for Image Analysis</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<!-- Default Statcounter code for cyber2a online course
http://cyber2a.github.io/cyber2a-course/ -->
<script type="text/javascript">
    var sc_project=13129980; 
    var sc_invisible=1; 
    var sc_security="fa33fcfd"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async=""></script>
    <noscript><div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img class="statcounter" src="https://c.statcounter.com/13129980/0/fa33fcfd/1/" alt="Web Analytics" referrerpolicy="no-referrer-when-downgrade"></a></div></noscript>
    <!-- End of Statcounter Code -->




</body></html>