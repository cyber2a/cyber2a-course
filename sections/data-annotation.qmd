# Data Annotation: The Foundation of Deep Learning Models

**>>>>THIS SECTION IS UNDER DEVELOPMENT<<<<**

---
title: "Data Annotation: The Foundation of Deep Learning Models"
toc: true
number-sections: true
from: markdown+emoji
---

<style>
figcaption {
  font-size: 0.5em;
}
.column-margin {
  figcaption {
    font-size: 0.5em;
  }
}
</style>


## Goals {.unnumbered}
This session explores the critical role of training data in deep learning, focusing on data annotation methods, tools, and strategies for acquiring high-quality data. Participants will learn how well-annotated data supports effective deep learning models, understanding the challenges and best practices in data annotation. By the end, participants will be equipped to prepare their datasets for deep learning.

## Key Elements {.unnumbered}
Training data's role, annotation methods/tools, annotated data's importance, annotation challenges, annotation best practices, dataset preparation

## Annotation Fundamentals

::: {.callout-tip appearance="simple" icon="false"}
### Highlights
- Reiterate ideas related to __*supervised learning*__, and the core idea of __*learning from examples*__
- Discuss key role of labeling/annotation in general for generating examples to learn from
- Take a quick tour of label/annotation examples across various ML applications (structured data, text, audio, image, video, etc)
- Talk about some general challenges of procuring/producing labeled data for Machine Learning
:::

### Fueling intelligence: Itâ€™s All About the Data!

The modern AI renaissance is driven by the synergistic combination of _**Computing advances**_, __*more & better data for training*__, and __*Algorithmic innovations*__.

![Source: [OECD.ai](https://oecd.ai/en/compute)](/images/data-annotation/ai-enablers.png){.lightbox width=60%}

Each of these is critical, but you really can't overstate the importance of massively upscaling _training and validation data_. Indeed, to a large extent, the most important advances in algorithms and compute are indeed the ones that are allowing us to efficiently use the large amount of data. **The more data available, the better the model can learn.**

Remember that in Machine Learning:

1. You are building a model to produce some __*desired output*__ for a __*given input*__. Imagine handing a model an aerial photo that contains a water body, or a camera trap video that contains a bear, or an audio recording that captures a the song of a particular bird species. In each case, you want the model to correctly detect, recognize, and report the relevant feature.
2. To achieve this, you do _not_ build this model by instructing the computer _how_ to detect the water body or the bear or the bird species. Instead, you assemble many (often many, many!) good _examples_ of the phenomena of interest, and feed them to an algorithm that allows the model to adaptively learn from these examples. Now, in practice there may be rule-based guardrails, but we can talk about that separately later in the course.

Much of this course is about understanding what kinds of model structures and learning algorithms allow this seemingly magical learning to happen inside the computer, and what the end-to-end process looks like. But for the sake of these next couple of sections, what is important is that this core concept makes sense to you:

_For any given project, you will quite likely be pulling a generic AI tool off the shelf that has basically no particular knowledge of your particular application area, and the way you will adapt it to apply to your project is not by hand-tweaking parameters or choosing functional forms or anything like that, but rather by (again) exposing the algorithm to many examples._

Bottom line, much like vehicles without fuel, even the best training algorithms in the world will just sit and gather dust if they don't have sufficient data to learn from!

![Source: [Walking Dead Fandom](https://walkingdead.fandom.com/wiki/Gas_Station_%28Season_1%29?file=S1GasStation.PNG)](/images/data-annotation/walking-dead-no-gas.webp){.lightbox width=60%}

Therefore although a lot of this week is about the models and how to operationalize them on compute platforms, your success in applying AI (especially if you are training and/or fine-tuning models, not simply applying pre-trained models) will depend on having a robust and effective _data pipeline_, from data collection methods to data annotation to data curation.

::: {.column-margin}
![Source: [DZone](https://dzone.com/articles/an-introduction-to-data-labeling-in-artificial-int)](/images/data-annotation/ml-time-allocation.webp){.lightbox}
:::

In this module, we focus on _data annotation_.

### What is annotation?

Data annotation is the process of labeling or marking up data with information that is not already explicit in the data itself.

In general, we do this to provide important and relevant context or meaning to the data. As humans, especially in knowledge work, we do this all the time for the purpose of sharing information with others.

![Source: [PowerPoint Tricks](https://www.youtube.com/watch?app=desktop&v=HP6Rb6mCQQM)](/images/data-annotation/labels-and-highlights.jpg){.lightbox width=60%}

In the context of Machine Learning and AI, our objective is to teach a model how to create accurate and useful annotations itself when it encounters new, unannotated data. In order to do this, we need to provide the model with annotated examples that it can train on.

To put it a different way, annotation is the process of taking some data just like the kind of data you will eventually feed into the model, and attaching to it the correct answer to whatever question you will be asking the model about that data.

Simply put, annotation refers to labeling data with information that a model needs to learn, and is not already inherently present in the data.

- **Note:** The term "_annotation_" is synonymous with "_labeling_"

#### Examples

<!-- set.seed(10); iris %>% sample_n(10) %>% kableExtra::kbl() %>% kableExtra::kable_styling(c("striped", "hold_position"), full_width = FALSE) -->
::: {.callout-note collapse="true" icon="false"
     title="Tabular Data Annotation"}
![](/images/data-annotation/iris-table.jpg){.lightbox width=80%}

- Label (aka Target) column: _Species_

When working with tabular data, we don't usually talk about "annotating" the data. Nevertheless, the concept of labeling for supervised learning tasks (such as classification and regression) still applies, and indeed it's common practice to refer to the data used for classification and regression model training as "_**labeled data**_". Labeled tabular data contains a column designated as the _target_ for learning, i.e. the column containing the value that a model learns to predict. Depending on the context (and background of the writer/speaker), you might also hear this referred to as the _label_, _outcome variable_, _dependent variable_, or even just _y variable_. If this is not already inherently present in the dataset, it must be added by an annotator before proceeding with modeling.
:::
::: {.callout-note collapse="true" icon="false"
     title="Text Annotation"}
![](/images/data-annotation/muir-quote.jpg){.lightbox width=80%}

- Sentiment: _Positive_
- Parts of speech: _most_::adv,  _beautiful_::adj
- Named entity: _Alaska_
:::
::: {.callout-note collapse="true" icon="false"
     title="Audio Annotation"}
![](/images/data-annotation/canyon-wren-song.png){.lightbox width=80%}

- Voice recognition
- Speech to text
:::
::: {.callout-note collapse="true" icon="false"
     title="Image Annotation"}
- ... our focus today and this week! See details below.
:::
::: {.callout-note collapse="true" icon="false"
     title="Video Annotation"}
![](/images/data-annotation/annotated-video.gif){.lightbox width=80%}

Like image annotation, but with many frames.
Focus is often on capturing motion, paths, change.
:::

- Examples from environmental science and satellite data (e.g., land cover classification, object detection).

### Why is annotation so important?

- Let's talk about the critical role of annotation in enabling _supervised learning_.

- Specifically, it will be used at _training time_, when a specific learning algorithm will use the information in your annotated data to update internal parameters to yield a specific parameterized (aka "trained") version of the model that can do a sufficiently good job at getting the right answer when exposed to new data that it hasn't seen before, and doesn't have labels

- Annotation affects model accuracy, precision, and generalizability
- Specific use cases in remote sensing and environmental applications
- Obviously there is a bit of tension here!
  - The point of the model, or the AI if you want to put it that way, is do something for you
  - But in order for the AI to be able to do this, you have to first teach it how, which means doing the very thing that you want it to do
  - It's kind of analogous to hiring an intern. Yeah, it takes extra time up front to get them trained up, but once you do that, you benefit
  - And this raises a few questions:
      - Hmm, is there an AI out there I can hire that already knows at least something about what I'm trying to do here? Maybe yes! This is what things like foundation models (and more generally, transfer learning) offer. If you can find an undergrad researcher, you're going to get going faster than if you hire a 7th grader.
      - How much annotated data do I need? Well, that depends! On lots of things that we'll touch on (complexity of task, clarity of the information, etc). So it would be nice

### Annotation challenges

This boils down to being able to _quickly_ and _correctly_ annotate a _large enough corpus of inputs_ that collectively provide an _adequate representation_ of information you want the model to learn.

* **Scalability**: Annotating large satellite datasets can be time-consuming.
* **Class Imbalance**: Some categories may have significantly more instances than others (e.g., more urban areas vs. forests).
* **Quality Control**: Balancing speed and accuracy.
  - Strategies to improve quality (e.g., annotation guidelines, quality checks).
  - Dealing with source data quality: Overlapping objects, distortion, other visual artifacts
* **Subjectivity**: Different annotators can interpret data differently, leading to inconsistent labels.
* **Cost**: In terms of both people time and potential tool/service/compute costs
* **Artifact management**: Managing the data, annotations, etc
* **Data privacy/security**: Esp if using a cloud-based tool
* **Annotation privacy/ethics**: Ethics is not an annotation-specific problem, but annotation is a key step when in some sense external knowledge (i.e., what the annotators know) is attached to the input data, creating an opportunity for injecting bias, exposing sensitive or private information, etc.

Labeling of satellite imagery brings its own specific challenges:
- Scenes are often highly complex and rich in detail
- Usually critical to maintain correspondence between pixels and their geospatial location
- Geographic distortion: Angle of sensor
- Atmospheric distortion: Haze, fog, clouds
- Variability over time:
  - What time of day? Angle of the sun affects visible characteristics
  - What time of year? Many features change seasonality (e.g. deciduous forest, grasslands in seasonally arid environments, snow cover, etc)
  - Features change! Forests are cut, etc. Be mindful of the difference between labeling an image and labeling a patch of the earth's surface.

Or another common way to breaks this down is the following 5 factors:

1. **Quality**
2. **Scale**
3. **Process**
4. **Security**
5. **Tools**

### Annotation best practices

* **Clear Labeling Instructions**: Guidelines to standardize annotations.
* **Annotator Training**: Ensure annotators understand the domain and use cases.
* **Quality Control**: Regular checks, cross-validation, and feedback loops.
  * Consensus mechanisms
  * Four ways of measuring quality:
    1. Gold standard: When there's a know correct answer, ask whether the provided annotation is correct or not
    2. Sample review: Randomly sample some, and measure based on correct vs incorrect
    3. Consensus: Multiple annotators, treat most common answer as correct
    4. Intersection over union (IoU): For bounding boxes, divide area over overlap by total area of the (unioned) boxes
* **Ensure compliance**: ...
* **Iterative Annotation**: Start small, refine, and scale gradually.
* To outsource/crowdsource or not?

### Dataset preparation

(Maybe move this to the end of the Annotation Methodology section?)

- **Data Preprocessing**: Cleaning and normalizing annotated data.
  - Satellite-specific preprocessing: removing noise, correcting for atmospheric conditions, correcting other distortion, adjust brightness/contrast/color esp for multispectral images.
- **Splitting Data**: Training, validation, and test sets.
  - Ensuring a balanced representation of classes across these sets.
- **Augmentation**: Techniques for enhancing datasets
  - In image-based tasks, common techniques are rotation, cropping, and scaling
  - Other transformations can also "bring out" useful information for the model to operate on, leading to faster and/or better model outcomes (see e.g. [UKFields project](https://github.com/Spiruel/UKFields))


## Annotation Methodology

::: {.callout-tip appearance="simple" icon="false"}
### Highlights
- Discuss the primary types of image annotations
- Discuss the common image-related AI/ML tasks requiring annotation
- Discuss different methods for annotating images
- Describe a high level annotation workflow
:::

It's important to understand and recognize the difference between image annotation _types_, _tasks_, and _methods_. Note that this isn't universal or standardized terminology, but it's pretty widespread.

In this context:

- An annotation _type_ describes the specific format or structure of the annotation used to convey information about the data critical for supporting the task.
- An annotation _task_ is the specific objective that the annotations are meant to support, i.e. the job you want your AI application to do. In the computer vision context, this typically means identifying or understanding something about an image, and conveying that information in some specific form.
- An annotation _method_ refers to the process or approach used to create the annotations.

### Image Annotation Types

The type of annotation you apply will depend partly on the task (see next section), as different annotation types are naturally suited for different tasks. However, the decision will also be driven in part by time, cost, and accuracy considerations.

::: {.callout-caution collapse="true" icon="false"
     title="Text labels"}
**Text labels** are words or phrases associated with the image as a whole, without explicit linkage to any localized portion of the image.
![](/images/data-annotation/santa-rosa-beach.jpg){.lightbox width=80%}

- Label: __*beach*__
- Caption: "__*Embracing the serenity of the shore, where the sky meets the ocean #outdoor #beachlife #nature*__"
:::

::: {.callout-caution collapse="true" icon="false"
     title="Bounding boxes"}
**Bounding boxes** are rectangles drawn around objects to localize them within an image.
![](/images/data-annotation/building-bbox.jpg){.lightbox width=80%}

Typically they are _axis-aligned_, meaning two sides are parallel with the image top/bottom, and two sides are parallel with the image sides, but sometimes rotation is supported.
:::

::: {.callout-caution collapse="true" icon="false"
     title="Polygons"}
Generalizing the bounding box concept, **polygons** are a series of 3 or more connected line segments (each with definable end coordinates) that form a closed shape (i.e. the end of the last segment is the beginning of the first segment), used to more precisely localize objects or areas by outlining their shape.
![](/images/data-annotation/building-polygon.jpg){.lightbox width=80%}
:::

::: {.callout-caution collapse="true" icon="false"
     title="Segmentations"}
**Segmentations** involve assigning a class label to individual pixels (or collectively, to regions of individual pixels) in an image. Segmentation may be done either fully for all pixels, or partially only for pixels associated with phenomena of interest.

In practice, pixel segmentations can be produced either by drawing a polygon or using a brush tool.

![](/images/data-annotation/building-segmentation.jpg){.lightbox width=80%}
:::

::: {.callout-caution collapse="true" icon="false"
     title="Keypoints"}
**Keypoints** are simply points, used for denoting specific landmarks or features (e.g., skeletal points in human pose estimation).
![](/images/data-annotation/buildings-points.jpg){.lightbox width=80%}
:::

::: {.callout-caution collapse="true" icon="false"
     title="Polylines"}
**Polylines** are conceptually similar to polygons, but they do not form a closed shape. Instead, the lines are used to mark linear features such as roads, rivers, powerlines, or boundaries.
![](/images/data-annotation/road-lines.jpg){.lightbox width=80%}
:::

::: {.callout-caution collapse="true" icon="false"
     title="3D Cuboids"}
**3D cuboids** are bounding boxes extended to three dimensions. These are often used in LiDAR data which is represented as a 3-dimensional point cloud, but can also be used to indicate depth of field in a 2D image when the modeling task involves understanding position in three dimensions.
![](/images/data-annotation/bear-3dcuboid.jpg){.lightbox width=80%}
:::

### Image Annotation Tasks

The task you choose will depend on the type of information you want the model to extract from the images. Here are the key types of annotation tasks in computer vision:

::: {.callout-note collapse="true" icon="false"
     title="Image Classification"}
**Image classification** is the task of assigning an entire image to a category.

The classification typically refers to some singular dominant object or feature (e.g., "Polar bear") within the image, or some defining characteristic of the image (e.g., "Grassland"), but the details depend on the specific use case motivating the modeling exercise.
:::

::: {.callout-note collapse="true" icon="false"
     title="Object Detection"}

**Object detection** is the task of identifying one or more objects or discrete entities within an image.

Note that object detection involves two distinct sub-tasks:

- _**Localization**_: Where is the object within the image?
- _**Classification**_: What is the localized object?
:::

::: {.callout-note collapse="true" icon="false"
     title="Image Segmentation"}
**Segmentation** is the task of associating individual pixels annotation for detailed image analysis (e.g., land-use segmentation).
- Think of it as object detection reported at the pixel level

There are three distinct kinds of segmentation:

- **Semantic Segmentation**
  - Assigns a class label to each pixel in the image, without differentiating individual instances of that class
  - Is good for amorphous and uncountable "stuff" - sky, water, vegetation, floor, wall, etc
- **Instance Segmentation**
  - Separately detects and segments each object instance
  - Hence similar to semantic segmentation, but _identifies the existence, location, shape, and count of objects_
  - Good for distinct and countable "things" - person, polar bear, tree, lake, building
- **[Panoptic Segmentation](https://ai.meta.com/research/publications/panoptic-segmentation/)**
  (see [research pub](https://arxiv.org/abs/1801.00868))
  - Combines semantic segmentation + instance segmentation by labeling all pixels, including differentiation of discrete and separately objects within categories
:::

::: {.callout-note collapse="true" icon="false"
    title="Temporal Annotation"}
**Temporal annotation** is the task of labeling satellite images over time to track changes in environmental features.
:::

::: {.callout-note collapse="true" icon="false"
    title="Image Captioning"}
**Image captioning** is the task of generating textual descriptions of the image. It is conceptually similar to image classification, but involves producing freeform text for each image rather than assigning the image to one of a set of pre-defined categorical classifications.
:::

### Image Annotation Methods

The annotation _method_ largely boils down to whether annotations are done manually versus with some level of supporting automation. Ultimately, the choice involves project-specific determination of the cost, speed, and quality of human annotation relative to what can be achieved with available AI assistance.

::: {.callout-tip collapse="true" icon="false"
     title="Manual Annotation"}
With **purely manual annotation**, all labeling is done by human annotators.

Note that good tooling may help make this process easier and more efficient, but ultimately it is up to the human annotator to fully apply the annotation to an unlabeled input.
:::

::: {.callout-tip collapse="true" icon="false"
     title="Semi-Automated Annotation"}
With **semi-automated annotation**, machines assist humans in generating annotations, but humans are still heavily involved in real time with labeling decisions, ranging from actually applying the annotations to refining AI-generated annotations.

Note that this can take many forms. For example:
- Model-based filtering: A model is trained to recognize images with _any_ objects (as compared to empty scenes), and is used to reduce the number of images passed to the human annotator.
- Model-suggested labels: A pre-trained model provides hints, e.g. multiple options of likely object classes, and the human makes the decision.
- Model-assisted labeling: A pre-trained model generates a candidate annotation, which the human can accept, reject, or modify in some way (e.g., size, position, category).
- Active Learning: A model is learning how to annotate the images alongside the human, and actively decides which images the human should label to accelarate model training the fastest.
:::

::: {.callout-tip collapse="true" icon="false"
     title="Automated Annotation with Human Validation"}
At the level of **automated annotation with human validation**, AI models generate most annotations autonomously, but humans review the results after the fact and address mistakes.

Example: A pre-trained model processes satellite images and automatically labels roads, rivers, and forests across thousands of images. A human reviewer then inspects a small percentage of these results to confirm the annotations are accurate, fixing any errors before the dataset is finalized.

  - This might seem impossible! If you already have a model that can do the annotation, then don't you already have a model to do the actual task you want to do?
  - In practice, here is _model distillation_: There is big, expensive, and/or proprietary (hidden behind an API) model that does what you want, and probably a lot more. You can use this to generate labels on your own data to train your own more compact model
    - Example:Â **U-Net**Â for image segmentation,Â **YOLO**Â for object detection.
:::

::: {.callout-tip collapse="true" icon="false"
     title="Fully Automated Annotation"}
Rare in practice! Under **fully automated annotation**, trained models generate annotations with no human involvement, and the quality is deemed sufficient without review.

This is typically only relevant in very limited cases:
- Environments in which the image data is very highly controlled (e.g. recorded in a lab setting, in video games, etc) or has been generated synthetically.
- Contexts in which the goal of annotation is not to train a model that can do the annotation task (which we already have!), but rather to produce many annotated examples for some other modeling task.

A related approach is to use _synthetic data_, wherein a trained AI model is used to generate both the image and its annotations.
:::

### Data Annotation Workflow

::: {.callout-important collapse="true" icon="false"
     title="1 - Data collection"}
Get a sufficiently large and diverse set of data
:::

::: {.callout-important collapse="true" icon="false"
     title="2 - Data preprocessing"}
Clean the data, prepare formats, etc
:::

::: {.callout-important collapse="true" icon="false"
     title="3 - Tool Selection"}
Time to choose your annotation tool/platform! See next section...
:::

::: {.callout-important collapse="true" icon="false"
     title="4 - Guideline Development"}
Before beginning in earnest, it's critical to develop specific guidelines for annotators to follow when doing the annotation using the selected tool.

Often this will be based on a combination of **prior knowledge** and **task familiarity**. To the extent that nobody on the project has extensive experience with the task at hand, it's often helpful to do some prototyping to inform development of the guidelines.
:::

::: {.callout-important collapse="true" icon="false"
     title="5 - Annotation"}
Annotate the data!
:::

::: {.callout-important collapse="true" icon="false"
     title="6 - Quality Assurance"}
Review and refine the annotations

Note that although QA is identified here as a discrete stage in the workflow, in practice quality is achieved through deliberate attention at multiple stages in the process, including:

- Initial annotator workforce training before any annotation is done
- Continuous monitoring during the annotation process
  - E.g. Consensus metrics: have more than one person annotate each item, and measure consistency
- Final post-annotation review
:::

::: {.callout-important collapse="true" icon="false"
     title="7 - Data Export"}
Prepare the annotated data for model training. Depending on the situation, this may include converting the annotations into a format compatible with modeling framework, if not supported directly by the annotation tool.
:::

Best practice: Iterate! Annotate, train, test, fix annotations, figure out whether/how to do more and/or better annotations, change annotation approach, etc.

## Annotation Tools & Platforms

::: {.callout-tip appearance="simple" icon="false"}
### Highlights
- Get a sense of what kind of tools are out there today!
- Discuss high level considerations for choosing a tool
- Review some specific tools out there today
- Highlight how fast things are changing!
:::

### High level considerations

Here are some questions you should be asking...

::: {.callout-tip collapse="false" icon="false"
     title="What annotation types are supported?"}
Does the tooling allow you do create the kinds of annotations necessary for your task?
:::

::: {.callout-tip collapse="false" icon="false"
     title="What import formats are supported?"}
- What input formats are supported?
:::

::: {.callout-tip collapse="false" icon="false"
     title="What output formats are supported?"}
- What output formats are supported?
  - Various JSON formats
    - COCO JSON
    - VGG Image Annotator JSON
    - LabelMe JSON
  - YOLO TXT
  - Pascal VOC XML
  - TensorFlow TFRecord
  - ... lots more ...
:::

::: {.callout-tip collapse="false" icon="false"
     title="Who is doing the annotation?"}
- Who will do the annotation?
  - In-house (your people)
  - Crowdsource (the community)
  - Outsource (people employed by 3rd party companies that you pay)
:::

::: {.callout-tip collapse="false" icon="false"
     title="How can I assess annotation quality?"}
- Reliability / accuracy -- for assisted tools (AI, crowdsource, outsource)
:::

::: {.callout-tip collapse="false" icon="false"
     title="Is the tool easy to use?"}
- Ease of use!
  - Effective UI
  - Support for keyboard shortcuts
:::

::: {.callout-tip collapse="false" icon="false"
     title="How much am I willing to pay for tooling?"}
- Free or Commercial?
:::

::: {.callout-tip collapse="false" icon="false"
     title="How is the softare licensed?"}
- Open source or Proprietary?
:::

::: {.callout-tip collapse="false" icon="false"
     title="Where does the software run?"}
- Desktop or Self-hosted or Cloud?
  - Security/privacy considerations
:::

::: {.callout-tip collapse="false" icon="false"
     title="What collaboration features are there?"}
- What collaborative features are offered?
:::

### Tools & services galore

Note that for geospatial image data annotation in particular, historically there's been a divide between these two approaches:

- Big GIS packages (QGIS, ArcGIS, etc) -
  - Draw polygons, but lots of menus and heavyweight UI
  - Lots of Geo support, but no real annotation support
- Streamlined, maybe semi-automated annotation tools (LabelBox, RoboFlow)
  - Really nice and improving, but quite generic wrt Image AI, not feature-full around Remote Sensing applications like data cubes (multispectral images)

Focus here is really on the latter.


#### Open-Source Tools for Image Annotation

- [**LabelImg**](https://pypi.org/project/labelImg/)
  - High level: An open-source tool for creating bounding boxes.
  - Used for object detection mainly, maybe??
  - Only supports _bounding boxes_ for annotation
  - "Graphical image annotation tool and label object bounding boxes in images"
  - It is written in Python and uses Qt for its graphical interface.
  - Annotations are saved as XML files in PASCAL VOC format, the format used byÂ [ImageNet](http://www.image-net.org/). Besides, it also supports YOLO and CreateML formats
  - See this third-party [video tutorial](https://www.youtube.com/watch?v=EGQyDla8JNU)
- [**VGG Image Annotator (VIA)**](https://www.robots.ox.ac.uk/~vgg/software/via/)
  - High level: A flexible (but manual) tool for image, video, and audio annotation.
  - Serverless web application, runs locally and self-contained in a browser, with no network connection required
  - Released in 2016, still maintained, based out of Oxford
  - See [demo](https://www.robots.ox.ac.uk/~vgg/software/via/via_demo.html)
- [LabelMe](https://github.com/labelmeai/labelme)
  - Originally built as an online annotation tool, now distributed
  - Now distributed as a [deployable web application](https://github.com/CSAILVision/LabelMeAnnotationTool) that you can ran on a local web server
  - Not to be confused with this independent Python/QT port of [labelme](https://github.com/labelmeai/labelme)
  - Wait and what about this [labelme](https://github.com/wkentaro/labelme) GitHub repo??

- **QGIS**
  - High level: For geospatial data and satellite imagery.

#### Hybrid solutions with both desktop and hosted options

- [CVAT](https://docs.cvat.ai/docs/) (Computer Vision Annotation Tool):
  - Open-source tool for video and image annotation, widely used in computer vision projects.
  - Uses pre-trained models to assist annotation?
  - See [GitHub repository](https://github.com/cvat-ai/cvat)
  - Also has [cloud-based offering](http://app.cvat.ai/) and offers [annotation services](https://www.cvat.ai/annotation-service)
  - Supports:
    - labeling images
    - drawing bounding boxes
    - model assisted labeling using models like [YOLO](https://pjreddie.com/darknet/yolo/)Â 
    - manual semantic segmentationÂ 
    - automatic semantic segmentation with [SAM](https://segment-anything.com/)Â 
- :fire: [Label Studio](https://labelstud.io/guide/labeling)
  - Multi-type data labeling and annotation tool with standardized output format
  - Works on various data types (text, image, audio)
  - Has both [open source option](https://github.com/HumanSignal/label-studio) and [paid cloud service](https://humansignal.com/goenterprise/)
  - See online [playground](https://labelstud.io/playground/)
- [Microsoft's Spatial imagely labeling toolkit](https://github.com/microsoft/satellite-imagery-labeling-tool)
- [imglab](https://github.com/NaturalIntelligence/imglab)

#### Commercial apps
- [RectLabel](https://rectlabel.com)
  - Offline image annotation tool for object detection and segmentation
  - Has regular and Pro version
  - Built for Mac
  - See [support page](https://github.com/ryouchinsa/Rectlabel-support)

#### Commercial services
- [Labelbox](https://docs.labelbox.com/docs/annotate-overview)
  - Cloud-based commercial platform, albeit with possible free options for academic researchers
- [Roboflow annotate](https://roboflow.com/annotate)
  - Online platform, with limited free tier
  - Free tier does not offer any privacy
- [SuperAnnotate](https://www.superannotate.com/)
  - High level: Full-featured collaborative annotation and modeling platform
  - [Commercial offering](https://www.superannotate.com/pricing) with free tier
- [MakeSense.ai](https://www.makesense.ai/)
  - Includes AI models!
  - [GitHub](https://github.com/SkalskiP/make-sense)0
- [Supervise.ly](https://supervisely.com/labeling-toolbox/images/) (commercial with free version)
- [Labelerr](https://www.labellerr.com/image-annotation-platform) (commercial with free researcher tier)
- [RMSI](https://www.rmsi.com/Annotation-and-Labeling/platform.html) annotation tools & services
- [Kili](https://kili-technology.com/platform/label-annotate/geospatial-annotation-tool) annotation platform (see [geoannotation docs](https://docs.kili-technology.com/docs/geospatialtiled-imagery))
- [Segments.ai](https://segments.ai/) labeling platform
- [Sama](https://www.sama.com/2d-image-annotation-services)
- [ScaleAI](https://scale.com/data-engine)
- [Diffgram](https://www.diffgram.com/) (see [tech docs](https://diffgram.readme.io/) and [GiHub](https://github.com/diffgram/diffgram)) -- commercial but locally installed? Hard to tell!
- [DarkLabel](https://github.com/darkpgmr/DarkLabel)
- [Groundwork](https://element84.com/groundwork/data-labeling) professional labeling services

#### Fully managed AI & annotation services
- [Alegion](https://alegion.com/about-us/)
- [Manthano](https://manthano.ai)

#### Other platforms
- [Zooniverse](https://www.zooniverse.org/projects)? Crowd-sourcing annotation platform
  - E.g. [The Arctic Bears Project](https://www.zooniverse.org/projects/douglas-clark/the-arctic-bears-project)

```{=html}
<iframe credentialless width="780" height="500" src="https://www.zooniverse.org/" title="Zooniverse"></iframe>
```
- [**IRIS**](https://github.com/ESA-PhiLab/iris) (Intelligently Reinforced Image Segmentation)
  - Tool for manual image segmentation of satellite imagery (or images in general).
  - _Semi-automated annotation for image segmentation_
  - See [YouTube video](https://www.youtube.com/watch?v=ERJA2-fTW6k) with the main creator Alistar Francis
  - Main premise:
    - In each image, there is a lot of correlation between the pixels
    - In one scene, might only be a few types of pixels
  - Runs as a JS application on the frontend with Python in the backend
  - Designed to accelerate the creation of ML training datasets for Earth Observation.
  - Flask app which can be run locally
  - Support by AI (gradient boosted decision tree) when doing image segmentation
  - Multiple and configurable views for multispectral imagery
  - Simple setup with pip and one configuration file
  - Platform independent app (runs on Linux, Windows and Mac OS)
  - Multi-user support: work in a team on your dataset and merge the results
- [DeepForest](https://deepforest.readthedocs.io/en/v1.3.3/landing.html)
  - From the Weecology lab
  - Python package for training and predicting ecological objects in airborne imagery
  - Comes with a tree crown object detection model and a bird detection model
  - See [GitHub repo](https://github.com/weecology/DeepForest)

### Miscellaneous links
- [Satellite image deep learning](https://github.com/satellite-image-deep-learning/annotation) (Robin Cole's site)
- [Open Source Data Annotation & Labeling Tools](https://github.com/zenml-io/awesome-open-data-annotation)
